# Langchain - Models

**Pages:** 803

---

## LangSmith Evaluations

**URL:** llms-txt#langsmith-evaluations

Source: https://docs.langchain.com/langsmith/evaluation

The following sections help you create datasets, run evaluations, and analyze results:

<Columns cols={3}>
  <Card title="Evaluation concepts" icon="circle-info" href="/langsmith/evaluation-concepts" arrow="true">
    Review core terminology and concepts to understand how evaluations work in LangSmith.
  </Card>

<Card title="Manage datasets" icon="database" href="/langsmith/manage-datasets" arrow="true">
    Create and manage datasets for evaluation through the UI or SDK.
  </Card>

<Card title="Run evaluations" icon="microscope" href="/langsmith/evaluate-llm-application" arrow="true">
    Evaluate your applications with different evaluators and techniques to measure quality.
  </Card>

<Card title="Analyze results" icon="chart-bar" href="/langsmith/analyze-an-experiment" arrow="true">
    View and analyze evaluation results, compare experiments, filter data, and export findings.
  </Card>

<Card title="Collect feedback" icon="comments" href="/langsmith/annotation-queues" arrow="true">
    Gather human feedback through annotation queues and inline annotation on outputs.
  </Card>

<Card title="Follow tutorials" icon="book" href="/langsmith/evaluate-chatbot-tutorial" arrow="true">
    Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## from langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal

**URL:** llms-txt#from-langchain_community.document_loaders.parsers-import-openaiwhisperparser,-openaiwhisperparserlocal

bash  theme={null}
  pip install youtube-transcript-api langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add youtube-transcript-api langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.document_loaders import YoutubeLoader
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/google.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### YouTube Transcripts Loader

Load video transcripts. Requires `youtube-transcript-api`.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See a [usage example](/oss/python/integrations/document_loaders/youtube_transcript).
```

---

## How to use Studio

**URL:** llms-txt#how-to-use-studio

**Contents:**
- Run application
- Manage assistants
- Manage threads
- Next steps

Source: https://docs.langchain.com/langsmith/use-studio

This page describes the core workflows you’ll use in Studio. It explains how to run your application, manage assistant configurations, and work with conversation threads. Each section includes steps in both graph mode (full-featured view of your graph’s execution) and chat mode (lightweight conversational interface):

* [Run application](#run-application): Execute your application or agent and observe its behavior.
* [Manage assistants](#manage-assistants): Create, edit, and select the assistant configuration used by your application.
* [Manage threads](#manage-threads): View and organize the threads, including forking or editing past runs for debugging.

<Tabs>
  <Tab title="Graph">
    ### Specify input

1. Define the input to your graph in the **Input** section on the left side of the page, below the graph interface. Studio will attempt to render a form for your input based on the graph's defined [state schema](/oss/python/langgraph/graph-api/#schema). To disable this, click the **View Raw** button, which will present you with a JSON editor.
    2. Click the up or down arrows at the top of the **Input** section to toggle through and use previously submitted inputs.

To specify the [assistant](/langsmith/assistants) that is used for the run:

1. Click the **Settings** button in the bottom left corner. If an assistant is currently selected the button will also list the assistant name. If no assistant is selected it will say **Manage Assistants**.
    2. Select the assistant to run.
    3. Click the **Active** toggle at the top of the modal to activate it.

For more information, refer to [Manage assistants](#manage-assistants).

Click the dropdown next to **Submit** and click the toggle to enable or disable streaming.

To run your graph with breakpoints:

1. Click **Interrupt**.
    2. Select a node and whether to pause before or after that node has executed.
    3. Click **Continue** in the thread log to resume execution.

For more information on breakpoints, refer to [Human-in-the-loop](/oss/python/langchain/human-in-the-loop).

To submit the run with the specified input and run settings:

1. Click the **Submit** button. This will add a [run](/langsmith/assistants#execution) to the existing selected [thread](/oss/python/langgraph/persistence#threads). If no thread is currently selected, a new one will be created.
    2. To cancel the ongoing run, click the **Cancel** button.
  </Tab>

<Tab title="Chat">
    Specify the input to your chat application in the bottom of the conversation panel.

1. Click the **Send message** button to submit the input as a Human message and have the response streamed back.

To cancel the ongoing run:

1. Click **Cancel**.
    2. Click the **Show tool calls** toggle to hide or show tool calls in the conversation.
  </Tab>
</Tabs>

Studio lets you view, edit, and update your assistants, and allows you to run your graph using these assistant configurations.

For more conceptual details, refer to the [Assistants overview](/langsmith/assistants/).

<Tabs>
  <Tab title="Graph">
    To view your assistants:

1. Click **Manage Assistants** in the bottom left corner. This opens a modal for you to view all the assistants for the selected graph.
    2. Specify the assistant and its version you would like to mark as **Active**. LangSmith will use this assistant when runs are submitted.

The **Default configuration** option will be active, which reflects the default configuration defined in your graph. Edits made to this configuration will be used to update the run-time configuration, but will not update or create a new assistant unless you click **Create new assistant**.
  </Tab>

<Tab title="Chat">
    Chat mode enables you to switch through the different assistants in your graph via the dropdown selector at the top of the page. To create, edit, or delete assistants, use Graph mode.
  </Tab>
</Tabs>

Studio provides tools to view all [threads](/oss/python/langgraph/persistence#threads) saved on the server and edit their state. You can create new threads, switch between threads, and modify past states both in graph mode and chat mode.

<Tabs>
  <Tab title="Graph">
    ### View threads

1. In the top of the right-hand pane, select the dropdown menu to view existing threads.
    2. Select the desired thread, and the thread history will populate in the right-hand side of the page.
    3. To create a new thread, click **+ New Thread** and [submit a run](#run-application).
    4. To view more granular information in the thread, drag the slider at the top of the page to the right. To view less information, drag the slider to the left. Additionally, collapse or expand individual turns, nodes, and keys of the state.
    5. Switch between `Pretty` and `JSON` mode for different rendering formats.

### Edit thread history

To edit the state of the thread:

1. Select <Icon icon="pencil" /> **Edit node state** next to the desired node.
    2. Edit the node's output as desired and click **Fork** to confirm. This will create a new forked run from the checkpoint of the selected node.

If you instead want to re-run the thread from a given checkpoint without editing the state, click **Re-run from here**. This will again create a new forked run from the selected checkpoint. This is useful for re-running with changes that are not specific to the state, such as the selected assistant.
  </Tab>

<Tab title="Chat">
    1. View all threads in the right-hand pane of the page.
    2. Select the desired thread and the thread history will populate in the center panel.
    3. To create a new thread, click **+** and submit a run.

To edit a human message in the thread:

1. Click <Icon icon="pencil" /> **Edit node state** below the human message.
    2. Edit the message as desired and submit. This will create a new fork of the conversation history.
    3. To re-generate an AI message, click the retry icon below the AI message.
  </Tab>
</Tabs>

Refer to the following guides for more detail on tasks you can complete in Studio:

* [Iterate on prompts](/langsmith/observability-studio)
* [Run experiments over datasets](/langsmith/observability-studio#run-experiments-over-a-dataset)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Conditional edge function to create llm_call workers that each write a section of the report

**URL:** llms-txt#conditional-edge-function-to-create-llm_call-workers-that-each-write-a-section-of-the-report

def assign_workers(state: State):
    """Assign a worker to each section in the plan"""

# Kick off section writing in parallel via Send() API
    return [Send("llm_call", {"section": s}) for s in state["sections"]]

---

## LangGraph Server

**URL:** llms-txt#langgraph-server

**Contents:**
- Application structure
- Parts of a deployment
  - Graphs
  - Persistence and task queue
- Learn more

Source: https://docs.langchain.com/langsmith/langgraph-server

**LangGraph Server** offers an API for creating and managing agent-based applications. It is built on the concept of [assistants](/langsmith/assistants), which are agents configured for specific tasks, and includes built-in [persistence](/oss/python/langgraph/persistence#memory-store) and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.

Use LangGraph Server to create and manage [assistants](/langsmith/assistants), [threads](/oss/python/langgraph/persistence#threads), [runs](/langsmith/assistants#execution), [cron jobs](/langsmith/cron-jobs), [webhooks](/langsmith/use-webhooks), and more.

<Tip>
  **API reference**<br />
  For detailed information on the API endpoints and data models, refer to the [API reference docs](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html).
</Tip>

To use the `Enterprise` version of the LangGraph Server, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, [contact our sales team](https://www.langchain.com/contact-sales).

You can run the `Enterprise` version of the LangGraph Server on the following LangSmith [hosting](/langsmith/hosting) options:

* [Cloud](/langsmith/cloud)
* [Hybrid](/langsmith/hybrid)
* [Self-hosted](/langsmith/self-hosted)

## Application structure

To deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.

Read the [application structure](/langsmith/application-structure) guide to learn how to structure your LangGraph application for deployment.

## Parts of a deployment

When you deploy LangGraph Server, you are deploying one or more [graphs](#graphs), a database for [persistence](/oss/python/langgraph/persistence), and a task queue.

When you deploy a graph with LangGraph Server, you are deploying a "blueprint" for an [Assistant](/langsmith/assistants).

An [Assistant](/langsmith/assistants) is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases
that can be served by the same graph.

Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph's default configuration settings.

<Note>
  We often think of a graph as implementing an [agent](/oss/python/langgraph/workflows-agents), but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple
  chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use [multiple agents](/oss/python/langchain/multi-agent) working in tandem.
</Note>

### Persistence and task queue

LangGraph Server leverages a database for [persistence](/oss/python/langgraph/persistence) and a task queue.

[PostgreSQL](https://www.postgresql.org/) is supported as a database for LangGraph Server and [Redis](https://redis.io/) as the task queue.

If you're deploying using [LangSmith cloud](/langsmith/cloud), these components are managed for you. If you're deploying LangGraph Server on your [own infrastructure](/langsmith/self-hosted), you'll need to set up and manage these components yourself.

For more information on how these components are set up and managed, review the [hosting options](/langsmith/hosting) guide.

* LangGraph [Application Structure](/langsmith/application-structure) guide explains how to structure your LangGraph application for deployment.
* The [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html) provides detailed information on the API endpoints and data models.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production-use.

store = InMemoryStore(index={"embed": embed, "dims": 2}) # [!code highlight]
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context) # [!code highlight]
store.put( # [!code highlight]
    namespace,
    "a-memory",
    {
        "rules": [
            "User likes short, direct language",
            "User only speaks English & python",
        ],
        "my-key": "my-value",
    },
)

---

## 2. Define an evaluator

**URL:** llms-txt#2.-define-an-evaluator

def is_concise(outputs: dict, reference_outputs: dict) -> bool:
    return len(outputs["answer"]) < (3 * len(reference_outputs["answer"]))

---

## The "messages" stream mode returns an iterator of tuples (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-an-iterator-of-tuples-(message_chunk,-metadata)

---

## Pricing plans

**URL:** llms-txt#pricing-plans

Source: https://docs.langchain.com/langsmith/pricing-plans

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-plans.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## autoscaling:

**URL:** llms-txt#autoscaling:

---

## Define the processing node

**URL:** llms-txt#define-the-processing-node

def answer_node(state: InputState):
    # Replace with actual logic and do something useful
    return {"answer": "bye", "question": state["question"]}

---

## This is loaded from the `.env` file you created above

**URL:** llms-txt#this-is-loaded-from-the-`.env`-file-you-created-above

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_SERVICE_KEY = os.environ["SUPABASE_SERVICE_KEY"]

@auth.authenticate
async def get_current_user(authorization: str | None):
    """Validate JWT tokens and extract user information."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

try:
        # Verify token with auth provider
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{SUPABASE_URL}/auth/v1/user",
                headers={
                    "Authorization": authorization,
                    "apiKey": SUPABASE_SERVICE_KEY,
                },
            )
            assert response.status_code == 200
            user = response.json()
            return {
                "identity": user["id"],  # Unique user identifier
                "email": user["email"],
                "is_authenticated": True,
            }
    except Exception as e:
        raise Auth.exceptions.HTTPException(status_code=401, detail=str(e))

---

## How to simulate multi-turn interactions

**URL:** llms-txt#how-to-simulate-multi-turn-interactions

**Contents:**
- Setup
- Running a simulation
- Running in LangSmith experiments
  - Using `pytest` or `Vitest/Jest`
  - Using `evaluate`
- Modifying the simulated user persona
- Next Steps

Source: https://docs.langchain.com/langsmith/multi-turn-simulation

<Info>
  * [Multi-turn interactions](/langsmith/evaluation-concepts#multi-turn-interactions)
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
  * [LLM-as-judge](/langsmith/evaluation-concepts#llm-as-judge)
  * [OpenEvals](https://github.com/langchain-ai/openevals)
</Info>

AI applications with conversational interfaces, like chatbots, operate over multiple interactions with a user, also called conversation *turns*. When evaluating the performance of such applications, core concepts such as [building a dataset](/langsmith/evaluation-concepts#datasets) and defining [evaluators](/langsmith/evaluation-concepts#evaluators) and metrics to judge your app outputs remain useful. However, you may also find it useful to run a *simulation* between your app and a user, then evaluate this dynamically created trajectory.

Some advantages of doing this are:

* Ease of getting started vs. an evaluation over a full dataset of pre-existing trajectories
* End-to-end coverage from an initial query until a successful or unsuccessful resolution
* The ability to detect repetitive behavior or context loss over several iterations of your app

The downside is that because you are broadening your evaluation surface area to contain multiple turns, there is less consistency than evaluating a single output from your app given a static input from a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

This guide will show you how to simulate multi-turn interactions and evaluate them using the open-source [`openevals`](https://github.com/langchain-ai/openevals) package, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.

First, ensure you have the required dependencies installed:

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:

## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

The response looks like this:

The simulation first generates an initial query from the simulated `user`, then passes response chat messages back and forth until it reaches `max_turns` (you can alternatively pass a `stopping_condition` that takes the current trajectory and returns `True` or `False` - [see the OpenEvals README for more information](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation)). The return value is the final list of chat messages that make up the converation's **trajectory**.

<Info>
  There are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out [the OpenEvals README](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation).
</Info>

The final trace will look something [like this](https://smith.langchain.com/public/648ca37d-1c4d-4f7b-9b6a-89e35dc5d4f0/r) with responses from your `app` and `user` interleaved:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

Congrats! You just ran your first multi-turn simulation. Next, we'll cover how to run it in a LangSmith experiment.

## Running in LangSmith experiments

You can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith's [`pytest`](/langsmith/pytest) (Python-only), [`Vitest`/`Jest`](/langsmith/vitest-jest) (JS only), or [`evaluate`](/langsmith/evaluate-llm-application) runners.

### Using `pytest` or `Vitest/Jest`

<Check>
  See the following guides to learn how to set up evals using LangSmith's integrations with test frameworks:

* [`pytest`](https://docs.smith.langchain.com/langsmith/pytest)
  * [`Vitest` or `Jest`](https://docs.smith.langchain.com/langsmith/vitest-jest)
</Check>

If you are using one of the [LangSmith test framework integrations](/langsmith/pytest), you can pass in an array of OpenEvals evaluators as a `trajectory_evaluators` param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an `outputs` kwarg. Your passed `trajectory_evaluator` must therefore accept this kwarg.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=04c56e67e7bb9f01cb905d8a184d62d5" alt="" data-og-width="3448" width="3448" data-og-height="1128" height="1128" data-path="langsmith/images/multi-turn-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=41ac969c6ceb99ac0976ab3027b00e89 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c10eece031225173dc0ded446e3e2e3c 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=90bda951b2cfa02bde0c8ad204a7dac7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2ca95c911f68412eb09e2f8a0a6b42e4 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=12be96d7f216cd8ce664c01f61f45288 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1faa3a34b3daf67705e2afeca748e353 2500w" />

LangSmith will automatically detect and log the feedback returned from the passed `trajectory_evaluators`, adding it to the experiment. Note also that the test case uses the `fixed_responses` param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.

You may also find it convenient to have the simulated user's system prompt to be part of your logged dataset as well.

You can also use the [`evaluate`](/langsmith/evaluate-llm-application) runner to evaluate simulated multi-turn interactions. This will be a little bit different from the `pytest`/`Vitest`/`Jest` example in the following ways:

* The simulation should be part of your `target` function, and your target function should return the final trajectory.
  * This will make the trajectory the `outputs` that LangSmith will pass to your evaluators.
* Instead of using the `trajectory_evaluators` param, you should pass your evaluators as a param into the `evaluate()` method.
* You will need an existing dataset of inputs and (optionally) reference trajectories.

## Modifying the simulated user persona

The above examples run using the same simulated user persona for all input examples, defined by the `system` parameter passed into `create_llm_simulated_user`. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired `system` prompt, then pass that field in when creating your simulated user like this:

You've just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals.

Here are some topics you might want to explore next:

* [Trace multiturn conversations across different traces](/langsmith/threads)
* [Use multiple messages in the playground UI](/langsmith/multiple-messages)
* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

You can also explore the [OpenEvals readme](https://github.com/langchain-ai/openevals) for more on prebuilt evaluators.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multi-turn-simulation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:
```

Example 3 (unknown):
```unknown
## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## We want this to be a `POST` endpoint since we will post data here

**URL:** llms-txt#we-want-this-to-be-a-`post`-endpoint-since-we-will-post-data-here

@web_endpoint(method="POST")

---

## required environment variables

**URL:** llms-txt#required-environment-variables

CONTROL_PLANE_HOST = os.getenv("CONTROL_PLANE_HOST")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
INTEGRATION_ID = os.getenv("INTEGRATION_ID")
MAX_WAIT_TIME = 1800  # 30 mins

def get_headers() -> dict:
    """Return common headers for requests to the control plane API."""
    return {
        "X-Api-Key": LANGSMITH_API_KEY,
    }

def create_deployment() -> str:
    """Create deployment. Return deployment ID."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

deployment_name = "my_deployment"

request_body = {
        "name": deployment_name,
        "source": "github",
        "source_config": {
            "integration_id": INTEGRATION_ID,
            "repo_url": "https://github.com/langchain-ai/langgraph-example",
            "deployment_type": "dev",
            "build_on_push": False,
            "custom_url": None,
            "resource_spec": None,
        },
        "source_revision_config": {
            "repo_ref": "main",
            "langgraph_config_path": "langgraph.json",
            "image_uri": None,
        },
        "secrets": [
            {
                "name": "OPENAI_API_KEY",
                "value": "test_openai_api_key",
            },
            {
                "name": "ANTHROPIC_API_KEY",
                "value": "test_anthropic_api_key",
            },
            {
                "name": "TAVILY_API_KEY",
                "value": "test_tavily_api_key",
            },
        ],
    }

response = requests.post(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments",
        headers=headers,
        json=request_body,
    )

if response.status_code != 201:
        raise Exception(f"Failed to create deployment: {response.text}")

deployment_id = response.json()["id"]
    print(f"Created deployment {deployment_name} ({deployment_id})")
    return deployment_id

def get_deployment(deployment_id: str) -> dict:
    """Get deployment."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get deployment ID {deployment_id}: {response.text}")

return response.json()

def list_revisions(deployment_id: str) -> list[dict]:
    """List revisions.

Return list is sorted by created_at in descending order (latest first).
    """
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(
            f"Failed to list revisions for deployment ID {deployment_id}: {response.text}"
        )

return response.json()

def get_revision(
    deployment_id: str,
    revision_id: str,
) -> dict:
    """Get revision."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions/{revision_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get revision ID {revision_id}: {response.text}")

return response.json()

def patch_deployment(deployment_id: str) -> None:
    """Patch deployment."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

# This creates a new revision because source_revision_config is included
    response = requests.patch(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=headers,
        json={
            "source_config": {
                "build_on_push": True,
            },
            "source_revision_config": {
                "repo_ref": "main",
                "langgraph_config_path": "langgraph.json",
            },
        },
    )

if response.status_code != 200:
        raise Exception(f"Failed to patch deployment: {response.text}")

print(f"Patched deployment ID {deployment_id}")

def wait_for_deployment(deployment_id: str, revision_id: str) -> None:
    """Wait for revision status to be DEPLOYED."""
    start_time = time.time()
    revision, status = None, None
    while time.time() - start_time < MAX_WAIT_TIME:
        revision = get_revision(deployment_id, revision_id)
        status = revision["status"]
        if status == "DEPLOYED":
            break
        elif "FAILED" in status:
            raise Exception(f"Revision ID {revision_id} failed: {revision}")

print(f"Waiting for revision ID {revision_id} to be DEPLOYED...")
        time.sleep(60)

if status != "DEPLOYED":
        raise Exception(
            f"Timeout waiting for revision ID {revision_id} to be DEPLOYED: {revision}"
        )

def delete_deployment(deployment_id: str) -> None:
    """Delete deployment."""
    response = requests.delete(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 204:
        raise Exception(
            f"Failed to delete deployment ID {deployment_id}: {response.text}"
        )

print(f"Deployment ID {deployment_id} deleted")

if __name__ == "__main__":
    # create deployment and get the latest revision
    deployment_id = create_deployment()
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# patch the deployment and get the latest revision
    patch_deployment(deployment_id)
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# delete the deployment
    delete_deployment(deployment_id)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/api-ref-control-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Explore the failed inputs and outputs.

**URL:** llms-txt#explore-the-failed-inputs-and-outputs.

for r in failed:
    print(r["example"].inputs)
    print(r["run"].outputs)

---

## BedrockChat

**URL:** llms-txt#bedrockchat

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you getting started with Amazon Bedrock [chat models](/oss/javascript/langchain/models). For detailed documentation of all `BedrockChat` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html).

<Tip>
  The newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/oss/javascript/integrations/chat/bedrock_converse) integration package. Use [tool calling](/oss/javascript/langchain/tools) with more models with this package.
</Tip>

### Integration details

| Class                                                                                                          | Package                                                          | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/bedrock/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- | :---: | :----------: | :------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [`BedrockChat`](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) |   ❌   |       ✅      |                                      ✅                                     | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

To access Bedrock models you'll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `@langchain/community` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

---

## You can access the store directly to get the value

**URL:** llms-txt#you-can-access-the-store-directly-to-get-the-value

store.get(("users",), "user_123").value
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Implement a CI/CD pipeline using LangSmith Deployments and Evaluation

**URL:** llms-txt#implement-a-ci/cd-pipeline-using-langsmith-deployments-and-evaluation

**Contents:**
- Overview
- Pipeline architecture
  - Trigger sources
  - Testing layers
- GitHub Actions Workflow
  - Prerequisites
- Deployment options
  - Prerequisites for manual deployment
  - Local development and testing

Source: https://docs.langchain.com/langsmith/cicd-pipeline-example

This guide demonstrates how to implement a comprehensive CI/CD pipeline for AI agent applications deployed in LangSmith Deployments. In this example, you'll use the [LangGraph](/oss/python/langgraph/overview) open source framework for orchestrating and building the agent, [LangSmith](/langsmith/home) for observability and evaluations. This pipeline is based on the [cicd-pipeline-example repository](https://github.com/langchain-ai/cicd-pipeline-example).

The CI/CD pipeline provides:

* <Icon icon="check-circle" /> **Automated testing**: Unit, integration, and end-to-end tests.
* <Icon icon="chart-line" /> **Offline evaluations**: Performance assessment using [AgentEvals](https://github.com/langchain-ai/agentevals), [OpenEvals](https://github.com/langchain-ai/openevals) and [LangSmith](https://docs.langchain.com/langsmith/home).
* <Icon icon="rocket" /> **Preview and production deployments**: Automated staging and quality-gated production releases using the Control Plane API.
* <Icon icon="eye" /> **Monitoring**: Continuous evaluation and alerting.

## Pipeline architecture

The CI/CD pipeline consists of several key components that work together to ensure code quality and reliable deployments:

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions Workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployments using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [LangGraph dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

<img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

<AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

<Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

<Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

<Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/hosting):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration or Docker image deployment.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployments.

### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:

### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="LangGraph Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Trigger sources

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

### Testing layers

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions Workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployments using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

  <img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [LangGraph dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

  <img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

  <AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

    <Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

    <Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

    <Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

  See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

### Prerequisites

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/hosting):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration or Docker image deployment.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployments.
```

Example 2 (unknown):
```unknown
### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:
```

Example 3 (unknown):
```unknown
### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="LangGraph Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):
```

---

## Get or create tracer provider

**URL:** llms-txt#get-or-create-tracer-provider

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## songs by "prince" and our DB records the artist as "Prince", ideally when we query our

**URL:** llms-txt#songs-by-"prince"-and-our-db-records-the-artist-as-"prince",-ideally-when-we-query-our

---

## Access memory

**URL:** llms-txt#access-memory

@tool
def get_user_info(user_id: str, runtime: ToolRuntime) -> str:
    """Look up user info."""
    store = runtime.store
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

---

## This means that this node is the first one called

**URL:** llms-txt#this-means-that-this-node-is-the-first-one-called

workflow.add_edge(START, "agent")

---

## Customize user management

**URL:** llms-txt#customize-user-management

**Contents:**
- Features
  - Workspace level invites to an organization
  - SSO New Member Login Flow
  - Disabling Organization Creating
  - Disabling Personal Organizations

Source: https://docs.langchain.com/langsmith/self-host-user-management

<Note>
  This guide assumes you have read the [admin guide](/langsmith/administration-overview) and [organization setup guide](/langsmith/set-up-a-workspace#set-up-an-organization).
</Note>

LangSmith offers additional customization features for user management using feature flags.

### Workspace level invites to an organization

The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace **at the workspace level**.

Once this feature is enabled via the configuration option below, workspace Admins may add new users in the `Workspace members` tab under `Settings` > `Workspaces`. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.

1. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace
2. Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).

Admins may invite users for both cases at the same time.

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-user-management.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

#### Configuration

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

#### Configuration

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

---

## Before model hook

**URL:** llms-txt#before-model-hook

@before_model
def log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]
    print(f"Processing request for user: {runtime.context.user_name}")  # [!code highlight]
    return None

---

## ]

**URL:** llms-txt#]

**Contents:**
  - Reasoning
  - Local models
  - Prompt caching
  - Server-side tool use
  - Rate limiting
  - Base URL or proxy
  - Log probabilities
  - Token usage
  - Invocation config
  - Configurable models

python Stream reasoning output theme={null}
  for chunk in model.stream("Why do parrots have colorful feathers?"):
      reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
      print(reasoning_steps if reasoning_steps else chunk.text)
  python Complete reasoning output theme={null}
  response = model.invoke("Why do parrots have colorful feathers?")
  reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
  print(" ".join(step["reasoning"] for step in reasoning_steps))
  python Invoke with server-side tool use theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
response.content_blocks
python Result expandable theme={null}
[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "Here are some positive news stories from today...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "article title",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]
python Define a rate limiter theme={null}
  from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
      requests_per_second=0.1,  # 1 request every 10s
      check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request
      max_bucket_size=10,  # Controls the maximum burst size.
  )

model = init_chat_model(
      model="gpt-5",
      model_provider="openai",
      rate_limiter=rate_limiter  # [!code highlight]
  )
  python  theme={null}
  model = init_chat_model(
      model="MODEL_NAME",
      model_provider="openai",
      base_url="BASE_URL",
      api_key="YOUR_API_KEY",
  )
  python  theme={null}
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(
      model="gpt-4o",
      openai_proxy="http://proxy.example.com:8080"
  )
  python  theme={null}
model = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])
python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="openai:gpt-4o-mini")
    model_2 = init_chat_model(model="anthropic:claude-3-5-haiku-latest")

callback = UsageMetadataCallbackHandler()
    result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
    result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
    callback.usage_metadata
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-3-5-haiku-20241022': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import get_usage_metadata_callback

model_1 = init_chat_model(model="openai:gpt-4o-mini")
    model_2 = init_chat_model(model="anthropic:claude-3-5-haiku-latest")

with get_usage_metadata_callback() as cb:
        model_1.invoke("Hello")
        model_2.invoke("Hello")
        print(cb.usage_metadata)
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-3-5-haiku-20241022': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python Invocation with config theme={null}
response = model.invoke(
    "Tell me a joke",
    config={
        "run_name": "joke_generation",      # Custom name for this run
        "tags": ["humor", "demo"],          # Tags for categorization
        "metadata": {"user_id": "123"},     # Custom metadata
        "callbacks": [my_callback_handler], # Callback handlers
    }
)
python  theme={null}
from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-5-nano"}},  # Run with GPT-5-Nano
)
configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5"}},  # Run with Claude
)
python  theme={null}
  first_model = init_chat_model(
          model="gpt-4.1-mini",
          temperature=0,
          configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
          config_prefix="first",  # Useful when you have a chain with multiple models
  )

first_model.invoke("what's your name")
  python  theme={null}
  first_model.invoke(
      "what's your name",
      config={
          "configurable": {
              "first_model": "claude-sonnet-4-5",
              "first_temperature": 0.5,
              "first_max_tokens": 100,
          }
      },
  )
  python  theme={null}
  from pydantic import BaseModel, Field

class GetWeather(BaseModel):
      """Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
      """Get the current population in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

model = init_chat_model(temperature=0)
  model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4.1-mini"}}
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York, NY'},
          'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
          'type': 'tool_call'
      }
  ]
  python  theme={null}
  model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC",
          config={"configurable": {"model": "claude-sonnet-4-5"}},
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York City, NY'},
          'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
          'type': 'tool_call'
      }
  ]
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers.

### Reasoning

Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.

**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical "tiers" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.

For details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.

### Local models

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.

[Ollama](/oss/python/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/python/integrations/providers/overview).

### Prompt caching

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

* **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google_generative_ai) (Gemini 2.5 and above).
* **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) (via `prompt_cache_key`), Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching) and [`cache_control`](https://docs.langchain.com/oss/python/integrations/chat/anthropic#prompt-caching) options, [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching), [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).

<Warning>
  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/python/integrations/chat) for details.
</Warning>

Cache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response.

### Server-side tool use

Some providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.

If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/python/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:
```

Example 4 (unknown):
```unknown

```

---

## it will take precedence for any "read" actions on the "threads" resource

**URL:** llms-txt#it-will-take-precedence-for-any-"read"-actions-on-the-"threads"-resource

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.read.value
):
    # Since we are reading (and not creating) a thread,
    # we don't need to set metadata. We just need to
    # return a filter to ensure users can only see their own threads
    return {"owner": ctx.user.identity}

---

## First, post the runs to create them

**URL:** llms-txt#first,-post-the-runs-to-create-them

posts = [parent_run, child_run]
batch_ingest_runs(api_url, api_key, posts=posts)

---

## shortlived: "604800"  # 7 days (default is 14 days)

**URL:** llms-txt#shortlived:-"604800"--#-7-days-(default-is-14-days)

frontend:
  deployment:
    replicas: 4 # OR enable autoscaling to this level (example below)

---

## >             {

**URL:** llms-txt#>-------------{

---

## enabled: true

**URL:** llms-txt#enabled:-true

---

## Model caches

**URL:** llms-txt#model-caches

Source: https://docs.langchain.com/oss/javascript/integrations/llm_caching/index

[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL Semantic Cache" icon="link" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## View trace counts across your organization

**URL:** llms-txt#view-trace-counts-across-your-organization

**Contents:**
- Programmatically fetch trace counts
  - Method 1: Use the LangSmith REST API
  - Method 2: Use PostgreSQL support queries

Source: https://docs.langchain.com/langsmith/self-host-organization-charts

<Note>
  This feature is available on Helm chart versions 0.9.5 and later.
</Note>

LangSmith automatically generates and syncs organization usage charts for self-hosted installations.

These charts are available under `Settings > Usage and billing > Usage graph`:

* Usage by Workspace: this counts traces (root runs) by workspace
* Organization Usage: this counts all traces (root runs) for the organization

The charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.

## Programmatically fetch trace counts

You can retrieve trace counts programmatically using two different methods:

### Method 1: Use the LangSmith REST API

If your self-hosted installation uses an online key, you can use the [LangSmith REST API](https://api.smith.langchain.com/redoc?_gl=1*w68t81*_gcl_au*MTgyNTQ5MDUxNy4xNzU2NzI3MDky*_ga*MTU3NDY5MzQyNC4xNzQyOTMyMTQ2*_ga_47WX3HKKY2*czE3NTgyMDAxMDAkbzM0MSRnMCR0MTc1ODIwMDEwMCRqNjAkbDAkaDA.#tag/orgs/operation/get_org_usage_api_v1_orgs_current_billing_usage_get) to fetch organization usage data.

### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).

For more detailed information about running support queries, see the [Run support queries against PostgreSQL](/langsmith/script-running-pg-support-queries) guide.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-organization-charts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).
```

---

## Prompt Engineering Concepts

**URL:** llms-txt#prompt-engineering-concepts

**Contents:**
- Why prompt engineering?
- Prompts vs. prompt templates
- Prompts in LangSmith
  - Chat vs Completion
  - F-string vs. mustache
  - Tools
  - Structured output
  - Model
- Prompt versioning
  - Commits

Source: https://docs.langchain.com/langsmith/prompt-engineering-concepts

While traditional software applications are built by writing code, AI applications often derive their logic from prompts.

This guide will walk through the key concepts of prompt engineering in LangSmith.

## Why prompt engineering?

A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's behavior without changing its underlying capabilities. Just as telling an actor to "be a pirate" determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.

Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.

We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.

## Prompts vs. prompt templates

Although we often use these terms interchangably, it is important to understand the difference between "prompts" and "prompt templates".

Prompts refer to the messages that are passed into the language model.

Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=714ad4962c85cfb8847ebbf01559c217" alt="" data-og-width="1084" width="1084" data-og-height="450" height="450" data-path="langsmith/images/prompt-vs-prompt-template.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=81497578f297dd5a7311de6f1c06ef85 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d13dfcf34430d876cdc1a2b77a5fd3c4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=feec2461df390de635a2534b50eab8e6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e1851de62aa377fc3d70ec62dd91c5b2 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6bccce7c02cb065f9ef8964fbe3112b6 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c3494cef2e70686d55493ded764cc8c0 2500w" />

## Prompts in LangSmith

You can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.

### Chat vs Completion

There are two different types of prompts: `chat` style prompts and `completion` style prompts.

Chat style prompts are a **list of messages**. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.

Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.

### F-string vs. mustache

You can format your prompt with input variables using either [f-string](https://realpython.com/python-f-strings/) or [mustache](https://mustache.github.io/mustache.5.html) format. Here is an example prompt with f-string format:

And here is one with mustache:

To add a conditional mustache prompt:

* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:

<Check>
  The LangSmith Playground uses `f-string` as the default template format, but you can switch to `mustache` format in the prompt settings/template format section. `mustache` gives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, you'll need to manually add json variables in the 'inputs' section. Read [the documentation](https://mustache.github.io/mustache.5.html)
</Check>

Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.

### Structured output

Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use [Tools](#tools) under the hood.

<Check>
  Structured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM **always** responds in this format. With tools, the LLM may select **multiple** tools; with structured output, only one response is generate.
</Check>

Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).

Verisioning is a key part of iterating and collaborating on your different prompts.

Every saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. `prompt_name:commit_hash`).

In the UI, you can compare a commit with its previous version by toggling the "diff" button in the top-right corner of the Commits tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a045b239f92dc1c616c7e42ae282c2b9" alt="" data-og-width="2884" width="2884" data-og-height="1426" height="1426" data-path="langsmith/images/commit-diff.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5f1f1e66762226af9ec78e7dcd88a40e 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9170a56b51a619a3aea586942c6d1825 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198ae20ca965b50d42178bb241bddf46 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e56c9323edf28650c4c667def63d7d1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=25b8995161ed4523c09526eee44aef8a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac495d702a4df63f35ac47e5584a1c4b 2500w" />

You may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with `dev` or `prod` tags. This allows you to track which versions of prompts are used where.

The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.

In the playground you can:

* Change the model being used
* Change prompt template being used
* Change the output schema
* Change the tools available
* Enter the input variables to run through the prompt template
* Run the prompt through the model
* Observe the outputs

## Testing multiple prompts

You can add more prompts to your playground to easily compare outputs and decide which version is better:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-prompt-to-playground.gif?s=1c6f0c32b45a3f480b16d704c09570fc" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/add-prompt-to-playground.gif" data-optimize="true" data-opv="3" />

## Testing over a dataset

To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/test-over-dataset-in-playground.gif?s=aaf0f90a0c61934a928f81d5e11e2c35" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/test-over-dataset-in-playground.gif" data-optimize="true" data-opv="3" />

You can click on the "View Experiment" button to dive deeper into the results of the test.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/h4f6bIWGkog?si=IVJFfhldC7M3HL4G" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
And here is one with mustache:
```

Example 2 (unknown):
```unknown
To add a conditional mustache prompt:
```

Example 3 (unknown):
```unknown
* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:
```

---

## ttl:

**URL:** llms-txt#ttl:

---

## Control plane API reference for LangSmith Deployment

**URL:** llms-txt#control-plane-api-reference-for-langsmith-deployment

**Contents:**
- Host
- Authentication
- Versioning
- Quick Start
- Example Code

Source: https://docs.langchain.com/langsmith/api-ref-control-plane

The control plane API is part of [LangSmith Deployment](/langsmith/deployments). With the control plane API, you can programmatically create, manage, and automate your [LangGraph Server](/langsmith/langgraph-server) deployments—for example, as part of a custom CI/CD workflow.

<Card title="API Reference" href="https://api.host.langchain.com/docs" icon="book">
  View the full Control Plane API reference documentation
</Card>

The control plane hosts for Cloud data regions:

| US                               | EU                                  |
| -------------------------------- | ----------------------------------- |
| `https://api.host.langchain.com` | `https://eu.api.host.langchain.com` |

**Note**: Self-hosted deployments of LangSmith will have a custom host for the control plane. The control plane APIs can be accessed at the path `/api-host`. For example, `http(s)://<host>/api-host/v2/deployments`. See [here](../langsmith/self-host-usage#configuring-the-application-you-want-to-use-with-langsmith) for more details.

To authenticate with the control plane API, set the `X-Api-Key` header to a valid LangSmith API key.

Example `curl` command:

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.

```python  theme={null}
import os
import time

import requests
from dotenv import load_dotenv

**Examples:**

Example 1 (unknown):
```unknown
## Versioning

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

## Quick Start

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

## Example Code

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.
```

---

## Trace with Anthropic

**URL:** llms-txt#trace-with-anthropic

Source: https://docs.langchain.com/langsmith/trace-anthropic

The `wrap_anthropic` methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_anthropic`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

```python  theme={null}
import anthropic
from langsmith import traceable
from langsmith.wrappers import wrap_anthropic

client = wrap_anthropic(anthropic.Anthropic())

---

## >    )

**URL:** llms-txt#>----)

---

## Log traces to a specific project

**URL:** llms-txt#log-traces-to-a-specific-project

**Contents:**
- Set the destination project statically
- Set the destination project dynamically

Source: https://docs.langchain.com/langsmith/log-traces-to-project

You can change the destination project of your traces both statically through environment variables and dynamically at runtime.

## Set the destination project statically

As mentioned in the [Tracing Concepts](/langsmith/observability-concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

<Warning>
  The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Warning>

If the project specified does not exist, it will be created automatically when the first trace is ingested.

## Set the destination project dynamically

You can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](/langsmith/annotate-code). This is useful when you want to log traces to different projects within the same application.

<Note>
  Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-traces-to-project.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
export LANGSMITH_PROJECT=my-custom-project
```

Example 2 (unknown):
```unknown

```

---

## Add the middleware to the app

**URL:** llms-txt#add-the-middleware-to-the-app

**Contents:**
- Configure `langgraph.json`
  - Customize middleware ordering
- Start server
- Deploying
- Next steps

app.add_middleware(CustomHeaderMiddleware)
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app",
    "middleware_order": "auth_first"
  },
  "auth": {
    "path": "./auth.py:my_auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
```

Now any request to your server will include the custom header `X-Custom-Header` in its response.

You can deploy this app as-is to cloud or to your self-hosted platform.

Now that you've added custom middleware to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or define [custom lifespan events](/langsmith/custom-lifespan) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
### Customize middleware ordering

By default, custom middleware runs before authentication logic. To run custom middleware *after* authentication, set `middleware_order` to `auth_first` in your `http` configuration. (This customization is supported starting with API server v0.4.35 and later.)
```

Example 3 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## The sky is

**URL:** llms-txt#the-sky-is

---

## Self-host LangSmith with deployment

**URL:** llms-txt#self-host-langsmith-with-deployment

**Contents:**
- Overview
- Prerequisites
- Setup

Source: https://docs.langchain.com/langsmith/deploy-self-hosted-full-platform

This guide shows you how to enable **deployment management** on your self-hosted LangSmith instance. This adds a [control plane](/langsmith/control-plane) and [data plane](/langsmith/data-plane) that let you deploy, scale, and manage agents and applications directly through the LangSmith UI.

<Info>**Important**<br /> Self-hosting LangSmith with deployment requires an [Enterprise](https://langchain.com/pricing) plan. </Info>

<Note>
  **This setup page is for adding [deployment](/langsmith/deployments) capabilities to an existing LangSmith instance.**

Review the [self-hosted options](/langsmith/self-hosted) to understand:

* [LangSmith (observability)](/langsmith/self-hosted#langsmith): What you should install first.
  * [LangSmith with deployment](/langsmith/self-hosted#langsmith-with-deployment): What this guide enables.
  * [Standalone Server](/langsmith/self-hosted#standalone-server): Lightweight alternative without the UI.
</Note>

This guide builds on top of the [Kubernetes installation guide](/langsmith/kubernetes). **You must complete that guide first** before continuing. This page covers the additional setup steps required to enable deployment functionality:

* Installing the LangGraph operator
* Configuring your ingress
* Connecting to the control plane

1. You are using Kubernetes.
2. You have an instance of [self-hosted LangSmith](/langsmith/kubernetes) running.
3. Use the [LangGraph CLI](/langsmith/cli) to [test your application locally](/langsmith/local-server).
4. Use the [LangGraph CLI](/langsmith/cli) to build a Docker image (i.e. `langgraph build`) and push it to a registry your Kubernetes cluster has access to.
5. `KEDA` is installed on your cluster.

6. Ingress Configuration
   1. You must set up an ingress, gateway, or use Istio for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress. Use this guide to [set up an ingress](/langsmith/self-host-ingress) for your instance.
7. You must have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
8. A valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:

9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the [Egress documentation](/langsmith/self-host-egress) for more details.

1. As part of configuring your self-hosted LangSmith instance, you enable the `deployment` option. This will provision a few key resources.
   1. `listener`: This is a service that listens to the [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs.
   2. `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith deployment.
   3. `operator`: This operator handles changes to your LangSmith CRDs.
   4. `host-backend`: This is the [control plane](/langsmith/control-plane).

<Note>
  As of v0.12.0, the `langgraphPlatform` option is deprecated. Use `config.deployment` for any version after v0.12.0.
</Note>

2. Two additional images will be used by the chart. Use the images that are specified in the latest release.

3. In your config file for langsmith (usually `langsmith_config.yaml`), enable the `deployment` option. Note that you must also have a valid ingress setup:

4. In your `values.yaml` file, configure the `hostBackendImage` and `operatorImage` options (if you need to mirror images)
5. You can also configure base templates for your agents by overriding the base templates [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L898).
6. You create a deployment from the [control plane UI](/langsmith/control-plane#control-plane-ui).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-self-hosted-full-platform.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
6. Ingress Configuration
   1. You must set up an ingress, gateway, or use Istio for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress. Use this guide to [set up an ingress](/langsmith/self-host-ingress) for your instance.
7. You must have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
8. A valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:
```

Example 2 (unknown):
```unknown
9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the [Egress documentation](/langsmith/self-host-egress) for more details.

## Setup

1. As part of configuring your self-hosted LangSmith instance, you enable the `deployment` option. This will provision a few key resources.
   1. `listener`: This is a service that listens to the [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs.
   2. `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith deployment.
   3. `operator`: This operator handles changes to your LangSmith CRDs.
   4. `host-backend`: This is the [control plane](/langsmith/control-plane).

<Note>
  As of v0.12.0, the `langgraphPlatform` option is deprecated. Use `config.deployment` for any version after v0.12.0.
</Note>

2. Two additional images will be used by the chart. Use the images that are specified in the latest release.
```

Example 3 (unknown):
```unknown
3. In your config file for langsmith (usually `langsmith_config.yaml`), enable the `deployment` option. Note that you must also have a valid ingress setup:
```

---

## Run support queries against PostgreSQL

**URL:** llms-txt#run-support-queries-against-postgresql

**Contents:**
- Prerequisites
- Running the query script
- Export usage data
  - Get customer information

Source: https://docs.langchain.com/langsmith/script-running-pg-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query).

This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `pg_get_trace_counts_daily.sql` input file in the `support_queries/postgres` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

5. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_pg.sh)

## Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.

```bash  theme={null}
curl https://<langsmith_url>/api/v1/info

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

Example 2 (unknown):
```unknown
which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

## Export usage data

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.
```

---

## Pull the images from the public registry

**URL:** llms-txt#pull-the-images-from-the-public-registry

**Contents:**
- Configuration

docker pull langchain/langsmith-backend:latest
docker tag langchain/langsmith-backend:latest <your-registry>/langsmith-backend:latest
docker push <your-registry>/langsmith-backend:latest
yaml Helm theme={null}
  images:
    imagePullSecrets: [] # Add your image pull secrets here if needed
    registry: "" # Set this to your registry URL if you mirrored all images to the same registry using our script. Then you can remove the repository prefix from the images below.
    aceBackendImage:
      repository: "(your-registry)/langchain/langsmith-ace-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    backendImage:
      repository: "(your-registry)/langchain/langsmith-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    frontendImage:
      repository: "(your-registry)/langchain/langsmith-frontend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    hostBackendImage:
      repository: "(your-registry)/langchain/hosted-langserve-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    operatorImage:
      repository: "(your-registry)/langchain/langgraph-operator"
      pullPolicy: IfNotPresent
      tag: "6cc83a8"
    platformBackendImage:
      repository: "(your-registry)/langchain/langsmith-go-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    playgroundImage:
      repository: "(your-registry)/langchain/langsmith-playground"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    postgresImage:
      repository: "(your-registry)/postgres"
      pullPolicy: IfNotPresent
      tag: "14.7"
    redisImage:
      repository: "(your-registry)/redis"
      pullPolicy: IfNotPresent
      tag: "7"
    clickhouseImage:
      repository: "(your-registry)/clickhouse/clickhouse-server"
      pullPolicy: Always
      tag: "24.8"
  bash Docker theme={null}
  # In your .env file
  _REGISTRY=your-registry # Set this to your registry URL if you mirrored all images to the same registry using our script. Otherwise you will need to manually set the repository for each image in the compose file.
  ```
</CodeGroup>

Once configured, you will need to update your LangSmith installation. You can follow our upgrade guide here: [Upgrading LangSmith](/langsmith/self-host-upgrades).If your upgrade is successful, your LangSmith instance should now be using the mirrored images from your Docker registry.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-mirroring-images.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You will need to repeat this for each image that you want to mirror.

## Configuration

Once the images are mirrored, you will need to configure your LangSmith installation to use the mirrored images. You can do this by modifying the `values.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation. Replace tag with the version you want to use, e.g. `0.10.66` for the latest version at the time of writing.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## List buckets

**URL:** llms-txt#list-buckets

aws s3 --endpoint-url=<endpoint_url> ls /

---

## Use the functional API

**URL:** llms-txt#use-the-functional-api

**Contents:**
- Creating a simple workflow
- Parallel execution
- Calling graphs
- Call other entrypoints
- Streaming
- Retry policy

Source: https://docs.langchain.com/oss/python/langgraph/use-functional-api

The [**Functional API**](/oss/python/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) — to your applications with minimal changes to your existing code.

<Tip>
  For conceptual information on the functional API, see [Functional API](/oss/python/langgraph/functional-api).
</Tip>

## Creating a simple workflow

When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.

<Accordion title="Extended example: simple workflow">
  
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).

<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.

This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.
</Accordion>

The **Functional API** and the [**Graph API**](/oss/python/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime.

<Accordion title="Extended example: calling a simple graph from the functional API">
  
</Accordion>

## Call other entrypoints

You can call other **entrypoints** from within an **entrypoint** or a **task**.

<Accordion title="Extended example: calling another entrypoint">
  
</Accordion>

The **Functional API** uses the same streaming mechanism as the **Graph API**. Please
read the [**streaming guide**](/oss/python/langgraph/streaming) section for more details.

Example of using the streaming API to stream both updates and custom data.

1. Import [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) from `langgraph.config`.
2. Obtain a stream writer instance within the entrypoint.
3. Emit custom data before computation begins.
4. Emit another custom message after computing the result.
5. Use `.stream()` to process streamed output.
6. Specify which streaming modes to use.

<Warning>
  **Async with Python \< 3.11**
  If using Python \< 3.11 and writing async code, using [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work. Instead please
  use the `StreamWriter` class directly. See [Async with Python \< 3.11](/oss/python/langgraph/streaming#async) for more details.

```python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import RetryPolicy

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: simple workflow">
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.
```

Example 3 (unknown):
```unknown
</Accordion>

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).
```

Example 4 (unknown):
```unknown
<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.
```

---

## Deprecated method call

**URL:** llms-txt#deprecated-method-call

text = response.text()
```

Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## In pyproject.toml

**URL:** llms-txt#in-pyproject.toml

**Contents:**
- Usage
- Custom Embeddings

[project]
dependencies = [
    "langchain>=0.3.8"
]

langchain>=0.3.8
python  theme={null}
def search_memory(state: State, *, store: BaseStore):
    # Search the store using semantic similarity
    # The namespace tuple helps organize different types of memories
    # e.g., ("user_facts", "preferences") or ("conversation", "summaries")
    results = store.search(
        namespace=("memory", "facts"),  # Organize memories by type
        query="your search query",
        limit=3  # number of results to return
    )
    return results
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "path/to/embedding_function.py:embed",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Or if using [requirements.txt](/langsmith/setup-app-requirements-txt):
```

Example 2 (unknown):
```unknown
## Usage

Once configured, you can use semantic search in your [nodes](/oss/python/langgraph/graph-api#nodes). The store requires a namespace tuple to organize memories:
```

Example 3 (unknown):
```unknown
## Custom Embeddings

If you want to use custom embeddings, you can pass a path to a custom embedding function:
```

Example 4 (unknown):
```unknown
The deployment will look for the function in the specified path. The function must be async and accept a list of strings:
```

---

## await aevaluate(...)

**URL:** llms-txt#await-aevaluate(...)

**Contents:**
- Related

results = await ls_client.aevaluate(
    researcher_app,
    data=dataset,
    evaluators=[concise],
    # Optional, add concurrency.
    max_concurrency=2,  # Optional, add concurrency.
    experiment_prefix="gpt-4o-mini-baseline"  # Optional, random by default.
)
```

* [Run an evaluation (synchronously)](/langsmith/evaluate-llm-application)
* [Handle model rate limits](/langsmith/rate-limiting)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-async.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Use in an async context

**URL:** llms-txt#use-in-an-async-context

results = await search_store()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/semantic-search.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Human-in-the-loop leverages LangGraph's persistence layer.

**URL:** llms-txt#human-in-the-loop-leverages-langgraph's-persistence-layer.

---

## Create and run the crew

**URL:** llms-txt#create-and-run-the-crew

**Contents:**
- Advanced usage
  - Custom metadata and tags

crew = Crew(
    agents=[market_researcher, data_analyst, content_strategist],
    tasks=[research_task, analysis_task, content_task],
    verbose=True,
    process="sequential"  # Tasks will be executed in order
)

def run_market_research_crew():
    """Run the market research crew and return results."""
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI market research process...")
    output = run_market_research_crew()
    print("\n" + "="*50)
    print("CrewAI Process Output:")
    print("="*50)
    print(output)
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your CrewAI application:
```

---

## How to run evaluations with pytest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-pytest-(beta)

**Contents:**
- Installation
- Define and run tests
- Log inputs, outputs, and reference outputs
- Log feedback
- Trace intermediate calls
- Grouping tests into a test suite
- Naming experiments
- Caching
- pytest features
  - Parametrize with `pytest.mark.parametrize`

Source: https://docs.langchain.com/langsmith/pytest

The LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases. Compared to the standard evaluation flow, this is useful when:

* Each example requires different evaluation logic
* You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
* You want pytest-like terminal outputs
* You already use pytest to test your app and want to add LangSmith tracking

<Warning>
  The pytest integration is in beta and is subject to change in upcoming releases.
</Warning>

<Info>
  The JS/TS SDK has an analogous [Vitest/Jest integration](/langsmith/vitest-jest).
</Info>

This functionality requires Python SDK version `langsmith>=0.3.4`.

For extra features like [rich terminal outputs](#rich-outputs) and [test caching](#caching) install:

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
  
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:

In most cases we recommend setting a test suite name:

Each time you run this test suite, LangSmith:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) for each test file. If a dataset for this test file already exists it will be updated
* creates an [experiment](/langsmith/evaluation-concepts#experiment) in each created/updated dataset
* creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you've logged
* collects the pass/fail rate under the `pass` feedback key for each test case

Here's what a test suite dataset looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f40d29e4260eebc87838ea7be78bd08d" alt="Dataset" data-og-width="1078" width="1078" data-og-height="437" height="437" data-path="langsmith/images/simple-pytest-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ac625f42dd28d99e4fedaf193421d7f5 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=169fc45042f5c75d61e5c9a0dc9117cf 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b63c8752c2356d297cc44969da407673 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9a727e2fed9369844b4e6de59d72c0f 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e5d14240d3d0df568adb1655a28dbc58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9adad58f251fc1dba96c9b6f4092a11f 2500w" />

And what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=09776ac389e88f7f5058f4f7cf44dc72" alt="Experiment" data-og-width="1077" width="1077" data-og-height="444" height="444" data-path="langsmith/images/simple-pytest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=250e774ba91ea54112577a40466fdf51 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e77b0e75d32f47f9b9ab86df83f0ac99 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1b1bd6106aaed9412a2e56b52598e412 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=65630e87475c1c8570e0d47d18fd1629 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a4507629b61206359c2546afb17ac77a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5ee150a76f3e40e704c7de4b2d74fdf 2500w" />

## Log inputs, outputs, and reference outputs

Every time we run a test we're syncing it to a dataset example and tracing it as a run. There's a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the `log_inputs`, `log_outputs`, and `log_reference_outputs` methods. You can run these any time in a test to update the example and run for that test:

Running this test will create/update an example with name "test\_foo", inputs `{"a": 1, "b": 2}`, reference outputs `{"foo": "bar"}` and trace a run with outputs `{"foo": "baz"}`.

**NOTE**: If you run `log_inputs`, `log_outputs`, or `log_reference_outputs` twice, the previous values will be overwritten.

Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using `@pytest.mark.langsmith(output_keys=["name_of_ref_output_arg"])`:

This will create/sync an example with name "test\_cd", inputs `{"c": 5}` and reference outputs `{"d": 6}`, and run output `{"d": 10}`.

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with `log_feedback`.

Note the use of the `trace_feedback()` context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the `correct` feedback key.

**NOTE**: Make sure that the `log_feedback` call associated with the feedback trace occurs inside the `trace_feedback` context. This way we'll be able to associate the feedback with the trace, and when seeing the feedback in the UI you'll be able to click on it to see the trace that generated it.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Grouping tests into a test suite

By default, all tests within a given file will be grouped as a single "test suite" with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@pytest.mark.langsmith` for case-by-case grouping, or you can set the `LANGSMITH_TEST_SUITE` env var to group all tests from an execution into a single test suite:

We generally recommend setting `LANGSMITH_TEST_SUITE` to get a consolidated view of all of your results.

## Naming experiments

You can name an experiment using the `LANGSMITH_EXPERIMENT` env var:

LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with `langsmith[pytest]` and set an env var: `LANGSMITH_TEST_CACHE=/my/cache/path`:

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.

In `langsmith>=0.4.10`, you may selectively enable caching for requests to individual URLs or hostnames like this:

`@pytest.mark.langsmith` is designed to stay out of your way and works well with familiar `pytest` features.

### Parametrize with `pytest.mark.parametrize`

You can use the `parametrize` decorator as before. This will create a new test case for each parametrized instance of the test.

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.

### Parallelize with `pytest-xdist`

You can use [pytest-xdist](https://pytest-xdist.readthedocs.io/en/stable/) as you normally would to parallelize test execution:

### Async tests with `pytest-asyncio`

`@pytest.mark.langsmith` works with sync or async tests, so you can run async tests exactly as before.

### Watch mode with `pytest-watch`

Use watch mode to quickly iterate on your tests. We *highly* recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:

If you'd like to see a rich display of the LangSmith results of your test run you can specify `--langsmith-output`:

**Note:** This flag used to be `--output=langsmith` in `langsmith<=0.3.3` but was updated to avoid collisions with other pytest plugins.

You'll get a nice table per test suite that updates live as the results are uploaded to LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=10712bc97e37900ca83cb70df1c9357d" alt="Rich pytest outputs" data-og-width="1340" width="1340" data-og-height="548" height="548" data-path="langsmith/images/rich-pytest-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b689a6512f89045fdda11112f344c0aa 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be71e1c2abb5e14616ab8f9c4cf05912 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d7beb30f4e5f72bc30def219831184d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=85478a1e6830a5bc01b35eda991e86c7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2980269c433c3a30c0da34e57a6232ba 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fc52452a3b17850b27d4922b392185f8 2500w" />

Some important notes for using this feature:

* Make sure you've installed `pip install -U "langsmith[pytest]"`
* Rich outputs do not currently work with `pytest-xdist`

**NOTE**: The custom output removes all the standard pytest outputs. If you're trying to debug some unexpected behavior it's often better to show the regular pytest outputs so to get full error traces.

If you want to run the tests without syncing the results to LangSmith, you can set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

LangSmith provides an [expect](https://docs.smith.langchain.com/reference/python/_expect/langsmith._expect._Expect#langsmith._expect._Expect) utility to help define expectations about your LLM output. For example:

This will log the binary "expectation" score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.

`expect` also provides "fuzzy match" methods. For example:

This test case will be assigned 4 scores:

1. The `embedding_distance` between the prediction and the expectation
2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
3. The `edit_distance` between the prediction and the expectation
4. The overall test pass/fail score (binary)

The `expect` utility is modeled off of [Jest](https://jestjs.io/docs/expect)'s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.

#### `@test` / `@unit` decorator

The legacy method for marking test cases is using the `@test` or `@unit` decorators:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pytest.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:
```

Example 4 (unknown):
```unknown
In most cases we recommend setting a test suite name:
```

---

## 4. Run an evaluation

**URL:** llms-txt#4.-run-an-evaluation

experiment = ls_client.evaluate(
    chatbot,
    data=dataset,
    evaluators=[is_concise],
    experiment_prefix="my-first-experiment",
    # 'upload_results' is the relevant arg.
    upload_results=False
)

---

## Define the nodes

**URL:** llms-txt#define-the-nodes

def node_a(state: State) -> Command[Literal["node_b", "node_c"]]:
    print("Called A")
    value = random.choice(["b", "c"])
    # this is a replacement for a conditional edge function
    if value == "b":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        # this is the state update
        update={"foo": value},
        # this is a replacement for an edge
        goto=goto,
    )

def node_b(state: State):
    print("Called B")
    return {"foo": state["foo"] + "b"}

def node_c(state: State):
    print("Called C")
    return {"foo": state["foo"] + "c"}
python  theme={null}
builder = StateGraph(State)
builder.add_edge(START, "node_a")
builder.add_node(node_a)
builder.add_node(node_b)
builder.add_node(node_c)

**Examples:**

Example 1 (unknown):
```unknown
We can now create the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/python/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) inside `node_a`.
```

---

## Node 2 input only requests the private data available after node_1

**URL:** llms-txt#node-2-input-only-requests-the-private-data-available-after-node_1

class Node2Input(TypedDict):
    private_data: str

def node_2(state: Node2Input) -> OverallState:
    output = {"a": "set by node_2"}
    print(f"Entered node `node_2`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## Compile with checkpointer for persistence

**URL:** llms-txt#compile-with-checkpointer-for-persistence

**Contents:**
- 3. Test the graph locally

graph = builder.compile(checkpointer=checkpointer)
python  theme={null}
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
python {highlight={2,13}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=1165c5d1a5c154b2491d6a5fca30853f" alt="LangGraph chatbot with one step: START routes to autogen, where call_autogen_agent sends the latest user message (with prior context) to the AutoGen agent." data-og-width="180" width="180" data-og-height="134" height="134" data-path="langsmith/images/autogen-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6a6671038776cd1784c968ee2ecf973e 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=94c98b5b118ae49006d2f56179e1dc0d 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=703b4822dfc1c3395c16dd9e7d0f1462 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0f6b4e65d2f036d5b28dde44afbb5fd8 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=17f044c125e4a480d4bd8814c19a0949 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=841b246f7eabe796de9c4ac0af4816dd 2500w" />

## 3. Test the graph locally

Before deploying to LangSmith, you can test the graph locally:
```

---

## Manage datasets

**URL:** llms-txt#manage-datasets

**Contents:**
- Version a dataset
  - Create a new version of a dataset
  - Tag a version

Source: https://docs.langchain.com/langsmith/manage-datasets

LangSmith provides tools for managing and working with your [*datasets*](/langsmith/evaluation-concepts#datasets). This page describes dataset operations including:

* [Versioning datasets](#version-a-dataset) to track changes over time.
* [Filtering](#evaluate-on-a-filtered-view-of-a-dataset) and [splitting](#evaluate-on-a-dataset-split) datasets for evaluation.
* [Sharing datasets](#share-a-dataset) publicly.
* [Exporting datasets](#export-a-dataset) in various formats.

You'll also learn how to [export filtered traces](#export-filtered-traces-from-experiment-to-dataset) from [experiments](/langsmith/evaluation-concepts#experiment) back to datasets for further analysis and iteration.

In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.

### Create a new version of a dataset

Any time you add, update, or delete examples in your dataset, a new [version](/langsmith/evaluation-concepts#versions) of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.

By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the **Examples** tab, you will find the state of the dataset at that point in time.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da312a60576f449797be71e24229ea31" alt="Version Datasets" data-og-width="2544" width="2544" data-og-height="1241" height="1241" data-path="langsmith/images/version-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4ab823875dfefd0578cf98b5e7722c59 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bbf9ef95b76653348454895c30475c88 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4f169ff2abd8401334aea31668389cff 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=71a528f7c942c60acf5aeff46a0abc56 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b36ec02adba0f4df157298892b4eddb0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=44b47e051fd375b396203b5912f5be13 2500w" />

Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.

<Note>
  By default, the latest version of the dataset is shown in the **Examples** tab and experiments from all versions are shown in the **Tests** tab.
</Note>

In the **Tests** tab, you will find the results of tests run on the dataset at different versions.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=42c5ac800ef7282fa65013f6de02e45a" alt="Version Datasets" data-og-width="2483" width="2483" data-og-height="963" height="963" data-path="langsmith/images/version-dataset-tests.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d4a0e67f595dce4ae3769d3ecf01705c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a7743a10620ad86b18cf84dcba7943c8 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=146ca947da81dbbdd8f88067d1630874 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=066ce46f2b51a55c28f9c43430b7db8b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3c970aa22e36c5d8a5cc17ee1398b95a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2eab0c66d507ab2f4f35f07b774aec19 2500w" />

You can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your dataset's history.

For example, you might tag a version of your dataset as "prod" and use it to run tests against your LLM pipeline.

You can tag a version of your dataset in the UI by clicking on **+ Tag this version** in the **Examples** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c3f7a97c92eb645f7b0888f4e35ffd48" alt="Tagging Datasets" data-og-width="662" width="662" data-og-height="124" height="124" data-path="langsmith/images/tag-this-version.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=93c44c3c1243429e10fea238ca078d37 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=99d4bd65c4b30f29c75443846c785b0a 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34c4a8a96a8024cdebcad7815825d0a0 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=af3c62be520a88604634349c8a5884db 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9a00d92647ebd171976304ad4def786 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21c96cdd5609bb4bfb5148ac7dd8e098 2500w" />

You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the [Python SDK](https://docs.smith.langchain.com/reference/python/reference):

```python  theme={null}
from langsmith import Client
from datetime import datetime

client = Client()
initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag

---

## User management

**URL:** llms-txt#user-management

**Contents:**
- Set up access control
  - Create a role
  - Assign a role to a user
- Set up SAML SSO for your organization
  - Just-in-time (JIT) provisioning
  - Login methods and access
  - Enforce SAML SSO only
  - Prerequisites
  - Initial configuration
  - Entra ID (Azure)

Source: https://docs.langchain.com/langsmith/user-management

This page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:

* [Set up access control](#set-up-access-control): Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.
* [SAML SSO (Enterprise plan)](#set-up-saml-sso-for-your-organization): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.
* [SCIM User Provisioning (Enterprise plan)](#set-up-scim-for-your-organization): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.

## Set up access control

<Note>
  RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the [`Admin` role](/langsmith/administration-overview) for all users.
</Note>

<Check>
  You may find it helpful to read the [Administration overview](/langsmith/administration-overview) page before setting up access control.
</Check>

LangSmith relies on RBAC to manage user permissions within a [workspace](/langsmith/administration-overview#workspaces). This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the `workspace:manage` permission can manage access control settings for a workspace.

By default, LangSmith comes with a set of system roles:

* `Admin`: has full access to all resources within the workspace.
* `Viewer`: has read-only access to all resources within the workspace.
* `Editor`: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).

If these do not fit your access model, `Organization Admins` can create custom roles to suit your needs.

To create a role, navigate to the **Roles** tab in the **Members and roles** section of the [Organization settings page](https://smith.langchain.com/settings). Note that new roles that you create will be usable across all workspaces within your organization.

Click on the **Create Role** button to create a new role. A **Create role** form will open.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6a413dbec076a37d680fa2ed8a91c495" alt="Create Role" data-og-width="3078" width="3078" data-og-height="1932" height="1932" data-path="langsmith/images/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9b92bb7e4743445999e92faac75163ec 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9ca40498a6740d4b73a5856461088af8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=77bde5767be13e160a54d06c5c016953 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c83fd7dfd2f046073d80682607c6b6e1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=318019bbd5d5e272aa6bad083f6713d5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=642e5b56a1dc12a29173f969ec08be45 2500w" />

Assign permissions for the different LangSmith resources that you want to control access to.

### Assign a role to a user

Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the `Workspace members` tab in the `Workspaces` section of the [Organization settings page](https://smith.langchain.com/settings)

Each user will have a **Role** dropdown that you can use to assign a role to them.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec5748a2c884693a24f984cc517a3860" alt="Assign Role" data-og-width="1888" width="1888" data-og-height="574" height="574" data-path="langsmith/images/assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9530054e9b95d485534b328b13c6df69 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=864bd5501ab0f253fc5f6db15dec9205 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=520648bd2bef4ed100193c0031b9b3e9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=53bbafe1b595daf0487a2e8daff8ce68 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b6b5ea2d4d814c28b243d170f0af6359 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ad29acab6d3c0e06b0f5ddbef793ca9 2500w" />

You can also invite new users with a given role.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3787cb55cbefcd5c95d6c01b6e9f6e75" alt="Invite User" data-og-width="1204" width="1204" data-og-height="886" height="886" data-path="langsmith/images/invite-user.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a300a15bd2572a18b8cc2f37921ffb6 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ea044cb3c58da8351dc60f2d3a040c23 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a3ab5fd8738cde3f2257e580daf96114 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e8b4ab3a1f758aaa93a8694da3ee9cf 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7642a1b08854289db7bb7939c23db52 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=33205dfa57d9a51af02c4f53395fc7f6 2500w" />

## Set up SAML SSO for your organization

Single Sign-On (SSO) functionality is **available for Enterprise Cloud** customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.

LangSmith's SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.

SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:

* Streamlines user management across systems for organization owners.
* Enables organizations to enforce their own security policies (e.g., MFA).
* Removes the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.

### Just-in-time (JIT) provisioning

LangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.

<Note>
  JIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a [different login method](/langsmith/authentication-methods#cloud).
</Note>

### Login methods and access

Once you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to [other login methods](/langsmith/authentication-methods#cloud), such as username/password or Google Authentication":

* When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.
* Users with SAML SSO as their only login method do not have [personal organizations](/langsmith/administration-overview#organizations).
* When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.

### Enforce SAML SSO only

<Note>
  User invites are not supported in organizations enforcing SAML SSO only. Initial workspace membership and role is determined by JIT provisioning, and changes afterwards can be managed in the UI.
  For additional flexibility in automated user management, LangSmith supports SCIM.
</Note>

To ensure users can only access the organization when logged in using SAML SSO and no other method, check the **Login via SSO only** checkbox and click **Save**. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking **Save**.

<Note>
  You must be logged in via SAML SSO in order to update this setting to `Only SAML SSO`. This is to ensure the SAML settings are valid and avoid locking users out of your organization.
</Note>

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SAML SSO, reach out to the LangChain support team at [support@langchain.dev](mailto:support@langchain.dev).

<Note>
  SAML SSO is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing-langsmith). Please [contact sales](https://www.langchain.com/contact-sales) to learn more.
</Note>

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support the SAML 2.0 standard.
* Only [`Organization Admins`](/langsmith/observability-concepts#organization-roles) can configure SAML SSO.

For instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the [SCIM setup](#set-up-scim-for-your-organization).

### Initial configuration

<Note>
  For IdP-specific configuration steps, refer to one of the following:

* [Entra ID](#entra-id-azure)
  * [Google](#google)
  * [Okta](#okta)
</Note>

1. In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.

<Note>
     The following URLs are different for the US and EU regions. Ensure you select the correct link.
   </Note>

1. Single sign-on URL (or ACS URL):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. Audience URI (or SP Entity ID):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Name ID format: email address.
   4. Application username: email address.
   5. Required claims: `sub` and `email`.

2. In LangSmith: Go to **Settings** -> **Members and roles** -> **SSO Configuration**. Fill in the required information and submit to activate SSO login:

1. Fill in either the `SAML metadata URL` or `SAML metadata XML`.
   2. Select the `Default workspace role` and `Default workspaces`. New users logging in via SSO will be added to the specified workspaces with the selected role.

* `Default workspace role` and `Default workspaces` are editable. The updated settings will apply to new users only, not existing users.
* (Coming soon) `SAML metadata URL` and `SAML metadata XML` are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso).

<div id="create-application-entra-id" />

**Step 1: Create a new Entra ID application integration**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`). On the left navigation pane, select the `Entra ID` service.

2. Navigate to **Enterprise Applications** and then select **All Applications**.

3. Click **Create your own application**.

4. In the **Create your own application** window:

1. Enter a name for your application (e.g., `LangSmith`).
   2. Select \*Integrate any other application you don't find in the gallery (Non-gallery)\*\*.

**Step 2: Configure the Entra ID application and obtain the SAML Metadata**

1. Open the enterprise application that you created.

2. In the left-side navigation, select **Manage** > **Single sign-on**.

3. On the Single sign-on page, click **SAML**.

4. Update the **Basic SAML Configuration**:

1. `Identifier (Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   2. `Reply URL (Assertion Consumer Service URL)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   3. Leave `Relay State`, `Logout Url`, and `Sign on URL` empty.
   4. Click **Save**.

5. Ensure required claims are present with **Namespace**: `http://schemas.xmlsoap.org/ws/2005/05/identity/claims`:

1. `sub`: `user.objectid`.
   2. `emailaddress`: `user.userprincipalname` or `user.mail` (if using the latter, ensure all users have the `Email` field filled in under `Contact Information`).
   3. (Optional) For SCIM, see the [setup documentation](/langsmith/user-management) for specific instructions about `Unique User Identifier (Name ID)`.

6. On the SAML-based Sign-on page, under **SAML Certificates**, copy the **App Federation Metadata URL**.

**Step 3: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the metadata URL from the previous step.

**Step 4: Verify the SSO setup**

1. Assign the application to users/groups in Entra ID:

1. Select **Manage** > **Users and groups**.

2. Click **Add user/group**.

3. In the **Add Assignment** window:

1. Under **Users**, click **None Selected**.
      2. Search for the user you want to assign to the enterprise application, and then click **Select**.
      3. Verify that the user is selected, and click **Assign**.

2. Have the user sign in via the unique login URL from the **SSO Configuration** page, or go to **Manage** > **Single sign-on** and select **Test single sign-on with (application name)**.

For additional information, see Google's [documentation](https://support.google.com/a/answer/6087519).

**Step 1: Create and configure the Google Workspace SAML application**

1. Make sure you're signed into an administrator account with the appropriate permissions.

2. In the Admin console, go to **Menu** -> **Apps** -> **Web and mobile apps**.

3. Click **Add App** and then **Add custom SAML app**.

4. Enter the app name and, optionally, upload an icon. Click **Continue**.

5. On the Google Identity Provider details page, download the **IDP metadata** and save it for Step 2. Click **Continue**.

6. In the `Service Provider Details` window, enter:

1. `ACS URL`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Entity ID`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Leave `Start URL` and the `Signed response` box empty.
   4. Set `Name ID` format to `EMAIL` and leave `Name ID` as the default (`Basic Information > Primary email`).
   5. Click `Continue`.

7. Use `Add mapping` to ensure required claims are present:
   1. `Basic Information > Primary email` -> `email`

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the `IDP metadata` from the previous step as the metadata XML.

**Step 3: Turn on the SAML app in Google**

1. Select the SAML app under `Menu -> Apps -> Web and mobile apps`

2. Click `User access`.

3. Turn on the service:

1. To turn the service on for everyone in your organization, click `On for everyone`, and then click `Save`.

2. To turn the service on for an organizational unit:

1. At the left, select the organizational unit then `On`.
      2. If the Service status is set to `Inherited` and you want to keep the updated setting, even if the parent setting changes, click `Override`.
      3. If the Service status is set to `Overridden`, either click `Inherit` to revert to the same setting as its parent, or click `Save` to keep the new setting, even if the parent setting changes.

3. To turn on a service for a set of users across or within organizational units, select an access group. For details, go to [Use groups to customize service access](https://support.google.com/a/answer/9050643).

4. Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the **SSO Configuration** page, or go to the SAML application page in Google and click **TEST SAML LOGIN**.

#### Supported features

* IdP-initiated SSO (Single Sign-On)
* SP-initiated SSO
* Just-In-Time provisioning
* Enforce SSO only

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_saml.htm).

**Step 1: Create and configure the Okta SAML application**

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Leave `ApiUrlBase` empty.
7. Fill in `AuthHost`:
   * US: `auth.langchain.com`
   * EU: `eu.auth.langchain.com`
8. (Optional, if planning to use [SCIM](#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`:
   * US: `api.smith.langchain.com`
   * EU: `eu.api.smith.langchain.com`
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `SAML 2.0`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`
    * `Update application username on`: `Create and update`
    * `Allow users to securely see their password`: leave **unchecked**.
13. Copy the **Metadata URL** from the **Sign On Options** page to use in the next step.

**Via Custom App Integration**

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.

2. Under **Applications** > **Applications** click **Create App Integration**.

3. Select **SAML 2.0**.

4. Enter an `App name` (e.g., `LangSmith`) and optionally an **App logo**, then click **Next**.

5. Enter the following information in the **Configure SAML** page:

1. `Single sign-on URL` (`ACS URL`). Keep `Use this for Recipient URL and Destination URL` checked:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Audience URI (SP Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. `Name ID format`: **Persistent**.
   4. `Application username`: `email`.
   5. Leave the rest of the fields empty or set to their default.
   6. Click **Next**.

7. Copy the **Metadata URL** from the **Sign On** page to use in the next step.

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the **Fill in required information** step, using the metadata URL from the previous step.

**Step 3: Assign users to LangSmith in Okta**

1. Under **Applications** > **Applications**, select the SAML application created in Step 1.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or have a user select the application from their Okta dashboard.

#### SP-initiated SSO

Once service-provider–initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under **Organization members and roles** then **SSO configuration**.

## Set up SCIM for your organization

System for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith [organization and workspaces](/langsmith/administration-overview), keeping user access synchronized with your organization's identity provider.

<Note>
  SCIM is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing). [Contact sales](https://www.langchain.com/contact-sales) to learn more.

SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.

SCIM support is API-only (see instructions below).
</Note>

SCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organization's identity system. This allows for:

* **Automated user management**: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.
* **Reduced administrative overhead**: No need to manage user access manually across multiple systems.
* **Improved security**: Users who leave your organization are automatically deprovisioned from LangSmith.
* **Consistent access control**: User attributes and group memberships are synchronized between systems.
* **Scaling team access control**: Efficiently manage large teams with many workspaces and custom roles.
* **Role assignment**: Select specific [Organization Roles](/langsmith/observability-concepts#organization-roles) and [Workspace Roles](/langsmith/observability-concepts#workspace-roles) for groups of users.

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support SCIM 2.0.
* Only [Organization Admins](/langsmith/administration-overview#organization-roles) can configure SCIM.
* For cloud customers: [SAML SSO](#set-up-saml-sso-for-your-organization) must be configurable for your organization.
* For self-hosted customers: [OAuth with Client Secret](/langsmith/self-host-sso#with-secret) authentication mode must be enabled.
* For self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:
  * Microsoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.
    ([details](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/use-scim-to-provision-users-and-groups#ip-ranges)).
  * Okta supports allow-listing IPs or domains ([details](https://help.okta.com/en-us/content/topics/security/ip-address-allow-listing.htm))
    or an agent-based solution ([details](https://help.okta.com/en-us/content/topics/provisioning/opp/opp-main.htm)) to provide connectivity.

When a user belongs to multiple groups for the same workspace, the following precedence applies:

1. **Organization Admin groups** take highest precedence. Users in these groups will be `Admin` in all workspaces.
2. **Most recently created workspace-specific group** takes precedence over other workspace groups.

<Note>
  When a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.

SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.
</Note>

#### Email verification

In cloud only, creating a new user with SCIM triggers an email to the user.
They must verify their email address by clicking the link in this email.
The link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.

### Attributes and Mapping

#### Group Naming Convention

Group membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:

**Organization Admin Groups**

Format: `<optional_prefix>Organization Admin` or `<optional_prefix>Organization Admins`

* `LS:Organization Admins`
* `Groups-Organization Admins`
* `Organization Admin`

**Workspace-Specific Groups**

Format: `<optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>`

* `LS:Organization User:Production:Annotators`
* `Groups-Organization User:Engineering:Developers`
* `Organization User:Marketing:Viewers`

While specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:

| **LangSmith App Attribute**    | **Identity Provider Attribute**                       | **Matching Precedence** |
| ------------------------------ | ----------------------------------------------------- | ----------------------- |
| `userName`<sup>1</sup>         | email address                                         |                         |
| `active`                       | `!deactivated`                                        |                         |
| `emails[type eq "work"].value` | email address<sup>2</sup>                             |                         |
| `name.formatted`               | `displayName` OR `givenName + familyName`<sup>3</sup> |                         |
| `givenName`                    | `givenName`                                           |                         |
| `familyName`                   | `familyName`                                          |                         |
| `externalId`                   | `sub`<sup>4</sup>                                     | 1                       |

1. `userName` is not required by LangSmith
2. Email address is required
3. Use the computed expression if your `displayName` does not match the format of `Firstname Lastname`
4. To avoid inconsistency, this should match the SAML `NameID` assertion for cloud customers, or the `sub` OAuth2.0 claim for self-hosted.

#### Group Attributes

| **LangSmith App Attribute** | **Identity Provider Attribute** | **Matching Precedence** |
| --------------------------- | ------------------------------- | ----------------------- |
| `displayName`               | `displayName`<sup>1</sup>       | 1                       |
| `externalId`                | `objectId`                      |                         |
| `members`                   | `members`                       |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` identity provider attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

### Step 1 - Configure SAML SSO (Cloud only)

There are two scenarios for [SAML SSO](#set-up-saml-sso-for-your-organization) configuration:

1. If SAML SSO is already configured for your organization, you should skip the steps to initially add the application ([Add application from Okta Integration Network](#add-application-okta-oin) or [Create a new Entra ID application integration](#create-application-entra-id)), as you already have an application configured and just need to enable provisioning.
2. If you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to [set up SAML SSO](#set-up-saml-sso-for-your-organization), *then* follow the instructions here to enable SCIM.

LangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.

1. Be unique to each user.
2. Be a persistent value that never changes, such as a randomly generated unique user ID.
3. Match exactly on each sign-in attempt. It should not rely on user input.

The NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.

The NameID format must be `Persistent`, unless you are using a field, like email, that requires a different format.

### Step 2 - Disable JIT provisioning

Before enabling SCIM, disable [Just-in-time (JIT) provisioning](/langsmith/user-management#just-in-time-jit-provisioning) to prevent conflicts between automatic and manual user provisioning.

#### Disabling JIT for Cloud

Use the `PATCH /orgs/current/info` [endpoint](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch):

#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:

### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:

Note that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:

* `GET /v1/platform/orgs/current/scim/tokens`
* `GET /v1/platform/orgs/current/scim/tokens/{scim_token_id}`
* `PATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id}` (only the `description` field is supported)
* `DELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}`

### Step 4 - Configure your Identity Provider

<Note>
  If you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to [Azure Entra ID](#azure-entra-id-configuration-steps), [Okta](#okta)). The requirements and steps above are applicable for all identity providers.
</Note>

#### Azure Entra ID configuration steps

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/user-provisioning).

<Note>
  In self-hosted installations, the `oid` JWT claim is used as the `sub`.
  See [this Microsoft Learn link](https://learn.microsoft.com/en-us/answers/questions/5546297/how-to-link-oidc-users-with-scim)
  and [the related configuration instructions](/langsmith/self-host-sso#override-sub-claim) for additional details.
</Note>

**Step 1: Configure SCIM in your Enterprise Application**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`).
2. Navigate to your existing LangSmith Enterprise Application.
3. In the left-side navigation, select **Manage** > **Provisioning**.
4. Click **Get started**.

**Step 2: Configure Admin credentials**

1. Under **Admin Credentials**:

* US: `https://api.smith.langchain.com/scim/v2`
     * EU: `https://eu.api.smith.langchain.com/scim/v2`
     * Self-hosted: `<langsmith_url>/scim/v2`

* **Secret Token**: Enter the SCIM Bearer Token generated in Step 3.

2. Click **Test Connection** to verify the configuration.

**Step 3: Configure Attribute Mappings**

Configure the following attribute mappings under `Mappings`:

Set **Target Object Actions** to `Create` and `Update` (start with `Delete` disabled for safety):

|   **LangSmith App Attribute**  |            **Microsoft Entra ID Attribute**           | **Matching Precedence** |
| :----------------------------: | :---------------------------------------------------: | :---------------------: |
|           `userName`           |                  `userPrincipalName`                  |                         |
|            `active`            |                 `Not([IsSoftDeleted])`                |                         |
| `emails[type eq "work"].value` |                        `mail`1                        |                         |
|        `name.formatted`        | `displayName` OR `Join(" ", [givenName], [surname])`2 |                         |
|          `externalId`          |                      `objectId`3                      |            1            |

1. User's email address must be present in Entra ID.
2. Use the `Join` expression if your `displayName` does not match the format of `Firstname Lastname`.
3. To avoid inconsistency, this should match the SAML NameID assertion and the `sub` OAuth2.0 claim. For SAML SSO in cloud, the `Unique User Identifier (Name ID)` required claim should be `user.objectID` and the `Name identifier format` should be `persistent`.

Set **Target Object Actions** to `Create` and `Update` only (start with `Delete` disabled for safety):

| **LangSmith App Attribute** | **Microsoft Entra ID Attribute** | **Matching Precedence** |
| :-------------------------: | :------------------------------: | :---------------------: |
|        `displayName`        |          `displayName`1          |            1            |
|         `externalId`        |            `objectId`            |                         |
|          `members`          |             `members`            |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` Microsoft Entra ID Attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

**Step 4: Assign Users and Groups**

1. Under **Applications** > **Applications**, select your LangSmith Enterprise Application.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 5: Enable Provisioning**

1. Set **Provisioning Status** to `On` under **Provisioning**.
2. Monitor the initial sync to ensure users and groups are provisioned correctly.
3. Once verified, enable `Delete` actions for both User and Group mappings.

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SCIM, reach out to the LangChain support team at [support@langchain.dev](mailto:support@langchain.dev).

#### Okta configuration steps

<Note>
  You must use the [Okta Lifecycle Management](https://www.okta.com/products/lifecycle-management/) product. This product tier is required to use SCIM on Okta.
</Note>

<div id="supported-features">
  <b>Supported features</b>
</div>

* Create users
* Update user attributes
* Deactivate users
* Group push
* Import users
* Import groups

<div id="add-application-okta-oin">
  <b>Step 1: Add application from Okta Integration Network</b>
</div>

<Note>
  If you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.
</Note>

See [SAML SSO setup](#okta) for cloud or [OAuth2.0 setup](/langsmith/self-host-sso#okta-idp-setup) for self-hosted.

**Step 2: Configure API Integration**

1. In the General tab, ensure the `LangSmithUrl` is filled in according to the instructions from [Step 1](#add-application-okta-oin)
2. In the Provisioning tab, select `Integration`.
3. Select `Edit` then `Enable API integration`.
4. For API Token, paste the SCIM token you [generated above](#step-3-generate-scim-bearer-token).
5. Keep `Import Groups` checked.
6. To verify the configuration, select Test API Credentials.
7. Select Save.
8. After saving the API integration details, new settings tabs appear on the left. Select `To App`.
9. Select Edit.
10. Select the Enable checkbox for Create Users, Update Users, and Deactivate Users.
11. Select Save.
12. Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.

**Step 3: Configure User Provisioning Settings**

1. Configure provisioning: under `Provisioning > To App > Provisioning to App`, click `Edit`, then check `Create Users`, `Update User Attributes`, and `Deactivate Users`.
2. Under `<application_name> Attribute Mappings`, set the user attribute mappings as shown below, and delete the rest:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=4951533e29e0f0f75e7aac74dcfab3bb" alt="SCIM Okta User Attributes Mapping" data-og-width="748" width="748" data-og-height="467" height="467" data-path="langsmith/images/scim_okta_user_attributes.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=280&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f883ed0bf6e0876126e5a7f79dadfbb1 280w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=560&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=0dcd59e75cc4e7b5532dfc9ad81347fa 560w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=840&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=3c9ee64fb3d1357bb67685ca85ada751 840w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1100&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=fb7274973d558c7973c6c00195d77341 1100w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1650&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=7830d972b9ad3cb036d8c08623bc891b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=2500&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f1e137b9faa8d6422cff5e3673cf98fb 2500w" />

**Step 4: Push Groups**

<Note>
  Okta does not support group attributes besides the group name itself, so group name *must* follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
</Note>

Follow Okta's [Enable Group Push](https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-enable-group-push.htm) instructions to configure groups to push by name or by rule.

#### Other Identity Providers

Other identity providers have not been tested but may function depending on their SCIM implementation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/user-management.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:
```

Example 2 (unknown):
```unknown
### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:
```

---

## Conversation 1: Learn about a project

**URL:** llms-txt#conversation-1:-learn-about-a-project

agent.invoke({
    "messages": [{"role": "user", "content": "We're building a web app with React. Save project notes."}]
})

---

## How to return multiple scores in one evaluator

**URL:** llms-txt#how-to-return-multiple-scores-in-one-evaluator

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/multiple-scores

Sometimes it is useful for a custom evaluator or summary evaluator to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.

To return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:

To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form

Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

Rows from the resulting experiment will display each of the scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7f0a67189b7202a46d5e093cce9ea283" alt="multiple_scores.png" data-og-width="1622" width="1622" data-og-height="1020" height="1020" data-path="langsmith/images/multiple-scores.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb2f322c66eadd3ef0eef99a5c11063f 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a52f0d760f2e079c6701e83c48fb73 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147f1a6046e9dabac499f68ec6abf68f 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0a807e000d2e2d7cd18b6af3e83e8a59 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f46442dafae69f9039fe1c9def5a1a01 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c182c52b39a42dbcab618fc8b2a0c525 2500w" />

* [Return categorical vs numerical metrics](/langsmith/metric-type)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-scores.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form
```

Example 2 (unknown):
```unknown
Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

Example:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## minReplicas: 5

**URL:** llms-txt#minreplicas:-5

backend:
  deployment:
    replicas: 16 # OR enable autoscaling to this level (example below)

---

## Troubleshooting

**URL:** llms-txt#troubleshooting

**Contents:**
- Getting helpful information
- Common issues
  - *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*
  - *error: Dirty database version 'version'. Fix and force version*
  - *413 - Request Entity Too Large*
  - *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*
  - *ClickHouse fails to start up when running a cluster with AquaSec*

Source: https://docs.langchain.com/langsmith/troubleshooting

This guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.

While running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.

## Getting helpful information

To diagnose and resolve an issue, you will first need to retrieve some relevant information. The following sections explain how to do this for a Kubernetes or a Docker setup, and how to pull helpful browser information.

Generally, the main services you will want to analyze are the:

* `langsmith-backend`: Handles CRUD API requests, business logic, requests from the frontend and SDK, trace preparation for ingestion, and the hub API.
* `langsmith-platform-backend`: Handles authentication, run ingestion, and other high-volume tasks.
* `langsmith-queue`: Handles incoming traces and feedback, asynchronous ingestion and persistence into the datastore, data integrity checks, and retries during database errors or connection issues.

For more details on these services, refer to the [Architectural overview](/langsmith/architectural-overview).

The first step in troubleshooting is to gather important debugging information about your LangSmith deployment. Service logs, kubernetes events, and resource utilization of containers can help identify the root cause of an issue.

You can run our [k8s troubleshooting script](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_k8s_debugging_info.sh) which will pull all of the relevant kubernetes information and output it to a folder for investigation. The script also compresses this folder into a zip file for sharing. Here is an example of how to run this script, assuming your langsmith deployment was brought up in a `langsmith` namespace:

You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

If running on Docker, you can check the logs your deployment by running the following command:

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

* If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:

2. Apply your changes to the cluster.

### *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*

This error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the `users.xml` file from the github repo as well. This adds the `<access_management>` tag to the `users.xml` file, which allows the user to create row policies. Below is the default `users.xml` file that we expect to be used.

In some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the `users.xml` file included.

Example `Dockerfile`:

Then take the following steps:

1. Build your custom image.

2. Update your `docker-compose.yaml` to use the custom image. Make sure to remove the users.xml mount point.

3. Restart your instance of LangSmith.

### *ClickHouse fails to start up when running a cluster with AquaSec*

In some environments, AquaSec may prevent ClickHouse from starting up correctly. This may manifest as the ClickHouse pod not emitting any logs and failing to get marked as ready.
Generally this is due to `LD_PRELOAD` being set by AquaSec, which interferes with ClickHouse. To resolve this, you can add the following environment variable to your ClickHouse deployment:

Edit your `langsmith_config.yaml` (or corresponding config file) and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

Edit your `docker-compose.yaml` and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

#### Docker

If running on Docker, you can check the logs your deployment by running the following command:
```

Example 2 (unknown):
```unknown
#### Browser Errors

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

## Common issues

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

#### Kubernetes

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

   * If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

#### Docker

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

#### Kubernetes

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 3 (unknown):
```unknown
1. Rerun your upgrade/migrations.

#### Docker

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 4 (unknown):
```unknown
1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

#### Kubernetes

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:
```

---

## Embedding models

**URL:** llms-txt#embedding-models

**Contents:**
- Overview
  - How it works
  - Similarity metrics
- Interface
- Top integrations
- Caching

Source: https://docs.langchain.com/oss/python/integrations/text_embedding/index

<Note>
  This overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.
</Note>

Embedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its **semantic meaning**. These vectors allow machines to compare and search text based on meaning rather than exact words.

In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase *"machine learning"*, embeddings can surface documents that discuss related concepts even when different wording is used.

1. **Vectorization** — The model encodes each input string as a high-dimensional vector.
2. **Similarity scoring** — Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.

### Similarity metrics

Several metrics are commonly used to compare embeddings:

* **Cosine similarity** — measures the angle between two vectors.
* **Euclidean distance** — measures the straight-line distance between points.
* **Dot product** — measures how much one vector projects onto another.

Here's an example of computing cosine similarity between two vectors:

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) interface.

Two main methods are available:

* `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.
* `embed_query(text: str) → List[float]`: Embeds a single query.

<Note>
  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.
</Note>

| Provider                                                                      | Package                                                                                                                                                            |
| ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [OpenAI](/oss/python/integrations/text_embedding/openai)                      | [`langchain-openai`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                              |
| [OpenAI on Azure](/oss/python/integrations/text_embedding/azure_openai)       | [`langchain-openai`](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                    |
| [Google Gemini](/oss/python/integrations/text_embedding/google_generative_ai) | [`langchain-google-genai`](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html) |
| [Ollama](/oss/python/integrations/text_embedding/ollama)                      | [`langchain-ollama`](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                               |
| [Together](/oss/python/integrations/text_embedding/together)                  | [`langchain-together`](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                       |
| [Fireworks](/oss/python/integrations/text_embedding/fireworks)                | [`langchain-fireworks`](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                   |
| [MistralAI](/oss/python/integrations/text_embedding/mistralai)                | [`langchain-mistralai`](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                   |
| [Cohere](/oss/python/integrations/text_embedding/cohere)                      | [`langchain-cohere`](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.cohere.Cohere.html)                                        |
| [Nomic](/oss/python/integrations/text_embedding/nomic)                        | [`langchain-nomic`](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                   |
| [Fake](/oss/python/integrations/text_embedding/fake)                          | [`langchain-core`](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                  |
| [Databricks](/oss/python/integrations/text_embedding/databricks)              | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings)   |
| [IBM](/oss/python/integrations/text_embedding/ibm_watsonx)                    | [`langchain-ibm`](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                       |
| [NVIDIA](/oss/python/integrations/text_embedding/nvidia_ai_endpoints)         | [`langchain-nvidia`](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)     |
| [AI/ML API](/oss/python/integrations/text_embedding/aimlapi)                  | [`langchain-aimlapi`](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html)                           |

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.

The main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:

* **`underlying_embedder`**: The embedder to use for embedding.
* **`document_embedding_cache`**: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings.
* **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.
* **`namespace`**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).
* **`query_embedding_cache`**: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.

<Important>
  - Always set the `namespace` parameter to avoid collisions when using different embedding models.
  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_cache`.
</Important>

```python  theme={null}
import time
from langchain_classic.embeddings import CacheBackedEmbeddings  # [!code highlight]
from langchain_classic.storage import LocalFileStore # [!code highlight]
from langchain_core.vectorstores import InMemoryVectorStore

**Examples:**

Example 1 (unknown):
```unknown
## Interface

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) interface.

Two main methods are available:

* `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.
* `embed_query(text: str) → List[float]`: Embeds a single query.

<Note>
  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.
</Note>

## Top integrations

| Provider                                                                      | Package                                                                                                                                                            |
| ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [OpenAI](/oss/python/integrations/text_embedding/openai)                      | [`langchain-openai`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                              |
| [OpenAI on Azure](/oss/python/integrations/text_embedding/azure_openai)       | [`langchain-openai`](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                    |
| [Google Gemini](/oss/python/integrations/text_embedding/google_generative_ai) | [`langchain-google-genai`](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html) |
| [Ollama](/oss/python/integrations/text_embedding/ollama)                      | [`langchain-ollama`](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                               |
| [Together](/oss/python/integrations/text_embedding/together)                  | [`langchain-together`](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                       |
| [Fireworks](/oss/python/integrations/text_embedding/fireworks)                | [`langchain-fireworks`](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                   |
| [MistralAI](/oss/python/integrations/text_embedding/mistralai)                | [`langchain-mistralai`](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                   |
| [Cohere](/oss/python/integrations/text_embedding/cohere)                      | [`langchain-cohere`](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.cohere.Cohere.html)                                        |
| [Nomic](/oss/python/integrations/text_embedding/nomic)                        | [`langchain-nomic`](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                   |
| [Fake](/oss/python/integrations/text_embedding/fake)                          | [`langchain-core`](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                  |
| [Databricks](/oss/python/integrations/text_embedding/databricks)              | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings)   |
| [IBM](/oss/python/integrations/text_embedding/ibm_watsonx)                    | [`langchain-ibm`](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                       |
| [NVIDIA](/oss/python/integrations/text_embedding/nvidia_ai_endpoints)         | [`langchain-nvidia`](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)     |
| [AI/ML API](/oss/python/integrations/text_embedding/aimlapi)                  | [`langchain-aimlapi`](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html)                           |

## Caching

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.

The main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:

* **`underlying_embedder`**: The embedder to use for embedding.
* **`document_embedding_cache`**: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings.
* **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.
* **`namespace`**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).
* **`query_embedding_cache`**: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.

<Important>
  - Always set the `namespace` parameter to avoid collisions when using different embedding models.
  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_cache`.
</Important>
```

---

## Trace with API

**URL:** llms-txt#trace-with-api

**Contents:**
- Basic tracing

Source: https://docs.langchain.com/langsmith/trace-with-api

Learn how to trace your LLM applications using the LangSmith API directly.

It is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.

The simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to

<Note>
  When using the LangSmith REST API, you will need to provide your API key in the request headers as `"x-api-key"`.

If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `"x-tenant-id"`.

In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.
</Note>

The following example shows how you might leverage our API directly in Python. The same principles apply to other languages.

```python  theme={null}
import openai
import os
import requests
from datetime import datetime, timezone
from uuid import uuid4

---

## ✅ Correct

**URL:** llms-txt#✅-correct

**Contents:**
  - Agents must use correct paths
  - No automatic cleanup

agent = create_deep_agent(
    use_longterm_memory=True,
    store=InMemoryStore()
)
```

### Agents must use correct paths

The agent must learn to use the `/memories/` prefix for persistence. The system prompt teaches this, but the agent must follow the instructions.

### No automatic cleanup

Long-term files persist indefinitely. There's no built-in TTL or automatic cleanup. You'll need to implement cleanup strategies if needed.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/long-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Get the current tracer

**URL:** llms-txt#get-the-current-tracer

**Contents:**
  - Combining with other instrumentors

tracer = trace.get_tracer(__name__)

async def main():
    with tracer.start_as_current_span("semantic_kernel_workflow") as span:
        # Add custom metadata
        span.set_attribute("langsmith.metadata.workflow_type", "code_analysis")
        span.set_attribute("langsmith.metadata.user_id", "developer_123")
        span.set_attribute("langsmith.span.tags", "semantic-kernel,code-analysis")

# Your Semantic Kernel code here
        result = await kernel.invoke(code_analyzer, code=sample_code)
        return result
python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.dspy import DSPyInstrumentor

**Examples:**

Example 1 (unknown):
```unknown
### Combining with other instrumentors

You can combine Semantic Kernel instrumentation with other instrumentors (e.g., DSPy, AutoGen) by adding them and initializing them as instrumentors:
```

---

## Run a local server

**URL:** llms-txt#run-a-local-server

**Contents:**
- Prerequisites
- 1. Install the LangGraph CLI
- 2. Create a LangGraph app 🌱
- 3. Install dependencies
- 4. Create a `.env` file
- 5. Launch LangGraph Server 🚀
- 6. Test your application in Studio
- 7. Test the API
- Next steps

Source: https://docs.langchain.com/oss/python/langgraph/local-server

This guide shows you how to run a LangGraph application locally.

Before you begin, ensure you have the following:

* An API key for [LangSmith](https://smith.langchain.com/settings) - free to sign up

## 1. Install the LangGraph CLI

## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project). This template demonstrates a single-node application you can extend with your own logic.

<Tip>
  **Additional templates**
  If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

## 4. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

## 5. Launch LangGraph Server 🚀

Start the LangGraph API server locally:

The `langgraph dev` command starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see the [Hosting overview](/langsmith/hosting).

## 6. Test your application in Studio

[Studio](/langsmith/studio) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

For a LangGraph Server running on a custom host/port, update the baseURL parameter.

<Accordion title="Safari compatibility">
  Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

<Tabs>
  <Tab title="Python SDK (async)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Python SDK (sync)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

* [Deployment quickstart](/langsmith/deployment-quickstart): Deploy your LangGraph app using LangSmith.

* [LangSmith](/langsmith/home): Learn about foundational LangSmith concepts.

* [Python SDK Reference](https://reference.langchain.com/python/platform/python_sdk/): Explore the Python SDK API Reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/local-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project). This template demonstrates a single-node application you can extend with your own logic.
```

Example 3 (unknown):
```unknown
<Tip>
  **Additional templates**
  If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to evaluate with repetitions

**URL:** llms-txt#how-to-evaluate-with-repetitions

**Contents:**
- Configuring repetitions on an experiment
- Viewing results of experiments run with repetitions

Source: https://docs.langchain.com/langsmith/repetition

Running multiple repetitions can give a more accurate estimate of the performance of your system since LLM outputs are not deterministic. Outputs can differ from one repetition to the next. Repetitions are a way to reduce noise in systems prone to high variability, such as agents.

## Configuring repetitions on an experiment

Add the optional `num_repetitions` param to the `evaluate` / `aevaluate` function ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)) to specify how many times to evaluate over each example in your dataset. For instance, if you have 5 examples in the dataset and set `num_repetitions=5`, each example will be run 5 times, for a total of 25 runs.

## Viewing results of experiments run with repetitions

If you've run your experiment with [repetitions](/langsmith/evaluation-concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=60962de04e5533d7718ca60fa9c7dcce" alt="Repetitions" data-og-width="1636" width="1636" data-og-height="959" height="959" data-path="langsmith/images/repetitions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8be83801a53f2544883faf173bc16ef1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7a924559be193efcc2c77dba3fea1231 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=25cbd580d06bda48419b83401c268c2d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9da3908c81d1c8fd44dde6d3ec7dfe1d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=775af0be371e662bea7ba7e29c2f21fd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d593460688be852a64638f092cba9f3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/repetition.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Environment variables

**URL:** llms-txt#environment-variables

**Contents:**
- `BG_JOB_ISOLATED_LOOPS`
- `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`
- `BG_JOB_TIMEOUT_SECS`
- `DD_API_KEY`
- `LANGCHAIN_TRACING_SAMPLING_RATE`
- `LANGGRAPH_AUTH_TYPE`
- `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`
- `LANGSMITH_API_KEY`
- `LANGSMITH_ENDPOINT`
- `LANGSMITH_TRACING`

Source: https://docs.langchain.com/langsmith/env-var

The LangGraph Server supports specific environment variables for configuring a deployment.

## `BG_JOB_ISOLATED_LOOPS`

Set `BG_JOB_ISOLATED_LOOPS` to `True` to execute background runs in an isolated event loop separate from the serving API event loop.

This environment variable should be set to `True` if the implementation of a graph/node contains synchronous code. In this situation, the synchronous code will block the serving API event loop, which may cause the API to be unavailable. A symptom of an unavailable API is continuous application restarts due to failing health checks.

## `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`

Specifies, in seconds, how long the server will wait for background jobs to finish after the queue receives a shutdown signal. After this period, the server will force termination. Defaults to `180` seconds. Set this to ensure jobs have enough time to complete cleanly during shutdown. Added in `langgraph-api==0.2.16`.

## `BG_JOB_TIMEOUT_SECS`

The timeout of a background run can be increased. However, the infrastructure for a Cloud deployment enforces a 1 hour timeout limit for API requests. This means the connection between client and server will timeout after 1 hour. This is not configurable.

A background run can execute for longer than 1 hour, but a client must reconnect to the server (e.g. join stream via `POST /threads/{thread_id}/runs/{run_id}/stream`) to retrieve output from the run if the run is taking longer than 1 hour.

Specify `DD_API_KEY` (your [Datadog API Key](https://docs.datadoghq.com/account_management/api-app-keys/)) to automatically enable Datadog tracing for the deployment. Specify other [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) to configure the tracing instrumentation.

If `DD_API_KEY` is specified, the application process is wrapped in the [`ddtrace-run` command](https://ddtrace.readthedocs.io/en/stable/installation_quickstart.html). Other `DD_*` environment variables (e.g. `DD_SITE`, `DD_ENV`, `DD_SERVICE`, `DD_TRACE_ENABLED`) are typically needed to properly configure the tracing instrumentation. See [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) for more details. You can enable `DD_TRACE_DEBUG=true` and set `DD_LOG_LEVEL=debug` to troubleshoot.

<Note>
  Enabling `DD_API_KEY` (and thus `ddtrace-run`) can override or interfere with other auto-instrumentation solutions (such as OpenTelemetry) that you may have instrumented into your application code.
</Note>

## `LANGCHAIN_TRACING_SAMPLING_RATE`

Sampling rate for traces sent to LangSmith. Valid values: Any float between `0` and `1`.

For more details, refer to [Set a sampling rate for traces](/langsmith/sample-traces).

## `LANGGRAPH_AUTH_TYPE`

Type of authentication for the LangGraph Server deployment. Valid values: `langsmith`, `noop`.

For deployments to LangSmith, this environment variable is set automatically. For local development or deployments where authentication is handled externally (e.g. self-hosted), set this environment variable to `noop`.

## `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`

Beginning with langgraph-api version `0.2.12`, the maximum size of the Postgres connection pool (per replica) can be controlled using the `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Postgres database.

For example, if a deployment is scaled up to 10 replicas and `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` is configured to `150`, then up to `1500` connections to Postgres can be established. This is particularly useful for deployments where database resources are limited (or more available) or where you need to tune connection behavior for performance or scaling reasons.

Defaults to `150` connections.

## `LANGSMITH_API_KEY`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_API_KEY` to an API key created from the self-hosted instance.

## `LANGSMITH_ENDPOINT`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted instance.

## `LANGSMITH_TRACING`

Set `LANGSMITH_TRACING` to `false` to disable tracing to LangSmith.

This is mainly relevant in the context of using the dev server via the `langgraph dev` command. Set `LOG_COLOR` to `true` to enable ANSI-colored console output when using the default console renderer. Disabling color output by setting this variable to `false` produces monochrome logs. Defaults to `true`.

Configure [log level](https://docs.python.org/3/library/logging.html#logging-levels). Defaults to `INFO`.

Set `LOG_JSON` to `true` to render all log messages as JSON objects using the configured `JSONRenderer`. This produces structured logs that can be easily parsed or ingested by log management systems. Defaults to `false`.

<Info>
  **Only Allowed in Self-Hosted Deployments**
  The `MOUNT_PREFIX` environment variable is only allowed in Self-Hosted Deployment models, LangSmith SaaS will not allow this environment variable.
</Info>

Set `MOUNT_PREFIX` to serve the LangGraph Server under a specific path prefix. This is useful for deployments where the server is behind a reverse proxy or load balancer that requires a specific path prefix.

For example, if the server is to be served under `https://example.com/langgraph`, set `MOUNT_PREFIX` to `/langgraph`.

## `N_JOBS_PER_WORKER`

Number of jobs per worker for the LangGraph Server task queue. Defaults to `10`.

## `POSTGRES_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Postgres instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `POSTGRES_URI_CUSTOM` to use a custom Postgres instance. The value of `POSTGRES_URI_CUSTOM` must be a valid [Postgres connection URI](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING-URIS).

* Version 15.8 or higher.
* An initial database must be present and the connection URI must reference the database.

Control Plane Functionality:

* If `POSTGRES_URI_CUSTOM` is specified, the control plane will not provision a database for the server.
* If `POSTGRES_URI_CUSTOM` is removed, the control plane will not provision a database for the server and will not delete the externally managed Postgres instance.
* If `POSTGRES_URI_CUSTOM` is removed, deployment of the revision will not succeed. Once `POSTGRES_URI_CUSTOM` is specified, it must always be set for the lifecycle of the deployment.
* If the deployment is deleted, the control plane will not delete the externally managed Postgres instance.
* The value of `POSTGRES_URI_CUSTOM` can be updated. For example, a password in the URI can be updated.

Database Connectivity:

* The custom Postgres instance must be accessible by the LangGraph Server. The user is responsible for ensuring connectivity.

<Warning>
  This feature is in Alpha.
</Warning>

<Info>
  **Only Allowed in Self-Hosted Deployments**
  Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default.
</Info>

Set `REDIS_CLUSTER` to `True` to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment.

## `REDIS_KEY_PREFIX`

<Info>
  **Available in API Server version 0.1.9+**
  This environment variable is supported in API Server version 0.1.9 and above.
</Info>

Specify a prefix for Redis keys. This allows multiple LangGraph Server instances to share the same Redis instance by using different key prefixes.

## `REDIS_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `REDIS_URI_CUSTOM` to use a custom Redis instance. The value of `REDIS_URI_CUSTOM` must be a valid [Redis connection URI](https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url).

## `REDIS_MAX_CONNECTIONS`

The maximum size of the Redis connection pool (per replica) can be controlled using the `REDIS_MAX_CONNECTIONS` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Redis instance.

For example, if a deployment is scaled up to 10 replicas and `REDIS_MAX_CONNECTIONS` is configured to `150`, then up to `1500` connections to Redis can be established.

## `RESUMABLE_STREAM_TTL_SECONDS`

Time-to-live in seconds for resumable stream data in Redis.

When a run is created and the output is streamed, the stream can be configured to be resumable (e.g. `stream_resumable=True`). If a stream is resumable, output from the stream is temporarily stored in Redis. The TTL for this data can be configured by setting `RESUMABLE_STREAM_TTL_SECONDS`.

See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.RunsClient.stream) and [JS/TS](https://langchain-ai.github.io/langgraphjs/reference/classes/sdk_client.RunsClient.html#stream) SDKs for more details on how to implement resumable streams.

Defaults to `120` seconds.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/env-var.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Store without embedding (still retrievable, but not searchable)

**URL:** llms-txt#store-without-embedding-(still-retrievable,-but-not-searchable)

**Contents:**
  - Using in LangGraph

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {"system_info": "Last updated: 2024-01-01"},
    index=False
)
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

**Examples:**

Example 1 (unknown):
```unknown
### Using in LangGraph

With this all in place, we use the `in_memory_store` in LangGraph. The `in_memory_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in_memory_store` allows us to store arbitrary information for access *across* threads. We compile the graph with both the checkpointer and the `in_memory_store` as follows.
```

---

## Create a parent run

**URL:** llms-txt#create-a-parent-run

parent_run = construct_run(
    name="Parent Run",
    run_type="chain",
    inputs={"main_question": "Tell me about France"},
)

---

## Graph node for extracting user info and routing to lookup/refund/END.

**URL:** llms-txt#graph-node-for-extracting-user-info-and-routing-to-lookup/refund/end.

async def gather_info(state: State) -> Command[Literal["lookup", "refund", END]]:
    info = await info_llm.ainvoke(
        [
            {"role": "system", "content": gather_info_instructions},
            *state["messages"],
        ]
    )
    parsed = info["parsed"]
    if any(parsed[k] for k in ("invoice_id", "invoice_line_ids")):
        goto = "refund"
    elif all(
        parsed[k]
        for k in ("customer_first_name", "customer_last_name", "customer_phone")
    ):
        goto = "lookup"
    else:
        goto = END
    update = {"messages": [info["raw"]], **parsed}
    return Command(update=update, goto=goto)

---

## use in our SQL queries.

**URL:** llms-txt#use-in-our-sql-queries.

def index_fields() -> tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]: ...

track_store, artist_store, album_store = index_fields()

---

## Use environment variables for model providers

**URL:** llms-txt#use-environment-variables-for-model-providers

**Contents:**
- Requirements
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-playground-environment-settings

<Note>
  This feature is only available on Helm chart versions 0.10.27 (application version 0.10.74) and later.
</Note>

Many model providers support setting credentials and other configuration options through environment variables. This is useful for self-hosted deployments where you want to avoid hardcoding sensitive information in your code or configuration files. In LangSmith, most model interactions are done through the `playground` service, which allows you to configure many of those environment variables directly on the pod itself. This can be useful to avoid having to set credentials in the UI.

* A self-hosted LangSmith instance with the `playground` service running.
* The provider you want to configure must support environment variables for configuration. Check the provider's Chat Model [documentation](https://python.langchain.com/docs/integrations/providers/) for more information.
* The secrets/roles you may want to attach to the `playground` service.
  * Note that for IRSA you may need to grant the `langsmith-playground` service account the necessary permissions to access the secrets or roles in your cloud provider.

With the parameters from above, you can configure your LangSmith instance to use environment variables for model providers. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation or the `docker-compose.yaml` file for your Docker installation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-playground-environment-settings.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## The

**URL:** llms-txt#the

---

## How to integrate LangGraph with AutoGen, CrewAI, and other frameworks

**URL:** llms-txt#how-to-integrate-langgraph-with-autogen,-crewai,-and-other-frameworks

**Contents:**
- Prerequisites
- Setup
- 1. Define AutoGen agent
- 2. Create the graph

Source: https://docs.langchain.com/langsmith/autogen-integration

This guide shows how to integrate AutoGen agents with LangGraph to leverage features like persistence, streaming, and memory, and then deploy the integrated solution to LangSmith for scalable production use. In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.

Integrating AutoGen with LangGraph provides several benefits:

* Enhanced features: Add [persistence](/oss/python/langgraph/persistence), [streaming](/langsmith/streaming), [short and long-term memory](/oss/python/concepts/memory) and more to your AutoGen agents.
* Multi-agent systems: Build [multi-agent systems](/oss/python/langchain/multi-agent) where individual agents are built with different frameworks.
* Production deployment: Deploy your integrated solution to [LangSmith](/langsmith/home) for scalable production use.

* Python 3.9+
* Autogen: `pip install autogen`
* LangGraph: `pip install langgraph`
* OpenAI API key

Set your your environment:

## 1. Define AutoGen agent

Create an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):

## 2. Create the graph

We will now create a LangGraph chatbot graph that calls AutoGen agent.

```python  theme={null}
from langchain_core.messages import convert_to_openai_messages
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver

def call_autogen_agent(state: MessagesState):
    # Convert LangGraph messages to OpenAI format for AutoGen
    messages = convert_to_openai_messages(state["messages"])

# Get the last user message
    last_message = messages[-1]

# Pass previous message history as context (excluding the last message)
    carryover = messages[:-1] if len(messages) > 1 else []

# Initiate chat with AutoGen
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=last_message,
        carryover=carryover
    )

# Extract the final response from the agent
    final_content = response.chat_history[-1]["content"]

# Return the response in LangGraph format
    return {"messages": {"role": "assistant", "content": final_content}}

**Examples:**

Example 1 (unknown):
```unknown
## 1. Define AutoGen agent

Create an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):
```

Example 2 (unknown):
```unknown
## 2. Create the graph

We will now create a LangGraph chatbot graph that calls AutoGen agent.
```

---

## AWS (Amazon)

**URL:** llms-txt#aws-(amazon)

**Contents:**
- Chat models
  - Bedrock Chat
  - Bedrock Converse
- LLMs
  - Bedrock
  - Amazon API Gateway
  - SageMaker Endpoint
- Embedding Models
  - Bedrock
  - SageMaker Endpoint

Source: https://docs.langchain.com/oss/python/integrations/providers/aws

All LangChain integrations with the [Amazon AWS](https://aws.amazon.com/) platform.

> [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of
> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`,
> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to
> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`,
> you can easily experiment with and evaluate top FMs for your use case, privately customize them with
> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build
> agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is
> serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy
> generative AI capabilities into your applications using the AWS services you are already familiar with.

See a [usage example](/oss/python/integrations/chat/bedrock).

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).

See a [usage example](/oss/python/integrations/llms/bedrock).

### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).

### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).

See a [usage example](/oss/python/integrations/text_embedding/bedrock).

### SageMaker Endpoint

See a [usage example](/oss/python/integrations/text_embedding/sagemaker-endpoint).

### AWS S3 Directory and File

> [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> is an object storage service.
> [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> [AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)

See a [usage example for S3DirectoryLoader](/oss/python/integrations/document_loaders/aws_s3_directory).

See a [usage example for S3FileLoader](/oss/python/integrations/document_loaders/aws_s3_file).

> [Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine
> learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.

See a [usage example](/oss/python/integrations/document_loaders/amazon_textract).

> [Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built
> on open-source frameworks, supporting open-table and file formats.

See a [usage example](/oss/python/integrations/document_loaders/athena).

> The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata
> repository that allows you to manage, access, and share metadata about
> your data stored in AWS. It acts as a metadata store for your data assets,
> enabling various AWS services and your applications to query and connect
> to the data they need efficiently.

See a [usage example](/oss/python/integrations/document_loaders/glue_catalog).

### Amazon OpenSearch Service

> [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs
> interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is
> an open source,
> distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the
> latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as
> visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service).

### Amazon DocumentDB Vector Search

> [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.
> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.
> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.

#### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/documentdb).

We need to install the `pymongo` python package.

#### Deploy DocumentDB on AWS

[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.

AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/).

See a [usage example](/oss/python/integrations/vectorstores/documentdb).

[Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,
enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.

InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.

See a [usage example](/oss/python/integrations/vectorstores/memorydb).

> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service
> provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine
> learning algorithms to enable powerful search capabilities across various data sources within an organization.
> `Kendra` is designed to help users find the information they need quickly and accurately,
> improving productivity and decision-making.

> With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases,
> manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and
> contextual meanings to provide highly relevant search results.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/amazon_kendra_retriever).

### Amazon Bedrock (Knowledge Bases)

> [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an
> `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your
> private data to customize foundation model response.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/bedrock).

> [`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by
> `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without
> provisioning or managing servers. This serverless architecture enables you to focus on writing and
> deploying code, while AWS automatically takes care of scaling, patching, and managing the
> infrastructure required to run your applications.

We need to install `boto3` python library.

See a [usage example](/oss/python/integrations/tools/awslambda).

> [Amazon Neptune](https://aws.amazon.com/neptune/)
> is a high-performance graph analytics and serverless database for superior scalability and availability.

For the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.

### Amazon Neptune with Cypher

See a [usage example](/oss/python/integrations/graphs/amazon_neptune_open_cypher).

### Amazon Neptune with SPARQL

### Bedrock token usage

### SageMaker Tracking

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly
> and easily build, train and deploy machine learning (ML) models.

> [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability
> of `Amazon SageMaker` that lets you organize, track,
> compare and evaluate ML experiments and model versions.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/callbacks/sagemaker_tracking).

### Amazon Comprehend Moderation Chain

> [Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that
> uses machine learning to uncover valuable insights and connections in text.

We need to install the `boto3` and `nltk` libraries.

See a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon_comprehend_chain/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/aws.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Bedrock Converse

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).
```

Example 2 (unknown):
```unknown
## LLMs

### Bedrock

See a [usage example](/oss/python/integrations/llms/bedrock).
```

Example 3 (unknown):
```unknown
### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).
```

Example 4 (unknown):
```unknown
### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).
```

---

## Create store with semantic search enabled

**URL:** llms-txt#create-store-with-semantic-search-enabled

**Contents:**
- Manage short-term memory
  - Trim messages
  - Delete messages
  - Summarize messages
  - Manage checkpoints
- Prebuilt memory tools

embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)
python  theme={null}

from langchain.embeddings import init_embeddings
  from langchain.chat_models import init_chat_model
  from langgraph.store.base import BaseStore
  from langgraph.store.memory import InMemoryStore
  from langgraph.graph import START, MessagesState, StateGraph

model = init_chat_model("openai:gpt-4o-mini")

# Create store with semantic search enabled
  embeddings = init_embeddings("openai:text-embedding-3-small")
  store = InMemoryStore(
      index={
          "embed": embeddings,
          "dims": 1536,
      }
  )

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
  store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

def chat(state, *, store: BaseStore):
      # Search based on user's last message
      items = store.search(
          ("user_123", "memories"), query=state["messages"][-1].content, limit=2
      )
      memories = "\n".join(item.value["text"] for item in items)
      memories = f"## Memories of user\n{memories}" if memories else ""
      response = model.invoke(
          [
              {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
              *state["messages"],
          ]
      )
      return {"messages": [response]}

builder = StateGraph(MessagesState)
  builder.add_node(chat)
  builder.add_edge(START, "chat")
  graph = builder.compile(store=store)

for message, metadata in graph.stream(
      input={"messages": [{"role": "user", "content": "I'm hungry"}]},
      stream_mode="messages",
  ):
      print(message.content, end="")
  python  theme={null}
from langchain_core.messages.utils import (  # [!code highlight]
    trim_messages,  # [!code highlight]
    count_tokens_approximately  # [!code highlight]
)  # [!code highlight]

def call_model(state: MessagesState):
    messages = trim_messages(  # [!code highlight]
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...
python  theme={null}
  from langchain_core.messages.utils import (
      trim_messages,  # [!code highlight]
      count_tokens_approximately  # [!code highlight]
  )
  from langchain.chat_models import init_chat_model
  from langgraph.graph import StateGraph, START, MessagesState

model = init_chat_model("anthropic:claude-sonnet-4-5")
  summarization_model = model.bind(max_tokens=128)

def call_model(state: MessagesState):
      messages = trim_messages(  # [!code highlight]
          state["messages"],
          strategy="last",
          token_counter=count_tokens_approximately,
          max_tokens=128,
          start_on="human",
          end_on=("human", "tool"),
      )
      response = model.invoke(messages)
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(MessagesState)
  builder.add_node(call_model)
  builder.add_edge(START, "call_model")
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  
  ================================== Ai Message ==================================

Your name is Bob, as you mentioned when you first introduced yourself.
  python  theme={null}
from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]
python  theme={null}
from langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]
python  theme={null}
  from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
      messages = state["messages"]
      if len(messages) > 2:
          # remove the earliest two messages
          return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": response}

builder = StateGraph(MessagesState)
  builder.add_sequence([call_model, delete_messages])
  builder.add_edge(START, "call_model")

checkpointer = InMemorySaver()
  app = builder.compile(checkpointer=checkpointer)

for event in app.stream(
      {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])

for event in app.stream(
      {"messages": [{"role": "user", "content": "what's my name?"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])
  
  [('human', "hi! I'm bob")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  [('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  python  theme={null}
from langgraph.graph import MessagesState
class State(MessagesState):
    summary: str
python  theme={null}
def summarize_conversation(state: State):

# First, we get any existing summary
    summary = state.get("summary", "")

# Create our summarization prompt
    if summary:

# A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )

else:
        summary_message = "Create a summary of the conversation above:"

# Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

# Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
python  theme={null}
  from typing import Any, TypedDict

from langchain.chat_models import init_chat_model
  from langchain.messages import AnyMessage
  from langchain_core.messages.utils import count_tokens_approximately
  from langgraph.graph import StateGraph, START, MessagesState
  from langgraph.checkpoint.memory import InMemorySaver
  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]

model = init_chat_model("anthropic:claude-sonnet-4-5")
  summarization_model = model.bind(max_tokens=128)

class State(MessagesState):
      context: dict[str, RunningSummary]  # [!code highlight]

class LLMInputState(TypedDict):  # [!code highlight]
      summarized_messages: list[AnyMessage]
      context: dict[str, RunningSummary]

summarization_node = SummarizationNode(  # [!code highlight]
      token_counter=count_tokens_approximately,
      model=summarization_model,
      max_tokens=256,
      max_tokens_before_summary=256,
      max_summary_tokens=128,
  )

def call_model(state: LLMInputState):  # [!code highlight]
      response = model.invoke(state["summarized_messages"])
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(State)
  builder.add_node(call_model)
  builder.add_node("summarize", summarization_node)  # [!code highlight]
  builder.add_edge(START, "summarize")
  builder.add_edge("summarize", "call_model")
  graph = builder.compile(checkpointer=checkpointer)

# Invoke the graph
  config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  print("\nSummary:", final_response["context"]["running_summary"].summary)
  
  ================================== Ai Message ==================================

From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.

Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled "The Mystery of Cats" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote "The Joy of Dogs," which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.
  python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    graph.get_state(config)  # [!code highlight]
    
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    checkpointer.get_tuple(config)  # [!code highlight]
    
    CheckpointTuple(
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        checkpoint={
            'v': 3,
            'ts': '2025-05-05T16:01:24.680462+00:00',
            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
            'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        },
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        pending_writes=[]
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(graph.get_state_history(config))  # [!code highlight]
    
    [
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            next=(),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:24.680462+00:00',
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
            next=('call_model',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863421+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=('__start__',),
            config={...},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863173+00:00',
            parent_config={...}
            tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=(),
            config={...},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.862295+00:00',
            parent_config={...}
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob")]},
            next=('call_model',),
            config={...},
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.278960+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': []},
            next=('__start__',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.277497+00:00',
            parent_config=None,
            tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
            interrupts=()
        )
    ]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(checkpointer.list(config))  # [!code highlight]
    
    [
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:24.680462+00:00',
                'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863421+00:00',
                'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863173+00:00',
                'id': '1f029ca3-1790-616e-8002-9e021694a0cd',
                'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}, 'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': "what's my name?"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.862295+00:00',
                'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.278960+00:00',
                'id': '1f029ca3-0874-6612-8000-339f2abc83b1',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.277497+00:00',
                'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',
                'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},
                'versions_seen': {'__input__': {}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            parent_config=None,
            pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': "hi! I'm bob"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]
        )
    ]
    python  theme={null}
thread_id = "1"
checkpointer.delete_thread(thread_id)
```

## Prebuilt memory tools

**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Long-term memory with semantic search">
```

Example 2 (unknown):
```unknown
</Accordion>

## Manage short-term memory

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) from LangGraph state permanently
* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history
* Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.

To trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:
```

Example 3 (unknown):
```unknown
<Accordion title="Full example: trim messages">
```

Example 4 (unknown):
```unknown

```

---

## Compile

**URL:** llms-txt#compile

**Contents:**
  - 1. Run the graph

checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)
graph
python  theme={null}
config = {
    "configurable": {
        "thread_id": uuid.uuid4(),
    }
}
state = graph.invoke({}, config)

print(state["topic"])
print()
print(state["joke"])

How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

**Examples:**

Example 1 (unknown):
```unknown
### 1. Run the graph
```

Example 2 (unknown):
```unknown
**Output:**
```

---

## Graph node for executing the refund.

**URL:** llms-txt#graph-node-for-executing-the-refund.

---

## The metadata contains information about the LLM invocation, including the tags

**URL:** llms-txt#the-metadata-contains-information-about-the-llm-invocation,-including-the-tags

async for msg, metadata in graph.astream(
    {"topic": "cats"},
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the tags field in the metadata to only include
    # the tokens from the LLM invocation with the "joke" tag
    if metadata["tags"] == ["joke"]:
        print(msg.content, end="|", flush=True)
python  theme={null}
  from typing import TypedDict

from langchain.chat_models import init_chat_model
  from langgraph.graph import START, StateGraph

# The joke_model is tagged with "joke"
  joke_model = init_chat_model(model="openai:gpt-4o-mini", tags=["joke"])
  # The poem_model is tagged with "poem"
  poem_model = init_chat_model(model="openai:gpt-4o-mini", tags=["poem"])

class State(TypedDict):
        topic: str
        joke: str
        poem: str

async def call_model(state, config):
        topic = state["topic"]
        print("Writing joke...")
        # Note: Passing the config through explicitly is required for python < 3.11
        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
        # The config is passed through explicitly to ensure the context vars are propagated correctly
        # This is required for Python < 3.11 when using async code. Please see the async section for more details
        joke_response = await joke_model.ainvoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}],
              config,
        )
        print("\n\nWriting poem...")
        poem_response = await poem_model.ainvoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}],
              config,
        )
        return {"joke": joke_response.content, "poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(call_model)
        .add_edge(START, "call_model")
        .compile()
  )

# The stream_mode is set to "messages" to stream LLM tokens
  # The metadata contains information about the LLM invocation, including the tags
  async for msg, metadata in graph.astream(
        {"topic": "cats"},
        stream_mode="messages",
  ):
      if metadata["tags"] == ["joke"]:
          print(msg.content, end="|", flush=True)
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: filtering by tags">
```

Example 2 (unknown):
```unknown
</Accordion>

#### Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:
```

---

## Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

resp = requests.post(
    "https://api.smith.langchain.com/api/v1/datasets/comparative",
    json={
        "experiment_ids": experiment_ids,
        "name": "Toxicity detection - API Example - Comparative - " + str(uuid4())[0:8],
        "description": "An optional description for the comparative experiment",
        "extra": {
            "metadata": {"foo": "bar"},  # Optional metadata
        },
        "reference_dataset_id": str(dataset_id),
    },
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()
comparative_experiment_id = comparative_experiment["id"]

---

## Node that updates instructions

**URL:** llms-txt#node-that-updates-instructions

**Contents:**
  - Writing memories
  - Memory storage

def update_instructions(state: State, store: BaseStore):
    namespace = ("instructions",)
    instructions = store.search(namespace)[0]
    # Memory logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
    output = llm.invoke(prompt)
    new_instructions = output['new_instructions']
    store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
    ...
python  theme={null}
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=13644c954ed79a45b8a1a762b3e39da1" alt="" data-og-width="493" width="493" data-og-height="515" height="515" data-path="oss/images/update-instructions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=90632c71febee5777be6ae2c338f0880 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=aefcc771a030a2d6a89f815b87e60fd4 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=9115490b76daffe987e3867bc9176386 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0df26f2e6f669f2fbea59a9a49482fb4 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=132e7b1f377e0b57c03ab31c6d788df4 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=651cd1bb14e445a972a671a196b6a893 2500w" />

### Writing memories

There are two primary methods for agents to write memories: ["in the hot path"](#in-the-hot-path) and ["in the background"](#in-the-background).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=edd006d6189dc29a2edcba57c41fd744" alt="" data-og-width="842" width="842" data-og-height="418" height="418" data-path="oss/images/hot_path_vs_background.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3efd9962012347a64b596d1d36925b33 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=add54b5469d7b4a8f22d7da250c19ddf 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1e763d0f2ee8aa4f5b302ad44bc19d2f 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ce84a68af250e53a4693332d39179136 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5f807accb9c63dae57c27c9a1d17f29a 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ecef974859d58f691dbb22a4a1cc1572 2500w" />

#### In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.

However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.

As an example, ChatGPT uses a [save\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.

#### In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.

However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.

See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.

### Memory storage

LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.
```

---

## Get email from command line

**URL:** llms-txt#get-email-from-command-line

email = getpass("Enter your email: ")
base_email = email.split("@")
password = "secure-password"  # CHANGEME
email1 = f"{base_email[0]}+1@{base_email[1]}"
email2 = f"{base_email[0]}+2@{base_email[1]}"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
if not SUPABASE_URL:
    SUPABASE_URL = getpass("Enter your Supabase project URL: ")

---

## {"type": "text", "text": "Here's a picture of a cat"},

**URL:** llms-txt#{"type":-"text",-"text":-"here's-a-picture-of-a-cat"},

---

## Anthropic (Claude)

**URL:** llms-txt#anthropic-(claude)

**Contents:**
- Model interfaces

Source: https://docs.langchain.com/oss/python/integrations/providers/anthropic

All LangChain integrations with [Anthropic](https://www.anthropic.com/).

<Columns cols={2}>
  <Card title="ChatAnthropic" href="/oss/python/integrations/chat/anthropic" cta="Get started" icon="message" arrow>
    Anthropic chat models.
  </Card>

<Card title="AnthropicLLM" href="/oss/python/integrations/llms/anthropic" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Anthropic text completion models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/anthropic.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Edges taken after the `action` node is called.

**URL:** llms-txt#edges-taken-after-the-`action`-node-is-called.

workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

---

## Must set dangerously_allow_filesystem to True if you want to use file paths

**URL:** llms-txt#must-set-dangerously_allow_filesystem-to-true-if-you-want-to-use-file-paths

@traceable(dangerously_allow_filesystem=True)
def trace_with_attachments(
    val: int,
    text: str,
    image: Attachment,
    audio: Attachment,
    video: Attachment,
    pdf: Attachment,
    csv: Attachment,
):
    return f"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data), {len(csv.data)}}"

---

## Example output:

**URL:** llms-txt#example-output:

---

## LangSmith JS/TS SDK

**URL:** llms-txt#langsmith-js/ts-sdk

Source: https://docs.langchain.com/langsmith/smith-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-js-ts-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Resume with the human's response

**URL:** llms-txt#resume-with-the-human's-response

---

## is a distance metric that varies inversely with similarity.

**URL:** llms-txt#is-a-distance-metric-that-varies-inversely-with-similarity.

**Contents:**
- 4. Retrievers
- Next steps

results = vector_store.similarity_search_with_score("What was Nike's revenue in 2023?")
doc, score = results[0]
print(f"Score: {score}\n")
print(doc)
output  theme={null}
Score: 0.23699893057346344

page_content='Table of Contents
FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS
The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:
FISCAL 2023 COMPARED TO FISCAL 2022
•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.
The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,
2 and 1 percentage points to NIKE, Inc. Revenues, respectively.
•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This
increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale
equivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
embedding = embeddings.embed_query("How were Nike's margins impacted in 2023?")

results = vector_store.similarity_search_by_vector(embedding)
print(results[0])
output  theme={null}
page_content='Table of Contents
GROSS MARGIN
FISCAL 2023 COMPARED TO FISCAL 2022
For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to
43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:
*Wholesale equivalent
The decrease in gross margin for fiscal 2023 was primarily due to:
•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as
product mix;
•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in
the prior period resulting from lower available inventory supply;
•Unfavorable changes in net foreign currency exchange rates, including hedges; and
•Lower off-price margin, on a wholesale equivalent basis.
This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import chain

@chain
def retriever(query: str) -> List[Document]:
    return vector_store.similarity_search(query, k=1)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
output  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
python  theme={null}
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 1},
)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
output  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
```

`VectorStoreRetriever` supports search types of `"similarity"` (default), `"mmr"` (maximum marginal relevance, described above), and `"similarity_score_threshold"`. We can use the latter to threshold documents output by the retriever by similarity score.

Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/python/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/python/langchain/rag) tutorial.

You've now seen how to build a semantic search engine over a PDF document.

For more on document loaders:

* [Overview](/oss/python/langchain/retrieval#document_loaders)
* [Available integrations](/oss/python/integrations/document_loaders/)

For more on embeddings:

* [Overview](/oss/python/langchain/retrieval#embedding_models/)
* [Available integrations](/oss/python/integrations/text_embedding/)

For more on vector stores:

* [Overview](/oss/python/langchain/retrieval#vectorstores/)
* [Available integrations](/oss/python/integrations/vectorstores/)

For more on RAG, see:

* [Build a Retrieval Augmented Generation (RAG) App](/oss/python/langchain/rag/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Return documents based on similarity to an embedded query:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
Learn more:

* [API Reference](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore)
* [Integration-specific docs](/oss/python/integrations/vectorstores)

## 4. Retrievers

LangChain [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects do not subclass @\[Runnable]. LangChain @\[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).

We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:
```

---

## OpenAI

**URL:** llms-txt#openai

**Contents:**
- Model interfaces
- Tools and toolkits
- Retrievers
- Document loaders
- Other

Source: https://docs.langchain.com/oss/python/integrations/providers/openai

All LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI)

<Columns cols={2}>
  <Card title="ChatOpenAI" href="/oss/python/integrations/chat/openai" cta="Get started" icon="message" arrow>
    OpenAI chat models.
  </Card>

<Card title="AzureChatOpenAI" href="/oss/python/integrations/chat/azure_chat_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI chat models hosted on Azure.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/llms/openai" cta="Get started" icon="i-cursor" arrow>
    (Legacy) OpenAI text completion models.
  </Card>

<Card title="AzureOpenAI" href="/oss/python/integrations/llms/azure_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for (legacy) OpenAI text completion models hosted on Azure.
  </Card>

<Card title="OpenAIEmbeddings" href="/oss/python/integrations/text_embedding/openai" cta="Get started" icon="layer-group" arrow>
    OpenAI embedding models.
  </Card>

<Card title="AzureOpenAIEmbeddings" href="/oss/python/integrations/text_embedding/azure_openai.mdx" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI embedding models hosted on Azure.
  </Card>
</Columns>

## Tools and toolkits

<Columns cols={2}>
  <Card title="Dall-E Image Generator" href="/oss/python/integrations/tools/dalle_image_generator" cta="Get started" icon="image" arrow>
    Text-to-image generation using OpenAI's Dall-E models.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTPluginRetriever" href="/oss/python/integrations/retrievers/chatgpt-plugin" cta="Get started" icon="download" arrow>
    Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTLoader" href="/oss/python/integrations/document_loaders/chatgpt_loader" cta="Get started" icon="file" arrow>
    Load `conversations.json` from your ChatGPT data export folder.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Adapter" href="/oss/python/integrations/adapters/openai" cta="Get started" icon="arrows-left-right" arrow>
    Adapt LangChain models to OpenAI APIs.
  </Card>

<Card title="OpenAIModerationChain" href="https://python.langchain.com/v0.1/docs/guides/productionization/safety/moderation" cta="Get started" icon="link" arrow>
    Detect text that could be hateful, violent, etc.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangGraph Server API reference for LangSmith Deployment

**URL:** llms-txt#langgraph-server-api-reference-for-langsmith-deployment

**Contents:**
- Authentication

Source: https://docs.langchain.com/langsmith/server-api-ref

The LangGraph Server API reference is available within each [deployment](/langsmith/deployments) at the `/docs` endpoint (e.g. `http://localhost:8124/docs`).

<Card title="API Reference" href="https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html" icon="book">
  View the full LangGraph Server API reference documentation
</Card>

For deployments to LangSmith, authentication is required. Pass the `X-Api-Key` header with each request to the LangGraph Server. The value of the header should be set to a valid LangSmith API key for the organization where the LangGraph Server is deployed.

Example `curl` command:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-api-ref.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Here is the user info for user with ID "abc123":

**URL:** llms-txt#here-is-the-user-info-for-user-with-id-"abc123":

---

## Note: Original doc listed VertexPairWiseStringEvaluator twice. Assuming this class exists.

**URL:** llms-txt#note:-original-doc-listed-vertexpairwisestringevaluator-twice.-assuming-this-class-exists.

**Contents:**
- Other Google Products
  - Document loaders
  - Vector Stores
  - Retrievers
  - Tools

from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator # Verify class name if needed
bash pip theme={null}
  pip install langchain-google-community[drive]
  bash uv theme={null}
  uv add langchain-google-community[drive]
  python  theme={null}
from langchain_google_community import GoogleDriveLoader
bash pip theme={null}
  pip install scann langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add scann langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.vectorstores import ScaNN
bash pip theme={null}
  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  bash uv theme={null}
  uv add google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  python  theme={null}
from langchain_googledrive.retrievers import GoogleDriveRetriever
bash pip theme={null}
  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  bash uv theme={null}
  uv add google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  python  theme={null}
from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper
from langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool
bash pip theme={null}
  pip install google-search-results langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add google-search-results langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.tools.google_finance import GoogleFinanceQueryRun
from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper
bash pip theme={null}
  pip install google-search-results langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add google-search-results langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.tools.google_jobs import GoogleJobsQueryRun

**Examples:**

Example 1 (unknown):
```unknown
## Other Google Products

Integrations with various Google services beyond the core Cloud Platform.

### Document loaders

#### Google Drive

> [Google Drive](https://en.wikipedia.org/wiki/Google_Drive) file storage. Currently supports Google Docs.

Install with Drive dependencies:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See [usage example and authorization instructions](/oss/python/integrations/document_loaders/google_drive).
```

Example 4 (unknown):
```unknown
### Vector Stores

#### ScaNN (Local Index)

> [Google ScaNN](https://github.com/google-research/google-research/tree/master/scann)
> (Scalable Nearest Neighbors) is a python package.
>
> `ScaNN` is a method for efficient vector similarity search at scale.

> `ScaNN` includes search space pruning and quantization for Maximum Inner
> Product Search and also supports other distance functions such as
> Euclidean distance. The implementation is optimized for x86 processors
> with AVX2 support. See its [Google Research github](https://github.com/google-research/google-research/tree/master/scann)
> for more details.

Install the `scann` package:

<CodeGroup>
```

---

## Create an AI message manually (e.g., for conversation history)

**URL:** llms-txt#create-an-ai-message-manually-(e.g.,-for-conversation-history)

ai_msg = AIMessage("I'd be happy to help you with that question!")

---

## This is our toy user database. Do not do this in production

**URL:** llms-txt#this-is-our-toy-user-database.-do-not-do-this-in-production

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

---

## Connect to an external ClickHouse database

**URL:** llms-txt#connect-to-an-external-clickhouse-database

**Contents:**
- Requirements
- HA Replicated Clickhouse Cluster
- LangSmith-managed ClickHouse
- Parameters
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-clickhouse

ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries.

LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application.

However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling. By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database. While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways:

* [LangSmith-managed ClickHouse](/langsmith/langsmith-managed-clickhouse)

* Provision a [ClickHouse Cloud](https://clickhouse.cloud/) either directly or through a cloud provider marketplace:

* [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud?tab=Overview)
  * [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud)
  * [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc)

* On a VM in your cloud provider

<Note>
  Using the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).

Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
</Note>

* A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).
* A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.
* We support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.
* We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later.
* We rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:

<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* Host: The hostname or IP address of the ClickHouse database
* HTTP Port: The port that the ClickHouse database listens on for HTTP connections
* Native Port: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* Database: The name of the ClickHouse database that LangSmith should use
* Username: The username to use to connect to the ClickHouse database
* Password: The password to use to connect to the ClickHouse database
* Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

* Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

* Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

* When using a clustered deployment, LangSmith will automatically:

* Run database migrations across all nodes in the cluster

* Configure tables for data replication across the cluster

Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-clickhouse.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

## Parameters

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* Host: The hostname or IP address of the ClickHouse database
* HTTP Port: The port that the ClickHouse database listens on for HTTP connections
* Native Port: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* Database: The name of the ClickHouse database that LangSmith should use
* Username: The username to use to connect to the ClickHouse database
* Password: The password to use to connect to the ClickHouse database
* Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

  * Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

  * Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

  * When using a clustered deployment, LangSmith will automatically:

  * Run database migrations across all nodes in the cluster

  * Configure tables for data replication across the cluster

  Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

## Configuration

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Run the graph until the interrupt is hit.

**URL:** llms-txt#run-the-graph-until-the-interrupt-is-hit.

result = agent.invoke(
    {
        "messages": [
            {
                "role": "user",
                "content": "Delete old records from the database",
            }
        ]
    },
    config=config # [!code highlight]
)

---

## >             }

**URL:** llms-txt#>-------------}

---

## Invoke

**URL:** llms-txt#invoke

**Contents:**
- Evaluator-optimizer
- Agents

state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})

from IPython.display import Markdown
Markdown(state["final_report"])
python Graph API theme={null}
  # Graph state
  class State(TypedDict):
      joke: str
      topic: str
      feedback: str
      funny_or_not: str

# Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  def llm_call_generator(state: State):
      """LLM generates a joke"""

if state.get("feedback"):
          msg = llm.invoke(
              f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {state['topic']}")
      return {"joke": msg.content}

def llm_call_evaluator(state: State):
      """LLM evaluates the joke"""

grade = evaluator.invoke(f"Grade the joke {state['joke']}")
      return {"funny_or_not": grade.grade, "feedback": grade.feedback}

# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator
  def route_joke(state: State):
      """Route back to joke generator or end based upon feedback from the evaluator"""

if state["funny_or_not"] == "funny":
          return "Accepted"
      elif state["funny_or_not"] == "not funny":
          return "Rejected + Feedback"

# Build workflow
  optimizer_builder = StateGraph(State)

# Add the nodes
  optimizer_builder.add_node("llm_call_generator", llm_call_generator)
  optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

# Add edges to connect nodes
  optimizer_builder.add_edge(START, "llm_call_generator")
  optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
  optimizer_builder.add_conditional_edges(
      "llm_call_evaluator",
      route_joke,
      {  # Name returned by route_joke : Name of next node to visit
          "Accepted": END,
          "Rejected + Feedback": "llm_call_generator",
      },
  )

# Compile the workflow
  optimizer_workflow = optimizer_builder.compile()

# Show the workflow
  display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = optimizer_workflow.invoke({"topic": "Cats"})
  print(state["joke"])
  python Functional API theme={null}
  # Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  @task
  def llm_call_generator(topic: str, feedback: Feedback):
      """LLM generates a joke"""
      if feedback:
          msg = llm.invoke(
              f"Write a joke about {topic} but take into account the feedback: {feedback}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {topic}")
      return msg.content

@task
  def llm_call_evaluator(joke: str):
      """LLM evaluates the joke"""
      feedback = evaluator.invoke(f"Grade the joke {joke}")
      return feedback

@entrypoint()
  def optimizer_workflow(topic: str):
      feedback = None
      while True:
          joke = llm_call_generator(topic, feedback).result()
          feedback = llm_call_evaluator(joke).result()
          if feedback.grade == "funny":
              break

# Invoke
  for step in optimizer_workflow.stream("Cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Using tools theme={null}
from langchain.tools import tool

**Examples:**

Example 1 (unknown):
```unknown
## Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.

Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9bd0474f42b6040b14ed6968a9ab4e3c" alt="evaluator_optimizer.png" data-og-width="1004" width="1004" data-og-height="340" height="340" data-path="oss/images/evaluator_optimizer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ab36856e5f9a518b22e71278aa8b1711 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ec597c92270278c2bac203d36b611c2 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ad3bfb734a0e509d9b87fdb4e808bfd 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e82bd25a463d3cdf76036649c03358a9 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d31717ae3e76243dd975a53f46e8c1f6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a9bb4fb1583f6ad06c0b13602cd14811 2500w" />

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Agents

Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38" alt="agent.png" data-og-width="1732" width="1732" data-og-height="712" height="712" data-path="oss/images/agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w" />

<Note>
  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.
</Note>
```

---

## Try with a valid token

**URL:** llms-txt#try-with-a-valid-token

client = get_client(
    url="http://localhost:2024", headers={"Authorization": "Bearer user1-token"}
)

---

## How to define a summary evaluator

**URL:** llms-txt#how-to-define-a-summary-evaluator

**Contents:**
- Basic example
- Summary evaluator args
- Summary evaluator output

Source: https://docs.langchain.com/langsmith/summary

Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.

Here, we'll compute the f1-score, which is a combination of precision and recall.

This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference\_outputs.

You can then pass this evaluator to the `evaluate` method as follows:

In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d0f259baa7d7467bf172ef8197c3bb17" alt="summary_eval.png" data-og-width="1535" width="1535" data-og-height="122" height="122" data-path="langsmith/images/summary-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=076d830ea3952a4a598d25a2830232e0 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5f96c7cf258a92be14f489bc1a05f8c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4a2a77c6e0ae855a7027888591733e13 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62eee3f7f104ae12e97ba22828a8bb2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=48d6045a20e1021aceadc98554e39e9e 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d72ea1cfc84b0cb33461eb36ea47c696 2500w" />

## Summary evaluator args

Summary evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: list[dict]`: A list of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs/referenceOutputs: list[dict]`: A list of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `examples: list[Example]`: All of the dataset [Example](/langsmith/example-data-format) objects, including the example inputs, outputs (if available), and metdata (if available).

## Summary evaluator output

Summary evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score": ..., "name": ...}` allow you to pass a numeric or boolean score and metric name.

Currently Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/summary.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

You can then pass this evaluator to the `evaluate` method as follows:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## List threads - each user only sees their own

**URL:** llms-txt#list-threads---each-user-only-sees-their-own

**Contents:**
- 3. Add scoped authorization handlers

alice_threads = await alice.threads.search()
bob_threads = await bob.threads.search()
print(f"✅ Alice sees {len(alice_threads)} thread")
print(f"✅ Bob sees {len(bob_threads)} thread")
bash  theme={null}
✅ Alice created assistant: fc50fb08-78da-45a9-93cc-1d3928a3fc37
✅ Alice created thread: 533179b7-05bc-4d48-b47a-a83cbdb5781d
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/533179b7-05bc-4d48-b47a-a83cbdb5781d'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 437c36ed-dd45-4a1e-b484-28ba6eca8819
✅ Alice sees 1 thread
✅ Bob sees 1 thread
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

Example 2 (unknown):
```unknown
This means:

1. Each user can create and chat in their own threads
2. Users can't see each other's threads
3. Listing threads only shows your own

<a id="scoped-authorization" />

## 3. Add scoped authorization handlers

The broad `@auth.on` handler matches on all [authorization events](/langsmith/auth#supported-resources). This is concise, but it means the contents of the `value` dict are not well-scoped, and the same user-level access control is applied to every resource. If you want to be more fine-grained, you can also control specific actions on resources.

Update `src/security/auth.py` to add handlers for specific resource types:
```

---

## Your application code using multiple frameworks

**URL:** llms-txt#your-application-code-using-multiple-frameworks

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-semantic-kernel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Push to your container registry

**URL:** llms-txt#push-to-your-container-registry

**Contents:**
  - Connect to Your Deployed Agent
  - Environment Configuration

docker push my-agent:latest
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can push to any container registry (Docker Hub, AWS ECR, Azure ACR, Google GCR, etc.) that your deployment environment has access to.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Use the Control Plane API to create deployments from your GitHub repository
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Use the Control Plane API to create deployments from your container registry

See the [LangGraph CLI build documentation](/langsmith/cli#build) for more details.

### Connect to Your Deployed Agent

* <Icon icon="code" /> **[LangGraph SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph-sdk-python)**: Use the LangGraph SDK for programmatic integration.
* <Icon icon="project-diagram" /> **[RemoteGraph](/langsmith/use-remote-graph)**: Connect using RemoteGraph for remote graph connections (to use your graph in other graphs).
* <Icon icon="globe" /> **[REST API](/langsmith/server-api-ref)**: Use HTTP-based interactions with your deployed agent.
* <Icon icon="desktop" /> **[Studio](/langsmith/studio)**: Access the visual interface for testing and debugging.

### Environment Configuration

#### Database & Cache Configuration

By default, LangSmith Deployments create PostgreSQL and Redis instances for you. To use external services, set the following environment variables in your new deployment or revision:
```

---

## Manage billing in your account

**URL:** llms-txt#manage-billing-in-your-account

**Contents:**
- Set up billing for your account
  - Developer Plan: set up billing on your personal organization
  - Plus Plan: set up billing on a shared organization
  - Set up billing for accounts created before pricing introduction
- Update your information
  - Invoice email
  - Business information and tax ID
- Optimize your tracing spend
  - Understand your current usage
  - Optimization 1: manage data retention

Source: https://docs.langchain.com/langsmith/billing

This page describes how to manage billing for your LangSmith organization:

* [Set up billing for your account](#set-up-billing-for-your-account): Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.
* [Update your information](#update-your-information): Modify invoice email addresses, business information, and tax IDs for your organization.
* [Optimize your tracing spend](#optimize-your-tracing-spend): Learn how to reduce costs through data retention management and usage limits.

## Set up billing for your account

<Note>
  Before using this guide, note the following:

* If you are interested in the [Enterprise](https://www.langchain.com/pricing) plan, please [contact sales](https://www.langchain.com/contact-sales). This guide is only for our self-serve billing plans.
  * If you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please [skip to the final section](#set-up-billing-for-accounts-created-before-pricing-was-introduced-on-april-2-2024).
</Note>

To set up billing for your LangSmith organization, navigate to the [Usage and Billing](https://smith.langchain.com/settings/payments) page under **Settings**. Depending on your organization's settings, there are different setup guides:

* [Developer plan](#developer-plan%3A-set-up-billing-on-your-personal-organization)
* [Plus plan](#plus-plan%3A-set-up-billing-on-a-shared-organization)
* [Setup for accounts created before April 2, 2024 pricing introduction](#set-up-billing-for-accounts-created-before-pricing-introduction)

### Developer Plan: set up billing on your personal organization

Personal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the **Plans and Billing** page as follows:

1. Click **Set up Billing**.
2. Add your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.

### Plus Plan: set up billing on a shared organization

If you have not yet created an organization, you need to follow [this guide](/langsmith/set-up-a-workspace#set-up-an-organization) before setting up billing. The following steps assume you are already in a new organization.

<Note>
  You can't use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith.
</Note>

1. Click **Subscribe** on the **Plus** page.
   <Note>
     If you are a startup building with AI, instead click **Apply Now** on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment.
   </Note>
2. Review your existing members. Before subscribing, LangSmith lets you remove any added users that you do **not** want to be included in the bill.
3. Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the **This is a business** checkbox and enter the information accordingly.

For more information, refer to the [Update your information section](#update-your-information).

Once this step is complete, your organization will have access to the rest of LangSmith.

### Set up billing for accounts created before pricing introduction

If you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.

1. Navigate to the [Settings](https://smith.langchain.com/settings) page.
2. Click **Set up Billing**.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=08c998935eacc89e1627c7b2f779ae85" alt="" data-og-width="3238" width="3238" data-og-height="1940" height="1940" data-path="langsmith/images/setup-billing-legacy.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=56bf31c4ae825446f76b858a6193fc6d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e72f35d8282ea3ce30d0b6f3e56f175 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0d44f6c0a8e356bfd81786be01a00053 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8781b4d23063bbdcb8aad7da484358dc 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f57c672319acaee29f0530d3c1b029a9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f451b47e0b730c4a59c69b54adf42781 2500w" />

3. Enter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the [Developer](#developer-plan%3A-set-up-billing-on-your-personal-organization) or [Plus](#plus-plan%3A-set-up-billing-on-a-shared-organization) plans respectively, starting at step 2.
4. Claim free credits as a thank you for being an early LangSmith user.

## Update your information

To update business information for your LangSmith organization, head to the [Usage and Billing](https://smith.langchain.com/settings/payments) page under **Settings** and click on the [Plans and Billing](https://smith.langchain.com/settings/payments?tab=2) tab.

<Note>
  Business information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=450d0bd275f48aeace29d930a60280ab" alt="" data-og-width="1460" width="1460" data-og-height="1268" height="1268" data-path="langsmith/images/update-invoice-email.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0178456f9cfde7888d240102888c3653 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6781adf6ee1fc51996e9dd5040d98efc 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=df5a872bda147196cfd3234981e379c8 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=beeef4657cd4455474f8482b25b8bdd5 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8581a701fb9bdcde17e9f31ef40a2375 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3538905553105a39ad253a58777623d8 2500w" />

To update the email address for invoices, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Locate the section beneath the payment method, where the current invoice email is displayed.
3. Enter the new email address for invoices in the provided field.
4. The new email address will be automatically saved.

You will receive all future invoices to the updated email address.

### Business information and tax ID

<Note>
  In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=651639e60dd1c62d8d745bd63f84bc64" alt="" data-og-width="2030" width="2030" data-og-height="1268" height="1268" data-path="langsmith/images/update-business-info.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c0a75f993ca566f9aee49d3abdc05fff 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=de0e8d12b53caca16cb615b23b517e22 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c8fc4605490916ed8362fd1a23eef711 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e4651854e9979a335e6976ce174f9d81 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4789d1e8bcddd8e4b9a864568301b9b5 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dfcc7869323c51030d69b80cefed7a41 2500w" />

To update your organization's business information, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Below the invoice email section, you will find a checkbox labeled **Business**.
3. Check the **Business** checkbox if your organization belongs to a business.
4. A business information section will appear, allowing you to enter or update the following details:
   * Business Name
   * Address
   * Tax ID for applicable jurisdictions
5. A Tax ID field will appear for applicable jurisdictions after you select a country.
6. After entering the necessary information, click the **Save** button to save your changes.

This ensures that your business information is up-to-date and accurate for billing and tax purposes.

## Optimize your tracing spend

<Check>
  You may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:

* [Data Retention Conceptual Docs](/langsmith/administration-overview#data-retention)
  * [Usage Limiting Conceptual Docs](/langsmith/administration-overview#usage-limits)
</Check>

<Note>
  Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or [support@langchain.dev](mailto:support@langchain.dev).
</Note>

You will learn how to optimize existing spend and prevent future overspend in LangSmith, which includes:

1. Reducing existing costs with data retention policies.
2. Preventing future overspend with usage limits.

This tutorial will use an existing LangSmith organization with high usage. You can transfer the concepts from this example to your own organization. The example organization has three [workspaces](/langsmith/administration-overview#workspaces), one for each deployment stage (`Dev`, `Staging`, and `Prod`):

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=56195e0ab898707658e8456028ce1b2d" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/workspaces.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f6d4464e1f6939b371eb1d5c07c93d23 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2a959511f37aeeefcc10fb789288c11c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f0a3270d5e8cd2604acf52476f51fc50 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dd4d5e690cd066fb6b0e293fa6ee5a4a 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bdf8558780bba78264fee212685bc0ae 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=659d88e90f8eaae9d0b6da822b4781d0 2500w" />

### Understand your current usage

The first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: [Usage graph](#usage-graph) and [Invoices](#invoices).

The usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).

You can navigate to the usage graph under **Settings** -> **Usage and Billing** -> **Usage Graph**.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e584af6597a02d72785204dadf72da54" alt="" data-og-width="2316" width="2316" data-og-height="1864" height="1864" data-path="langsmith/images/usage-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6dcfe25fe752262bc22d64dce2f8e4ac 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=67bdc7704b8388f23c84a760216dca6c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c4ae70ea92f0e29a2c984f34942a0c4f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4baf1b3f573716301e7bd4792af7e533 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=69ac54e38c7a44e907c7e569493421f0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1727041cd6b318ca57f7ca0421beb6ff 2500w" />

This graph shows that there are two usage metrics that LangSmith charges for:

* LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.
* LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.

For more details, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention). Notice that these graphs look identical, which you will review later in the tutorial.

LangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization. As a LangSmith administrator, you may want to understand spend granularly per each of these units. In this case where you just want to cut spend, you can focus on the environment responsible for the majority of costs first for the greatest savings.

You understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the **Invoices** tab. The first invoice that will appear on screen is a draft of your current month's invoice, which shows your running spend thus far this month.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invoice-investigation-v2.gif?s=303cde242c17b0cf1e5e7bd14d6a1d8c" alt="" data-og-width="1148" width="1148" data-og-height="720" height="720" data-path="langsmith/images/invoice-investigation-v2.gif" data-optimize="true" data-opv="3" />

<Note>
  LangSmith's Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.
</Note>

In the GIF, you'll see that the charges for LangSmith Traces are broken up by "tenant\_id" (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly \$2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.

These upgrades occur for two reasons:

1. You use extended data retention tracing, which means by default your traces are retained for 400 days.
2. You use base data retention tracing and use a feature that automatically extends the data retention of a trace. ([Refer to the Auto-Upgrade conceptual docs](/langsmith/administration-overview#data-retention).)

Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the case that this organization is using extended data retention tracing everywhere. As a result, start by optimizing the retention settings.

### Optimization 1: manage data retention

LangSmith charges differently based on a trace's [data retention](/langsmith/administration-overview#data-retention), where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, you'll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.

#### Change org level retention defaults for new projects

Navigate to the **Usage configuration** tab, and look at the organization level retention settings. Modifying this setting affects all **new projects** that are created going forward in all workspaces in the organizaton.

<Note>
  For backwards compatibility, older organizations may have this defaulted to **Extended**. Organizations created after June 3rd, 2024 have this defaulted to **Base**.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=135edb99a5db7123452fddfed8eb85f5" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p1orgretention-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0cbeba57f9d57a309a425a30f48b80c8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2bd1b692312d207fe53ab19e2724695b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0f1ce4db73658d634b76406c74b74235 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=01fe2a6fc65bf6c20033d3094b42d86f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ceb966a10860622f20be10d95fabe616 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7399b5721195c33f79d0a4419361338d 2500w" />

#### Change project level retention defaults

Data retention settings are adjustable per project on the tracing project page.

Navigate to **Projects** > ***Your project name*** > Select **Retention** and modify the default retention of the project to **Base**. This will only affect retention (and pricing) for **traces going forward**.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=394b513df5ef31d0309f5f3c78bd315a" alt="" data-og-width="1358" width="1358" data-og-height="452" height="452" data-path="langsmith/images/p1projectretention.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ebc83ac05d14858da153e707bd02f6b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0baaf55ed3a4719c2b3c3545778e1ded 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=30989a6636683bca7e456ea1897c2986 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ee7c87a83e1a15bc4d843bae3ad32811 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7947cbde975873a53d197019453485e5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53e1f6edb92789ee5034d5f7f1153af6 2500w" />

#### Apply extended data retention to a percentage of traces

You may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an [automation rule](/langsmith/rules). You might want to apply extended data retention to specific types of traces, such as:

* 10% of all traces: For general analysis or analyzing trends long term.
* Errored traces: To investigate and debug issues thoroughly.
* Traces with specific metadata: For long-term examination of particular features or user flows.

1. Navigate to **Projects** > ***Your project name*** > Select **+ New** > Select **New Automation**.
2. Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to [filtering techniques](/langsmith/filter-traces-in-application#filter-operators).

<Note>
  When an automation rule matches any [run](/langsmith/observability-concepts#runs) within a [trace](/langsmith/observability-concepts#traces), then all runs within the trace are upgraded to be retained for 400 days.
</Note>

For example, this is the expected configuration to keep 10% of all traces for extended data retention:

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=09bbdf5ef7cf3a5a99d6bf0a704e2143" alt="" data-og-width="640" width="640" data-og-height="610" height="610" data-path="langsmith/images/P2SampleTraces.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=b40ff90a0e3ef5f52ecda25965ec1daa 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4381f3239c4d5c71c8ccd9dcdb4fe859 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=81da02c26dcc067fbbc4a4d7de318dee 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5f703039c242ba31d192fdb8f4742362 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=e293a6cca1f7da6c8ca62b64e249f6b2 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=558b6d18d598241a2f81e8327b13ce3d 2500w" />

If you want to keep a subset of traces for **longer than 400 days** for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.

#### See results after 7 days

While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly \$900 in the last 7 days, as opposed to \$2,000 in the previous 4. That's a cost reduction of nearly 75% per day.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6ca63d6ab2e222fe98afeefb53c39276" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p1endresultinvoice-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03bc474effd5dd77a62d255b6bafa278 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6de7c29f70e1db30eb270380386aa447 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebe1dcde6d3270fcad3d209b9cc67950 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4ec173a612b3e60ef6f29d1aef383662 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b2b1590c5bc6af8083d91136eb613a8f 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=15c2d1ef150646f045ecb79adcf3d766 2500w" />

### Optimization 2: limit usage

In the previous section, you managed data retention settings to **optimize existing spend**. In this section, you will use usage limits to **prevent future overspend**.

LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the [usage graph](#usage-graph). You can use these in tandem to have granular control over spend.

To set limits, navigate back to **Settings** -> **Usage and Billing** -> **Usage configuration**. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bbd59c73ade4d3f52062bb8261ca3cdb" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2usagelimitsempty-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b5b3ff431b66b82373a5ad092c7a7569 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bbbb1b2ec896daa2e313f0d904842c44 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2969c0fdd5dd9ac09452d013e6648d87 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=023637ca6beb9ba97913f052d4429498 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5a3b1b709d8bb4bc7ff9db2c5bdca576 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=76dd6e90d1e69a469aa78c37aa71ce46 2500w" />

Start by setting limits on production usage, since that is where the majority of spend comes from.

#### Set a good total traces limit

Picking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:

* **Current Load**: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day.
* **Expected Growth in Load**: The expectation is that this will double in size in the near future.

From these assumptions, you can calculate an approximate limit:

Click on the edit icon on the right side of the table for the **Prod** row to enter the limit.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=35073744719e152c1766c5d57309e131" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2alllimitonly-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=04a36fe432e50025a8371371e905f3dd 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03ade0debf4fad232aeba1bd9a3d25ae 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c1b245c764ddfcd4886795a8cfc791 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0f6ee99dede0d9e9b4728e47b0e9c9fc 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=48c63eeaf566fb85ce91340ef109836d 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c489dbb1a608a646147bde274df81146 2500w" />

<Note>
  When set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.
</Note>

#### Cut maximum spend with an extended data retention limit

From [Optimization 1](#optimization-1-manage-data-retention), you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in `.10 * 7,800,000 = 780,000`.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7c9e489f6513fa29b1ce6b47789a402a" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2bothlimits-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=157b5be0d03a9ce5c597d6de43ebd397 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cd60626cc2e355caf40a09ec61712391 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5149c8505dfe51d6e6a4f92877e0d928 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=147e457bb693f14daa6b6e4a0761e305 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cc15640d5a46026da950354ad2456b03 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4e5d52aabfa18020d558e5706ee7d439 2500w" />

The maximum cost is cut from \~40k per month to \~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon.

<Note>
  The extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, read more about its [functionality and side effects](/langsmith/administration-overview#side-effects-of-extended-data-retention-traces-limit).
</Note>

#### Set dev/staging limits and view total spent limit across workspaces

Following a similar logic for the `dev` and `staging` environments, you can set limits at 10% of the production limit on usage for each workspace.

While this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures.

With the limits set, LangSmith shows a maximum spend estimate across all workspaces:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=928eb58f3d41623fa2e72e191b3d2dfb" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2totalspendlimits-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=808034f2e5697b33344968cd4ee722db 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5da581360d8adca6c93ac8ba6c7980dd 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a2fcbdccff84d7f12102a8012557a59a 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8f9eafa50af5140dff3166f629d7180d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ba8e03537bf499021ee642fd4a93517 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2576e5daea8e6b715a3ae3f46371d2c8 2500w" />

You can use the cost estimate to plan for your invoice total.

If you have questions about further optimizing your spend, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/billing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## minReplicas: 20

**URL:** llms-txt#minreplicas:-20

**Contents:**
- Ensure your Redis cache is at least 200 GB

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

---

## Manage your organization using the API

**URL:** llms-txt#manage-your-organization-using-the-api

**Contents:**
- Workspaces
- User management
  - RBAC
  - Membership management
- API keys
- Security settings
- User-only endpoints
- Sample code

Source: https://docs.langchain.com/langsmith/manage-organization-by-api

LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted in [User-only endpoints](#user-only-endpoints).

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
  * [Organization setup how-to guild](/langsmith/set-up-a-workspace#set-up-an-organization)
</Check>

<Note>
  There are a few limitations that will be lifted soon:

* The LangSmith SDKs do not support these organization management actions yet.
  * Organization-scoped [service keys](/langsmith/administration-overview#service-keys) with Organization Admin permission may be used for these actions.
</Note>

<Warning>
  Use the `X-Tenant-Id` header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.

**If `X-Tenant-Id` is not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with `403 Forbidden`.**
</Warning>

Some commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the [API docs](https://api.smith.langchain.com/redoc). **The `X-Organization-Id` header should be present on all requests, and `X-Tenant-Id` header should be present on requests that are scoped to a particular workspace.**

* [List workspaces](https://api.smith.langchain.com/redoc#tag/workspaces/operation/list_workspaces_api_v1_workspaces_get)
* [Create workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/create_workspace_api_v1_workspaces_post)
* [Update workspace name](https://api.smith.langchain.com/redoc#tag/workspaces/operation/patch_workspace_api_v1_workspaces__workspace_id__patch)

* [List roles](https://api.smith.langchain.com/redoc#tag/orgs/operation/list_organization_roles_api_v1_orgs_current_roles_get)
* [List permissions](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)
* [Create role](https://api.smith.langchain.com/redoc#tag/orgs/operation/create_organization_roles_api_v1_orgs_current_roles_post)
* [Update role](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)

### Membership management

`List roles` under [RBAC](#rbac) should be used for retrieving role IDs of these operations. `List [organization|workspace] members` endpoints (below) response `"id"`s should be used as `identity_id` in these operations.

* [List active organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_active_org_members_api_v1_orgs_current_members_active_get)
* [List pending organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_pending_org_members_api_v1_orgs_current_members_pending_get)
* [Invite a user to the organization and one or more workspaces](https://api.smith.langchain.com/redoc#tag/orgs/operation/add_members_to_current_org_batch_api_v1_orgs_current_members_batch_post). This should be used when the user is not already a member in the organization.
* [Update a user's organization role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from the organization](https://api.smith.langchain.com/redoc#tag/orgs/operation/remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete)

* [List workspace members](https://api.smith.langchain.com/redoc#tag/workspaces/operation/get_current_workspace_members_api_v1_workspaces_current_members_get)
* [Add a member to a workspace that is already part of the organization](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Update a user's workspace role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from a workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete)

<Note>
  These params should be omitted: `read_only` (deprecated), `password` and `full_name` ([basic auth](/langsmith/authentication-methods) only)
</Note>

* [Create a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/generate_api_key_api_v1_api_key_post)
* [Delete a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/delete_api_key_api_v1_api_key__api_key_id__delete)

<Note>
  "Shared resources" in this context refer to [public prompts](/langsmith/create-a-prompt#save-your-prompt), [shared runs](/langsmith/share-trace), and [shared datasets](/langsmith/manage-datasets#share-a-dataset).
</Note>

<Warning>
  Updating these settings affects **all resources in the organization**.
</Warning>

You can update these settings under the **Settings > Shared** tab for a workspace, or via API:

* [Update organization sharing settings](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch)
  * use `unshare_all` to unshare **ALL** shared resources in the organization - use `disable_public_sharing` to prevent future sharing of resources

## User-only endpoints

These endpoints are user-scoped and require a logged-in user's JWT, so they should only be executed through the UI.

* `/api-key/current` endpoints: these are related a user's PATs
* `/sso/email-verification/send` (Cloud-only): this endpoint is related to [SAML SSO](/langsmith/user-management)

The sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever `<replace_me>` is in the code.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-organization-by-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## The "messages" stream mode returns a tuple of (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-a-tuple-of-(message_chunk,-metadata)

---

## Observability in Studio

**URL:** llms-txt#observability-in-studio

**Contents:**
- Iterate on prompts
  - Direct node editing
  - Graph Configuration
  - Playground
- Run experiments over a dataset
  - Prerequisites
  - Experiment setup
- Debug LangSmith traces
  - Open deployed threads
  - Testing local agents with remote traces

Source: https://docs.langchain.com/langsmith/observability-studio

LangSmith [Studio](/langsmith/studio) provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:

* [Iterate on prompts](#iterate-on-prompts): Modify prompts inside graph nodes directly or with the LangSmith playground.
* [Run experiments over a dataset](#run-experiments-over-a-dataset): Execute your assistant over a LangSmith dataset to score and compare results.
* [Debug LangSmith traces](#debug-langsmith-traces): Import traced runs into Studio and optionally clone them into your local agent.
* [Add a node to a dataset](#add-node-to-dataset): Turn parts of thread history into dataset examples for evaluation or further analysis.

## Iterate on prompts

Studio supports the following methods for modifying prompts in your graph:

* [Direct node editing](#direct-node-editing)
* [Playground interface](#playground)

### Direct node editing

Studio allows you to edit prompts used inside individual nodes, directly from the graph interface.

### Graph Configuration

Define your [configuration](/oss/python/langgraph/use-graph-api#add-runtime-configuration) to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.

#### `langgraph_nodes`

* **Description**: Specifies which nodes of the graph a configuration field is associated with.
* **Value Type**: Array of strings, where each string is the name of a node in your graph.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

<Accordion title="Full example configuration">
  
</Accordion>

#### Editing prompts in the UI

1. Locate the gear icon on nodes with associated configuration fields.
2. Click to open the configuration modal.
3. Edit the values.
4. Save to update the current assistant version or create a new one.

The [playground](/langsmith/create-a-prompt) interface allows testing individual LLM calls without running the full graph:

1. Select a thread.
2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.
3. Select an LLM run to open in the playground.
4. Modify prompts and test different model and tool settings.
5. Copy updated prompts back to your graph.

## Run experiments over a dataset

Studio lets you run [evaluations](/langsmith/evaluation-concepts) by executing your assistant against a predefined LangSmith [dataset](/langsmith/evaluation-concepts#datasets). This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured [evaluators](/langsmith/evaluation-concepts#evaluators).

This guide shows you how to run a full end-to-end experiment directly from Studio.

Before running an experiment, ensure you have the following:

* **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see [here](/oss/python/langgraph/use-graph-api#schema). For more on creating datasets, refer to [How to Manage Datasets](/langsmith/manage-datasets-in-application#set-up-your-dataset).
* **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.
* **A running application**: The experiment can be run against:
  * An application deployed on [LangSmith](/langsmith/deployments).
  * A locally running application started via the [langgraph-cli](/langsmith/local-server).

1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.
2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.
3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment's progress via the badge in the top right corner.
4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.

## Debug LangSmith traces

This guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.

### Open deployed threads

1. Open the LangSmith trace, selecting the root run.
2. Click **Run in Studio**.

This will open Studio connected to the associated deployment with the trace's parent thread selected.

### Testing local agents with remote traces

This section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.

* A LangSmith traced thread
* A [locally running agent](/langsmith/local-server#local-development-server).

<Info>
  **Local agent requirements**

* langgraph>=0.3.18
  * langgraph-api>=0.0.32
  * Contains the same set of nodes present in the remote trace
</Info>

1. Open the LangSmith trace, selecting the root run.
2. Click the dropdown next to **Run in Studio**.
3. Enter your local agent's URL.
4. Select **Clone thread locally**.
5. If multiple graphs exist, select the target graph.

A new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.

## Add node to dataset

Add [examples](/langsmith/evaluation-concepts#examples) to [LangSmith datasets](/langsmith/manage-datasets) from nodes in the thread log. This is useful to evaluate individual steps of the agent.

1. Select a thread.
2. Click **Add to Dataset**.
3. Select nodes whose input/output you want to add to a dataset.
4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.
5. Edit the example's input/output as needed before adding it to the dataset.
6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.

For more details, refer to [How to evaluate an application's intermediate steps](/langsmith/evaluate-on-intermediate-steps).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:
```

Example 2 (unknown):
```unknown
<Accordion title="Full example configuration">
```

---

## This converts the runs to a dataset + experiment

**URL:** llms-txt#this-converts-the-runs-to-a-dataset-+-experiment

**Contents:**
- Benchmark against new system
  - Define evaluators
  - Evaluate baseline

convert_runs_to_test(
    prod_runs,
    # Name of the resulting dataset
    dataset_name=dataset_name,
    # Whether to include the run outputs as reference/ground truth
    include_outputs=False,
    # Whether to include the full traces in the resulting experiment
    # (default is to just include the root run)
    load_child_runs=True,
    # Name of the experiment so we can apply evalautors to it after
    test_project_name=baseline_experiment_name
)
python  theme={null}
import emoji
from pydantic import BaseModel, Field
from langchain_core.messages import convert_to_openai_messages

class Grade(BaseModel):
    """Grade whether a response is supported by some context."""
    grounded: bool = Field(..., description="Is the majority of the response supported by the retrieved context?")

grounded_instructions = f"""You have given somebody some contextual information and asked them to write a statement grounded in that context.

Grade whether their response is fully supported by the context you have provided. \
If any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \
Otherwise it is grounded."""
grounded_model = init_chat_model(model="gpt-4o").with_structured_output(Grade)

def lt_280_chars(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(messages[-1]['content']) <= 280

def gte_3_emojis(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(emoji.emoji_list(messages[-1]['content'])) >= 3

async def is_grounded(outputs: dict) -> bool:
    context = ""
    messages = convert_to_openai_messages(outputs["messages"])
    for message in messages:
        if message["role"] == "tool":
            # Tool message outputs are the results returned from the Tavily/DuckDuckGo tool
            context += "\n\n" + message["content"]
    tweet = messages[-1]["content"]
    user = f"""CONTEXT PROVIDED:
    {context}

RESPONSE GIVEN:
    {tweet}"""
    grade = await grounded_model.ainvoke([
        {"role": "system", "content": grounded_instructions},
        {"role": "user", "content": user}
    ])
    return grade.grounded
python  theme={null}
baseline_results = await client.aevaluate(
    baseline_experiment_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
)

**Examples:**

Example 1 (unknown):
```unknown
Once this step is complete, you should see a new dataset in your LangSmith project called "Tweet Writing Task-backtesting TODAYS DATE", with a single experiment like so:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=73b60a75d6b33f2830f5ed68464c586b" alt="" data-og-width="3456" width="3456" data-og-height="1852" height="1852" data-path="langsmith/images/baseline-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e459a884bbec6e3741617830b9e70848 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6116dd6057709be29d84f0cbad32e7a1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a86ab5cffeac0ceace81adbc59dba649 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0058d68f4061df5fe4249c9e3954c38 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=270a7eb33f39fd612d1732aadaa0b373 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fe9dc037876fcf1c4eb22317d7bb3f45 2500w" />

## Benchmark against new system

Now we can start the process of benchmarking our production runs against a new system.

### Define evaluators

First let's define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we'll need to come up with evaluation metrics that only require the actual outputs.
```

Example 2 (unknown):
```unknown
### Evaluate baseline

Now, let's run our evaluators against the baseline experiment.
```

---

## Prevent logging of sensitive data in traces

**URL:** llms-txt#prevent-logging-of-sensitive-data-in-traces

**Contents:**
- Rule-based masking of inputs and outputs
- Processing Inputs & Outputs for a Single Function
- Quick starts
  - Regex

Source: https://docs.langchain.com/langsmith/mask-inputs-outputs

In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.

If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:

This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

* Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.

<Note>
  Improving the performance of `anonymizer` API is on our roadmap! If you are encountering performance issues, please contact us at [support@langchain.dev](mailto:support@langchain.dev).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ac9ba9a6729029a7fa38da03e1466a1a" alt="" data-og-width="1708" width="1708" data-og-height="717" height="717" data-path="langsmith/images/hide-inputs-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7ded12c0345f47d55e9802083c5032d0 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cbe74d09660d8c65e8a75dd78cdb24e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cb8c0b5c926e46522b9539b0262ee7a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9b8ef244796fad943ec76b0aa5733f80 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=87f35d63f42f05c49a220d5b8a87787a 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5173e30032c065646c13e9b9c6a95fb5 2500w" />

Older versions of LangSmith SDKs can use the `hide_inputs` and `hide_outputs` parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.

## Processing Inputs & Outputs for a Single Function

<Info>
  The `process_outputs` parameter is available in LangSmith SDK version 0.1.98 and above for Python.
</Info>

In addition to client-level input and output processing, LangSmith provides function-level processing through the `process_inputs` and `process_outputs` parameters of the `@traceable` decorator.

These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.

Here's an example of how to use `process_inputs` and `process_outputs`:

In this example, `process_inputs` creates a new dictionary with processed input data, and `process_outputs` transforms the output into a specific format before logging to LangSmith.

<Warning>
  It's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.
</Warning>

For asynchronous functions, the usage is similar:

These function-level processors take precedence over client-level processors (`hide_inputs` and `hide_outputs`) when both are defined.

You can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we'll cover working with regex, Microsoft Presidio, and Amazon Comprehend.

<Info>
  The implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.
</Info>

You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.

```python  theme={null}
import re
import openai
from langsmith import Client
from langsmith.wrappers import wrap_openai

**Examples:**

Example 1 (unknown):
```unknown
This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

  * Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## This will NOT be traced (if LANGSMITH_TRACING is not set)

**URL:** llms-txt#this-will-not-be-traced-(if-langsmith_tracing-is-not-set)

**Contents:**
- Log to a project
- Add metadata to traces
- Use anonymizers to prevent logging of sensitive data in traces

agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})
bash  theme={null}
  export LANGSMITH_PROJECT=my-agent-project
  python  theme={null}
  import langsmith as ls

with ls.tracing_context(project_name="email-agent-test", enabled=True):
      response = agent.invoke({
          "messages": [{"role": "user", "content": "Send a welcome email"}]
      })
  python  theme={null}
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Send a welcome email"}]},
    config={
        "tags": ["production", "email-assistant", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
python  theme={null}
with ls.tracing_context(
    project_name="email-agent-test",
    enabled=True,
    tags=["production", "email-assistant", "v1.0"],
    metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "Send a welcome email"}]}
    )
python Python theme={null}
from langchain_core.tracers.langchain import LangChainTracer
from langgraph.graph import StateGraph, MessagesState
from langsmith import Client
from langsmith.anonymizer import create_anonymizer

anonymizer = create_anonymizer([
    # Matches SSNs
    { "pattern": r"\b\d{3}-?\d{2}-?\d{4}\b", "replace": "<ssn>" }
])

tracer_client = Client(anonymizer=anonymizer)
tracer = LangChainTracer(client=tracer_client)

**Examples:**

Example 1 (unknown):
```unknown
## Log to a project

<Accordion title="Statically">
  You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Dynamically">
  You can set the project name programmatically for specific operations:
```

Example 3 (unknown):
```unknown
</Accordion>

## Add metadata to traces

You can annotate your traces with custom metadata and tags:
```

Example 4 (unknown):
```unknown
`tracing_context` also accepts tags and metadata for fine-grained control:
```

---

## Retrieval

**URL:** llms-txt#retrieval

**Contents:**
- Building a knowledge base
  - From retrieval to RAG
  - Retrieval Pipeline
  - Building Blocks
- RAG Architectures
  - 2-step RAG
  - Agentic RAG
  - Hybrid RAG

Source: https://docs.langchain.com/oss/python/langchain/retrieval

Large language models (LLMs) are powerful, but they have two key limitations:

* **Finite context** — they can’t ingest entire corpora at once.
* **Static knowledge** — their training data is frozen at a point in time.

Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.

## Building a knowledge base

A **knowledge base** is a repository of documents or structured data used during retrieval.

If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.

<Note>
  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:

* Connect it as a **tool** for an agent in Agentic RAG.
  * Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).
</Note>

See the following tutorial to build a searchable knowledge base and minimal RAG workflow:

<Card title="Tutorial: Semantic search" icon="database" href="/oss/python/langchain/knowledge-base" arrow cta="Learn more">
  Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.
  In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.
</Card>

### From retrieval to RAG

Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.

This is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.

### Retrieval Pipeline

A typical retrieval workflow looks like this:

Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/python/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.
  </Card>

<Card title="Text splitters" icon="scissors" href="/oss/python/integrations/splitters" arrow cta="Learn more">
    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.
  </Card>

<Card title="Embedding models" icon="diagram-project" href="/oss/python/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

<Card title="Vector stores" icon="database" href="/oss/python/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

<Card title="Retrievers" icon="binoculars" href="/oss/python/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>

<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.

Typical components include:

* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.
* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.
* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.

The architecture often supports multiple iterations between these steps:

This architecture is suitable for:

* Applications with ambiguous or underspecified queries
* Systems that require validation or quality control steps
* Workflows involving multiple sources or iterative refinement

<Card title="Tutorial: Agentic RAG with Self-Correction" icon="robot" href="/oss/python/langgraph/agentic-rag" arrow cta="Learn more">
  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

### Building Blocks

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/python/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.
  </Card>

  <Card title="Text splitters" icon="scissors" href="/oss/python/integrations/splitters" arrow cta="Learn more">
    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.
  </Card>

  <Card title="Embedding models" icon="diagram-project" href="/oss/python/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

  <Card title="Vector stores" icon="database" href="/oss/python/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

  <Card title="Retrievers" icon="binoculars" href="/oss/python/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

## RAG Architectures

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

### 2-step RAG

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.
```

Example 2 (unknown):
```unknown
<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

### Agentic RAG

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.
```

---

## Do NOT mistake this for the secret service role key

**URL:** llms-txt#do-not-mistake-this-for-the-secret-service-role-key

SUPABASE_ANON_KEY = os.environ.get("SUPABASE_ANON_KEY")
if not SUPABASE_ANON_KEY:
    SUPABASE_ANON_KEY = getpass("Enter your public Supabase anon  key: ")

async def sign_up(email: str, password: str):
    """Create a new user account."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/signup",
            json={"email": email, "password": password},
            headers={"apiKey": SUPABASE_ANON_KEY},
        )
        assert response.status_code == 200
        return response.json()

---

## Deploy an observability stack for your LangSmith deployment

**URL:** llms-txt#deploy-an-observability-stack-for-your-langsmith-deployment

Source: https://docs.langchain.com/langsmith/observability-stack

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

LangSmith applications expose telemetry data that can be sent to the backend of your choice. If you don’t already have an observability stack, or prefer to keep LangSmith telemetry separate from your main application, you can use the LangSmith Observability Helm chart to deploy a basic observability stack.

---

## Contributing

**URL:** llms-txt#contributing

**Contents:**
- Ways to Contribute

Source: https://docs.langchain.com/oss/python/contributing/overview

**Welcome! Thank you for your interest in contributing.**

LangChain has helped form the largest developer community in generative AI, and we're always open to new contributors. Whether you're fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone.

## Ways to Contribute

<AccordionGroup>
  <Accordion title="Report bugs" icon="bug">
    Found a bug? Please help us fix it by following these steps:

<Steps>
      <Step title="Search">
        Check if the issue already exists in our GitHub Issues for the respective repo:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/issues">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/issues">Issues</Card>
        </Columns>
      </Step>

<Step title="Create issue">
        If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example). Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.
      </Step>

<Step title="Wait">
        A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.
      </Step>
    </Steps>

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please [link them](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) rather than combining them. For example,

<Accordion title="Suggest features" icon="wand-magic-sparkles">
    Have an idea for a new feature or enhancement?

<Steps>
      <Step title="Search">
        Search the issues for the respective repository for existing feature requests:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3A%22feature%20request%22">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Aenhancement">Issues</Card>
        </Columns>
      </Step>

<Step title="Discuss">
        If no requests exist, start a new discussion under the [relevant category](https://forum.langchain.com/c/help/langchain/14) so that project maintainers and the community can provide feedback.
      </Step>

<Step title="Describe">
        Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.
      </Step>
    </Steps>
  </Accordion>

<Accordion title="Improve documentation" icon="book">
    Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference.

<Card title="How to propose changes to the documentation" href="/oss/python/contributing/documentation" arrow>Guide</Card>
  </Accordion>

<Accordion title="Contribute code" icon="code">
    With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!

<Card title="How to make your first Pull Request" href="/oss/python/contributing/code" arrow>Guide</Card>

If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.

If you are looking for something to work on, check out the issues labeled "good first issue" or "help wanted" in our repos:

<Columns cols={2}>
      <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/labels">Labels</Card>
      <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/labels">Labels</Card>
    </Columns>
  </Accordion>

<Accordion title="Add a new integration" icon="plug-circle-plus">
    <Columns cols={2}>
      <Card title="LangChain" icon="link" href="/oss/python/contributing/integrations-langchain" arrow>Guide to adding a new LangChain integration</Card>
      <Card title="LangGraph" icon="circle-nodes" href="/oss/python/contributing/integrations-langgraph" arrow>Guide to adding a new LangGraph integration</Card>
    </Columns>
  </Accordion>
</AccordionGroup>

Thank you for helping make LangChain better! 🦜❤️

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Runtime

**URL:** llms-txt#runtime

**Contents:**
- Overview
- Access
  - Inside tools
  - Inside middleware

Source: https://docs.langchain.com/oss/python/langchain/runtime

LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) runs on LangGraph's runtime under the hood.

LangGraph exposes a [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object with the following information:

1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation
2. **Store**: a [BaseStore](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory)
3. **Stream writer**: an object used for streaming information via the `"custom"` stream mode

You can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).

When creating an agent with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), you can specify a `context_schema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime).

When invoking the agent, pass the `context` argument with the relevant configuration for the run:

You can access the runtime information inside tools to:

* Access the context
* Read or write long-term memory
* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)

Use the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool.

### Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.

Use `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions.

```python  theme={null}
from dataclasses import dataclass

from langchain.messages import AnyMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model
from langgraph.runtime import Runtime

@dataclass
class Context:
    user_name: str

**Examples:**

Example 1 (unknown):
```unknown
### Inside tools

You can access the runtime information inside tools to:

* Access the context
* Read or write long-term memory
* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates)

Use the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool.
```

Example 2 (unknown):
```unknown
### Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.

Use `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions.
```

---

## This isn't for production use, but is useful for local

**URL:** llms-txt#this-isn't-for-production-use,-but-is-useful-for-local

store = LocalFileStore("./cache/") # [!code highlight]

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings,
    store,
    namespace=underlying_embeddings.model
)

---

## Troubleshoot variable caching

**URL:** llms-txt#troubleshoot-variable-caching

**Contents:**
- 1. Verify Your Environment Variables
- 2. Clear the cache
- 3. Reload the Environment Variables

Source: https://docs.langchain.com/langsmith/troubleshooting-variable-caching

If you're not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmith's default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:

## 1. Verify Your Environment Variables

First, check that the environment variables are set correctly by running:

If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:

## 3. Reload the Environment Variables

Reload your environment variables from the .env file by executing:

After reloading, your environment variables should be set correctly.

If you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in the [LangChain Forum](https://forum.langchain.com/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-variable-caching.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:
```

Example 2 (unknown):
```unknown
## 3. Reload the Environment Variables

Reload your environment variables from the .env file by executing:
```

---

## Enforce previous behavior with output_version flag

**URL:** llms-txt#enforce-previous-behavior-with-output_version-flag

**Contents:**
  - Default `max_tokens` in `langchain-anthropic`
  - Legacy code moved to `langchain-classic`
  - Removal of deprecated APIs
  - `.text()` is now a property

model = ChatOpenAI(model="gpt-4o-mini", output_version="v0")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Default `max_tokens` in `langchain-anthropic`

The `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.

### Legacy code moved to `langchain-classic`

Existing functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-namespace) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`.

### Removal of deprecated APIs

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.

### `.text()` is now a property

Use of the `.text()` method on message objects should drop the parentheses:
```

---

## Manage assistants

**URL:** llms-txt#manage-assistants

**Contents:**
- Create an assistant
  - LangGraph SDK
  - LangSmith UI
- Use an assistant
  - LangGraph SDK
  - LangSmith UI
- Create a new version for your assistant
  - LangGraph SDK
  - LangSmith UI
- Use a previous assistant version

Source: https://docs.langchain.com/langsmith/configuration-cloud

In this guide we will show how to create, configure, and manage an [assistant](/langsmith/assistants).

First, as a brief refresher on the concept of context, consider the following simple `call_model` node and context schema.
Observe that this node tries to read and use the `model_name` as defined by the `context` object's `model_name` field.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

For more information on configurations, [see here](/langsmith/configuration-cloud#configuration).

## Create an assistant

To create an assistant, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create) SDK reference docs for more information.

This example uses the same context schema as above, and creates an assistant with `model_name` set to `openai`.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

You can also create assistants from the LangSmith UI.

Inside your deployment, select the "Assistants" tab. This will load a table of all of the assistants in your deployment, across all graphs.

To create a new assistant, select the "+ New assistant" button. This will open a form where you can specify the graph this assistant is for, as well as provide a name, description, and the desired configuration for the assistant based on the configuration schema for that graph.

To confirm, click "Create assistant". This will take you to [Studio](/langsmith/studio) where you can test the assistant. If you go back to the "Assistants" tab in the deployment, you will see the newly created assistant in the table.

We have now created an assistant called "Open AI Assistant" that has `model_name` defined as `openai`. We can now use this assistant with this configuration:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Inside your deployment, select the "Assistants" tab. For the assistant you would like to use, click the **Studio** button. This will open Studio with the selected assistant. When you submit an input (either in Graph or Chat mode), the selected assistant and its configuration will be used.

## Create a new version for your assistant

To edit the assistant, use the `update` method. This will create a new version of the assistant with the provided edits. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.update) and [JS](/langsmith/langgraph-js-ts-sdk#update) SDK reference docs for more information.

<Note>
  **Note**
  You must pass in the ENTIRE context (and metadata if you are using it). The update endpoint creates new versions completely from scratch and does not rely on previous versions.
</Note>

For example, to update your assistant's system prompt:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

This will create a new version of the assistant with the updated parameters and set this as the active version of your assistant. If you now run your graph and pass in this assistant id, it will use this latest version.

You can also edit assistants from the LangSmith UI.

Inside your deployment, select the "Assistants" tab. This will load a table of all of the assistants in your deployment, across all graphs.

To edit an existing assistant, select the "Edit" button for the specified assistant. This will open a form where you can edit the assistant's name, description, and configuration.

Additionally, if using Studio, you can edit the assistants and create new versions via the "Manage Assistants" button.

## Use a previous assistant version

You can also change the active version of your assistant. To do so, use the `setLatest` method.

In the example above, to rollback to the first version of the assistant:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

If you now run your graph and pass in this assistant id, it will use the first version of the assistant.

If using Studio, to set the active version of your assistant, click the "Manage Assistants" button and locate the assistant you would like to use. Select the assistant and the version, and then click the "Active" toggle. This will update the assistant to make the selected version active.

<Warning>
  **Deleting Assistants**
  Deleting as assistant will delete ALL of its versions. There is currently no way to delete a single version, but by pointing your assistant to the correct version you can skip any versions that you don't wish to use.
</Warning>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configuration-cloud.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

For more information on configurations, [see here](/langsmith/configuration-cloud#configuration).

## Create an assistant

### LangGraph SDK

To create an assistant, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create) SDK reference docs for more information.

This example uses the same context schema as above, and creates an assistant with `model_name` set to `openai`.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Check what was interrupted

**URL:** llms-txt#check-what-was-interrupted

---

## ❌ Bad: Generic and unorganized

**URL:** llms-txt#❌-bad:-generic-and-unorganized

**Contents:**
  - Document what gets persisted
  - Isolate storage by assistant ID
  - Use persistent stores in production

/memories/temp.txt
/memories/data.txt
/memories/file1.txt
python  theme={null}
system_prompt="""You have access to two types of storage:

SHORT-TERM (paths without /memories/):
- Current conversation notes
- Temporary scratch work
- Draft documents

LONG-TERM (paths starting with /memories/):
- User preferences and settings
- Completed reports and documents
- Knowledge that should persist across conversations
- Project state and progress

Always use /memories/ for information that should survive beyond this conversation."""
python  theme={null}
config = {
    "configurable": {
        "thread_id": "thread-123",
    },
    "metadata": {
        "assistant_id": "user-456"  # Namespace isolation
    }
}

agent.invoke({"messages": [...]}, config=config)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Document what gets persisted

In system prompts, clarify when to use long-term vs short-term storage:
```

Example 2 (unknown):
```unknown
### Isolate storage by assistant ID

For multi-tenant applications, provide an `assistant_id` to isolate storage:
```

Example 3 (unknown):
```unknown
Each assistant gets its own namespace in the Store, preventing cross-contamination.

### Use persistent stores in production
```

---

## - /memories/project_status.txt

**URL:** llms-txt#--/memories/project_status.txt

**Contents:**
- Limitations
  - Store is required

**Examples:**

Example 1 (unknown):
```unknown
Files from the Store are prefixed with `/memories/` in listings.

## Limitations

### Store is required

You must provide a Store when enabling long-term memory:
```

---

## This invocation will take ~1 second due to the slow_task execution

**URL:** llms-txt#this-invocation-will-take-~1-second-due-to-the-slow_task-execution

**Contents:**
- Human-in-the-loop
  - Basic human-in-the-loop workflow

try:
    # First invocation will raise an exception due to the `get_info` task failing
    main.invoke({'any_input': 'foobar'}, config=config)
except ValueError:
    pass  # Handle the failure gracefully
python  theme={null}
main.invoke(None, config=config)
pycon  theme={null}
'Ran slow task.'
python  theme={null}
from langgraph.func import entrypoint, task
from langgraph.types import Command, interrupt

@task
def step_1(input_query):
    """Append bar."""
    return f"{input_query} bar"

@task
def human_feedback(input_query):
    """Append user input."""
    feedback = interrupt(f"Please provide feedback: {input_query}")
    return f"{input_query} {feedback}"

@task
def step_3(input_query):
    """Append qux."""
    return f"{input_query} qux"
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def graph(input_query):
    result_1 = step_1(input_query).result()
    result_2 = human_feedback(result_1).result()
    result_3 = step_3(result_2).result()

return result_3
python  theme={null}
config = {"configurable": {"thread_id": "1"}}

for event in graph.stream("foo", config):
    print(event)
    print("\n")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
When we resume execution, we won't need to re-run the `slow_task` as its result is already saved in the checkpoint.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
## Human-in-the-loop

The functional API supports [human-in-the-loop](/oss/python/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function and the `Command` primitive.

### Basic human-in-the-loop workflow

We will create three [tasks](/oss/python/langgraph/functional-api#task):

1. Append `"bar"`.
2. Pause for human input. When resuming, append human input.
3. Append `"qux"`.
```

Example 4 (unknown):
```unknown
We can now compose these tasks in an [entrypoint](/oss/python/langgraph/functional-api#entrypoint):
```

---

## Run creation, streaming, updates, etc.

**URL:** llms-txt#run-creation,-streaming,-updates,-etc.

---

## Define the nodes we will cycle between

**URL:** llms-txt#define-the-nodes-we-will-cycle-between

workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

---

## Assuming it might be related or a typo in the original doc:

**URL:** llms-txt#assuming-it-might-be-related-or-a-typo-in-the-original-doc:

**Contents:**
  - Tools

from langchain_google_community import VertexAISearchRetriever # Verify class name if needed
python  theme={null}
from langchain_google_community import VertexAISearchSummaryTool
bash pip theme={null}
  pip install langchain-google-community # Add specific docai dependencies if needed
  bash uv theme={null}
  uv add langchain-google-community # Add specific docai dependencies if needed
  python  theme={null}
from langchain_google_community.documentai_warehouse import DocumentAIWarehouseRetriever
bash pip theme={null}
  pip install google-cloud-text-to-speech langchain-google-community
  bash uv theme={null}
  uv add google-cloud-text-to-speech langchain-google-community
  python  theme={null}
from langchain_google_community import TextToSpeechTool
bash pip theme={null}
  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  bash uv theme={null}
  uv add google-api-python-client google-auth-httplib2 google-auth-oauthlib langchain-googledrive
  python  theme={null}
from langchain_googledrive.utilities.google_drive import GoogleDriveAPIWrapper
from langchain_googledrive.tools.google_drive.tool import GoogleDriveSearchTool
bash pip theme={null}
  pip install google-search-results langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add google-search-results langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.tools.google_finance import GoogleFinanceQueryRun
from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper
bash pip theme={null}
  pip install google-search-results langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add google-search-results langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.tools.google_jobs import GoogleJobsQueryRun

**Examples:**

Example 1 (unknown):
```unknown
##### VertexAISearchSummaryTool
```

Example 2 (unknown):
```unknown
#### Document AI Warehouse

> Search, store, and manage documents using [Document AI Warehouse](https://cloud.google.com/document-ai-warehouse).

Note: `GoogleDocumentAIWarehouseRetriever` (from `langchain`) is deprecated. Use `DocumentAIWarehouseRetriever` from `langchain-google-community`.

Requires installation of relevant Document AI packages (check specific docs).

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
```

---

## Optionally wrap the OpenAI client to trace all model calls.

**URL:** llms-txt#optionally-wrap-the-openai-client-to-trace-all-model-calls.

oai_client = wrappers.wrap_openai(OpenAI())

def valid_reasoning(inputs: dict, outputs: dict) -> bool:
    """Use an LLM to judge if the reasoning and the answer are consistent."""
    instructions = """
Given the following question, answer, and reasoning, determine if the reasoning
for the answer is logically valid and consistent with the question and the answer."""

class Response(BaseModel):
        reasoning_is_valid: bool

msg = f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"
    response = oai_client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
        response_format=Response
    )
    return response.choices[0].message.parsed.reasoning_is_valid

---

## Note: The example code shows VertexAIMultiTurnSearchRetriever, confirm if VertexAISearchRetriever is separate or related.

**URL:** llms-txt#note:-the-example-code-shows-vertexaimultiturnsearchretriever,-confirm-if-vertexaisearchretriever-is-separate-or-related.

---

## How to create and manage datasets programmatically

**URL:** llms-txt#how-to-create-and-manage-datasets-programmatically

**Contents:**
- Create a dataset
  - Create a dataset from list of values
  - Create a dataset from traces
  - Create a dataset from a CSV file
  - Create a dataset from pandas DataFrame (Python only)
- Fetch datasets
  - Query all datasets
  - List datasets by name
  - List datasets by type
- Fetch examples

Source: https://docs.langchain.com/langsmith/manage-datasets-programmatically

You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.

### Create a dataset from list of values

The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.

Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.

<Check>
  If you have many examples to create, consider using the `create_examples`/`createExamples` method to create multiple examples in a single request. If creating a single example, you can use the `create_example`/`createExample` method.
</Check>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

### Create a dataset from pandas DataFrame (Python only)

The python client offers an additional convenience method to upload a dataset from a pandas dataframe.

You can programmatically fetch datasets from LangSmith using the `list_datasets`/`listDatasets` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### Query all datasets

### List datasets by name

If you want to search by the exact name, you can do the following:

If you want to do a case-invariant substring search, try the following:

### List datasets by type

You can filter datasets by type. Below is an example querying for chat datasets.

You can programmatically fetch examples from LangSmith using the `list_examples`/`listExamples` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### List all examples for a dataset

You can filter by dataset ID:

Or you can filter by dataset name (this must exactly match the dataset name you want to query)

### List examples by id

You can also list multiple examples all by ID.

### List examples by metadata

You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.

For example, if you have an example with metadata `{"foo": "bar", "baz": "qux"}`, both `{foo: bar}` and `{baz: qux}` would match, as would `{foo: bar, baz: qux}`.

### List examples by structured filter

Similar to how you can use the structured filter query language to [fetch runs](/langsmith/export-traces#use-filter-query-language), you can use it to fetch examples.

<Note>
  This is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.

Additionally, the structured filter query language is only supported for `metadata` fields.
</Note>

You can use the `has` operator to fetch examples with metadata fields that contain specific key/value pairs and the `exists` operator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the `and` operator and negate a filter using the `not` operator.

### Update single example

You can programmatically update examples from LangSmith using the `update_example`/`updateExample` method in the Python and TypeScript SDKs. Below is an example.

### Bulk update examples

You can also programmatically update multiple examples in a single request with the `update_examples`/`updateExamples` method in the Python and TypeScript SDKs. Below is an example.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-programmatically.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

<CodeGroup>
```

---

## First let's just say hi to the AI

**URL:** llms-txt#first-let's-just-say-hi-to-the-ai

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates"
):
    print(update)
python  theme={null}
def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

# Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# ... Analyze conversation and create a new memory

# Create a new memory ID
    memory_id = str(uuid.uuid4())

# We create a new memory
    store.put(namespace, memory_id, {"memory": memory})

python  theme={null}
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    # Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# Search based on the most recent message
    memories = store.search(
        namespace,
        query=state["messages"][-1].content,
        limit=3
    )
    info = "\n".join([d.value["memory"] for d in memories])

# ... Use memories in the model call
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can access the `in_memory_store` and the `user_id` in *any node* by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here's how we might use semantic search in a node to find relevant memories:
```

Example 2 (unknown):
```unknown
As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.
```

Example 3 (unknown):
```unknown
We can access the memories and use them in our model call.
```

Example 4 (unknown):
```unknown
If we create a new thread, we can still access the same memories so long as the `user_id` is the same.
```

---

## Initialize an in-memory checkpointer for persistence

**URL:** llms-txt#initialize-an-in-memory-checkpointer-for-persistence

checkpointer = InMemorySaver()

@task
def slow_task():
    """
    Simulates a slow-running task by introducing a 1-second delay.
    """
    time.sleep(1)
    return "Ran slow task."

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer: StreamWriter):
    """
    Main workflow function that runs the slow_task and get_info tasks sequentially.

Parameters:
    - inputs: Dictionary containing workflow input values.
    - writer: StreamWriter for streaming custom data.

The workflow first executes `slow_task` and then attempts to execute `get_info`,
    which will fail on the first invocation.
    """
    slow_task_result = slow_task().result()  # Blocking call to slow_task
    get_info().result()  # Exception will be raised here on the first attempt
    return slow_task_result

---

## Define the schema for the input

**URL:** llms-txt#define-the-schema-for-the-input

class InputState(TypedDict):
    question: str

---

## Publicly available test files

**URL:** llms-txt#publicly-available-test-files

pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
wav_url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
img_url = "https://www.w3.org/Graphics/PNG/nurbcup2si.png"

---

## The `authenticate` decorator tells LangGraph to call this function as middleware

**URL:** llms-txt#the-`authenticate`-decorator-tells-langgraph-to-call-this-function-as-middleware

---

## Check if execution was interrupted

**URL:** llms-txt#check-if-execution-was-interrupted

if result.get("__interrupt__"):
    # Extract interrupt information
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]
    review_configs = interrupts["review_configs"]

# Create a lookup map from tool name to review config
    config_map = {cfg["action_name"]: cfg for cfg in review_configs}

# Display the pending actions to the user
    for action in action_requests:
        review_config = config_map[action["name"]]
        print(f"Tool: {action['name']}")
        print(f"Arguments: {action['args']}")
        print(f"Allowed decisions: {review_config['allowed_decisions']}")

# Get user decisions (one per action_request, in order)
    decisions = [
        {"type": "approve"}  # User approved the deletion
    ]

# Resume execution with decisions
    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config  # Must use the same config!
    )

---

## Messages

**URL:** llms-txt#messages

**Contents:**
- Basic usage

Source: https://docs.langchain.com/oss/python/langchain/messages

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.

Messages are objects that contain:

* <Icon icon="user" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)
* <Icon icon="folder-closed" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)
* <Icon icon="tag" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage

LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.

The simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/python/langchain/models#invocation).

```python  theme={null}
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage

model = init_chat_model("openai:gpt-5-nano")

system_msg = SystemMessage("You are a helpful assistant.")
human_msg = HumanMessage("Hello, how are you?")

---

## Deploy your app to Cloud

**URL:** llms-txt#deploy-your-app-to-cloud

**Contents:**
- Prerequisites
- 1. Create a repository on GitHub
- 2. Deploy to LangSmith
- 3. Test your application in Studio
- 4. Get the API URL for your deployment
- 5. Test the API
- Next steps

Source: https://docs.langchain.com/langsmith/deployment-quickstart

This is a quickstart guide for deploying your first application to LangSmith Cloud.

<Tip>
  For a comprehensive Cloud deployment guide with all configuration options, refer to the [Cloud deployment setup guide](/langsmith/deploy-to-cloud).
</Tip>

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

## 1. Create a repository on GitHub

To deploy an application to **LangSmith**, your application code must reside in a GitHub repository. Both public and private repositories are supported. For this quickstart, use the [`new-langgraph-project` template](https://github.com/langchain-ai/react-agent) for your application:

1. Go to the [`new-langgraph-project` repository](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraphjs-project` template](https://github.com/langchain-ai/new-langgraphjs-project).
2. Click the `Fork` button in the top right corner to fork the repository to your GitHub account.
3. Click **Create fork**.

## 2. Deploy to LangSmith

1. Log in to [LangSmith](https://smith.langchain.com/).
2. In the left sidebar, select **Deployments**.
3. Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
4. If you are a first time user or adding a private repository that has not been previously connected, click the **Import from GitHub** button and follow the instructions to connect your GitHub account.
5. Select your New LangGraph Project repository.
6. Click **Submit** to deploy.
   This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

## 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. [Studio](/langsmith/studio) will open to display your graph.

## 4. Get the API URL for your deployment

1. In the **Deployment details** view, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python SDK (Async)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

You've successfully deployed your application to LangSmith Cloud. Here are some next steps:

* **Explore Studio**: Use [Studio](/langsmith/studio) to visualize and debug your graph interactively.
* **Monitor your app**: Set up [observability](/langsmith/observability) with traces, dashboards, and alerts.
* **Learn more about Cloud**: See the [complete Cloud setup guide](/langsmith/deploy-to-cloud) for all configuration options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deployment-quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:
```

Example 3 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK
```

---

## Durable execution

**URL:** llms-txt#durable-execution

**Contents:**
- Requirements
- Determinism and Consistent Replay
- Durability modes
  - `"exit"`
  - `"async"`
  - `"sync"`
- Using tasks in nodes
- Resuming Workflows
- Starting Points for Resuming Workflows

Source: https://docs.langchain.com/oss/python/langgraph/durable-execution

**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](/oss/python/langgraph/interrupts), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).

LangGraph's built-in [persistence](/oss/python/langgraph/persistence) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](/oss/python/langgraph/interrupts) interactions -- it can be resumed from its last recorded state.

<Tip>
  If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
  To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](/oss/python/langgraph/functional-api#task). You can use [tasks](/oss/python/langgraph/functional-api#task) from both the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).
</Tip>

To leverage durable execution in LangGraph, you need to:

1. Enable [persistence](/oss/python/langgraph/persistence) in your workflow by specifying a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) that will save workflow progress.

2. Specify a [thread identifier](/oss/python/langgraph/persistence#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.

3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside @\[`task`] to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).

## Determinism and Consistent Replay

When you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.

As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](/oss/python/langgraph/functional-api#task) or [nodes](/oss/python/langgraph/graph-api#nodes).

To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

* **Avoid Repeating Work**: If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
* **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
* **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow's resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the [Common Pitfalls](/oss/python/langgraph/functional-api#common-pitfalls) section in the functional API, which shows
how to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph).

LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. The durability modes, from least to most durable, are as follows:

* [`"exit"`](#exit)
* [`"async"`](#async)
* [`"sync"`](#sync)

A higher durability mode adds more overhead to the workflow execution.

<Tip>
  **Added in v0.6.0**
  Use the `durability` parameter instead of `checkpoint_during` (deprecated in v0.6.0) for persistence policy management:

* `durability="async"` replaces `checkpoint_during=True`
  * `durability="exit"` replaces `checkpoint_during=False`

for persistence policy management, with the following mapping:

* `checkpoint_during=True` -> `durability="async"`
  * `checkpoint_during=False` -> `durability="exit"`
</Tip>

Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.

Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.

Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

You can specify the durability mode when calling any graph execution method:

## Using tasks in nodes

If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

<Tabs>
  <Tab title="Original">
    
  </Tab>

<Tab title="With task">
    
  </Tab>
</Tabs>

## Resuming Workflows

Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

* **Pausing and Resuming Workflows:** Use the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function to pause a workflow at specific points and the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive to resume it with updated state. See [**Interrupts**](/oss/python/langgraph/interrupts) for more details.
* **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](/oss/python/langgraph/use-functional-api#resuming-after-an-error) with the functional API).

## Starting Points for Resuming Workflows

* If you're using a [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph), the starting point is the beginning of the [**node**](/oss/python/langgraph/graph-api#nodes) where execution stopped.
* If you're making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.
  Inside the subgraph, the starting point will be the specific [**node**](/oss/python/langgraph/graph-api#nodes) where execution stopped.
* If you're using the Functional API, the starting point is the beginning of the [**entrypoint**](/oss/python/langgraph/functional-api#entrypoint) where execution stopped.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/durable-execution.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Using tasks in nodes

If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

<Tabs>
  <Tab title="Original">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="With task">
```

---

## Access custom state fields

**URL:** llms-txt#access-custom-state-fields

@tool
def get_user_preference(
    pref_name: str,
    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model
) -> str:
    """Get a user preference value."""
    preferences = runtime.state.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")
python wrap theme={null}
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  The `tool_runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `tool_runtime` is *not* included in the request.
</Warning>

**Updating state:**

Use [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:
```

---

## Subgraphs

**URL:** llms-txt#subgraphs

**Contents:**
- Setup
- Invoke a graph from a node

Source: https://docs.langchain.com/oss/python/langgraph/use-subgraphs

This guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/python/langgraph/graph-api#graphs) that is used as a [node](/oss/python/langgraph/graph-api#nodes) in another graph.

Subgraphs are useful for:

* Building [multi-agent systems](/oss/python/langchain/multi-agent)
* Re-using a set of nodes in multiple graphs
* Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

* [Invoke a graph from a node](#invoke-a-graph-from-a-node) — subgraphs are called from inside a node in the parent graph
* [Add a graph as a node](#add-a-graph-as-a-node) — a subgraph is added directly as a node in the parent and **shares [state keys](/oss/python/langgraph/graph-api#state)** with the parent

<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.

```python  theme={null}
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START

class SubgraphState(TypedDict):
    bar: str

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.
```

---

## Run a specific test function in a file

**URL:** llms-txt#run-a-specific-test-function-in-a-file

uv run --group test pytest tests/integration_tests/test_chat_models.py::test_chat_completions

---

## Chat models

**URL:** llms-txt#chat-models

**Contents:**
- Featured providers
- Chat Completions API
- All chat models

Source: https://docs.langchain.com/oss/python/integrations/chat/index

[Chat models](/oss/python/langchain/models) are language models that use a sequence of [messages](/oss/python/langchain/messages) as inputs and return messages as outputs <Tooltip tip="Models that do not include the prefix 'Chat' in their name or include 'LLM' as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.">(as opposed to traditional, plaintext LLMs)</Tooltip>.

## Featured providers

<Info>
  **While all these LangChain classes support the indicated advanced feature**, you may have to open the provider-specific documentation to learn which hosted models or backends support the feature.
</Info>

| Provider                                                                       | [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output/) | JSON mode | Local | [Multimodal](/oss/python/langchain/messages#multimodal) | Package                                                                                                                                                    |
| ------------------------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------- | --------- | ----- | ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)                     | ✅                                           | ✅                                                             | ❌         | ❌     | ✅                                                       | [`langchain-anthropic`](https://reference.langchain.com/python/integrations/langchain_anthropic/)                                                          |
| [`ChatOpenAI`](/oss/python/integrations/chat/openai)                           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/)                                                     |
| [`AzureChatOpenAI`](/oss/python/integrations/chat/azure_chat_openai)           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain_openai/AzureChatOpenAI/)                                                |
| [`ChatVertexAI`](/oss/python/integrations/chat/google_vertex_ai_palm)          | ✅                                           | ✅                                                             | ❌         | ❌     | ✅                                                       | [`langchain-google-vertexai`](https://reference.langchain.com/python/integrations/langchain_google_vertexai/)                                              |
| [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google_generative_ai) | ✅                                           | ✅                                                             | ❌         | ❌     | ✅                                                       | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai/)                                                    |
| [`ChatGroq`](/oss/python/integrations/chat/groq)                               | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq/)                                                                    |
| [`ChatBedrock`](/oss/python/integrations/chat/bedrock)                         | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       | [`langchain-aws`](https://reference.langchain.com/python/integrations/langchain_aws/)                                                                      |
| [`ChatHuggingFace`](/oss/python/integrations/chat/huggingface)                 | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       | [`langchain-huggingface`](https://reference.langchain.com/python/integrations/langchain_huggingface/)                                                      |
| [`ChatOllama`](/oss/python/integrations/chat/ollama)                           | ✅                                           | ✅                                                             | ✅         | ✅     | ❌                                                       | [`langchain-ollama`](https://reference.langchain.com/python/integrations/langchain_ollama/)                                                                |
| [`ChatXAI`](/oss/python/integrations/chat/xai)                                 | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       | [`langchain-xai`](https://reference.langchain.com/python/integrations/langchain_xai/)                                                                      |
| [`ChatNVIDIA`](/oss/python/integrations/chat/nvidia_ai_endpoints)              | ✅                                           | ✅                                                             | ✅         | ✅     | ✅                                                       | [`langchain-nvidia-ai-endpoints`](https://reference.langchain.com/python/integrations/langchain_nvidia_ai_endpoints/)                                      |
| [`ChatCohere`](/oss/python/integrations/chat/cohere)                           | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       | [`langchain-cohere`](https://reference.langchain.com/python/integrations/langchain_cohere/)                                                                |
| [`ChatMistralAI`](/oss/python/integrations/chat/mistralai)                     | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       | [`langchain-mistralai`](https://reference.langchain.com/python/integrations/langchain_mistralai/)                                                          |
| [`ChatTogether`](/oss/python/integrations/chat/together)                       | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       | [`langchain-together`](https://reference.langchain.com/python/integrations/langchain_together/)                                                            |
| [`ChatFireworks`](/oss/python/integrations/chat/fireworks)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       | [`langchain-fireworks`](https://reference.langchain.com/python/integrations/langchain_fireworks/)                                                          |
| [`ChatLlamaCpp`](/oss/python/integrations/chat/llamacpp)                       | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       | [`langchain-community`](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html)       |
| [`ChatDatabricks`](/oss/python/integrations/chat/databricks)                   | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.ChatDatabricks) |
| [`ChatPerplexity`](/oss/python/integrations/chat/perplexity)                   | ❌                                           | ✅                                                             | ✅         | ❌     | ✅                                                       | [`langchain-perplexity`](https://reference.langchain.com/python/integrations/langchain_perplexity/)                                                        |

## Chat Completions API

Certain model providers offer endpoints that are compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). In such case, you can use [`ChatOpenAI`](/oss/python/integrations/chat/openai) with a custom `base_url` to connect to these endpoints.

<Accordion title="Example: OpenRouter">
  To use OpenRouter, you will need to sign up for an account and obtain an [API key](https://openrouter.ai/docs/api-reference/authentication).

Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.

<Note>
    To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens),

1. Switch imports from `langchain_openai` to `langchain_deepseek`
    2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.
    3. Adjust reasoning parameters as needed under `extra_body`, e.g.:

This is a known limitation with `ChatOpenAI` and will be addressed in a future release.
  </Note>
</Accordion>

<Columns cols={3}>
  <Card title="Abso" icon="link" href="/oss/python/integrations/chat/abso" arrow="true" cta="View guide" />

<Card title="AI21 Labs" icon="link" href="/oss/python/integrations/chat/ai21" arrow="true" cta="View guide" />

<Card title="AI/ML API" icon="link" href="/oss/python/integrations/chat/aimlapi" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud PAI EAS" icon="link" href="/oss/python/integrations/chat/alibaba_cloud_pai_eas" arrow="true" cta="View guide" />

<Card title="Anthropic" icon="link" href="/oss/python/integrations/chat/anthropic" arrow="true" cta="View guide" />

<Card title="AzureAIChatCompletionsModel" icon="link" href="/oss/python/integrations/chat/azure_ai" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/python/integrations/chat/azure_chat_openai" arrow="true" cta="View guide" />

<Card title="Azure ML Endpoint" icon="link" href="/oss/python/integrations/chat/azureml_chat_endpoint" arrow="true" cta="View guide" />

<Card title="Baichuan Chat" icon="link" href="/oss/python/integrations/chat/baichuan" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/python/integrations/chat/baidu_qianfan_endpoint" arrow="true" cta="View guide" />

<Card title="Baseten" icon="link" href="/oss/python/integrations/chat/baseten" arrow="true" cta="View guide" />

<Card title="AWS Bedrock" icon="link" href="/oss/python/integrations/chat/bedrock" arrow="true" cta="View guide" />

<Card title="Cerebras" icon="link" href="/oss/python/integrations/chat/cerebras" arrow="true" cta="View guide" />

<Card title="CloudflareWorkersAI" icon="link" href="/oss/python/integrations/chat/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/python/integrations/chat/cohere" arrow="true" cta="View guide" />

<Card title="ContextualAI" icon="link" href="/oss/python/integrations/chat/contextual" arrow="true" cta="View guide" />

<Card title="Coze Chat" icon="link" href="/oss/python/integrations/chat/coze" arrow="true" cta="View guide" />

<Card title="Dappier AI" icon="link" href="/oss/python/integrations/chat/dappier" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/chat/databricks" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/python/integrations/chat/deepinfra" arrow="true" cta="View guide" />

<Card title="DeepSeek" icon="link" href="/oss/python/integrations/chat/deepseek" arrow="true" cta="View guide" />

<Card title="Eden AI" icon="link" href="/oss/python/integrations/chat/edenai" arrow="true" cta="View guide" />

<Card title="EverlyAI" icon="link" href="/oss/python/integrations/chat/everlyai" arrow="true" cta="View guide" />

<Card title="Featherless AI" icon="link" href="/oss/python/integrations/chat/featherless_ai" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/python/integrations/chat/fireworks" arrow="true" cta="View guide" />

<Card title="ChatFriendli" icon="link" href="/oss/python/integrations/chat/friendli" arrow="true" cta="View guide" />

<Card title="Goodfire" icon="link" href="/oss/python/integrations/chat/goodfire" arrow="true" cta="View guide" />

<Card title="Google Gemini" icon="link" href="/oss/python/integrations/chat/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Cloud Vertex AI" icon="link" href="/oss/python/integrations/chat/google_vertex_ai_palm" arrow="true" cta="View guide" />

<Card title="GPTRouter" icon="link" href="/oss/python/integrations/chat/gpt_router" arrow="true" cta="View guide" />

<Card title="DigitalOcean Gradient" icon="link" href="/oss/python/integrations/chat/gradientai" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/chat/greennode" arrow="true" cta="View guide" />

<Card title="Groq" icon="link" href="/oss/python/integrations/chat/groq" arrow="true" cta="View guide" />

<Card title="ChatHuggingFace" icon="link" href="/oss/python/integrations/chat/huggingface" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/chat/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="JinaChat" icon="link" href="/oss/python/integrations/chat/jinachat" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/chat/kinetica" arrow="true" cta="View guide" />

<Card title="Konko" icon="link" href="/oss/python/integrations/chat/konko" arrow="true" cta="View guide" />

<Card title="LiteLLM" icon="link" href="/oss/python/integrations/chat/litellm" arrow="true" cta="View guide" />

<Card title="Llama 2 Chat" icon="link" href="/oss/python/integrations/chat/llama2_chat" arrow="true" cta="View guide" />

<Card title="Llama API" icon="link" href="/oss/python/integrations/chat/llama_api" arrow="true" cta="View guide" />

<Card title="LlamaEdge" icon="link" href="/oss/python/integrations/chat/llama_edge" arrow="true" cta="View guide" />

<Card title="Llama.cpp" icon="link" href="/oss/python/integrations/chat/llamacpp" arrow="true" cta="View guide" />

<Card title="maritalk" icon="link" href="/oss/python/integrations/chat/maritalk" arrow="true" cta="View guide" />

<Card title="MiniMax" icon="link" href="/oss/python/integrations/chat/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/python/integrations/chat/mistralai" arrow="true" cta="View guide" />

<Card title="MLX" icon="link" href="/oss/python/integrations/chat/mlx" arrow="true" cta="View guide" />

<Card title="ModelScope" icon="link" href="/oss/python/integrations/chat/modelscope_chat_endpoint" arrow="true" cta="View guide" />

<Card title="Moonshot" icon="link" href="/oss/python/integrations/chat/moonshot" arrow="true" cta="View guide" />

<Card title="Naver" icon="link" href="/oss/python/integrations/chat/naver" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/chat/nebius" arrow="true" cta="View guide" />

<Card title="Netmind" icon="link" href="/oss/python/integrations/chat/netmind" arrow="true" cta="View guide" />

<Card title="NVIDIA AI Endpoints" icon="link" href="/oss/python/integrations/chat/nvidia_ai_endpoints" arrow="true" cta="View guide" />

<Card title="ChatOCIModelDeployment" icon="link" href="/oss/python/integrations/chat/oci_data_science" arrow="true" cta="View guide" />

<Card title="OCIGenAI" icon="link" href="/oss/python/integrations/chat/oci_generative_ai" arrow="true" cta="View guide" />

<Card title="ChatOctoAI" icon="link" href="/oss/python/integrations/chat/octoai" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/python/integrations/chat/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/python/integrations/chat/openai" arrow="true" cta="View guide" />

<Card title="Outlines" icon="link" href="/oss/python/integrations/chat/outlines" arrow="true" cta="View guide" />

<Card title="Perplexity" icon="link" href="/oss/python/integrations/chat/perplexity" arrow="true" cta="View guide" />

<Card title="Pipeshift" icon="link" href="/oss/python/integrations/chat/pipeshift" arrow="true" cta="View guide" />

<Card title="ChatPredictionGuard" icon="link" href="/oss/python/integrations/chat/predictionguard" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/python/integrations/chat/premai" arrow="true" cta="View guide" />

<Card title="PromptLayer ChatOpenAI" icon="link" href="/oss/python/integrations/chat/promptlayer_chatopenai" arrow="true" cta="View guide" />

<Card title="Qwen QwQ" icon="link" href="/oss/python/integrations/chat/qwq" arrow="true" cta="View guide" />

<Card title="Qwen" icon="link" href="/oss/python/integrations/chat/qwen" arrow="true" cta="View guide" />

<Card title="Reka" icon="link" href="/oss/python/integrations/chat/reka" arrow="true" cta="View guide" />

<Card title="RunPod Chat Model" icon="link" href="/oss/python/integrations/chat/runpod" arrow="true" cta="View guide" />

<Card title="SambaNovaCloud" icon="link" href="/oss/python/integrations/chat/sambanova" arrow="true" cta="View guide" />

<Card title="SambaStudio" icon="link" href="/oss/python/integrations/chat/sambastudio" arrow="true" cta="View guide" />

<Card title="ChatSeekrFlow" icon="link" href="/oss/python/integrations/chat/seekrflow" arrow="true" cta="View guide" />

<Card title="Snowflake Cortex" icon="link" href="/oss/python/integrations/chat/snowflake" arrow="true" cta="View guide" />

<Card title="SparkLLM Chat" icon="link" href="/oss/python/integrations/chat/sparkllm" arrow="true" cta="View guide" />

<Card title="Nebula (Symbl.ai)" icon="link" href="/oss/python/integrations/chat/symblai_nebula" arrow="true" cta="View guide" />

<Card title="Tencent Hunyuan" icon="link" href="/oss/python/integrations/chat/tencent_hunyuan" arrow="true" cta="View guide" />

<Card title="Together" icon="link" href="/oss/python/integrations/chat/together" arrow="true" cta="View guide" />

<Card title="Tongyi Qwen" icon="link" href="/oss/python/integrations/chat/tongyi" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/chat/upstage" arrow="true" cta="View guide" />

<Card title="vLLM Chat" icon="link" href="/oss/python/integrations/chat/vllm" arrow="true" cta="View guide" />

<Card title="Volc Engine Maas" icon="link" href="/oss/python/integrations/chat/volcengine_maas" arrow="true" cta="View guide" />

<Card title="ChatWriter" icon="link" href="/oss/python/integrations/chat/writer" arrow="true" cta="View guide" />

<Card title="xAI" icon="link" href="/oss/python/integrations/chat/xai" arrow="true" cta="View guide" />

<Card title="Xinference" icon="link" href="/oss/python/integrations/chat/xinference" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/python/integrations/chat/yandex" arrow="true" cta="View guide" />

<Card title="ChatYI" icon="link" href="/oss/python/integrations/chat/yi" arrow="true" cta="View guide" />

<Card title="Yuan2.0" icon="link" href="/oss/python/integrations/chat/yuan2" arrow="true" cta="View guide" />

<Card title="ZHIPU AI" icon="link" href="/oss/python/integrations/chat/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.

  <Note>
    To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens),

    1. Switch imports from `langchain_openai` to `langchain_deepseek`
    2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.
    3. Adjust reasoning parameters as needed under `extra_body`, e.g.:
```

---

## Create your underlying embeddings model

**URL:** llms-txt#create-your-underlying-embeddings-model

underlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.

---

## How to set up an application with requirements.txt

**URL:** llms-txt#how-to-set-up-an-application-with-requirements.txt

**Contents:**
- Specify Dependencies
- Specify Environment Variables
- Define Graphs

Source: https://docs.langchain.com/langsmith/setup-app-requirements-txt

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `requirements.txt` to specify project dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `requirements.txt` file:

Example file directory:

## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [LangGraph configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `requirements.txt` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## Build the graph

**URL:** llms-txt#build-the-graph

builder = StateGraph(MessagesState)
builder.add_node("autogen", call_autogen_agent)
builder.add_edge(START, "autogen")

---

## Generate query stats

**URL:** llms-txt#generate-query-stats

**Contents:**
  - Prerequisites
  - Running the query stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-query-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_query_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate query stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_query_stats.sh)

### Running the query stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, query\_stats.csv, has been created with LangSmith query statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-query-stats.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Target function for running the relevant step

**URL:** llms-txt#target-function-for-running-the-relevant-step

async def run_intent_classifier(inputs: dict) -> dict:
    # Note that we can access and run the intent_classifier node of our graph directly.
    command = await graph.nodes['intent_classifier'].ainvoke(inputs)
    return {"route": command.goto}

---

## Once these are set, start Claude Code, and events will be traced to LangSmith

**URL:** llms-txt#once-these-are-set,-start-claude-code,-and-events-will-be-traced-to-langsmith

<Note>
  Claude Code emits [open telemetry standard events](https://docs.claude.com/en/docs/claude-code/monitoring-usage#events) for monitoring usage, but this does not include the actual prompts and messages that go to the LLM.
</Note>

<Note>
  If you're self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append `/api/v1`. For example: `OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://ai-company.com/api/v1/otel/v1/claude_code`
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2a6e8481d9451ba0775d1dd175ad893e" alt="Claude Code Trace" data-og-width="3406" width="3406" data-og-height="1972" height="1972" data-path="langsmith/images/claude-code-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3a00d0f6cbcd7fb5b69121e4a69abb48 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8b9c25576a05a142506b30ce068bddd4 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5a3a902b15e71eb551eb394d5caf641 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cfa9e38c765e191020680f34010d4fb8 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ee44593b5516434c6ea05c1d49306d3d 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e037152a80fdb00763f7f64532521950 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-claude-code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Make conversations private

**URL:** llms-txt#make-conversations-private

**Contents:**
- Prerequisites
- 1. Add resource authorization

Source: https://docs.langchain.com/langsmith/resource-auth

In this tutorial, you will extend [the chatbot created in the last tutorial](/langsmith/set-up-custom-auth) to give each user their own private conversations. You'll add [resource-level access control](/langsmith/auth#single-owner-resources) so users can only see their own threads.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=8daa07dd8efb13d7f9d7aa35117b2138" alt="Authorization flow: after authentication, an authorization handler tags each resource with owner=user id and returns a filter so users only see their own threads." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authorization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=a617e2e62772c307a7b69a78e627ac40 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c978257553e23b1cb19348959ed72ffc 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=62370abd0f9ca0093d252fd9c1f7cda8 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6388bdfb9d7a61105c32683b8b750db1 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=95ff1d87241c5b94fd38483350159abb 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=bd94714727ddb6d79a49109d4ca7014f 2500w" />

Before you start this tutorial, ensure you have the [bot from the first tutorial](/langsmith/set-up-custom-auth) running without errors.

## 1. Add resource authorization

Recall that in the last tutorial, the [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object lets you register an [authentication function](/langsmith/auth#authentication), which LangSmith uses to validate the bearer tokens in incoming requests. Now you'll use it to register an **authorization** handler.

Authorization handlers are functions that run **after** authentication succeeds. These handlers can add [metadata](/langsmith/auth#filter-operations) to resources (like who owns them) and filter what each user can see.

Update your `src/security/auth.py` and add one authorization handler to run on every request:

```python {highlight={29-39}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

---

## results in `Weather(temperature=70.0, condition='sunny')`

**URL:** llms-txt#results-in-`weather(temperature=70.0,-condition='sunny')`

**Contents:**
- Standard content blocks

python  theme={null}
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5")
response = model.invoke("What's the capital of France?")

**Examples:**

Example 1 (unknown):
```unknown
**Error handling**: Control error handling via the `handle_errors` parameter to `ToolStrategy`:

* **Parsing errors**: Model generates data that doesn't match desired structure
* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas

***

## Standard content blocks

<Note>
  Content block support is currently only available for the following integrations:

  * [`langchain-anthropic`](https://pypi.org/project/langchain-anthropic/)
  * [`langchain-aws`](https://pypi.org/project/langchain-aws/)
  * [`langchain-openai`](https://pypi.org/project/langchain-openai/)
  * [`langchain-google-genai`](https://pypi.org/project/langchain-google-genai/)
  * [`langchain-ollama`](https://pypi.org/project/langchain-ollama/)

  Broader support for content blocks will be rolled out gradually across more providers.
</Note>

The new [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property introduces a standard representation for message content that works across providers:
```

---

## Define the function that determines whether to continue or not

**URL:** llms-txt#define-the-function-that-determines-whether-to-continue-or-not

def should_continue(state: State) -> Literal["tools", END]:
    messages = state['messages']
    last_message = messages[-1]

# If the LLM makes a tool call, then we route to the "tools" node
    if last_message.tool_calls:
        return "tools"

# Otherwise, we stop (reply to the user)
    return END

---

## INVALID_GRAPH_NODE_RETURN_VALUE

**URL:** llms-txt#invalid_graph_node_return_value

**Contents:**
- Troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE

A LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)
received a non-dict return type from a node. Here's an example:

Invoking the above graph will result in an error like this:

Nodes in your graph must return a dict containing one or more keys defined in your state.

The following may help resolve this error:

* If you have complex logic in your node, make sure all code paths return an appropriate dict for your defined state.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Invoking the above graph will result in an error like this:
```

Example 2 (unknown):
```unknown

```

---

## Service A: Create a span and propagate context to Service B

**URL:** llms-txt#service-a:-create-a-span-and-propagate-context-to-service-b

def service_a():
    with tracer.start_as_current_span("service_a_operation") as span:
        # Create a chain
        prompt = ChatPromptTemplate.from_template("Summarize: {text}")
        model = ChatOpenAI()
        chain = prompt | model

# Run the chain
        result = chain.invoke({"text": "OpenTelemetry is an observability framework"})

# Propagate context to Service B
        headers = {}
        inject(headers)  # Inject trace context into headers

# Call Service B with the trace context
        response = requests.post(
            "http://service-b.example.com/process",
            headers=headers,
            json={"summary": result.content}
        )
        return response.json()

---

## Claude Code Logs are translated to Spans by LangSmith

**URL:** llms-txt#claude-code-logs-are-translated-to-spans-by-langsmith

export OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://api.smith.langchain.com/otel/v1/claude_code

---

## Instrument AutoGen and OpenAI

**URL:** llms-txt#instrument-autogen-and-openai

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Graph state

**URL:** llms-txt#graph-state

class State(TypedDict):
    topic: str  # Report topic
    sections: list[Section]  # List of report sections
    completed_sections: Annotated[
        list, operator.add
    ]  # All workers write to this key in parallel
    final_report: str  # Final report

---

## Configure API URL and key

**URL:** llms-txt#configure-api-url-and-key

---

## ... Same as before

**URL:** llms-txt#...-same-as-before

---

## __interrupt__ contains the payload that was passed to interrupt()

**URL:** llms-txt#__interrupt__-contains-the-payload-that-was-passed-to-interrupt()

print(result["__interrupt__"])

---

## Wrap-style: dynamic prompts

**URL:** llms-txt#wrap-style:-dynamic-prompts

@dynamic_prompt
def personalized_prompt(request: ModelRequest) -> str:
    user_id = request.runtime.context.get("user_id", "guest")
    return f"You are a helpful assistant for user {user_id}. Be concise and friendly."

---

## Integration Packages

**URL:** llms-txt#integration-packages

**Contents:**
- Popular providers
- All providers

Source: https://docs.langchain.com/oss/javascript/integrations/providers/overview

LangChain integrates with a wide variety of chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

These providers have standalone `langchain-provider` packages for improved versioning, dependency management, and testing.

| Provider                                                                                 | Package                                                                                          | Downloads                                                                  | Latest                                                              |
| :--------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------ |
| [Anthropic](/oss/javascript/integrations/providers/anthropic)                            | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/anthropic)           | ![NPM](https://img.shields.io/npm/v/@langchain/anthropic)           |
| [Azure CosmosDB](/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql)         | [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb)           | ![Downloads](https://img.shields.io/npm/dm/@langchain/azure-cosmosdb)      | ![NPM](https://img.shields.io/npm/v/@langchain/azure-cosmosdb)      |
| [Cerebras](/oss/javascript/integrations/chat/cerebras)                                   | [`@langchain/cerebras`](https://www.npmjs.com/package/@langchain/cerebras)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/cerebras)            | ![NPM](https://img.shields.io/npm/v/@langchain/cerebras)            |
| Cloudflare                                                                               | [`@langchain/cloudflare`](https://www.npmjs.com/package/@langchain/cloudflare)                   | ![Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare)          | ![NPM](https://img.shields.io/npm/v/@langchain/cloudflare)          |
| [Cohere](/oss/javascript/integrations/chat/cohere)                                       | [`@langchain/cohere`](https://www.npmjs.com/package/@langchain/cohere)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/cohere)              | ![NPM](https://img.shields.io/npm/v/@langchain/cohere)              |
| [Exa](/oss/javascript/integrations/retrievers/exa)                                       | [`langchain-exa`](https://www.npmjs.com/package/@langchain/exa)                                  | ![Downloads](https://img.shields.io/npm/dm/@langchain/exa)                 | ![NPM](https://img.shields.io/npm/v/@langchain/exa)                 |
| [Google GenAI](/oss/javascript/integrations/chat/google_generative_ai)                   | [`@langchain/google-genai`](https://www.npmjs.com/package/@langchain/google-genai)               | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-genai)        | ![NPM](https://img.shields.io/npm/v/@langchain/google-genai)        |
| [Google VertexAI](/oss/javascript/integrations/chat/google_vertex_ai)                    | [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai)         | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai)     | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai)     |
| [Google VertexAI (Web Environments)](/oss/javascript/integrations/chat/google_vertex_ai) | [`@langchain/google-vertexai-web`](https://www.npmjs.com/package/@langchain/google-vertexai-web) | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai-web) | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai-web) |
| [Groq](/oss/javascript/integrations/chat/groq)                                           | [`@langchain/groq`](https://www.npmjs.com/package/@langchain/groq)                               | ![Downloads](https://img.shields.io/npm/dm/@langchain/groq)                | ![NPM](https://img.shields.io/npm/v/@langchain/groq)                |
| [MistralAI](/oss/javascript/integrations/chat/mistral)                                   | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/mistralai)           | ![NPM](https://img.shields.io/npm/v/@langchain/mistralai)           |
| [MongoDB](/oss/javascript/integrations/vectorstores/mongodb_atlas)                       | [`@langchain/mongodb`](https://www.npmjs.com/package/@langchain/mongodb)                         | ![Downloads](https://img.shields.io/npm/dm/@langchain/mongodb)             | ![NPM](https://img.shields.io/npm/v/@langchain/mongodb)             |
| [Nomic](/oss/javascript/integrations/text_embedding/nomic)                               | [`@langchain/nomic`](https://www.npmjs.com/package/@langchain/nomic)                             | ![Downloads](https://img.shields.io/npm/dm/@langchain/nomic)               | ![NPM](https://img.shields.io/npm/v/@langchain/nomic)               |
| [OpenAI](/oss/javascript/integrations/providers/openai)                                  | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/openai)              | ![NPM](https://img.shields.io/npm/v/@langchain/openai)              |
| [Pinecone](/oss/javascript/integrations/vectorstores/pinecone)                           | [`@langchain/pinecone`](https://www.npmjs.com/package/@langchain/pinecone)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/pinecone)            | ![NPM](https://img.shields.io/npm/v/@langchain/pinecone)            |
| [Qdrant](/oss/javascript/integrations/vectorstores/qdrant)                               | [`@langchain/qdrant`](https://www.npmjs.com/package/@langchain/qdrant)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/qdrant)              | ![NPM](https://img.shields.io/npm/v/@langchain/qdrant)              |
| [Tavily](/oss/javascript/integrations/retrievers/tavily)                                 | [`@langchain/tavily`](https://www.npmjs.com/package/@langchain/tavily)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/tavily)              | ![NPM](https://img.shields.io/npm/v/@langchain/tavily)              |
| [Weaviate](/oss/javascript/integrations/vectorstores/weaviate)                           | [`@langchain/weaviate`](https://www.npmjs.com/package/@langchain/weaviate)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/weaviate)            | ![NPM](https://img.shields.io/npm/v/@langchain/weaviate)            |
| [xAI](/oss/javascript/integrations/chat/xai)                                             | [`@langchain/xai`](https://www.npmjs.com/package/@langchain/xai)                                 | ![Downloads](https://img.shields.io/npm/dm/@langchain/xai)                 | ![NPM](https://img.shields.io/npm/v/@langchain/xai)                 |
| [Yandex](/oss/javascript/integrations/chat/yandex)                                       | [`@langchain/yandex`](https://www.npmjs.com/package/@langchain/yandex)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/yandex)              | ![NPM](https://img.shields.io/npm/v/@langchain/yandex)              |

[See all providers](/oss/javascript/integrations/providers/all_providers) or search for a provider using the search field.

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## - Age: 25

**URL:** llms-txt#--age:-25

---

## Often used with whisper parsers:

**URL:** llms-txt#often-used-with-whisper-parsers:

---

## Models

**URL:** llms-txt#models

**Contents:**
- Basic usage
  - Initialize a model
  - Key methods
- Parameters
- Invocation
  - Invoke
  - Stream

Source: https://docs.langchain.com/oss/python/langchain/models

[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.

In addition to text generation, many models support:

* <Icon icon="hammer" size={16} /> [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses.
* <Icon icon="shapes" size={16} /> [Structured output](#structured-outputs) - where the model's response is constrained to follow a defined format.
* <Icon icon="image" size={16} /> [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video.
* <Icon icon="brain" size={16} /> [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion.

Models are the reasoning engine of [agents](/oss/python/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.

The quality and capabilities of the model you choose directly impact your agent's reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.

LangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your case.

<Info>
  For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/python/integrations/chat).
</Info>

Models can be utilized in two ways:

1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/python/langchain/agents#model).
2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.

The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.

### Initialize a model

The easiest way to get started with a standalone model in LangChain is to use [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) to initialize one from a [chat model provider](/oss/python/integrations/chat) of your choice (examples below):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="AWS Bedrock">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)

</CodeGroup>
  </Tab>
</Tabs>

See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) for more detail, including information on how to pass model [parameters](#parameters).

<Card title="Invoke" href="#invoke" icon="paper-plane" arrow="true" horizontal>
  The model takes messages as input and outputs messages after generating a complete response.
</Card>

<Card title="Stream" href="#stream" icon="tower-broadcast" arrow="true" horizontal>
  Invoke the model, but stream the output as it is generated in real-time.
</Card>

<Card title="Batch" href="#batch" icon="grip" arrow="true" horizontal>
  Send multiple requests to a model in a batch for more efficient processing.
</Card>

<Info>
  In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/python/integrations/providers/overview) for details.
</Info>

A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:

<ParamField body="model" type="string" required>
  The name or identifier of the specific model you want to use with a provider.
</ParamField>

<ParamField body="api_key" type="string">
  The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip="A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.">environment variable</Tooltip>.
</ParamField>

<ParamField body="temperature" type="number">
  Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.
</ParamField>

<ParamField body="timeout" type="number">
  The maximum time (in seconds) to wait for a response from the model before canceling the request.
</ParamField>

<ParamField body="max_tokens" type="number">
  Limits the total number of <Tooltip tip="The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.">tokens</Tooltip> in the response, effectively controlling how long the output can be.
</ParamField>

<ParamField body="max_retries" type="number">
  The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.
</ParamField>

Using [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), pass these parameters as inline <Tooltip tip="Arbitrary keyword arguments" cta="Learn more" href="https://www.w3schools.com/python/python_args_kwargs.asp">`**kwargs`</Tooltip>:

<Info>
  Each chat model integration may have additional params used to control provider-specific functionality. For example, [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.

To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/python/integrations/chat) page.
</Info>

A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.

The most straightforward way to call a model is to use [`invoke()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.invoke) with a single message or a list of messages.

A list of messages can be provided to a model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation. See the [messages](/oss/python/langchain/messages) guide for more detail on roles, types, and content.

Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.

Calling [`stream()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.stream) returns an <Tooltip tip="An object that progressively provides access to each item of a collection, in order.">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

As opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

```python Construct an AIMessage theme={null}
full = None  # None | AIMessageChunk
for chunk in model.stream("What color is the sky?"):
    full = chunk if full is None else full + chunk
    print(full.text)

**Examples:**

Example 1 (unknown):
```unknown
<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Tab>

  <Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)
```

Example 4 (unknown):
```unknown
<CodeGroup>
```

---

## Add edges

**URL:** llms-txt#add-edges

**Contents:**
- Create branches
  - Run graph nodes in parallel
  - Defer node execution
  - Conditional branching
- Map-Reduce and the Send API

builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")
python  theme={null}
builder = StateGraph(State).add_sequence([step_1, step_2, step_3])
builder.add_edge(START, "step_1")
python  theme={null}
  from typing_extensions import TypedDict

class State(TypedDict):
      value_1: str
      value_2: int
  python  theme={null}
  def step_1(state: State):
      return {"value_1": "a"}

def step_2(state: State):
      current_value_1 = state["value_1"]
      return {"value_1": f"{current_value_1} b"}

def step_3(state: State):
      return {"value_2": 10}
  python  theme={null}
  from langgraph.graph import START, StateGraph

builder = StateGraph(State)

# Add nodes
  builder.add_node(step_1)
  builder.add_node(step_2)
  builder.add_node(step_3)

# Add edges
  builder.add_edge(START, "step_1")
  builder.add_edge("step_1", "step_2")
  builder.add_edge("step_2", "step_3")
  python  theme={null}
    builder.add_node("my_node", step_1)
    python  theme={null}
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  graph.invoke({"value_1": "c"})
  
  {'value_1': 'a b', 'value_2': 10}
  python  theme={null}
    builder = StateGraph(State).add_sequence([step_1, step_2, step_3])  # [!code highlight]
    builder.add_edge(START, "step_1")

graph = builder.compile()

graph.invoke({"value_1": "c"})
    python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []}, {"configurable": {"thread_id": "foo"}})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "D" to ['A', 'B', 'C']
python  theme={null}
  graph.invoke({"value_1": "c"}, {"configurable": {"max_concurrency": 10}})
  python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def b_2(state: State):
    print(f'Adding "B_2" to {state["aggregate"]}')
    return {"aggregate": ["B_2"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(b_2)
builder.add_node(c)
builder.add_node(d, defer=True)  # [!code highlight]
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "b_2")
builder.add_edge("b_2", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "B_2" to ['A', 'B', 'C']
Adding "D" to ['A', 'B', 'C', 'B_2']
python  theme={null}
import operator
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    aggregate: Annotated[list, operator.add]
    # Add a key to the state. We will set this key to determine
    # how we branch.
    which: str

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"], "which": "c"}  # [!code highlight]

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_edge(START, "a")
builder.add_edge("b", END)
builder.add_edge("c", END)

def conditional_edge(state: State) -> Literal["b", "c"]:
    # Fill in arbitrary logic here that uses the state
    # to determine the next node
    return state["which"]

builder.add_conditional_edges("a", conditional_edge)  # [!code highlight]

graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
result = graph.invoke({"aggregate": []})
print(result)

Adding "A" to []
Adding "C" to ['A']
{'aggregate': ['A', 'C'], 'which': 'c'}
python  theme={null}
  def route_bc_or_cd(state: State) -> Sequence[str]:
  if state["which"] == "cd":
  return ["c", "d"]
  return ["b", "c"]
  python  theme={null}
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from typing_extensions import TypedDict, Annotated
import operator

class OverallState(TypedDict):
    topic: str
    subjects: list[str]
    jokes: Annotated[list[str], operator.add]
    best_selected_joke: str

def generate_topics(state: OverallState):
    return {"subjects": ["lions", "elephants", "penguins"]}

def generate_joke(state: OverallState):
    joke_map = {
        "lions": "Why don't lions like fast food? Because they can't catch it!",
        "elephants": "Why don't elephants use computers? They're afraid of the mouse!",
        "penguins": "Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice."
    }
    return {"jokes": [joke_map[state["subject"]]]}

def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

def best_joke(state: OverallState):
    return {"best_selected_joke": "penguins"}

builder = StateGraph(OverallState)
builder.add_node("generate_topics", generate_topics)
builder.add_node("generate_joke", generate_joke)
builder.add_node("best_joke", best_joke)
builder.add_edge(START, "generate_topics")
builder.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
builder.add_edge("generate_joke", "best_joke")
builder.add_edge("best_joke", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can also use the built-in shorthand `.add_sequence`:
```

Example 2 (unknown):
```unknown
<Accordion title="Why split application steps into a sequence with LangGraph?">
  LangGraph makes it easy to add an underlying persistence layer to your application.
  This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:

  * How state updates are [checkpointed](/oss/python/langgraph/persistence)
  * How interruptions are resumed in [human-in-the-loop](/oss/python/langgraph/interrupts) workflows
  * How we can "rewind" and branch-off executions using LangGraph's [time travel](/oss/python/langgraph/use-time-travel) features

  They also determine how execution steps are [streamed](/oss/python/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio).

  Let's demonstrate an end-to-end example. We will create a sequence of three steps:

  1. Populate a value in a key of the state
  2. Update the same value
  3. Populate a different value

  Let's first define our [state](/oss/python/langgraph/graph-api#state). This governs the [schema of the graph](/oss/python/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.

  In our case, we will just keep track of two values:
```

Example 3 (unknown):
```unknown
Our [nodes](/oss/python/langgraph/graph-api#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state:
```

Example 4 (unknown):
```unknown
<Note>
    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.

    By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](/oss/python/langgraph/graph-api#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.
  </Note>

  Finally, we define the graph. We use [StateGraph](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state.

  We will then use [`add_node`](/oss/python/langgraph/graph-api#messagesstate) and [`add_edge`](/oss/python/langgraph/graph-api#edges) to populate our graph and define its control flow.
```

---

## It's not something you will have in your actual code.

**URL:** llms-txt#it's-not-something-you-will-have-in-your-actual-code.

@task()
def get_info():
    """
    Simulates a task that fails once before succeeding.
    Raises an exception on the first attempt, then returns "OK" on subsequent tries.
    """
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError("Failure")  # Simulate a failure on the first attempt
    return "OK"

---

## Then, update the runs with their end times and any outputs

**URL:** llms-txt#then,-update-the-runs-with-their-end-times-and-any-outputs

child_run_update = {
    **child_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"answer": "Paris is the capital of France."},
}

parent_run_update = {
    **parent_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"summary": "Discussion about France, including its capital."},
}

patches = [parent_run_update, child_run_update]
batch_ingest_runs(api_url, api_key, patches=patches)

---

## How to run an evaluation locally (Python only)

**URL:** llms-txt#how-to-run-an-evaluation-locally-(python-only)

**Contents:**
- Example

Source: https://docs.langchain.com/langsmith/local

Sometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if you're quickly iterating on a prompt and want to smoke test it on a few examples, or if you're validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.

You can do this by using the LangSmith Python SDK and passing `upload_results=False` to `evaluate()` / `aevaluate()`.

This will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.

Let's take a look at an example:

Requires `langsmith>=0.2.0`. Example also uses `pandas`.

```python  theme={null}
from langsmith import Client

---

## [{"type": "text", "text": "The sky is typically blue..."}]

**URL:** llms-txt#[{"type":-"text",-"text":-"the-sky-is-typically-blue..."}]

**Contents:**
  - Batch
- Tool calling
- Structured outputs
- Supported models
- Advanced topics
  - Multimodal

python  theme={null}
    async for event in model.astream_events("Hello"):

if event["event"] == "on_chat_model_start":
            print(f"Input: {event['data']['input']}")

elif event["event"] == "on_chat_model_stream":
            print(f"Token: {event['data']['chunk'].text}")

elif event["event"] == "on_chat_model_end":
            print(f"Full message: {event['data']['output'].text}")

else:
            pass
    txt  theme={null}
    Input: Hello
    Token: Hi
    Token:  there
    Token: !
    Token:  How
    Token:  can
    Token:  I
    ...
    Full message: Hi there! How can I help today?
    python Batch theme={null}
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
python Yield batch responses upon completion theme={null}
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
python Batch with max concurrency theme={null}
  model.batch(
      list_of_inputs,
      config={
          'max_concurrency': 5,  # Limit to 5 parallel calls
      }
  )
  python Binding user tools theme={null}
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."

model_with_tools = model.bind_tools([get_weather])  # [!code highlight]

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
python Tool execution loop theme={null}
    # Bind (potentially multiple) tools to the model
    model_with_tools = model.bind_tools([get_weather])

# Step 1: Model generates tool calls
    messages = [{"role": "user", "content": "What's the weather in Boston?"}]
    ai_msg = model_with_tools.invoke(messages)
    messages.append(ai_msg)

# Step 2: Execute tools and collect results
    for tool_call in ai_msg.tool_calls:
        # Execute the tool with the generated arguments
        tool_result = get_weather.invoke(tool_call)
        messages.append(tool_result)

# Step 3: Pass results back to model for final response
    final_response = model_with_tools.invoke(messages)
    print(final_response.text)
    # "The current weather in Boston is 72°F and sunny."
    python Force use of any tool theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="any")
      python Force use of specific tools theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
      python Parallel tool calls theme={null}
    model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
        "What's the weather in Boston and Tokyo?"
    )

# The model may generate multiple tool calls
    print(response.tool_calls)
    # [
    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
    # ]

# Execute all tools (can be done in parallel with async)
    results = []
    for tool_call in response.tool_calls:
        if tool_call['name'] == 'get_weather':
            result = get_weather.invoke(tool_call)
        ...
        results.append(result)
    python  theme={null}
      model.bind_tools([get_weather], parallel_tool_calls=False)
      python Streaming tool calls theme={null}
    for chunk in model_with_tools.stream(
        "What's the weather in Boston and Tokyo?"
    ):
        # Tool call chunks arrive progressively
        for tool_chunk in chunk.tool_call_chunks:
            if name := tool_chunk.get("name"):
                print(f"Tool: {name}")
            if id_ := tool_chunk.get("id"):
                print(f"ID: {id_}")
            if args := tool_chunk.get("args"):
                print(f"Args: {args}")

# Output:
    # Tool: get_weather
    # ID: call_SvMlU1TVIZugrFLckFE2ceRE
    # Args: {"lo
    # Args: catio
    # Args: n": "B
    # Args: osto
    # Args: n"}
    # Tool: get_weather
    # ID: call_QMZdy6qInx13oWKE7KhuhOLR
    # Args: {"lo
    # Args: catio
    # Args: n": "T
    # Args: okyo
    # Args: "}
    python Accumulate tool calls theme={null}
    gathered = None
    for chunk in model_with_tools.stream("What's the weather in Boston?"):
        gathered = chunk if gathered is None else gathered + chunk
        print(gathered.tool_calls)
    python  theme={null}
    from pydantic import BaseModel, Field

class Movie(BaseModel):
        """A movie with details."""
        title: str = Field(..., description="The title of the movie")
        year: int = Field(..., description="The year the movie was released")
        director: str = Field(..., description="The director of the movie")
        rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
    python  theme={null}
    from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
        """A movie with details."""
        title: Annotated[str, ..., "The title of the movie"]
        year: Annotated[int, ..., "The year the movie was released"]
        director: Annotated[str, ..., "The director of the movie"]
        rating: Annotated[float, ..., "The movie's rating out of 10"]

model_with_structure = model.with_structured_output(MovieDict)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
    python  theme={null}
    import json

json_schema = {
        "title": "Movie",
        "description": "A movie with details",
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the movie"
            },
            "year": {
                "type": "integer",
                "description": "The year the movie was released"
            },
            "director": {
                "type": "string",
                "description": "The director of the movie"
            },
            "rating": {
                "type": "number",
                "description": "The movie's rating out of 10"
            }
        },
        "required": ["title", "year", "director", "rating"]
    }

model_with_structure = model.with_structured_output(
        json_schema,
        method="json_schema",
    )
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, ...}
    python  theme={null}
  from pydantic import BaseModel, Field

class Movie(BaseModel):
      """A movie with details."""
      title: str = Field(..., description="The title of the movie")
      year: int = Field(..., description="The year the movie was released")
      director: str = Field(..., description="The director of the movie")
      rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
  response = model_with_structure.invoke("Provide details about the movie Inception")
  response
  # {
  #     "raw": AIMessage(...),
  #     "parsed": Movie(title=..., year=..., ...),
  #     "parsing_error": None,
  # }
  python Pydantic BaseModel theme={null}
    from pydantic import BaseModel, Field

class Actor(BaseModel):
        name: str
        role: str

class MovieDetails(BaseModel):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: float | None = Field(None, description="Budget in millions USD")

model_with_structure = model.with_structured_output(MovieDetails)
    python TypedDict theme={null}
    from typing_extensions import Annotated, TypedDict

class Actor(TypedDict):
        name: str
        role: str

class MovieDetails(TypedDict):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: Annotated[float | None, ..., "Budget in millions USD"]

model_with_structure = model.with_structured_output(MovieDetails)
    python Multimodal output theme={null}
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)

**Examples:**

Example 1 (unknown):
```unknown
The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) - for example, it can be aggregated into a message history and passed back to the model as conversational context.

<Warning>
  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.
</Warning>

<Accordion title="Advanced streaming topics">
  <Accordion title="&#x22;Auto-streaming&#x22; chat models">
    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.

    In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

    #### How it works

    When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) events in LangChain's callback system.

    Callback events allow LangGraph `stream()` and [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) to surface the chat model's output in real-time.
  </Accordion>

  <Accordion title="Streaming events">
    LangChain chat models can also stream semantic events using [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events).

    This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
<Tip>
      See the [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) reference for event types and other details.
    </Tip>
  </Accordion>
</Accordion>

### Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:
```

Example 4 (unknown):
```unknown
<Note>
  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch), which parallelizes model calls client-side.

  It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://docs.claude.com/en/docs/build-with-claude/batch-processing#message-batches-api).
</Note>

By default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):
```

---

## Define the function that calls the model

**URL:** llms-txt#define-the-function-that-calls-the-model

def call_model(state: State):
    messages = state['messages']
    response = model.invoke(messages)

# We return a list, because this will get added to the existing list
    return {"messages": [response]}

---

## Build a semantic search engine with LangChain

**URL:** llms-txt#build-a-semantic-search-engine-with-langchain

**Contents:**
- Overview
  - Concepts
- Setup
  - Installation
  - LangSmith
- 1. Documents and Document Loaders
  - Loading documents
  - Splitting
- 2. Embeddings
- 3. Vector stores

Source: https://docs.langchain.com/oss/python/langchain/knowledge-base

This tutorial will familiarize you with LangChain's [document loader](/oss/python/langchain/retrieval#document-loaders), [embedding](/oss/python/langchain/retrieval#embedding-models), and [vector store](/oss/python/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/python/langchain/retrieval).

Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.

This guide focuses on retrieval of text data. We will cover the following concepts:

* [Documents and document loaders](/oss/python/integrations/document_loaders);
* [Text splitters](/oss/python/integrations/splitters);
* [Embeddings](/oss/python/integrations/text_embedding);
* [Vector stores](/oss/python/integrations/vectorstores) and [retrievers](/oss/python/integrations/retrievers).

This tutorial requires the `langchain-community` and `pypdf` packages:

For more details, see our [Installation guide](/oss/python/langchain/install).

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

Or, if in a notebook, you can set them with:

## 1. Documents and Document Loaders

LangChain implements a [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `page_content`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object often represents a chunk of a larger document.

We can generate sample documents when desired:

However, the LangChain ecosystem implements [document loaders](/oss/python/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/python/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.

### Loading documents

Let's load a PDF into a sequence of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/python/integrations/document_loaders/#pdfs).

`PyPDFLoader` loads one [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object per PDF page. For each, we can easily access:

* The string content of the page;
* Metadata containing the file name and page number.

For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not "washed out" by surrounding text.

We can use [text splitters](/oss/python/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the
`RecursiveCharacterTextSplitter`,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.

We set `add_start_index=True` so that the character index where each
split Document starts within the initial Document is preserved as
metadata attribute “start\_index”.

Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/python/langchain/retrieval#embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.

LangChain supports embeddings from [dozens of providers](/oss/python/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:

<Tabs>
  <Tab title="OpenAI">

<Tab title="Google Gemini">

<Tab title="Google Vertex">

<Tab title="HuggingFace">

<Tab title="MistralAI">

<Tab title="Voyage AI">

<Tab title="IBM watsonx">

Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.

LangChain [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects contain methods for adding text and [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/python/langchain/retrieval#embedding_models) models, which determine how text data is translated to numeric vectors.

LangChain includes a suite of [integrations](/oss/python/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/python/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:

<Tabs>
  <Tab title="In-memory">

<Tab title="AstraDB">

<Tab title="MongoDB">

<Tab title="PGVector">

<Tab title="PGVectorStore">

<Tab title="Pinecone">

Having instantiated our vector store, we can now index the documents.

Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/python/integrations/vectorstores) for more detail.

Once we've instantiated a [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) that contains documents, we can query it. [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) includes methods for querying:

* Synchronously and asynchronously;
* By string query and by vector;
* With and without returning similarity scores;
* By similarity and @\[maximum marginal relevance]\[VectorStore.max\_marginal\_relevance\_search] (to balance similarity with query to diversity in retrieved results).

The methods will generally include a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects in their outputs.

Embeddings typically represent text as a "dense" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.

Return documents based on similarity to a string query:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/python/langchain/install).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:
```

Example 3 (unknown):
```unknown
Or, if in a notebook, you can set them with:
```

Example 4 (unknown):
```unknown
## 1. Documents and Document Loaders

LangChain implements a [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `page_content`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object often represents a chunk of a larger document.

We can generate sample documents when desired:
```

---

## get a state snapshot for a specific checkpoint_id

**URL:** llms-txt#get-a-state-snapshot-for-a-specific-checkpoint_id

**Contents:**
  - Get state history
  - Replay
  - Update state
- Memory Store
  - Basic Usage
  - Semantic Search

config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}}
graph.get_state(config)

StateSnapshot(
    values={'foo': 'b', 'bar': ['a', 'b']},
    next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
    created_at='2024-08-29T19:19:38.821749+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()
)
python  theme={null}
config = {"configurable": {"thread_id": "1"}}
list(graph.get_state_history(config))

[
    StateSnapshot(
        values={'foo': 'b', 'bar': ['a', 'b']},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
        created_at='2024-08-29T19:19:38.821749+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        tasks=(),
    ),
    StateSnapshot(
        values={'foo': 'a', 'bar': ['a']},
        next=('node_b',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},
        created_at='2024-08-29T19:19:38.819946+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'foo': '', 'bar': []},
        next=('node_a',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        metadata={'source': 'loop', 'writes': None, 'step': 0},
        created_at='2024-08-29T19:19:38.817813+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'bar': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},
        created_at='2024-08-29T19:19:38.816205+00:00',
        parent_config=None,
        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),
    )
]
python  theme={null}
config = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}
graph.invoke(None, config=config)
python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

{"foo": 1, "bar": ["a"]}
python  theme={null}
graph.update_state(config, {"foo": 2, "bar": ["b"]})

{"foo": 2, "bar": ["a", "b"]}
python  theme={null}
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()
python  theme={null}
user_id = "1"
namespace_for_memory = (user_id, "memories")
python  theme={null}
memory_id = str(uuid.uuid4())
memory = {"food_preference" : "I like pizza"}
in_memory_store.put(namespace_for_memory, memory_id, memory)
python  theme={null}
memories = in_memory_store.search(namespace_for_memory)
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
from langchain.embeddings import init_embeddings

store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # Embedding provider
        "dims": 1536,                              # Embedding dimensions
        "fields": ["food_preference", "$"]              # Fields to embed
    }
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
In our example, the output of `get_state` will look like this:
```

Example 2 (unknown):
```unknown
### Get state history

You can get the full history of the graph execution for a given thread by calling [`graph.get_state_history(config)`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history). This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.
```

Example 3 (unknown):
```unknown
In our example, the output of [`get_state_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history) will look like this:
```

Example 4 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=38ffff52be4d8806b287836295a3c058" alt="State" data-og-width="2692" width="2692" data-og-height="1056" height="1056" data-path="oss/images/get_state.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e932acac5021614d0eb99b90e54be004 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=2eaf153fd49ba728e1d679c12bb44b6f 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=0ac091c7dbe8b1f0acff97615a3683ee 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9921a482f1c4f86316fca23a5150b153 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9412cd906f6d67a9fe1f50a5d4f4c674 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ccc5118ed85926bda3715c81ce728fcc 2500w" />

### Replay

It's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will *re-play* the previously executed steps *before* a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps *after* the checkpoint.

* `thread_id` is the ID of a thread.
* `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:
```

---

## LangGraph runtime

**URL:** llms-txt#langgraph-runtime

**Contents:**
- Overview
- Actors
- Channels
- Examples
- High-level API

Source: https://docs.langchain.com/oss/python/langgraph/pregel

[`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) implements LangGraph's runtime, managing the execution of LangGraph applications.

Compiling a [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or creating an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) instance that can be invoked with input.

This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.

> **Note:** The [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) runtime is named after [Google's Pregel algorithm](https://research.google/pubs/pub37252/), which describes an efficient method for large-scale parallel computation using graphs.

In LangGraph, Pregel combines [**actors**](https://en.wikipedia.org/wiki/Actor_model) and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/**Bulk Synchronous Parallel** model.

Each step consists of three phases:

* **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.
* **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
* **Update**: Update the channels with the values written by the **actors** in this step.

Repeat until no **actors** are selected for execution, or a maximum number of steps is reached.

An **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain's Runnable interface.

Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

* [`LastValue`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.LastValue): The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
* [`Topic`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.Topic): A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.
* [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate): stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`

While most users will interact with Pregel through the [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) API or the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator, it is possible to interact with Pregel directly.

Below are a few different examples to give you a sense of the Pregel API.

<Tabs>
  <Tab title="Single node">

<Tab title="Multiple nodes">

<Tab title="BinaryOperatorAggregate">
    This example demonstrates how to use the [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate) channel to implement a reducer.

<Tab title="Cycle">
    This example demonstrates how to introduce a cycle in the graph, by having
    a chain write to a channel it subscribes to. Execution will continue
    until a `None` value is written to the channel.

LangGraph provides two high-level APIs for creating a Pregel application: the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).

<Tabs>
  <Tab title="StateGraph (Graph API)">
    The [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.

You will see something like this:

You should see something like this

<Tab title="Functional API">
    In the [Functional API](/oss/python/langgraph/functional-api), you can use an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/pregel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Multiple nodes">
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Topic">
```

---

## Set environment variables for LangChain

**URL:** llms-txt#set-environment-variables-for-langchain

os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
os.environ["LANGSMITH_TRACING"] = "true"

---

## Note that providers implement different scores; the score here

**URL:** llms-txt#note-that-providers-implement-different-scores;-the-score-here

---

## Monorepo support

**URL:** llms-txt#monorepo-support

**Contents:**
- Repository Structure
- LangGraph.json configuration
- Building the application
- Tips and Best Practices

Source: https://docs.langchain.com/langsmith/monorepo-support

LangSmith supports deploying agents from monorepo setups where your agent code may depend on shared packages located elsewhere in the repository. This guide shows how to structure your monorepo and configure your `langgraph.json` file to work with shared dependencies.

## Repository Structure

For complete working examples, see:

* [Python monorepo example](https://github.com/langchain-ai/python-langraph-monorepo-example)
* [JS monorepo example](https://github.com/langchain-ai/js-langgraph-monorepo-example)

## LangGraph.json configuration

Place the langgraph.json file in your agent’s directory (not in the monorepo root). Ensure the file follows the required structure:

The Python implementation automatically handles packages in parent directories by:

* Detecting relative paths that start with `"."`.
* Adding parent directories to the Docker build context as needed.
* Supporting both real packages (with `pyproject.toml`/`setup.py`) and simple Python modules.

For JavaScript monorepos:

* Shared workspace dependencies are resolved automatically by your package manager.
* Your `package.json` should reference shared packages using workspace syntax.

Example `package.json` in the agent directory:

## Building the application

Run `langgraph build`:

The Python build process:

1. Automatically detects relative dependency paths.
2. Copies shared packages into the Docker build context.
3. Installs all dependencies in the correct order.
4. No special flags or commands required.

The JavaScript build process:

1. Uses the directory you called `langgraph build` from (the monorepo root in this case) as the build context.
2. Automatically detects your package manager (yarn, npm, pnpm, bun)
3. Runs the appropriate install command.
   * If you have one or both of a custom build/install command it will run from the directory you called `langgraph build` from.
   * Otherwise, it will run from the directory where the `langgraph.json` file is located.
4. Optionally runs a custom build command from the directory where the `langgraph.json` file is located (only if you pass the `--build-command` flag).

## Tips and Best Practices

1. **Keep agent configs in agent directories**: Place `langgraph.json` files in the specific agent directories, not at the monorepo root. This allows you to support multiple agents in the same monorepo, without having to deploy them all in the same LangSmith deployment.

2. **Use relative paths for Python**: For Python monorepos, use relative paths like `"../../shared-package"` in the `dependencies` array.

3. **Leverage workspace features for JS**: For JavaScript/TypeScript, use your package manager's workspace features to manage dependencies between packages.

4. **Test locally first**: Always test your build locally before deploying to ensure all dependencies are correctly resolved.

5. **Environment variables**: Keep environment files (`.env`) in your agent directories for environment-specific configuration.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/monorepo-support.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## LangGraph.json configuration

Place the langgraph.json file in your agent’s directory (not in the monorepo root). Ensure the file follows the required structure:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

The Python implementation automatically handles packages in parent directories by:

* Detecting relative paths that start with `"."`.
* Adding parent directories to the Docker build context as needed.
* Supporting both real packages (with `pyproject.toml`/`setup.py`) and simple Python modules.

For JavaScript monorepos:

* Shared workspace dependencies are resolved automatically by your package manager.
* Your `package.json` should reference shared packages using workspace syntax.

Example `package.json` in the agent directory:
```

---

## How to customize the Dockerfile

**URL:** llms-txt#how-to-customize-the-dockerfile

Source: https://docs.langchain.com/langsmith/custom-docker

Users can add an array of additional lines to add to the Dockerfile following the import from the parent LangGraph image. In order to do this, you simply need to modify your `langgraph.json` file by passing in the commands you want run to the `dockerfile_lines` key. For example, if we wanted to use `Pillow` in our graph you would need to add the following dependencies:

This would install the system packages required to use Pillow if we were working with `jpeg` or `png` image formats.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-docker.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:agent",
    },
    "env": "./.env",
    "dockerfile_lines": [
        "RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev libpng-dev",
        "RUN pip install Pillow"
    ]
}
```

---

## - /draft.txt

**URL:** llms-txt#--/draft.txt

---

## Dynamic few shot example selection

**URL:** llms-txt#dynamic-few-shot-example-selection

**Contents:**
- Pre-conditions
- Index your dataset for few shot search
- Test search quality in the few shot playground
- Adding few shot search to your application
  - Code snippets

Source: https://docs.langchain.com/langsmith/index-datasets-for-dynamic-few-shot-example-selection

<Note>
  This feature is in open beta. It is only available to paid team plans. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) if you have questions about enablement.
</Note>

Configure your datasets so that you can search for few shot examples based on an incoming request.

1. Your dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)
2. You must have an input schema defined for your dataset. See our docs on setting up schema validation [in our UI](/langsmith/manage-datasets-in-application#dataset-schema-validation) for details.
3. You must be on a paid team plan (e.g. Plus plan)
4. You must be on LangSmith cloud

## Index your dataset for few shot search

Navigate to the datasets UI, and click the new `Few-Shot search` tab. Hit the `Start sync` button, which will create a new index on your dataset to make it searchable.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7bc50f1a3d4ea7b9e7458f3ed9770179" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-tab-unsynced.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c2ee3688a24b8d142d4a5dd89c8797c1 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=44a4d8b59948893b4bd20d4c1b62198c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=16a303eab8e2aede574c12353f782789 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fbcf4566ec48c219bb7828bdc2cfe6b9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=15da2b6b3e4a29ac3a2b4e139e2cf345 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6bd99520313da2fedbbbaf388c956ba7 2500w" />

By default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under `Few-shot index` on the lefthand side of the screen in the next section.

## Test search quality in the few shot playground

Now that you have turned on indexing for your dataset, you will see the new few shot playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2b714b5a281ab8f39761566afd452f7" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-synced-empty-state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b8e4e648ce36f12a141f98815fd2e63d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=49044eef3af8c08b60bcfeabadde6d16 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b0fb897716bfac9521d0490c11eb006e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f21c2ad1fc5177af66c5f0c5ac59f660 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dd9e2f9aa42366063ca14ad2b365e6fc 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=72158d44bcf916f2548bc5c7a6dabbce 2500w" />

You can type in a sample input, and check which results would be returned by our search API.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f64d39010f85b68d4d9ae3febc2b59d7" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-search-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cbbd01172c4514eb3d775a4c0f3ca2e4 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a15ea167365f8d9105ab5aab50ed241 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=94ac208df6e860b9c194a646e3183779 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec68d9679e4cc86eebe352a9364e8894 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5650c218963b5e8bd8d56e0bf886cd3e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89e90c66c065bee2a728cd5dad69432a 2500w" />

Each result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.

<Note>
  Search uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.
</Note>

## Adding few shot search to your application

Click the `Get Code Snippet` button in the previous diagram, you'll be taken to a screen that has code snippets from our LangSmith SDK in different languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=015d9416ef4f2708a6f2dfedceb1ea07" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-code-snippet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a5b15cf813569f4f218d972f2dbe89a5 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=231a3aad9937aab1fa92e65839bb5869 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8aa86a24694342bca8ab387c83338afe 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d92247c22e6fb2fd85e80713616b7fba 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=28f6e81c67c52687cd24431b83145675 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=aeafca83ae2489c43aa38d24165e5d62 2500w" />

For code samples on using few shot search in LangChain python applications, please see our [how-to guide in the LangChain docs](https://python.langchain.com/v0.2/docs/how_to/example_selectors_langsmith/).

<Note>
  Please ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43
</Note>

For copy and paste convenience, you can find the similar code snippets to the ones shown in the screenshot above here:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/index-datasets-for-dynamic-few-shot-example-selection.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

---

## Keep our test users from the previous tutorial

**URL:** llms-txt#keep-our-test-users-from-the-previous-tutorial

**Contents:**
- 2. Test private conversations

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Our authentication handler from the previous tutorial."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,  # Contains info about the current user
    value: dict,  # The resource being created/accessed
):
    """Make resources private to their creator."""
    # Examples:
    # ctx: AuthContext(
    #     permissions=[],
    #     user=ProxyUser(
    #         identity='user1',
    #         is_authenticated=True,
    #         display_name='user1'
    #     ),
    #     resource='threads',
    #     action='create_run'
    # )
    # value:
    # {
    #     'thread_id': UUID('1e1b2733-303f-4dcd-9620-02d370287d72'),
    #     'assistant_id': UUID('fe096781-5601-53d2-b2f6-0d3403f7e9ca'),
    #     'run_id': UUID('1efbe268-1627-66d4-aa8d-b956b0f02a41'),
    #     'status': 'pending',
    #     'metadata': {},
    #     'prevent_insert_if_inflight': True,
    #     'multitask_strategy': 'reject',
    #     'if_not_exists': 'reject',
    #     'after_seconds': 0,
    #     'kwargs': {
    #         'input': {'messages': [{'role': 'user', 'content': 'Hello!'}]},
    #         'command': None,
    #         'config': {
    #             'configurable': {
    #                 'langgraph_auth_user': ... Your user object...
    #                 'langgraph_auth_user_id': 'user1'
    #             }
    #         },
    #         'stream_mode': ['values'],
    #         'interrupt_before': None,
    #         'interrupt_after': None,
    #         'webhook': None,
    #         'feedback_keys': None,
    #         'temporary': False,
    #         'subgraphs': False
    #     }
    # }

# Does 2 things:
    # 1. Add the user's ID to the resource's metadata. Each LangGraph resource has a `metadata` dict that persists with the resource.
    # this metadata is useful for filtering in read and update operations
    # 2. Return a filter that lets users only see their own resources
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)

# Only let users see their own resources
    return filters
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The handler receives two parameters:

1. `ctx` ([AuthContext](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.types.AuthContext)): contains info about the current `user`, the user's `permissions`, the `resource` ("threads", "crons", "assistants"), and the `action` being taken ("create", "read", "update", "delete", "search", "create\_run")
2. `value` (`dict`): data that is being created or accessed. The contents of this dict depend on the resource and action being accessed. See [adding scoped authorization handlers](#scoped-authorization) below for information on how to get more tightly scoped access control.

Notice that the simple handler does two things:

1. Adds the user's ID to the resource's metadata.
2. Returns a metadata filter so users only see resources they own.

## 2. Test private conversations

Test your authorization. If you have set things up correctly, you will see all ✅ messages. Be sure to have your development server running (run `langgraph dev`):
```

---

## ❌ Development only - data lost on restart

**URL:** llms-txt#❌-development-only---data-lost-on-restart

store = InMemoryStore()

---

## Run the graph

**URL:** llms-txt#run-the-graph

**Contents:**
- Use user-scoped MCP tools in your deployment
- Session behavior
- Authentication
- Disable MCP

print(graph.invoke({"question": "hi"}))
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient

def mcp_tools_node(state, config):
    user = config["configurable"].get("langgraph_auth_user")
         , user["github_token"], user["email"], etc.

client = MultiServerMCPClient({
        "github": {
            "transport": "streamable_http", # (1)
            "url": "https://my-github-mcp-server/mcp", # (2)
            "headers": {
                "Authorization": f"Bearer {user['github_token']}"
            }
        }
    })
    tools = await client.get_tools() # (3)

# Your tool-calling logic here

tool_messages = ...
    return {"messages": tool_messages}
json  theme={null}
{
  "http": {
    "disable_mcp": true
  }
}
```

This will prevent the server from exposing the `/mcp` endpoint.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-mcp.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For more details, see the [low-level concepts guide](/oss/python/langgraph/graph-api#state).

## Use user-scoped MCP tools in your deployment

<Tip>
  **Prerequisites**
  You have added your own [custom auth middleware](/langsmith/custom-auth) that populates the `langgraph_auth_user` object, making it accessible through configurable context for every node in your graph.
</Tip>

To make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:
```

Example 2 (unknown):
```unknown
1. MCP only supports adding headers to requests made to `streamable_http` and `sse` `transport` servers.
2. Your MCP server URL.
3. Get available tools from your MCP server.

*This can also be done by [rebuilding your graph at runtime](/langsmith/graph-rebuild) to have a different configuration for a new run*

## Session behavior

The current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.

## Authentication

The `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](/langsmith/auth) for setup details.

## Disable MCP

To disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:
```

---

## Let's configure the RetryPolicy to retry on ValueError.

**URL:** llms-txt#let's-configure-the-retrypolicy-to-retry-on-valueerror.

---

## How to add TTLs to your application

**URL:** llms-txt#how-to-add-ttls-to-your-application

**Contents:**
- Configuring Checkpoint TTL
- Configuring Store Item TTL
- Combining TTL Configurations
- Runtime Overrides
- Deployment Process

Source: https://docs.langchain.com/langsmith/configure-ttl

<Tip>
  **Prerequisites**
  This guide assumes familiarity with [LangSmith](/langsmith/home), [Persistence](/oss/python/langgraph/persistence), and [Cross-thread persistence](/oss/python/langgraph/persistence#memory-store) concepts.
</Tip>

LangSmith persists both [checkpoints](/oss/python/langgraph/persistence#checkpoints) (thread state) and [cross-thread memories](/oss/python/langgraph/persistence#memory-store) (store items). Configure Time-to-Live (TTL) policies in `langgraph.json` to automatically manage the lifecycle of this data, preventing indefinite accumulation.

## Configuring Checkpoint TTL

Checkpoints capture the state of conversation threads. Setting a TTL ensures old checkpoints and threads are automatically deleted.

Add a `checkpointer.ttl` configuration to your `langgraph.json` file:

* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring Store Item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:

* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL Configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:

The default `store.ttl` settings from `langgraph.json` can be overridden at runtime by providing specific TTL values in SDK method calls like `get`, `put`, and `search`.

## Deployment Process

After configuring TTLs in `langgraph.json`, deploy or restart your LangGraph application for the changes to take effect. Use `langgraph dev` for local development or `langgraph up` for Docker deployment.

See the [langgraph.json CLI reference](https://langchain-ai.github.io/langgraph/reference/configuration/#configuration-file) for more details on the other configurable options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configure-ttl.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring Store Item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:
```

Example 2 (unknown):
```unknown
* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL Configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:
```

---

## Release versions

**URL:** llms-txt#release-versions

**Contents:**
- Support levels
  - Active
  - Critical
  - End of life (EOL)
  - Deprecated
- Version support policy
  - Minor version support
  - Patch releases
- Recommendations
- Version compatibility

Source: https://docs.langchain.com/langsmith/release-versions

export const product_0 = "LangSmith"

{product_0} provides different support levels for different versions, which may include new features, bug fixes, or security patches.

There are four support levels:

* Active
* Critical
* End of life (EOL)
* Deprecated

Where N represents the latest minor version (e.g., 0.3, 0.4, etc.).

The current minor version (N) receives full support, including:

* New features and capabilities
* Bug fixes and regressions
* Security patches
* Quality-of-life improvements
* High confidence changes that are narrowly scoped

The previous minor version (N-1) receives limited support:

* Critical security fixes
* Installation fixes
* No new features or general bug fixes
* Transitioned from Active when a newer minor version is released

### End of life (EOL)

Versions older than N-2 (N-2, N-3, etc.) receive no support:

* No new patch releases
* No bug fixes, including known bugs
* No security updates
* Users should upgrade to a supported version

Versions that are no longer maintained:

* All versions prior to the first stable release
* Versions that have been explicitly deprecated
* No support or maintenance provided

## Version support policy

{product_0} follows an N-2 support policy for minor versions:

* **N (Current)**: Active support
* **N-1**: Critical support
* **N-2 and older**: End of Life

### Minor version support

Minor versions include new features and capabilities and are supported according to the N-2 policy. When we refer to a minor version, such as v0.3, we always mean its latest available patch release (v0.3.x).

During the support window for each version:

* **Active Support**: Regular patch releases with bug fixes, regressions, and new features
* **Critical Support**: Security-only releases for critical fixes related to security and installation
* **End of Life**: No new patches released

* **Stay Current**: We recommend upgrading to the latest minor version to receive full support and access to new features
* **Plan Upgrades**: Monitor the changelog for upcoming version changes and plan upgrades accordingly
* **Security**: Critical security fixes are only provided for Active and Critical support versions
* **Testing**: Test your applications with newer versions before upgrading in production

## Version compatibility

When upgrading between minor versions:

* Review the changelog for breaking changes
* Test your applications thoroughly
* Follow the upgrade guides provided in the documentation
* Consider the support timeline for your current version

## Current version support

To check the current supported versions and their support levels, refer to the [LangGraph Server Changelog](/langsmith/langgraph-server-changelog) for the latest release information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/release-versions.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Studio

**URL:** llms-txt#studio

**Contents:**
- Prerequisites
- Setup local LangGraph server
  - 1. Install the LangGraph CLI

Source: https://docs.langchain.com/oss/python/langgraph/studio

This guide will walk you through how to use **Studio** to visualize, interact, and debug your agent locally.

Studio is our free-to-use, powerful agent IDE that integrates with [LangSmith](/langsmith/home) to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents.

<Frame>
  <iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Mi1gSlHwZLM?si=zA47TNuTC5aH0ahd" title="Studio" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />
</Frame>

Before you begin, ensure you have the following:

* An API key for [LangSmith](https://smith.langchain.com/settings) (free to sign up)

## Setup local LangGraph server

### 1. Install the LangGraph CLI

```shell  theme={null}

---

## Trace with the Vercel AI SDK (JS/TS only)

**URL:** llms-txt#trace-with-the-vercel-ai-sdk-(js/ts-only)

**Contents:**
- Installation
- Environment configuration
- Basic setup
  - With `traceable`
- Tracing in serverless environments
- Passing LangSmith config
- Redacting data

Source: https://docs.langchain.com/langsmith/trace-with-vercel-ai-sdk

You can use LangSmith to trace runs from the Vercel AI SDK. This guide will walk through an example.

<Note>
  This wrapper requires AI SDK v5 and `langsmith>=0.3.63`. If you are using an older version of the AI SDK or `langsmith`, see the OpenTelemetry (OTEL)
  based approach [on this page](/langsmith/legacy-trace-with-vercel-ai-sdk).
</Note>

Install the Vercel AI SDK. This guide uses Vercel's OpenAI integration for the code snippets below, but you can use any of their other options as well.

## Environment configuration

<CodeGroup>
  
</CodeGroup>

Import and wrap AI SDK methods, then use them as you normally would:

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/4f0e689e-c801-44d3-8857-93b47ab100cc/r).

You can also trace runs with tool calls:

Which results in a trace like [this one](https://smith.langchain.com/public/6075fa2c-d255-4885-a66a-4fc798afaa9f/r).

You can use other AI SDK methods exactly as you usually would.

You can wrap `traceable` calls around AI SDK calls or within AI SDK tool calls. This is useful if you
want to group runs together in LangSmith:

The resulting trace will look [like this](https://smith.langchain.com/public/ff25bc26-9389-4798-8b91-2bdcc95d4a8e/r).

## Tracing in serverless environments

When tracing in serverless environments, you must wait for all runs to flush before your environment
shuts down. To do this, you can pass a LangSmith [`Client`](https://docs.smith.langchain.com/reference/js/classes/client.Client) instance when wrapping the AI SDK method,
then call `await client.awaitPendingTraceBatches()`.
Make sure to also pass it into any `traceable` wrappers you create as well:

If you are using `Next.js`, there is a convenient [`after`](https://nextjs.org/docs/app/api-reference/functions/after) hook
where you can put this logic:

See [this page](/langsmith/serverless-environments) for more detail, including information
around managing rate limits in serverless environments.

## Passing LangSmith config

You can pass LangSmith-specific config to your wrapper both when initially wrapping your
AI SDK methods and while running them via `providerOptions.langsmith`.
This includes metadata (which you can later use to filter runs in LangSmith), top-level run name,
tags, custom client instances, and more.

Config passed while wrapping will apply to all future calls you make with the wrapped method:

While passing config at runtime via `providerOptions.langsmith` will apply only to that run.
We suggest importing and wrapping your config in `createLangSmithProviderOptions` to ensure
proper typing:

You can customize what inputs and outputs the AI SDK sends to LangSmith by specifying custom input/output
processing functions. This is useful if you are dealing with sensitive data that you would like to
avoid sending to LangSmith.

Because output formats vary depending on which AI SDK method you are using, we suggest defining and passing config
individually into wrapped methods. You will also need to provide separate functions for child LLM runs within
AI SDK calls, since calling `generateText` at top level calls the LLM internally and can do so multiple times.

We also suggest passing a generic parameter into `createLangSmithProviderOptions` to get proper types for inputs and outputs.
Here's an example for `generateText`:

The actual return value will contain the original, non-redacted result but the trace in LangSmith
will be redacted. [Here's an example](https://smith.langchain.com/public/b4c69c8e-285b-4c0c-8492-e571e2cf562f/r).

For redacting tool input/output, wrap your `execute` method in a `traceable` like this:

The `traceable` return type is complex, which makes the cast necessary. You may also omit the AI SDK `tool` wrapper function
if you wish to avoid the cast.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-vercel-ai-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Environment configuration

<CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

## Basic setup

Import and wrap AI SDK methods, then use them as you normally would:
```

---

## Configure the OTLP exporter for your custom endpoint

**URL:** llms-txt#configure-the-otlp-exporter-for-your-custom-endpoint

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    # Change to your provider's endpoint
    endpoint="https://otel.your-provider.com/v1/traces",
    # Add any required headers for authentication
    headers={"api-key": "your-api-key"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

---

## Create a code analysis prompt template

**URL:** llms-txt#create-a-code-analysis-prompt-template

code_analysis_prompt = """
Analyze the following code and provide insights:

Please provide:
1. A brief summary of what the code does
2. Any potential improvements
3. Code quality assessment
"""

prompt_template_config = PromptTemplateConfig(
    template=code_analysis_prompt,
    name="code_analyzer",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="code", description="The code to analyze", is_required=True),
    ],
)

---

## Define an example with attachments

**URL:** llms-txt#define-an-example-with-attachments

example_id = uuid.uuid4()
example = {
  "id": example_id,
  "inputs": inputs,
  "outputs": outputs,
  "attachments": {
      "my_pdf": {"mime_type": "application/pdf", "data": pdf_bytes},
      "my_wav": {"mime_type": "audio/wav", "data": wav_bytes},
      "my_img": {"mime_type": "image/png", "data": img_bytes},
      # Example of an attachment specified via a local file path:
      # "my_local_img": {"mime_type": "image/png", "data": Path(__file__).parent / "my_local_img.png"},
  },
}

---

## maxReplicas: 40

**URL:** llms-txt#maxreplicas:-40

---

## Fetch the comparative experiment

**URL:** llms-txt#fetch-the-comparative-experiment

resp = requests.get(
    f"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative",
    params={"id": comparative_experiment_id},
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()[0]
experiment_ids = [info["id"] for info in comparative_experiment["experiments_info"]]

from collections import defaultdict
example_id_to_runs_map = defaultdict(list)

---

## Build the graph with explicit schemas

**URL:** llms-txt#build-the-graph-with-explicit-schemas

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)
builder.add_edge(START, "answer_node")
builder.add_edge("answer_node", END)
graph = builder.compile()

---

## For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.

**URL:** llms-txt#for-langsmith-api-keys-linked-to-multiple-workspaces,-set-the-langsmith_workspace_id-environment-variable-to-specify-which-workspace-to-use.

export LANGSMITH_WORKSPACE_ID=<your-workspace-id>
bash pip theme={null}
  pip install -U langsmith
  bash uv theme={null}
  uv add langsmith
  python  theme={null}
from openai import OpenAI
from langsmith import wrappers

client = wrappers.wrap_openai(OpenAI())
python  theme={null}
import instructor

client = instructor.patch(client)
python  theme={null}
from pydantic import BaseModel

class UserDetail(BaseModel):
    name: str
    age: int

user = client.chat.completions.create(
    model="gpt-4o-mini",
    response_model=UserDetail,
    messages=[
        {"role": "user", "content": "Extract Jason is 25 years old"},
    ]
)
python {highlight={2}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Next, you will need to install the LangSmith SDK:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Wrap your OpenAI client with `langsmith.wrappers.wrap_openai`
```

Example 4 (unknown):
```unknown
After this, you can patch the wrapped OpenAI client using `instructor`:
```

---

## Update the conversation history by removing all messages

**URL:** llms-txt#update-the-conversation-history-by-removing-all-messages

@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

---

## Traces: [OTel Example](/langsmith/langsmith-collector#traces)

**URL:** llms-txt#traces:-[otel-example](/langsmith/langsmith-collector#traces)

The LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit [Otel](https://opentelemetry.io/do/langsmith/observability-concepts/signals/traces/) traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in your `langsmith_config.yaml` (or equivalent) file:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-backend.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## We will use GPT-3.5 Turbo as the baseline and compare against GPT-4o

**URL:** llms-txt#we-will-use-gpt-3.5-turbo-as-the-baseline-and-compare-against-gpt-4o

gpt_3_5_turbo = init_chat_model(
    "gpt-3.5-turbo",
    temperature=1,
    configurable_fields=("model", "model_provider"),
)

---

## To reject

**URL:** llms-txt#to-reject

**Contents:**
  - Review and edit state
  - Interrupts in tools
  - Validating human input
- Rules of interrupts
  - Do not wrap `interrupt` calls in try/except
  - Do not reorder `interrupt` calls within a node
  - Do not return complex values in `interrupt` calls
  - Side effects called before `interrupt` must be idempotent
- Using with subgraphs called as functions
- Debugging with interrupts

graph.invoke(Command(resume=False), config=config)
python  theme={null}
  import sqlite3
  from typing import Literal, Optional, TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ApprovalState(TypedDict):
      action_details: str
      status: Optional[Literal["pending", "approved", "rejected"]]

def approval_node(state: ApprovalState) -> Command[Literal["proceed", "cancel"]]:
      # Expose details so the caller can render them in a UI
      decision = interrupt({
          "question": "Approve this action?",
          "details": state["action_details"],
      })

# Route to the appropriate node after resume
      return Command(goto="proceed" if decision else "cancel")

def proceed_node(state: ApprovalState):
      return {"status": "approved"}

def cancel_node(state: ApprovalState):
      return {"status": "rejected"}

builder = StateGraph(ApprovalState)
  builder.add_node("approval", approval_node)
  builder.add_node("proceed", proceed_node)
  builder.add_node("cancel", cancel_node)
  builder.add_edge(START, "approval")
  builder.add_edge("approval", "proceed")
  builder.add_edge("approval", "cancel")
  builder.add_edge("proceed", END)
  builder.add_edge("cancel", END)

# Use a more durable checkpointer in production
  checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "approval-123"}}
  initial = graph.invoke(
      {"action_details": "Transfer $500", "status": "pending"},
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'question': ..., 'details': ...})]

# Resume with the decision; True routes to proceed, False to cancel
  resumed = graph.invoke(Command(resume=True), config=config)
  print(resumed["status"])  # -> "approved"
  python  theme={null}
from langgraph.types import interrupt

def review_node(state: State):
    # Pause and show the current content for review (surfaces in result["__interrupt__"])
    edited_content = interrupt({
        "instruction": "Review and edit this content",
        "content": state["generated_text"]
    })

# Update the state with the edited version
    return {"generated_text": edited_content}
python  theme={null}
graph.invoke(
    Command(resume="The edited and improved text"),  # Value becomes the return from interrupt()
    config=config
)
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ReviewState(TypedDict):
      generated_text: str

def review_node(state: ReviewState):
      # Ask a reviewer to edit the generated content
      updated = interrupt({
          "instruction": "Review and edit this content",
          "content": state["generated_text"],
      })
      return {"generated_text": updated}

builder = StateGraph(ReviewState)
  builder.add_node("review", review_node)
  builder.add_edge(START, "review")
  builder.add_edge("review", END)

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "review-42"}}
  initial = graph.invoke({"generated_text": "Initial draft"}, config=config)
  print(initial["__interrupt__"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]

# Resume with the edited text from the reviewer
  final_state = graph.invoke(
      Command(resume="Improved draft after review"),
      config=config,
  )
  print(final_state["generated_text"])  # -> "Improved draft after review"
  python  theme={null}
from langchain.tools import tool
from langgraph.types import interrupt

@tool
def send_email(to: str, subject: str, body: str):
    """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
    response = interrupt({
        "action": "send_email",
        "to": to,
        "subject": subject,
        "body": body,
        "message": "Approve sending this email?"
    })

if response.get("action") == "approve":
        # Resume value can override inputs before executing
        final_to = response.get("to", to)
        final_subject = response.get("subject", subject)
        final_body = response.get("body", body)
        return f"Email sent to {final_to} with subject '{final_subject}'"
    return "Email cancelled by user"
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langchain.tools import tool
  from langchain_anthropic import ChatAnthropic
  from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class AgentState(TypedDict):
      messages: list[dict]

@tool
  def send_email(to: str, subject: str, body: str):
      """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
      response = interrupt({
          "action": "send_email",
          "to": to,
          "subject": subject,
          "body": body,
          "message": "Approve sending this email?",
      })

if response.get("action") == "approve":
          final_to = response.get("to", to)
          final_subject = response.get("subject", subject)
          final_body = response.get("body", body)

# Actually send the email (your implementation here)
          print(f"[send_email] to={final_to} subject={final_subject} body={final_body}")
          return f"Email sent to {final_to}"

return "Email cancelled by user"

model = ChatAnthropic(model="claude-sonnet-4-5").bind_tools([send_email])

def agent_node(state: AgentState):
      # LLM may decide to call the tool; interrupt pauses before sending
      result = model.invoke(state["messages"])
      return {"messages": state["messages"] + [result]}

builder = StateGraph(AgentState)
  builder.add_node("agent", agent_node)
  builder.add_edge(START, "agent")
  builder.add_edge("agent", END)

checkpointer = SqliteSaver(sqlite3.connect("tool-approval.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "email-workflow"}}
  initial = graph.invoke(
      {
          "messages": [
              {"role": "user", "content": "Send an email to alice@example.com about the meeting"}
          ]
      },
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'action': 'send_email', ...})]

# Resume with approval and optionally edited arguments
  resumed = graph.invoke(
      Command(resume={"action": "approve", "subject": "Updated subject"}),
      config=config,
  )
  print(resumed["messages"][-1])  # -> Tool result returned by send_email
  python  theme={null}
from langgraph.types import interrupt

def get_age_node(state: State):
    prompt = "What is your age?"

while True:
        answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

# Validate the input
        if isinstance(answer, int) and answer > 0:
            # Valid input - continue
            break
        else:
            # Invalid input - ask again with a more specific prompt
            prompt = f"'{answer}' is not a valid age. Please enter a positive number."

return {"age": answer}
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class FormState(TypedDict):
      age: int | None

def get_age_node(state: FormState):
      prompt = "What is your age?"

while True:
          answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

if isinstance(answer, int) and answer > 0:
              return {"age": answer}

prompt = f"'{answer}' is not a valid age. Please enter a positive number."

builder = StateGraph(FormState)
  builder.add_node("collect_age", get_age_node)
  builder.add_edge(START, "collect_age")
  builder.add_edge("collect_age", END)

checkpointer = SqliteSaver(sqlite3.connect("forms.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "form-1"}}
  first = graph.invoke({"age": None}, config=config)
  print(first["__interrupt__"])  # -> [Interrupt(value='What is your age?', ...)]

# Provide invalid data; the node re-prompts
  retry = graph.invoke(Command(resume="thirty"), config=config)
  print(retry["__interrupt__"])  # -> [Interrupt(value="'thirty' is not a valid age...", ...)]

# Provide valid data; loop exits and state updates
  final = graph.invoke(Command(resume=30), config=config)
  print(final["age"])  # -> 30
  python Separating logic theme={null}
  def node_a(state: State):
      # ✅ Good: interrupting first, then handling
      # error conditions separately
      interrupt("What's your name?")
      try:
          fetch_data()  # This can fail
      except Exception as e:
          print(e)
      return state
  python Explicit exception handling theme={null}
  def node_a(state: State):
      # ✅ Good: catching specific exception types
      # will not catch the interrupt exception
      try:
          name = interrupt("What's your name?")
          fetch_data()  # This can fail
      except NetworkException as e:
          print(e)
      return state
  python  theme={null}
def node_a(state: State):
    # ❌ Bad: wrapping interrupt in bare try/except
    # will catch the interrupt exception
    try:
        interrupt("What's your name?")
    except Exception as e:
        print(e)
    return state
python  theme={null}
def node_a(state: State):
    # ✅ Good: interrupt calls happen in the same order every time
    name = interrupt("What's your name?")
    age = interrupt("What's your age?")
    city = interrupt("What's your city?")

return {
        "name": name,
        "age": age,
        "city": city
    }
python Skipping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: conditionally skipping interrupts changes the order
      name = interrupt("What's your name?")

# On first run, this might skip the interrupt
      # On resume, it might not skip it - causing index mismatch
      if state.get("needs_age"):
          age = interrupt("What's your age?")

city = interrupt("What's your city?")

return {"name": name, "city": city}
  python Looping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: looping based on non-deterministic data
      # The number of interrupts changes between executions
      results = []
      for item in state.get("dynamic_list", []):  # List might change between runs
          result = interrupt(f"Approve {item}?")
          results.append(result)

return {"results": results}
  python Simple values theme={null}
  def node_a(state: State):
      # ✅ Good: passing simple types that are serializable
      name = interrupt("What's your name?")
      count = interrupt(42)
      approved = interrupt(True)

return {"name": name, "count": count, "approved": approved}
  python Structured data theme={null}
  def node_a(state: State):
      # ✅ Good: passing dictionaries with simple values
      response = interrupt({
          "question": "Enter user details",
          "fields": ["name", "email", "age"],
          "current_values": state.get("user", {})
      })

return {"user": response}
  python Functions theme={null}
  def validate_input(value):
      return len(value) > 0

def node_a(state: State):
      # ❌ Bad: passing a function to interrupt
      # The function cannot be serialized
      response = interrupt({
          "question": "What's your name?",
          "validator": validate_input  # This will fail
      })
      return {"name": response}
  python Class instances theme={null}
  class DataProcessor:
      def __init__(self, config):
          self.config = config

def node_a(state: State):
      processor = DataProcessor({"mode": "strict"})

# ❌ Bad: passing a class instance to interrupt
      # The instance cannot be serialized
      response = interrupt({
          "question": "Enter data to process",
          "processor": processor  # This will fail
      })
      return {"result": response}
  python Idempotent operations theme={null}
  def node_a(state: State):
      # ✅ Good: using upsert operation which is idempotent
      # Running this multiple times will have the same result
      db.upsert_user(
          user_id=state["user_id"],
          status="pending_approval"
      )

approved = interrupt("Approve this change?")

return {"approved": approved}
  python Side effects after interrupt theme={null}
  def node_a(state: State):
      # ✅ Good: placing side effect after the interrupt
      # This ensures it only runs once after approval is received
      approved = interrupt("Approve this change?")

if approved:
          db.create_audit_log(
              user_id=state["user_id"],
              action="approved"
          )

return {"approved": approved}
  python Separating into different nodes theme={null}
  def approval_node(state: State):
      # ✅ Good: only handling the interrupt in this node
      approved = interrupt("Approve this change?")

return {"approved": approved}

def notification_node(state: State):
      # ✅ Good: side effect happens in a separate node
      # This runs after approval, so it only executes once
      if (state.approved):
          send_notification(
              user_id=state["user_id"],
              status="approved"
          )

return state
  python Creating records theme={null}
  def node_a(state: State):
      # ❌ Bad: creating a new record before interrupt
      # This will create duplicate records on each resume
      audit_id = db.create_audit_log({
          "user_id": state["user_id"],
          "action": "pending_approval",
          "timestamp": datetime.now()
      })

approved = interrupt("Approve this change?")

return {"approved": approved, "audit_id": audit_id}
  python Appending to lists theme={null}
  def node_a(state: State):
      # ❌ Bad: appending to a list before interrupt
      # This will add duplicate entries on each resume
      db.append_to_history(state["user_id"], "approval_requested")

approved = interrupt("Approve this change?")

return {"approved": approved}
  python  theme={null}
def node_in_parent_graph(state: State):
    some_code()  # <-- This will re-execute when resumed
    # Invoke a subgraph as a function.
    # The subgraph contains an `interrupt` call.
    subgraph_result = subgraph.invoke(some_input)

async function node_in_subgraph(state: State) {
    someOtherCode(); # <-- This will also re-execute when resumed
    result = interrupt("What's your name?")
    ...
}
python  theme={null}
    graph = builder.compile(
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        checkpointer=checkpointer,
    )

# Pass a thread ID to the graph
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(inputs, config=config)  # [!code highlight]

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(
        inputs,
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        config=config,
    )

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    ```

1. `graph.invoke` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
    4. The graph is run until the first breakpoint is hit.
    5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>
</Tabs>

### Using LangGraph Studio

You can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1" alt="image" data-og-width="1252" width="1252" data-og-height="1040" height="1040" data-path="oss/images/static-interrupt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Full example">
```

Example 2 (unknown):
```unknown
</Accordion>

### Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.
```

Example 3 (unknown):
```unknown
When resuming, provide the edited content:
```

Example 4 (unknown):
```unknown
<Accordion title="Full example">
```

---

## maxReplicas: 6

**URL:** llms-txt#maxreplicas:-6

---

## Set up a workspace

**URL:** llms-txt#set-up-a-workspace

**Contents:**
- Set up an organization
  - Create an organization
  - Manage and navigate workspaces
  - Manage users
- Set up a workspace
  - Create a workspace
  - Manage users
  - Configure workspace settings
  - Delete a workspace
  - Delete a workspace via the UI

Source: https://docs.langchain.com/langsmith/set-up-a-workspace

This page describes setting up and managing your LangSmith [*organization*](/langsmith/administration-overview#organizations) and [*workspaces*](/langsmith/administration-overview#workspaces):

* [Set up an organization](#set-up-an-organization): Create and manage organizations for team collaboration, including user management and role assignments.
* [Set up a workspace](#set-up-a-workspace): Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.

<Check>
  You may find it helpful to refer to the [overview on LangSmith resource hierarchy](/langsmith/administration-overview) before you read this setup page.
</Check>

## Set up an organization

<Note>
  If you're interested in managing your organization and workspaces programmatically, see [this how-to guide](/langsmith/manage-organization-by-api).
</Note>

### Create an organization

When you log in for the first time, LangSmith will create a personal organization for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.

To do this, open the Organizations drawer by clicking your profile icon in the bottom left and click **+ New**. Shared organizations require a credit card before they can be used. You will need to [set up billing](/langsmith/billing#set-up-billing-for-your-account) to proceed.

### Manage and navigate workspaces

Once you've subscribed to a plan that allows for multiple users per organization, you can [set up workspaces](/langsmith/administration-overview#workspaces) to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=91c38be270a4e9f7d613fca83192dc6b" alt="" data-og-width="2992" width="2992" data-og-height="478" height="478" data-path="langsmith/images/select-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9c1ba64d54177f72feab7394b4a8ff28 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=772aa2168f043813208bd0a9af628b8b 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae8d1e560af5d320a772ef6edb23f934 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cea6e37801913f953c0fd750ba3ff2a3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e40edbdce2c538e64c4833f4a10b44fc 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b7598fd5dc01cb62f6fa22f9adfead8b 2500w" />

Manage membership in your shared organization in the **Members and roles** tabs on the [Settings page](https://smith.langchain.com/settings). Here you can:

* Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.
* Edit a user's organization role.
* Remove users from your organization.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f6b85051e5dcca2f074ba0ef4801ddd" alt="" data-og-width="3008" width="3008" data-og-height="890" height="890" data-path="langsmith/images/organization-members-and-roles.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2285444015681aadc12dee88d8485294 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5643ea639fa21f9df8faeb7bbcf4db4c 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=011efce2e1f2c89ef26e1c3c5c280d5b 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fe2dff2c95618365c5269c7fc0f6337f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cd20d709570a0e9e447e0a29b9f457c7 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8dba05df82360444c35d94fe90a64dd4 2500w" />

Organizations on the Enterprise plan may set up custom workspace roles in the **Roles** tab. For more details, refer to the [access control setup guide](/langsmith/user-management).

#### Organization roles

Organization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:

* `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. Any `Organization Admin` has `Admin` access to all workspaces in an organization.

- `Organization User` may read organization information, but cannot execute any write actions at the organization level. You can add an `Organization User` to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.

<Info>
  The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available.
</Info>

For a full list of permissions associated with each role, refer to the [Administration overview](/langsmith/administration-overview#organization-roles) page.

## Set up a workspace

When you log in for the first time, a default [workspace](/langsmith/administration-overview#workspaces) will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.

To organize resources within a workspace, you can use [resource tags](/langsmith/set-up-resource-tags).

### Create a workspace

To create a new workspace, navigate to the [Settings page](https://smith.langchain.com/settings) **Workspaces** tab in your shared organization and click **Add Workspace**. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a26994889b28911c59daa8de557c7271" alt="" data-og-width="3014" width="3014" data-og-height="532" height="532" data-path="langsmith/images/create-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e7542ce1dcc74278722aaa5b707eb7f8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c30f0578754fa71812905d1b964c2ebb 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=79f02189dc33d5f730defa4792d89f19 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4138278a2a6f8b11c3df64a51d710b55 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1a5964c468159d57a812efe66e8bd822 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e5efd5a083c29f17b9bddad1f7423fe9 2500w" />

<Note>
  Different plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the [pricing page](https://www.langchain.com/pricing-langsmith).
</Note>

<Info>
  Only workspace `Admins` can manage workspace membership and, if RBAC is enabled, change a user's workspace role.
</Info>

For users that are already members of an organization, a workspace `Admin` may add them to a workspace in the **Workspace members** tab under [Workspaces settings page](https://smith.langchain.com/settings/workspaces). Users may also be invited directly to one or more workspaces when they are [invited to an organization](#manage-users).

### Configure workspace settings

Workspace configuration exists in the [Workspaces settings page](https://smith.langchain.com/settings/workspaces) tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the **API keys**, and other configuration options including secrets, models, and shared URLs are available here as well.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0b95739c014bc31f2950d9d586303cbb" alt="" data-og-width="3012" width="3012" data-og-height="1226" height="1226" data-path="langsmith/images/workspace-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ddd4f1738c7142be44e6966b0079cad6 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8dcfe014fc2584946019acebc59fd3b 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=659cffc42334b972d3a1f01f2926120b 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7fdae2696aed94f5e7391d3e88c92d49 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1071e84f39202761f635e141b7828a82 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6edc496967555f2cfd3365bb846ce698 2500w" />

### Delete a workspace

<Warning>
  Deleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.
</Warning>

You can delete a workspace through the LangSmith UI or via [API](https://api.smith.langchain.com/redoc?#tag/workspaces/operation/delete_workspace_api_v1_workspaces__workspace_id__delete). You must be a workspace `Admin` in order to delete a workspace.

### Delete a workspace via the UI

1. Navigate to **Settings**.
2. Select the workspace you want to delete.
3. Click **Delete** in the top-right corner of the screen.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33038784e813f06dae3c87e5d34a3dc1" alt="Delete a workspace" data-og-width="1106" width="1106" data-og-height="250" height="250" data-path="langsmith/images/delete-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=829f2ad5874457f3023bf4441e408203 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c907fade390eff674deb3fafc038e885 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b5c0c1dec248c82ef12cd61d4da9fed 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c9005738241ade3dc2516c6e9b395d39 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=19a256c90e69d4db054d4a360a84cd40 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5cfe1a5279c11bdd0e33812b58418e92 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-a-workspace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## The sky is typically

**URL:** llms-txt#the-sky-is-typically

---

## Run a specific test function within a class

**URL:** llms-txt#run-a-specific-test-function-within-a-class

**Contents:**
- Troubleshooting

uv run --group test pytest tests/integration_tests/test_chat_models.py::TestChatParrotLinkIntegration::test_chat_completions
```

For a full list of the standard test suites that are available, as well as information on which tests are included and how to troubleshoot common issues, see the [Standard Tests API Reference](https://reference.langchain.com/python/langchain_tests).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/standard-tests-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Use annotation queues

**URL:** llms-txt#use-annotation-queues

**Contents:**
- Create an annotation queue
  - Basic Details
  - Annotation Rubric
  - Collaborator Settings
- Assign runs to an annotation queue
- Review runs in an annotation queue
- Video guide

Source: https://docs.langchain.com/langsmith/annotation-queues

Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs. While you can always annotate traces inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them.

## Create an annotation queue

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0d6717509d856a6c0a8234854507471f" alt="" data-og-width="1659" width="1659" data-og-height="190" height="190" data-path="langsmith/images/create-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=25c3093c231edfe3beb8f79704646780 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=caa3607e3895f583aad74ee817bc5171 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=62263e2761923e633cc3b72d185fb8c6 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5c097463244558d564faa6147692c8cf 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b5d054b7fe719e92a03fe2f90c160cc4 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=40a0cbf5aa2de93a3c5fb22ec3bd0c00 2500w" />

To create an annotation queue, navigate to the **Annotation queues** section through the homepage or left-hand navigation bar. Then click **+ New annotation queue** in the top right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c5c28c10a5522af0a37f40236ed57510" alt="" data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/create-annotation-queue-new.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=daa5c44976804eae5ca8bbfef1d0a9d0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=167955e0202671425e6cd1476c31a756 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71627eeab271c6d4581f00506731cc09 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cd30341efa9d5eea82d85b63518b53a0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3e71d31b42b3411946f73d79e8735599 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=832e7c8b99d332176bc9d9de702a6bac 2500w" />

Fill in the form with the **name** and **description** of the queue. You can also assign a **default dataset** to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace.

### Annotation Rubric

Begin by drafting some high-level instructions for your annotators, which will be shown in the sidebar on every run.

Next, click "+ Desired Feedback" to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run. Add a description for each, as well as a short description of each category if the feedback is categorical.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8adfdba2649847f82543674978b0d1b1" alt="annotation queue rubric" data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/create-annotation-rubric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5d73a7688b61b3b9489aacac1223f7c6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ede9b22be4e3ce82e4feabf86575e8a5 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=768747aa9e314c66631f27e794d9174b 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f3061b6ba68c4d9cab997bbed2efe76e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=27545c5b64b3b82ae9ebed853ff02168 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=801397056da06004808b6c38df30c139 2500w" />

Reviewers will see this:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=44452f7da89329acc06672beba4e4c0e" alt="rubric for annotators" data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/rubric-for-annotators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa18a86229854c27a85c341da2638501 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=67f5880b9d79e05e2dcd3703db92f79a 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=01436264374f87f9baab4ac4f3f8c161 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=80b8569a8ef3acc5b7d94a9276dc0d26 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b4c6f43fe2e91f57de4a2790757a2083 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aae4a68208b5b6838c3fdf77f2c57efe 2500w" />

### Collaborator Settings

There are a few settings related to multiple annotators:

* **Number of reviewers per run**: This determines the number of reviewers that must mark a run as "Done" for it to be removed from the queue. If you check "All workspace members review each run," then a run will remain in the queue until all workspace members have marked it "Done".

* Reviewers cannot view the feedback left by other reviewers.
  * Comments on runs are visible to all reviewers.

* **Enable reservations on runs**: We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.

1. **How do reservations work?**

When a reviewer views a run, the run is reserved for that reviewer for the specified "reservation length". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.

2. **What happens if time runs out?**

If a reviewer has viewed a run and then leaves the run without marking it "Done", the reservation will expire after the specified "reservation length". The run is then released back into the queue and can be reserved by another reviewer.

<Note>
  Clicking "Requeue at end" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run.
</Note>

Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size.

You can update these settings at any time by clicking on the pencil icon in the **Annotation Queues** section.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=afdec947a31e91c50588d551c32e236a" alt="" data-og-width="2402" width="2402" data-og-height="196" height="196" data-path="langsmith/images/annotation-queue-edit.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2f28d279afab3b7d6ec6c73c318be091 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3f46153057464851a6f55b8dc87058b3 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e3586bf74c528d336c18213d8d949e96 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b350884d7e7825bf7cf4bbf3d0c637e4 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5b1d665eca3fbeade94bcc4468ac18c 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-queue-edit.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3076a6154b12c13c6d1dd4e4cf8ee92b 2500w" />

## Assign runs to an annotation queue

To assign runs to an annotation queue, either:

1. Click on **Add to Annotation Queue** in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fc604c7f91bc8795dc688c4f9db73ce9" alt="" data-og-width="1373" width="1373" data-og-height="1028" height="1028" data-path="langsmith/images/add-to-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ff1545d09984dfb766067ad65ecbfb9 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=055554579c01cc8c48471e0d74fe27d6 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=10412c26d4042358e098631386cddbf2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be2c6162599ab18ef34975a439f16e93 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e0b51a16246b1e28d415f23d86643c7 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c7744f75389cd270f60bbbee9571582a 2500w" />

2. Select multiple runs in the runs table then click **Add to Annotation Queue** at the bottom of the page. <img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c6781e6a7345ef7e16ea7a0bb306a474" alt="" data-og-width="1323" width="1323" data-og-height="1317" height="1317" data-path="langsmith/images/multi-select-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fff2a1f8cc40bf86f4b9251dacc0e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2cc50c1d4f1b9ec24f9e3bc4d5fabbff 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b37ce7b181457582652a22405f240750 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=badc9ef126c220b9aa8f5e8212421b93 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae02333d194a955c1a6c86a2bf87c75f 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9dce63a0eedfc4a02a5ad822cee67bce 2500w" />

3. [Set up an automation rule](/langsmith/rules) that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue.

4. Select one or multiple experiments from the dataset page and click **Annotate**. From the resulting popup, you may either create a new queue or add the runs to an existing one: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7622e6db855711542de24270ddc129dc" alt="" data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/annotate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc0abf70504c439b413dfb6a3ff59f7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c511826c68b2ecf9fb75addc50df48c3 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=450345b069f8de91a982d66bfe6ce8a9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb67c24cd3f6fee0042ad3ef94fc9a59 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=83aab2b2cd3cc7a84440caa0d57a68be 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08a045816ca42285ad46570280ee7061 2500w" />

<Check>
  It is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow [this guide](/langsmith/attach-user-feedback).
</Check>

## Review runs in an annotation queue

To review runs in an annotation queue, navigate to the **Annotation Queues** section through the homepage or left-hand navigation bar. Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.

You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the **Trash** icon next to "View run".

The keyboard shortcuts shown can help streamline the review process.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9065d4b85e6165084b65d3908d61778a" alt="" data-og-width="1532" width="1532" data-og-height="1080" height="1080" data-path="langsmith/images/review-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f69f94916e247ff498e1d9e5ed2755a2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=64f16b8b83ffc56d1ad078ae1bfacd65 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=085b9e1ea9b3797bc10117c326a80018 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4321324e5d276d12864e446cd2a97c82 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7eeb685e4d4064518613283e502c9395 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=059f57fb238aa0cd1ebcf6a2fb0ede95 2500w" />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/rxKYHA-2KS0?si=V4EnrUmzJaUVJh0m" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotation-queues.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Evaluator functions can be sync or async

**URL:** llms-txt#evaluator-functions-can-be-sync-or-async

def concise(inputs: dict, outputs: dict) -> bool:
    return len(outputs["output"]) < 3 * len(inputs["idea"])

ls_client = Client()
ideas = [
    "universal basic income",
    "nuclear fusion",
    "hyperloop",
    "nuclear powered rockets",
]
dataset = ls_client.create_dataset("research ideas")
ls_client.create_examples(
    dataset_name=dataset.name,
    examples=[{"inputs": {"idea": i}} for i in ideas],
)

---

## Optimize a classifier

**URL:** llms-txt#optimize-a-classifier

**Contents:**
- The objective
- Getting started
- Set up automations
- Update the application
  - NEW CODE ###

Source: https://docs.langchain.com/langsmith/optimize-classifier

This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.

In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.

To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:

Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.

Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.

## Set up automations

We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.

The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let's create a dataset called `classifier-github-issues` to add this data to.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e36c57f7e0e224ff1ea29bcfbe9891fc" alt="Optimization Negative" data-og-width="1033" width="1033" data-og-height="558" height="558" data-path="langsmith/images/class-optimization-neg.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4091f7ae7d447eab035b32b66788eaee 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9971db3fd8992d31d94b58095381e575 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=96f35883e764b0f207692ce1fba46d08 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f1aa5d6d49137a2f8d254ad2d327dabf 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c9d20165b113f53fe5638c66ff47eddb 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0109460c47883c4225fd54115acadb2b 2500w" />

The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to "Use Corrections". This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6485ca961ed1c29d33f25f75f90ba939" alt="Optimization Positive" data-og-width="1038" width="1038" data-og-height="506" height="506" data-path="langsmith/images/class-optimization-pos.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=07ee0c534c8d8ce3e34e7c17058af5c0 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d14fc77148d349e02b363af96f0752ad 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=56592428dbb8d34492bbbc2bee911500 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=81a4d0d24ca94ad2adb5b4463bb3f776 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c3086f1fa332b846e7b6f668084fada 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=41ba1a118c1f220018e619edf2fecee2 2500w" />

## Update the application

We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!

```python  theme={null}
### NEW CODE ###

**Examples:**

Example 1 (unknown):
```unknown
We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.
```

Example 2 (unknown):
```unknown
We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:
```

Example 3 (unknown):
```unknown
Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.
```

Example 4 (unknown):
```unknown
Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.
```

---

## Note that we're (optionally) passing the memory when compiling the graph

**URL:** llms-txt#note-that-we're-(optionally)-passing-the-memory-when-compiling-the-graph

**Contents:**
  - Create a dataset
  - Create an evaluator
  - Run evaluations

app = workflow.compile()
python  theme={null}
from langsmith import Client

questions = [
    "what's the weather in sf",
    "whats the weather in san fran",
    "whats the weather in tangier"
]

answers = [
    "It's 60 degrees and foggy.",
    "It's 60 degrees and foggy.",
    "It's 90 degrees and sunny.",
]

ls_client = Client()
dataset = ls_client.create_dataset(
    "weather agent",
    inputs=[{"question": q} for q in questions],
    outputs=[{"answers": a} for a in answers],
)
python  theme={null}
judge_llm = init_chat_model("gpt-4o")

async def correct(outputs: dict, reference_outputs: dict) -> bool:
    instructions = (
        "Given an actual answer and an expected answer, determine whether"
        " the actual answer contains all of the information in the"
        " expected answer. Respond with 'CORRECT' if the actual answer"
        " does contain all of the expected information and 'INCORRECT'"
        " otherwise. Do not include anything else in your response."
    )
    # Our graph outputs a State dictionary, which in this case means
    # we'll have a 'messages' key and the final message should
    # be our actual answer.
    actual_answer = outputs["messages"][-1].content
    expected_answer = reference_outputs["answer"]
    user_msg = (
        f"ACTUAL ANSWER: {actual_answer}"
        f"\n\nEXPECTED ANSWER: {expected_answer}"
    )
    response = await judge_llm.ainvoke(
        [
            {"role": "system", "content": instructions},
            {"role": "user", "content": user_msg}
        ]
    )
    return response.content.upper() == "CORRECT"
python  theme={null}
from langsmith import aevaluate

def example_to_state(inputs: dict) -> dict:
  return {"messages": [{"role": "user", "content": inputs['question']}]}

**Examples:**

Example 1 (unknown):
```unknown
### Create a dataset

Let's create a simple dataset of questions and expected responses:
```

Example 2 (unknown):
```unknown
### Create an evaluator

And a simple evaluator:

Requires `langsmith>=0.2.0`
```

Example 3 (unknown):
```unknown
### Run evaluations

Now we can run our evaluations and explore the results. We'll just need to wrap our graph function so that it can take inputs in the format they're stored on our example:

<Note>
  If all of your graph nodes are defined as sync functions then you can use `evaluate` or `aevaluate`. If any of you nodes are defined as async, you'll need to use `aevaluate`
</Note>

Requires `langsmith>=0.2.0`
```

---

## model_2 is tagged with "poem"

**URL:** llms-txt#model_2-is-tagged-with-"poem"

model_2 = init_chat_model(model="openai:gpt-4o-mini", tags=['poem'])

graph = ... # define a graph that uses these LLMs

---

## What's new in v1

**URL:** llms-txt#what's-new-in-v1

**Contents:**
- Deprecation of `create_react_agent`
- Reporting issues
- Additional resources
- See also

Source: https://docs.langchain.com/oss/python/releases/langgraph-v1

**LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.

It's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.

<CardGroup cols={1}>
  <Card title="Stable core APIs" icon="diagram-project">
    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.
  </Card>

<Card title="Reliability, by default" icon="database">
    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.
  </Card>

<Card title="Seamless with LangChain v1" icon="link">
    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.
  </Card>
</CardGroup>

## Deprecation of `create_react_agent`

The LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.

* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).
* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangGraph 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Overview" icon="book" href="/oss/python/langgraph/overview" arrow>
    What LangGraph is and when to use it
  </Card>

<Card title="Graph API" icon="diagram-project" href="/oss/python/langgraph/graph-api" arrow>
    Build graphs with state, nodes, and edges
  </Card>

<Card title="LangChain Agents" icon="robot" href="/oss/python/langchain/agents" arrow>
    High-level agents built on LangGraph
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/python/migrate/langgraph-v1" arrow>
    How to migrate to LangGraph v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langgraph">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/python/versioning) - Understanding version numbers
* [Release policy](/oss/python/release-policy) - Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## LangGraph SDK

**URL:** llms-txt#langgraph-sdk

Source: https://docs.langchain.com/oss/python/reference/langgraph-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langgraph-python.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Manage prompts programmatically

**URL:** llms-txt#manage-prompts-programmatically

**Contents:**
- Install packages
- Configure environment variables
- Push a prompt
- Pull a prompt
- Use a prompt without LangChain
  - OpenAI
  - Anthropic
- List, delete, and like prompts

Source: https://docs.langchain.com/langsmith/manage-prompts-programmatically

You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.

<Note>
  Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.
</Note>

In Python, you can directly use the LangSmith SDK (*recommended, full functionality*) or you can use through the LangChain package (limited to pushing and pulling prompts).

In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.

<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: [Supported Providers](https://langsmith.com/playground))

To pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.

To pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).

To pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt's author.

Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include\_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.

When pulling a prompt, you can also specify a specific commit hash or [commit tag](/langsmith/manage-prompts#commit-tags) to pull a specific version of the prompt.

To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.

<Note>
  For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.

If you are in a non-Node environment, "includeModel" is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.
</Note>

## Use a prompt without LangChain

If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.

These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:

## List, delete, and like prompts

You can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the [LangSmith SDK client](https://github.com/langchain-ai/langsmith-sdk) for extensive documentation on these methods.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts-programmatically.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.
```

Example 4 (unknown):
```unknown
<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

## Push a prompt

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

<CodeGroup>
```

---

## This is the state before last (states are listed in chronological order)

**URL:** llms-txt#this-is-the-state-before-last-(states-are-listed-in-chronological-order)

**Contents:**
  - 3. Update the state
  - 4. Resume execution from the checkpoint

selected_state = states[1]
print(selected_state.next)
print(selected_state.values)

('write_joke',)
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}
python  theme={null}
new_config = graph.update_state(selected_state.config, values={"topic": "chickens"})
print(new_config)

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}
python  theme={null}
graph.invoke(None, new_config)
python  theme={null}
{'topic': 'chickens',
 'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-time-travel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown
<a id="optional" />

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.
```

Example 3 (unknown):
```unknown
**Output:**
```

Example 4 (unknown):
```unknown
### 4. Resume execution from the checkpoint
```

---

## Define the overall schema, combining both input and output

**URL:** llms-txt#define-the-overall-schema,-combining-both-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Add OtelSpanProcessor to the tracer provider

**URL:** llms-txt#add-otelspanprocessor-to-the-tracer-provider

tracer_provider.add_span_processor(OtelSpanProcessor())

---

## Create config with thread_id for state persistence

**URL:** llms-txt#create-config-with-thread_id-for-state-persistence

config = {"configurable": {"thread_id": str(uuid.uuid4())}}

---

## Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,

**URL:** llms-txt#since-this-is-**more-specific**-than-both-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler,

---

## Define the node that processes the input and generates an answer

**URL:** llms-txt#define-the-node-that-processes-the-input-and-generates-an-answer

def answer_node(state: InputState):
    # Example answer and an extra key
    return {"answer": "bye", "question": state["question"]}

---

## Routing model with structured output

**URL:** llms-txt#routing-model-with-structured-output

router_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    UserIntent, method="json_schema", strict=True
)

---

## Observability concepts

**URL:** llms-txt#observability-concepts

**Contents:**
- Runs
- Traces
- Threads
- Projects
- Feedback
- Tags
- Metadata
- Data storage and retention
- Deleting traces from LangSmith

Source: https://docs.langchain.com/langsmith/observability-concepts

This page covers key concepts that are important to understand when logging traces to LangSmith.

A [*trace*](#traces) records the sequence of steps your application takes—from receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a [*run*](#runs). Multiple traces are grouped together within a [*project*](#projects), and traces from multi-turn conversations can be linked together as a [*thread*](#threads).

The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=50c5f4d966f8fe4f8ae0be0beaf11bc4" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2701" width="2701" data-og-height="1739" height="1739" data-path="langsmith/images/primitives.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=e0b6083af11ec78c1650c907a6b8649a 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9054d8620c453c520e161c1ba8fb1fdc 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=8ece2e0b84019b722446e9bfdbf067f9 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=c4e3700fe862b539954b8c6c0124bfac 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=f9dfe46dbb576c1faeb1f4ad52324527 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9137d815270a89f9d4df799381a88aac 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=3ca35a8a6cec65b9a5139e7ac8cac470" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2919" width="2919" data-og-height="1752" height="1752" data-path="langsmith/images/primitives-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=41bff3be3eabb386bd11347b77908625 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=1e7d10f532b2a099ca74cd80f1f0f90d 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=10e2a103221b086ed8985a8478c9992c 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=a82db1fc02ffd0db0222ed0a14e48a86 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=70a1cd6dd3f30b5863bba85e92a30733 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=33a6bfeb3258ced236732b0be95b6ab3 2500w" />
</div>

A *run* is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a692d614eb441aef6e3a1f02f4a37e8a" alt="Run" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=911a09dc4da0d94015cc2a22f95efce6 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6ae06bcbba3e3b240cfa41950f4e451d 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e886c505427057c726f30b495fc7c4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48dfe76850483f46da1bbe4bcccb5f8b 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eff65c5910eaa68456e26712dfa64d1d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d99e6ba31cd3f4039ad86b0ade0b78b4 2500w" />

A *trace* is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dd692f6433fbcf0413a4516c170062f2" alt="Trace" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fea7f898d3f25c76bed212af157b1a1d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5a75129cb5ab5fab0a6618d0c3160ec2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3102493a51a280b38004ff8628231ad6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0e4d194f6a6b18b4505a2b4a4f40fc3 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d1f0bb5ae6e5dec024c5e3726b848394 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f5d14619201af89c134fd1fb99b7cdf0 2500w" />

A *thread* is a sequence of traces representing a single conversation. Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. Each turn in the conversation is represented as its own trace, but these traces are linked together by being part of the same thread. The most recent trace in a thread is the latest message exchange.

To group traces into threads, you pass a special metadata key (`session_id`, `thread_id`, or `conversation_id`) with a unique identifier value that links the traces together.

[Learn how to configure threads](/langsmith/threads).

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />

A *project* is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2426200ab2e619674636e41f11246c0d" alt="Project" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=97510cd2501c9d2dc529680172664219 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8624fbec9c58a1999d702aed79a606f6 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d918966500e84ce29550591755d98f6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d9a6a4b2365de4e4cf80506d3fc0a467 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=11761179be51c8cfc58193d6d0215269 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=779669a7f2f2106946d1f7eb3f427ed5 2500w" />

*Feedback* allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.

You can collect feedback on runs in a number of ways:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application.
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues).
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application).
4. Generated by an [online evaluator](/langsmith/online-evaluations).

To learn more about how feedback is stored in the application, refer to the [Feedback data format guide](/langsmith/feedback-data-format).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13fa69590caa5ba050e3cda9dbb6a336" alt="Feedback" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ef7168bb4e77c0b70f5cc6b7062be002 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=75059818a510d95abea177fb5e3a1b4e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a29438f9e3126eb0aa01023e02c0b621 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=904b4e5e7aa4dcfd49121b82fb269c5f 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f5d68cfe719b0cc0a9d5e2b0fefb12ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3be4d77ff49c91b49effcceeb96b38d3 2500w" />

*Tags* are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:

* Categorize runs for easier search.
* Filter runs.
* Group runs together for analysis.

[Learn how to attach tags to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=44e6b6242fd76e811dcc28d88c5c6db5" alt="Tags" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=35d294b4a295130dfb7f23a8237fe53d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ab7e4b06eced56da2b51eb9f88738e6f 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ecd67b36985d21a275b3b6aa34922a84 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dfc774fcd984d08202f178eb75cc14e6 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e5c4c571cbaf45fdab26f21e854bc29 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b792075475d671a84e3efb1ca47d527b 2500w" />

*Metadata* is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.

[Learn how to add metadata to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e48730c02b7974035bbb312734e86a92" alt="Metadata" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fbe88a76fd74d9fb36e89465ef068cc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7e3c71b02884707ca26863a88e4d04f6 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8799298d81f72725c01a937c549fdb81 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6dbb9c568c187cbd8bd14a471f6977b3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=be48eaaa028a65c6ca6698c27cf9da6b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=810f9462214e87aba38b724a9b5083af 2500w" />

## Data storage and retention

For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.

After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.

<Note>
  If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A [dataset](/langsmith/manage-datasets) allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.
</Note>

## Deleting traces from LangSmith

If you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.

You can delete a project with one of the following ways:

* In the [LangSmith UI](https://smith.langchain.com), select the **Delete** option on the project's overflow menu.
* With the [`delete_tracer_sessions`](https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/delete_tracer_session_api_v1_sessions__session_id__delete) API endpoint
* With the `delete_project()` ([Python](/langsmith/smith-python-sdk)) or `deleteProject()` ([JS/TS](/langsmith/smith-js-ts-sdk)) in the LangSmith SDK.

LangSmith does not support self-service deletion of individual traces.

If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should reach out to [LangSmith Support](mailto:support@langchain.dev) with the organization ID and trace IDs.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Instrument AutoGen and OpenAI calls

**URL:** llms-txt#instrument-autogen-and-openai-calls

**Contents:**
  - 3. Create and run your AutoGen application

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()
python  theme={null}
import autogen
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor
from langsmith.integrations.otel import configure
import os
import dotenv

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set any OpenTelemetry environment variables or configure exporters manually—`configure()` handles everything automatically.
</Note>

### 3. Create and run your AutoGen application

Once configured, your AutoGen application will automatically send traces to LangSmith:
```

---

## Enable TTL and data retention

**URL:** llms-txt#enable-ttl-and-data-retention

**Contents:**
- Requirements
- ClickHouse TTL Cleanup Job
  - Default Schedule
  - Disabling the Job
  - Configuring the Schedule
  - Configuring Minimum Expired Rows Per Part
  - Configuring Maximum Active Mutations
  - Emergency: Stopping Running Mutations
  - Backups and Data Retention

Source: https://docs.langchain.com/langsmith/self-host-ttl

LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications.

You can configure retention through helm or environment variable settings. There are a few options that are configurable:

* *Enabled:* Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see [data retention guide](/langsmith/administration-overview#data-retention) for details).
* *Retention Periods:* You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:

### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:

<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data

#### Checking Expired Rows

Use this query to analyze expired rows in your tables, and tweak your minimum value accordingly:

### Configuring Maximum Active Mutations

Delete operations can be time-consuming (\~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:

<Warning>
  Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.
</Warning>

### Emergency: Stopping Running Mutations

If you experience latency spikes and need to terminate a running mutation:

1. **Find active mutations**:

Look for the `mutation_id` where the `command` column contains a `DELETE` statement.

2. **Kill the mutation**:

### Backups and Data Retention

If disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data.

To verify, check the following directories inside your ClickHouse pod:

* `/var/lib/clickhouse/backup`
* `/var/lib/clickhouse/shadow`

If backups are present, copy them to an external filesystem or blob storage (e.g., S3), then clear the directories. Within a few minutes, you will notice disk space releasing.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-ttl.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

### Default Schedule

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:
```

Example 3 (unknown):
```unknown
### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:
```

Example 4 (unknown):
```unknown
<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data
```

---

## Create and manage datasets in the UI

**URL:** llms-txt#create-and-manage-datasets-in-the-ui

**Contents:**
- Create a dataset and add examples
  - Manually from a tracing project
  - Automatically from a tracing project
  - From examples in an Annotation Queue
  - From the Prompt Playground
  - Import a dataset from a CSV or JSONL file
  - Create a new dataset from the Datasets & Experiments page
  - Add synthetic examples created by an LLM
- Manage a dataset
  - Create a dataset schema

Source: https://docs.langchain.com/langsmith/manage-datasets-in-application

[*Datasets*](/langsmith/evaluation-concepts#datasets) enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of [*examples*](/langsmith/evaluation-concepts#examples), which store inputs, outputs, and optionally, reference outputs.

This page outlines the various methods for [creating](#create-a-dataset-and-add-examples) and [managing](#manage-a-dataset) datasets in the [LangSmith UI](https://smith.langchain.com).

## Create a dataset and add examples

The following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:

* [Manually from a tracing project](#manually-from-a-tracing-project)
* [Automatically from a tracing project](#automatically-from-a-tracing-project)
* [From examples in an Annotation Queue](#from-examples-in-an-annotation-queue)
* [From the Prompt Playground](#from-the-prompt-playground)
* [Import a dataset from a CSV or JSONL file](#import-a-dataset-from-a-csv-or-jsonl-file)
* [Create a new dataset from the dataset page](#create-a-new-dataset-from-the-dataset-page)
* [Add synthetic examples created by an LLM via the Datasets UI](#add-synthetic-examples-created-by-an-llm-via-the-datasets-ui)

### Manually from a tracing project

A common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have [configured tracing to LangSmith](/langsmith/observability-concepts#tracing-configuration).

<Check>
  A technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to [Filter traces](/langsmith/filter-traces-in-application) guide.
</Check>

There are two ways to add data manually from a tracing project to datasets. Navigate to **Tracing Projects** and select a project.

1. Multi-select runs from the runs table. On the **Runs** tab, multi-select runs. At the bottom of the page, click <Icon icon="database" /> **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fe666b6e1888554d0573df770bd9cda2" alt="The Runs table with a run selected and the Add to Dataset button visible at the bottom of the page." data-og-width="2912" width="2912" data-og-height="1464" height="1464" data-path="langsmith/images/multiselect-add-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2a5526d48109c7e33cb5b72f19c0ae2d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0434d04eb3c301ea49a369e5def47701 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f0d3184b1dea71c3794ba653637a513 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=69e347a4a8f26bf7343ba05e88d9fe24 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4187008b9cc8f5936b7d64251d4641b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9419ca3c63abd40cfb899c5cffc36e33 2500w" />

2. On the **Runs** tab, select a run from the table. On the individual run details page, select  **Add to** -> **Dataset** in the top right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fd03b2cf578c3e524223afc5b09d0589" alt="Add to dataset" data-og-width="2898" width="2898" data-og-height="1462" height="1462" data-path="langsmith/images/add-to..dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=70baaf45d7471f218c95af5ee77530d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd8fcd49d9b39e3937c3abcb0b2afc31 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9477bdbe20caa5ccdb50ea1e4e18f234 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8c527bf07a2e032583496c9ba4ddf0c5 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c538e79a32bc8bb0e6458f1bf7a2276 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7a739ceda9692ae6661ce365a9ca46ce 2500w" />

When you select a dataset from the run details page, a modal will pop up letting you know if any [transformations](/langsmith/dataset-transformations) were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4ac9ca81489294ac40bf4b88a68ba1c9" alt="Confirmation" data-og-width="2898" width="2898" data-og-height="1452" height="1452" data-path="langsmith/images/confirmation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3c09ffaacb03ec55aeddda61a3a58111 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9146d5498be48425a6de3caf28db394f 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=299ac17e36643502a8ddc0a7d4d49780 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7607f473d263d7a9d55e1720571d3755 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=deb6514e2202d020882f4fc810207795 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4141f028af5084d808297d936f168121 2500w" />

You can then optionally edit the run before adding it to the dataset.

### Automatically from a tracing project

You can use [run rules](/langsmith/rules) to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are [tagged](/langsmith/observability-concepts#tags) with a specific use case or have a [low feedback score](/langsmith/observability-concepts#feedback).

### From examples in an Annotation Queue

<Check>
  If you rely on subject matter experts to build meaningful datasets, use [annotation queues](/langsmith/annotation-queues) to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset.
</Check>

Annotation queues can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click **Add to Dataset** or hit the hot key `D` to add the run to it.

Any modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=885c58ab30d94b2371b79730468e0be3" alt="Add to dataset from annotation queue" data-og-width="2290" width="2290" data-og-height="1468" height="1468" data-path="langsmith/images/add-to-dataset-from-aq.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=90c1279193bf00acf6112980ebd94558 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9cb62b05c7d90e5904573d7992a141cc 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5c547cc7f74a3b3b34eb7788ad324fcd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6c623a6015dcb818677b6af9d41ad923 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=47c9a570a9c1e568c0a035aa82adc5d0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c76a247a72ca269c29c99c1da46852e6 2500w" />

Note you can also set up rules to add runs that meet specific criteria to an annotation queue using [automation rules](/langsmith/rules).

### From the Prompt Playground

On the [**Prompt Playground**](/langsmith/observability-concepts#prompt-playground) page, select **Set up Evaluation**, click **+New** if you're starting a new dataset or select from an existing dataset.

<Note>
  Creating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit [from the datasets page](/langsmith/manage-datasets-in-application#from-the-datasets-page).
</Note>

To edit the examples:

* Use **+Row** to add a new example to the dataset
* Delete an example using the **⋮** dropdown on the right hand side of the table
* If you're creating a reference-free dataset remove the "Reference Output" column using the **x** button in the column. Note: this action is not reversible.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=583ce80f84902ccb5ccab36a44dddb9b" alt="Create a dataset in the playground" data-og-width="1318" width="1318" data-og-height="981" height="981" data-path="langsmith/images/playground-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=31fe00c30e17e901334f46845dba1464 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6e63d19dd4be4761d1f6c0d4746395cc 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f625e8480f96e644b4ff2205a2905c9d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5126df423f04d152ad638f70f1a7765d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53a13fe730bbe7c2393049dac76266db 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=998729ce541d92f078382984c76c2501 2500w" />

### Import a dataset from a CSV or JSONL file

On the **Datasets & Experiments** page, click **+New Dataset**, then **Import** an existing dataset from CSV or JSONL file.

### Create a new dataset from the Datasets & Experiments page

1. Navigate to the **Datasets & Experiments** page from the left-hand menu.
2. Click **+ New Dataset**.
3. On the **New Dataset** page, select the **Create from scratch** tab.
4. Add a name and description for the dataset.
5. (Optional) Create a [dataset schema](#create-a-dataset-schema) to validate your dataset.
6. Click **Create**, which will create an empty dataset.
7. To add examples inline, on the dataset's page, go to the **Examples** tab. Click **+ Example**.
8. Define examples in JSON and click **Submit**. For more details on dataset splits, refer to [Create and manage dataset splits](#create-and-manage-dataset-splits).

### Add synthetic examples created by an LLM

If you have existing examples and a [schema](#create-a-dataset-schema) defined on your dataset, when you click **+ Example** there is an option to <Icon icon="sparkles" /> **Add AI-Generated Examples**. This will use an LLM to create [synthetic](/langsmith/evaluation-concepts#synthetic-data) examples.

In **Generate examples**, do the following:

1. Click **API Key** in the top right of the pane to set your OpenAI API key as a [workspace secret](/langsmith/administration-overview#workspaces). If your workspace already has an OpenAI API key set, you can skip this step.

2. Select <Tooltip tip="A few sample input–output pairs that guide the model on how to perform a task.">few-shot examples</Tooltip>: Toggle **Automatic** or **Manual** reference examples. You can select these examples manually from your dataset or use the automatic selection option.

3. Enter the number of synthetic examples you want to generate.

4. Click **Generate**.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=4ec726f80ee38a829ade96caedb61925" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="689" width="689" data-og-height="383" height="383" data-path="langsmith/images/generate-synthetic-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=2a49e15cf0be32a284ab6749941367d4 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=c6d6ee9674352b0bc75573cd7f4a5151 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=782626122e3c9ebef0cf67c0cd2060cf 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6e1a7c8b25bc8d15324dfd7c50a8bd5c 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=bb757aa11517627441a4c8317f7fb313 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6bab6b7f6f500ac9044ab3b816484af5 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6c0ba9da5bf342e702c23406bdfdf18c" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="674" width="674" data-og-height="361" height="361" data-path="langsmith/images/generate-synthetic-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=f4f82b37bd0e5b33bb7d113f5d67de38 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=b07f30b95f9c7dd7c3c21919d2a6ee86 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=64a4cbcd28b292e109f2807125516981 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=fd764de7b0999e27b1b336260f206cd8 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=77c5081c2232e183b372a418bffce330 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=3be24e6515512dda957b4ce6ba2cd732 2500w" />
   </div>

5. The examples will appear on the **Select generated examples** page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click **Save Examples**.

6. Each example will be validated against your specified dataset schema and tagged as **synthetic** in the source metadata.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=146c5f6238415bb8d77da15a8a17c839" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1781" width="1781" data-og-height="856" height="856" data-path="langsmith/images/select-generated-examples-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1175f7421bb4c5456ebccccddcc2bd56 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=d4d85335f1ec8826acce6be4a56d20d2 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=167488b8860682ee6b3ceb6dee4e8604 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=468acc35c4f46ce0997f1c552504c3ff 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3131cd4260d4cff89da156c774243500 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=23b27fab3ff759c7f7d48d7057b19409 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1f1235b31b2d86cf5c7c615c84061e9c" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1779" width="1779" data-og-height="838" height="838" data-path="langsmith/images/select-generated-examples-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=704f33356493a5913bae1c6a744acc2d 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=9222e34f0b8693ecfde55d33378c887a 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=8e96a9e18d8142347fbcd3e6ecbb64c5 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3e6c57060fb9bb371fafbe272ed83381 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=a81b2b4775f89ce9004a3cc1ea38ae57 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1917622069d093a1f43388bac001a050 2500w" />
   </div>

### Create a dataset schema

LangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard [JSON schema](https://json-schema.org/), with the addition of a few [prebuilt types](/langsmith/dataset-json-types) that make it easier to type common primitives like messages and tools.

Certain fields in your schema have a `+ Transformations` option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the `convert to OpenAI messages` transformation will convert message-like objects, like LangChain messages, to OpenAI message format.

For the full list of available transformations, see [our reference](/langsmith/dataset-transformations).

<Note>
  If you plan to collect production traces in your dataset from LangChain [ChatModels](https://python.langchain.com/do/langsmith/observability-concepts/chat_models/) or from OpenAI calls using the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client), we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.

Please see the [dataset transformations reference](/langsmith/dataset-transformations) for more information.
</Note>

### Create and manage dataset splits

Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin.

In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.

In order to create and manage splits in the app, you can select some examples in your dataset and click "Add to Split". From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=014aa1fdc735f055c9e66a2a18720d4c" alt="Add to Split" data-og-width="1309" width="1309" data-og-height="915" height="915" data-path="langsmith/images/add-to-split2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c1ef18d892f91c218d51d47fb313d81 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1168e3dd272772dde9cd92d767b832f8 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be237dec928908095462f5314bb795fd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d78b720e3dd1e98bb4880be2f18b4e1 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d68fd43c3f0e68fad3cc47f3c80d4f04 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95711a7ca8fe56f1a724568454f6fde5 2500w" />

### Edit example metadata

You can add metadata to your examples by clicking on an example and then clicking "Edit" on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then [group by](/langsmith/analyze-an-experiment#group-results-by-metadata) when analyzing experiment results or [filter by](/langsmith/manage-datasets-programmatically#list-examples-by-metadata) when you call `list_examples` in the SDK.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-metadata.gif?s=d7235cc2b83913561faccb5083780f17" alt="Add Metadata" data-og-width="1010" width="1010" data-og-height="720" height="720" data-path="langsmith/images/add-metadata.gif" data-optimize="true" data-opv="3" />

You can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table.

* **Filter by split**: Select split > Select a split to filter by
* **Filter by metadata**: Filters > Select "Metadata" from the dropdown > Select the metadata key and value to filter on
* **Full-text search**: Filters > Select "Full Text" from the dropdown > Enter your search criteria

You may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2d1f300884d5e886267a137a3cb3e4c7" alt="Filters Applied to Examples" data-og-width="1307" width="1307" data-og-height="370" height="370" data-path="langsmith/images/filters-applied.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bd615f72fbf783850caac0b6f06bb7 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4fcef71fff0a7251a559f3d275f527da 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=591c3a65c909f386e745e6f76107722f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1bc1167f20d8a0b80ce88f3001735a4d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=769ab5f104184d1f5841185666c50dbf 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7130de5c7f60a1e597ddbffa8575f056 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-in-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## maxReplicas: 160

**URL:** llms-txt#maxreplicas:-160

---

## Create a dataset

**URL:** llms-txt#create-a-dataset

**Contents:**
- Run a single experiment

examples = [
    {
        "inputs": {"text": "Shut up, idiot"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "You're a wonderful person"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "This is the worst thing ever"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "I had a great day today"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "Nobody likes you"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "This is unacceptable. I want to speak to the manager."},
        "outputs": {"label": "Not toxic"},
    },
]

dataset_name = "Toxic Queries - API Example"
dataset = client.create_dataset(dataset_name=dataset_name)
client.create_examples(dataset_id=dataset.id, examples=examples)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Run a single experiment

First, pull all of the examples you'd want to use in your experiment.
```

---

## How to kick off background runs

**URL:** llms-txt#how-to-kick-off-background-runs

**Contents:**
- Setup
- Check runs on thread
- Start runs on thread

Source: https://docs.langchain.com/langsmith/background-run

This guide covers how to kick off background runs for your agent.
This can be useful for long running jobs.

First let's set up our client and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Start runs on thread

Now let's kick off a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

The first time we poll it, we can see `status=pending`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can join the run, wait for it to finish and check that status again:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Perfect! The run succeeded as we would expect. We can double check that the run worked as expected by printing out the final state:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can also just print the content of the last AIMessage:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/background-run.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
```

---

## Define the graph

**URL:** llms-txt#define-the-graph

graph = (
    StateGraph(MessagesState)
    ...
    .compile()
    .with_config({'callbacks': [tracer]})
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/observability.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## BedrockEmbeddings

**URL:** llms-txt#bedrockembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you get started with Amazon Bedrock [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html).

### Integration details

| Class                                                                                | Package                                                                   | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/bedrock/) |                                            Downloads                                           |                                           Version                                           |
| :----------------------------------------------------------------------------------- | :------------------------------------------------------------------------ | :---: | :----------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------: |
| [Bedrock](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html) | [@langchain/aws](https://api.js.langchain.com/modules/langchain_aws.html) |   ❌   |                                           ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/aws?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/aws?style=flat-square\&label=%20&) |

To access Bedrock embedding models you'll need to create an AWS account, get an API key, and install the `@langchain/aws` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

---

## Create dataset

**URL:** llms-txt#create-dataset

examples = [
    {
        "inputs": {"messages": [{"role": "user", "content": "i bought some tracks recently and i dont like them"}]},
        "outputs": {"route": "refund_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "I was thinking of purchasing some Rolling Stones tunes, any recommendations?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i want a refund on purchase 237"}, {"role": "assistant", "content": "I've refunded you a total of $1.98. How else can I help you today?"}, {"role": "user", "content": "did prince release any albums in 2000?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
]

dataset_name = "Chinook Customer Service Bot: Intent Classifier"
if not client.has_dataset(dataset_name=dataset_name):
    dataset = client.create_dataset(dataset_name=dataset_name)
    client.create_examples(
        dataset_id=dataset.id,
        examples=examples
    )

---

## longlived: "7776000"  # 90 days (default is 400 days)

**URL:** llms-txt#longlived:-"7776000"--#-90-days-(default-is-400-days)

---

## How to evaluate a runnable

**URL:** llms-txt#how-to-evaluate-a-runnable

**Contents:**
- Setup
- Evaluate
- Related

Source: https://docs.langchain.com/langsmith/langchain-runnable

<Info>
  * `langchain`: [Python](https://python.langchain.com) and [JS/TS](https://js.langchain.com)
  * Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) and [JS/TS](https://js.langchain.com/docs/concepts/runnables/)
</Info>

`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) objects (such as chat models, retrievers, chains, etc.) can be passed directly into `evaluate()` / `aevaluate()`.

Let's define a simple chain to evaluate. First, install all the required packages:

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

The runnable is traced appropriately for each output.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b9dac41dafb9a1cbb3b90fc508f212f7" alt="Runnable Evaluation" data-og-width="2288" width="2288" data-og-height="1052" height="1052" data-path="langsmith/images/runnable-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=39f7bda57df5d29c72729390065342c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2bbfa58f877541adff85056d2d4910c7 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=198967ebb494d0577fac294f879f348c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dd0758a55517d6899d445bd203bc7d03 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6fa8f6a044a0b978ef727390f18f5ce3 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=40dad8febfdaf0756c90b6326e2c4415 2500w" />

* [How to evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langchain-runnable.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Now define a chain:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Evaluate

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

<CodeGroup>
```

---

## get the "memory" by ID

**URL:** llms-txt#get-the-"memory"-by-id

item = store.get(namespace, "a-memory") # [!code highlight]

---

## Create two test users

**URL:** llms-txt#create-two-test-users

print(f"Creating test users: {email1} and {email2}")
await sign_up(email1, password)
await sign_up(email2, password)
python  theme={null}
async def login(email: str, password: str):
    """Get an access token for an existing user."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/token?grant_type=password",
            json={
                "email": email,
                "password": password
            },
            headers={
                "apikey": SUPABASE_ANON_KEY,
                "Content-Type": "application/json"
            },
        )
        assert response.status_code == 200
        return response.json()["access_token"]

**Examples:**

Example 1 (unknown):
```unknown
⚠️ Before continuing: Check your email and click both confirmation links. Supabase will reject `/login` requests until after you have confirmed your users' email.

Now test that users can only see their own data. Make sure the server is running (run `langgraph dev`) before proceeding. The following snippet requires the "anon public" key that you copied from the Supabase dashboard while [setting up the auth provider](#setup-auth-provider) previously.
```

---

## my_graph.py.

**URL:** llms-txt#my_graph.py.

**Contents:**
  - Opt-out of configurable headers

@contextlib.asynccontextmanager
async def generate_agent(config):
  organization_id = config["configurable"].get("x-organization-id")
  if organization_id == "org1":
    graph = ...
    yield graph
  else:
    graph = ...
    yield graph

json  theme={null}
{
  "graphs": {"agent": "my_grph.py:generate_agent"}
}
json  theme={null}
{
  "http": {
    "configurable_headers": {
      "exclude": ["*"]
    }
  }
}
```

This will exclude all headers from being added to your run's configuration.

Note that exclusions take precedence over inclusions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configurable-headers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Opt-out of configurable headers

If you'd like to opt-out of configurable headers, you can simply set a wildcard pattern in the `exclude` list:
```

---

## Schema for routing user intent.

**URL:** llms-txt#schema-for-routing-user-intent.

---

## meaning you can use it as you would any other runnable.

**URL:** llms-txt#meaning-you-can-use-it-as-you-would-any-other-runnable.

---

## Trace Claude Code

**URL:** llms-txt#trace-claude-code

**Contents:**
- Quick Start

Source: https://docs.langchain.com/langsmith/trace-claude-code

[Claude Code](https://docs.claude.com/en/docs/claude-code/overview) is one of the most impressive and useful AI coding tools to date. Claude code emits events for monitoring and observability. LangSmith can collect and display these events to give you a full detailed log on what Claude Code does under the hood.

You can integrate LangSmith tracing with Claude Code by setting the following environment variables in the environment in which you run Claude Code.

```bash  theme={null}

---

## Initialize the LangSmith client with the anonymization functions

**URL:** llms-txt#initialize-the-langsmith-client-with-the-anonymization-functions

langsmith_client = Client(
    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize
)

---

## Trace with CrewAI

**URL:** llms-txt#trace-with-crewai

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-crewai

LangSmith can capture traces generated by [CrewAI](https://github.com/crewAIInc/crewAI) using OpenInference's CrewAI instrumentation. This guide shows you how to automatically capture traces from your CrewAI multi-agent workflows and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import OtelSpanProcessor
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from openinference.instrumentation.crewai import CrewAIInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:
```

---

## Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,

**URL:** llms-txt#reading-a-thread.-since-this-is-also-more-specific-than-the-generic-@auth.on-handler,-and-the-@auth.on.threads-handler,

---

## Example usage

**URL:** llms-txt#example-usage

**Contents:**
- Advanced usage
  - Custom metadata and tags

if __name__ == "__main__":
    task = """
    Create a Python function that implements a binary search algorithm.
    The function should:
    - Take a sorted list and a target value as parameters
    - Return the index of the target if found, or -1 if not found
    - Include proper error handling and documentation
    """

result = run_code_review_session(task)
    print(f"Result: {result}")
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your AutoGen application:
```

---

## Initialize the LangSmith Client so we can use to get the dataset

**URL:** llms-txt#initialize-the-langsmith-client-so-we-can-use-to-get-the-dataset

---

## Hosting LangSmith

**URL:** llms-txt#hosting-langsmith

**Contents:**
- Choose your hosting option
  - Comparison
  - Related

Source: https://docs.langchain.com/langsmith/hosting

This section covers how to host and manage **LangSmith**: the infrastructure, services, and datastores that power your observability, evaluation, and agent deployment capabilities.

<Callout icon="building" color="#2563EB" iconType="regular">
  **Start here if you're setting up or maintaining LangSmith infrastructure.** If you're ready to deploy your agent application, the [Deployment section](/langsmith/deployments) covers application structure, deployment configuration, and guides for deploying to Cloud.
</Callout>

## Choose your hosting option

You'll deploy LangSmith in one of three modes:

* [**Cloud**](/langsmith/cloud): fully managed by LangChain
* [**Hybrid**](/langsmith/hybrid): LangChain manages the <Tooltip tip="The LangSmith UI and APIs for managing deployments.">control plane</Tooltip>; you host the <Tooltip tip="The runtime environment where your LangGraph Servers and agents execute.">data plane</Tooltip>
* [**Self-hosted**](/langsmith/self-hosted): you manage the full stack within your infrastructure

<Columns cols={3}>
  <Card title="Cloud" icon="cloud" iconType="solid" href="/langsmith/cloud" cta="Get started">
    Run all components fully managed in LangChain’s cloud.
  </Card>

<Card title="Hybrid" icon="cloud" href="/langsmith/hybrid" cta="Set up Hybrid">
    **(Enterprise)** Manage the data plane running in your cloud while LangChain manages the control plane.
  </Card>

<Card title="Self-hosted" icon="server" iconType="solid" href="/langsmith/self-hosted" cta="Run self-hosted">
    **(Enterprise)** Run the full LangSmith or run standalone LangGraph Servers without the control plane UI.
  </Card>
</Columns>

Refer to the following table for a comparison of hosting options:

| Feature                                        | **Cloud**                           | **Hybrid**                                                        | **Self-Hosted**                   |
| ---------------------------------------------- | ----------------------------------- | ----------------------------------------------------------------- | --------------------------------- |
| **Infrastructure location**                    | LangChain's cloud                   | Split: Control plane in LangChain cloud, data plane in your cloud | Your cloud                        |
| **Who manages updates**                        | LangChain                           | LangChain (control plane), You (data plane)                       | You                               |
| **Who manages CI/CD for your apps**            | LangChain                           | You                                                               | You                               |
| **Can deploy applications?**                   | ✅ Yes                               | ✅ Yes                                                             | ✅ Yes (with full platform option) |
| **Observability data location**                | LangChain cloud                     | LangChain cloud                                                   | Your cloud                        |
| **[Pricing](https://www.langchain.com/plans)** | Plus tier                           | Enterprise                                                        | Enterprise                        |
| **Best for**                                   | Quick setup, managed infrastructure | Data residency requirements + managed control plane               | Full control, data isolation      |

<Tip>
  You can [run a LangGraph Server locally for free](/langsmith/local-server) for testing and development.
</Tip>

* [Plans](https://langchain.com/pricing)
* [Pricing](https://www.langchain.com/plans)
* [Observability](/langsmith/observability)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/hosting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Decide whether to retrieve

**URL:** llms-txt#decide-whether-to-retrieve

workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `retriever_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

---

## How to evaluate an existing experiment (Python only)

**URL:** llms-txt#how-to-evaluate-an-existing-experiment-(python-only)

Source: https://docs.langchain.com/langsmith/evaluate-existing-experiment

Evaluation of existing experiments is currently only supported in the Python SDK.

If you have already run an experiment and want to add additional evaluation metrics, you can apply any evaluators to the experiment using the `evaluate()` / `aevaluate()` methods as before. Just pass in the experiment name / ID instead of a target function:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-existing-experiment.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create a function that will take in a list of examples and format them into a string

**URL:** llms-txt#create-a-function-that-will-take-in-a-list-of-examples-and-format-them-into-a-string

**Contents:**
  - NEW CODE ###
- Semantic search over examples

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)
### NEW CODE ###

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    # We can now pull down the examples from the dataset
    # We do this inside the function so it always get the most up-to-date examples,
    # But this can be done outside and cached for speed if desired
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))  # <- New Code
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
python  theme={null}
ls_client = Client()
run_id = uuid.uuid4()
topic_classifier(
    "address bug in documentation",
    langsmith_extra={"run_id": run_id})
python  theme={null}
import numpy as np

def find_similar(examples, topic, k=5):
    inputs = [e.inputs['topic'] for e in examples] + [topic]
    vectors = client.embeddings.create(input=inputs, model="text-embedding-3-small")
    vectors = [e.embedding for e in vectors.data]
    vectors = np.array(vectors)
    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]
    examples = [examples[i] for i in args]
    return examples
python  theme={null}
ls_client = Client()

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))
    examples = find_similar(examples, topic)
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/optimize-classifier.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`
```

Example 2 (unknown):
```unknown
## Semantic search over examples

One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.

In order to do this, we can first define an example to find the `k` most similar examples:
```

Example 3 (unknown):
```unknown
We can then use that in the application
```

---

## [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10},-'__metadata__':-{'cached':-true}}]

**Contents:**
- Edges
  - Normal Edges
  - Conditional Edges
  - Entry Point
  - Conditional Entry Point
- `Send`
- `Command`
  - When should I use Command instead of conditional edges?
  - Navigating to a node in a parent graph
  - Using inside tools

python  theme={null}
graph.add_edge("node_a", "node_b")
python  theme={null}
graph.add_conditional_edges("node_a", routing_function)
python  theme={null}
graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
python  theme={null}
graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state['subjects']]

graph.add_conditional_edges("node_a", continue_to_jokes)
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    if state["foo"] == "bar":
        return Command(update={"foo": "baz"}, goto="my_other_node")
python  theme={null}
def my_node(state: State) -> Command[Literal["other_subgraph"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
@dataclass
class ContextSchema:
    llm_provider: str = "openai"

graph = StateGraph(State, context_schema=ContextSchema)
python  theme={null}
graph.invoke(inputs, context={"llm_provider": "anthropic"})
python  theme={null}
from langgraph.runtime import Runtime

def node_a(state: State, runtime: Runtime[ContextSchema]):
    llm = get_llm(runtime.context.llm_provider)
    # ...
python  theme={null}
graph.invoke(inputs, config={"recursion_limit": 5}, context={"llm": "anthropic"})
```

Read [this how-to](/oss/python/langgraph/graph-api#impose-a-recursion-limit) to learn more about how the recursion limit works.

It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](/oss/python/langgraph/graph-api.md#visualize-your-graph) for more info.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
1. First run takes two seconds to run (due to mocked expensive computation).
2. Second run utilizes cache and returns quickly.

## Edges

Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

* Normal Edges: Go directly from one node to the next.
* Conditional Edges: Call a function to determine which node(s) to go to next.
* Entry Point: Which node to call first when user input arrives.
* Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.

### Normal Edges

If you **always** want to go from node A to node B, you can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.
```

Example 2 (unknown):
```unknown
### Conditional Edges

If you want to **optionally** route to one or more edges (or optionally terminate), you can use the [`add_conditional_edges`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a "routing function" to call after that node is executed:
```

Example 3 (unknown):
```unknown
Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.

By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.

You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.
```

Example 4 (unknown):
```unknown
<Tip>
  Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.
</Tip>

### Entry Point

The entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.
```

---

## Add metadata and tags to traces

**URL:** llms-txt#add-metadata-and-tags-to-traces

Source: https://docs.langchain.com/langsmith/add-metadata-tags

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the [Concepts](/langsmith/observability-concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](/langsmith/filter-traces-in-application) page.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-metadata-tags.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Data storage and privacy

**URL:** llms-txt#data-storage-and-privacy

**Contents:**
- CLI
- LangGraph Server
  - LangSmith Tracing
  - In-memory development server
  - Standalone Server
- Studio
- Quick reference

Source: https://docs.langchain.com/langsmith/data-storage-and-privacy

This document describes how data is processed in the LangGraph CLI and the LangGraph Server for both the in-memory server (`langgraph dev`) and the local Docker server (`langgraph up`). It also describes what data is tracked when interacting with the hosted Studio frontend.

LangGraph **CLI** is the command-line interface for building and running LangGraph applications; see the [CLI guide](/langsmith/cli) to learn more.

By default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process's OS, OS version, Python version, the CLI version, the command name (`dev`, `up`, `run`, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic [here](https://github.com/langchain-ai/langgraph/blob/main/libs/cli/langgraph-cli/analytics.py).

You can disable all CLI telemetry by setting `LANGGRAPH_CLI_NO_ANALYTICS=1`.

<a id="in-memory-docker" />

The [LangGraph Server](/langsmith/langgraph-server) provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for `langgraph dev`) or a PostgreSQL database (for `langgraph up` and in all deployments).

### LangSmith Tracing

When running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting `LANGSMITH_TRACING=false` in your server's runtime environment.

<a id="langgraph-dev" />

### In-memory development server

`langgraph dev` runs an [in-memory development server](/langsmith/local-server) as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a `.langgraph_api` directory in the current working directory. Apart from the telemetry data described in the [CLI](#cli) section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.

<a id="langgraph-up" />

### Standalone Server

`langgraph up` builds your local package into a Docker image and runs the server as the [data plane](/langsmith/self-hosted) consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid `LANGGRAPH_AES_KEY` environment variable. You can also specify [TTLs](/langsmith/configure-ttl) for checkpoints and cross-thread memories in `langgraph.json` to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.

Additional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).

If you've disabled [tracing](#langsmith-tracing), no user data is persisted externally unless your graph code explicitly contacts an external service.

[Studio](/langsmith/studio) is a graphical interface for interacting with your LangGraph server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at [smith.langchain.com](https://smith.langchain.com), it is run in your browser and connects directly to your local LangGraph server so that no data needs to be sent to LangSmith.

If you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:

* Page visits and navigation patterns
* User actions (button clicks)
* Browser type and version
* Screen resolution and viewport size

Importantly, no application data or code (or other sensitive configuration details) are collected. All of that is stored in the persistence layer of your LangGraph server. When using Studio anonymously, no account creation is required and usage analytics are not collected.

In summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.

| Variable                       | Purpose                   | Default                |
| ------------------------------ | ------------------------- | ---------------------- |
| `LANGGRAPH_CLI_NO_ANALYTICS=1` | Disable CLI analytics     | Analytics enabled      |
| `LANGSMITH_API_KEY`            | Enable LangSmith tracing  | Tracing disabled       |
| `LANGSMITH_TRACING=false`      | Disable LangSmith tracing | Depends on environment |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-storage-and-privacy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Instrument OpenAI calls

**URL:** llms-txt#instrument-openai-calls

OpenAIInstrumentor().instrument()

---

## Custom state can be passed in invoke

**URL:** llms-txt#custom-state-can-be-passed-in-invoke

**Contents:**
- Common patterns
  - Trim messages
  - Delete messages
  - Summarize messages
- Access memory
  - Tools

result = agent.invoke(
    {
        "messages": [{"role": "user", "content": "Hello"}],
        "user_id": "user_123",  # [!code highlight]
        "preferences": {"theme": "dark"}  # [!code highlight]
    },
    {"configurable": {"thread_id": "1"}})
python  theme={null}
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig
from typing import Any

@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

if len(messages) <= 3:
        return None  # No changes needed

first_msg = messages[0]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }

agent = create_agent(
    model,
    tools=tools,
    middleware=[trim_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""
python  theme={null}
from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]
python  theme={null}
from langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]
python  theme={null}
from langchain.messages import RemoveMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig

@after_model
def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:
    """Remove old messages to keep conversation manageable."""
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
    return None

agent = create_agent(
    "openai:gpt-5-nano",
    tools=[],
    system_prompt="Please be concise and to the point.",
    middleware=[delete_old_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

for event in agent.stream(
    {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])

for event in agent.stream(
    {"messages": [{"role": "user", "content": "what's my name?"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])

[('human', "hi! I'm bob")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
[('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

checkpointer = InMemorySaver()

agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="openai:gpt-4o-mini",
            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens
            messages_to_keep=20,  # Keep last 20 messages after summary
        )
    ],
    checkpointer=checkpointer,
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob!
"""
python  theme={null}
from langchain.agents import create_agent, AgentState
from langchain.tools import tool, ToolRuntime

class CustomState(AgentState):
    user_id: str

@tool
def get_user_info(
    runtime: ToolRuntime
) -> str:
    """Look up user info."""
    user_id = runtime.state["user_id"]
    return "User is John Smith" if user_id == "user_123" else "Unknown user"

agent = create_agent(
    model="openai:gpt-5-nano",
    tools=[get_user_info],
    state_schema=CustomState,
)

result = agent.invoke({
    "messages": "look up user information",
    "user_id": "user_123"
})
print(result["messages"][-1].content)

**Examples:**

Example 1 (unknown):
```unknown
## Common patterns

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

<CardGroup cols={2}>
  <Card title="Trim messages" icon="scissors" href="#trim-messages" arrow>
    Remove first or last N messages (before calling LLM)
  </Card>

  <Card title="Delete messages" icon="trash" href="#delete-messages" arrow>
    Delete messages from LangGraph state permanently
  </Card>

  <Card title="Summarize messages" icon="layer-group" href="#summarize-messages" arrow>
    Summarize earlier messages in the history and replace them with a summary
  </Card>

  <Card title="Custom strategies" icon="gears">
    Custom strategies (e.g., message filtering, etc.)
  </Card>
</CardGroup>

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens).

One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.

To trim message history in an agent, use the [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) middleware decorator:
```

Example 2 (unknown):
```unknown
### Delete messages

You can delete messages from the graph state to manage the message history.

This is useful when you want to remove specific messages or clear the entire message history.

To delete messages from the graph state, you can use the `RemoveMessage`.

For `RemoveMessage` to work, you need to use a state key with [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) [reducer](/oss/python/langgraph/graph-api#reducers).

The default [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) provides this.

To remove specific messages:
```

Example 3 (unknown):
```unknown
To remove **all** messages:
```

Example 4 (unknown):
```unknown
<Warning>
  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

  * Some providers expect message history to start with a `user` message
  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>
```

---

## ... Setup authenticate, etc.

**URL:** llms-txt#...-setup-authenticate,-etc.

**Contents:**
- Learn more

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,
    value: dict  # The payload being sent to this access method
) -> dict:  # Returns a filter dict that restricts access to resources
    if is_studio_user(ctx.user):
        return {}

filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
```

Only use this if you want to permit developer access to a graph deployed on the managed LangSmith SaaS.

* [Authentication & Access Control](/langsmith/auth)
* [Setting up custom authentication tutorial](/langsmith/set-up-custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create the graph with memory for persistence

**URL:** llms-txt#create-the-graph-with-memory-for-persistence

checkpointer = MemorySaver()

---

## Dataset prebuilt JSON schema types

**URL:** llms-txt#dataset-prebuilt-json-schema-types

Source: https://docs.langchain.com/langsmith/dataset-json-types

LangSmith recommends that you set a schema on the inputs and outputs of your dataset schemas to ensure data consistency and that your examples are in the right format for downstream processing, like running evals.

In order to better support LLM workflows, LangSmith has support for a few different predefined prebuilt types. These schemas are hosted publicly by the LangSmith API, and can be defined in your dataset schemas using [JSON Schema references](https://json-schema.org/understanding-json-schema/structuring#dollarref). The table of available schemas can be seen below

| Type    | JSON Schema Reference Link                                                                                                       | Usage                                                                                                                     |
| ------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| Message | [https://api.smith.langchain.com/public/schemas/v1/message.json](https://api.smith.langchain.com/public/schemas/v1/message.json) | Represents messages sent to a chat model, following the OpenAI standard format.                                           |
| Tool    | [https://api.smith.langchain.com/public/schemas/v1/tooldef.json](https://api.smith.langchain.com/public/schemas/v1/tooldef.json) | Tool definitions available to chat models for function calling, defined in OpenAI's JSON Schema inspired function format. |

LangSmith lets you define a series of transformations that collect the above prebuilt types from your traces and add them to your dataset. For more info on available transformations, see our [reference](/langsmith/dataset-transformations)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dataset-json-types.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Subgraph

**URL:** llms-txt#subgraph

def subgraph_node_1(state: State):
    return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

---

## Trace with Google ADK

**URL:** llms-txt#trace-with-google-adk

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-google-adk

LangSmith supports tracing Google Agent Development Kit (ADK) applications through the OpenTelemetry integration. This guide shows you how to automatically capture traces from your [Google ADK](https://github.com/google/adk-python) agents and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your LangSmith API key and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Google ADK application, import and configure the LangSmith OpenTelemetry integration. This will automatically instrument Google ADK spans for OpenTelemetry.

```python  theme={null}
from langsmith.integrations.otel import configure

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your LangSmith API key and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Google ADK application, import and configure the LangSmith OpenTelemetry integration. This will automatically instrument Google ADK spans for OpenTelemetry.
```

---

## Release policy

**URL:** llms-txt#release-policy

Source: https://docs.langchain.com/oss/python/release-policy

This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:

<Tabs>
  <Tab title="LangChain">
    The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, partner packages, etc.)

With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `langchain-core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.

Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.

With LangChain 1.0's adoption of semantic versioning:

* Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)
    * Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes
    * Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### Stability of other packages

The stability of other packages in the LangChain ecosystem may vary:

* **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.

* **`langchain-community`** is a community maintained package that contains 3rd party integrations. Due to the number of integrations there, `langchain-community` does not follow the same strict semantic versioning policy as `langchain` and `langchain-core`. See the "Special considerations" section under Long-term support for more details.

## Deprecation policy

We will generally avoid deprecating features until a better alternative is available.

With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).

When a feature is deprecated in `langchain` or `langchain-core`, we will:

* Clearly mark it as deprecated in the code and documentation
    * Provide migration guidance to the recommended alternative
    * Provide security updates for the deprecated feature through all 1.x minor releases

In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.

## Long-term support (LTS)

LangChain follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangChain 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: Security patches and critical bug fixes

### Special considerations

**langchain-community 0.4**: Due to the nature of community contributions and third-party integrations, `langchain-community` may have breaking changes on minor releases. It has been released as version 0.4 to reflect this different stability policy.
  </Tab>

<Tab title="LangGraph">
    LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.

We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.

**Minor** releases are typically released every 1-2 months with new features and improvements.

**Patch** releases are released as needed, often weekly, to address bugs and security issues.

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.

Features marked as `beta` in the documentation are:

* Feature-complete and tested
    * Safe for production use with the understanding they may change
    * Subject to minor API adjustments based on user feedback

### Experimental features

Features marked as `experimental` or `alpha`:

* Are under active development
    * May change significantly or be removed
    * Should be used with caution in production

APIs prefixed with underscore (`_`) or explicitly marked as internal:

* Are not part of the public API
    * May change without notice
    * Should not be used directly

## Deprecation policy

When deprecating features:

1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance
    2. **Grace Period**: Deprecated features remain functional for at least one minor version
    3. **Removal**: Features are removed only in major version releases
    4. **Migration Support**: We provide migration guides and, when possible, automated tools

## Platform compatibility

* We support Python versions that are actively maintained by the Python Software Foundation
    * Python version requirements may change only in major releases
    * Currently requires Python 3.10 or later

Breaking changes are only introduced in major versions and include:

* Removal of deprecated APIs
    * Changes to required parameters
    * Changes to default behavior that affect existing applications
    * Minimum Python/Node.js version updates

For major version upgrades, we provide:

* Comprehensive migration guides
    * Automated migration scripts when feasible
    * Extended support period for the previous major version
    * Clear documentation of all breaking changes

## Long-term support (LTS)

LangGraph follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangGraph 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: All security patches and critical bug fixes

* [Versioning](/oss/python/versioning) - Version numbering and support details
    * [Releases](/oss/python/releases) - Version-specific release notes and migration guides
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Call the graph: here we call it to generate a list of jokes

**URL:** llms-txt#call-the-graph:-here-we-call-it-to-generate-a-list-of-jokes

**Contents:**
- Create and control loops

for step in graph.stream({"topic": "animals"}):
    print(step)

{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}
{'best_joke': {'best_selected_joke': 'penguins'}}
python  theme={null}
builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

def route(state: State) -> Literal["b", END]:
    if termination_condition(state):
        return END
    else:
        return "b"

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke(inputs, {"recursion_limit": 3})
except GraphRecursionError:
    print("Recursion Error")
python  theme={null}
import operator
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Node A sees {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Node B sees {state["aggregate"]}')
    return {"aggregate": ["B"]}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Create and control loops

When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/python/langgraph/graph-api#end-node) node once we reach some termination condition.

You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/python/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/python/langgraph/graph-api#recursion-limit).

Let's consider a simple graph with a loop to better understand how these mechanisms work.

<Tip>
  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).
</Tip>

When creating a loop, you can include a conditional edge that specifies a termination condition:
```

Example 3 (unknown):
```unknown
To control the recursion limit, specify `"recursionLimit"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:
```

Example 4 (unknown):
```unknown
Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.
```

---

## Observability

**URL:** llms-txt#observability

**Contents:**
- Prerequisites
- Enable tracing
- Trace selectively

Source: https://docs.langchain.com/oss/python/langgraph/observability

Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following:

* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces).
* [Evaluate the application performance](/oss/python/langchain/evals).
* [Monitor the application](/langsmith/dashboards).

Before you begin, ensure you have the following:

* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

To enable tracing for your application, set the following environment variables:

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:

```python  theme={null}
import langsmith as ls

**Examples:**

Example 1 (unknown):
```unknown
By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

## Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:
```

---

## Conversation 2: Use that knowledge

**URL:** llms-txt#conversation-2:-use-that-knowledge

agent.invoke({
    "messages": [{"role": "user", "content": "What framework are we using?"}]
})

---

## Property access

**URL:** llms-txt#property-access

---

## Worker state

**URL:** llms-txt#worker-state

class WorkerState(TypedDict):
    section: Section
    completed_sections: Annotated[list, operator.add]

---

## TypedDict defines the structure of user information for the LLM

**URL:** llms-txt#typeddict-defines-the-structure-of-user-information-for-the-llm

class UserInfo(TypedDict):
    name: str

---

## GoogleGenerativeAIEmbeddings

**URL:** llms-txt#googlegenerativeaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/google_generativeai

This will help you get started with Google Generative AI [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html).

### Integration details

| Class                                                                                                                           | Package                                                                | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/) |                                                Downloads                                                |                                                Version                                               |
| :------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [`GoogleGenerativeAIEmbeddings`](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html) | [`@langchain/google-genai`](https://npmjs.com/@langchain/google-genai) |   ❌   |                                                 ✅                                                 | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\&label=%20&) |

To access Google Generative AI embedding models you'll need to sign up for a Google AI account, get an API key, and install the `@langchain/google-genai` integration package.

Get an API key here: [ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup).

Next, set your key as an environment variable named `GOOGLE_API_KEY`:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## Frequently asked questions

**URL:** llms-txt#frequently-asked-questions

**Contents:**
- Observability
  - *I can't create API keys or manage users in the UI, what's wrong?*
  - *How does load balancing/ingress work*?
  - *How can we authenticate to the application?*
  - *Can I use external storage services?*
  - *Does my application need egress to function properly?*
  - *Resource requirements for the application?*
  - SAML SSO FAQs
  - SCIM FAQs
- Deployment

Source: https://docs.langchain.com/langsmith/faq

### *I can't create API keys or manage users in the UI, what's wrong?*

* You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the [configuration section.](/langsmith/self-host-sso)

### *How does load balancing/ingress work*?

* You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.
* You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx.

### *How can we authenticate to the application?*

* Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.

You can find more information on setting up SSO in the [configuration section.](/langsmith/self-host-sso)

### *Can I use external storage services?*

* You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the [configuration section](/langsmith/self-hosted) for more information.

### *Does my application need egress to function properly?*

Our deployment only needs egress for a few things (most of which can reside within your VPC):

* Fetching images (If mirroring your images, this may not be needed)

* Talking to any LLM endpoints

* Talking to any external storage services you may have configured

* Fetching OAuth information

* Subscription Metrics and Operational Metadata (if not running in offline mode)

* Requires egress to `https://beacon.langchain.com`
  * See [Egress](/langsmith/self-host-egress) for more information

Your VPC can set up rules to limit any other access. Note: We require the `X-Organization-Id` and `X-Tenant-Id` headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called "tenant") the request is for.

### *Resource requirements for the application?*

* In kubernetes, we recommend a minimum helm configuration which can be found in [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/medium_size.yaml). For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.
* For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.
* For Redis, we recommend 4GB of RAM and 2 CPUs.
* For Clickhouse, we recommend 32GB of RAM and 8 CPUs.

#### *How do I change a SAML SSO user's email address?*

Some identity providers retain the original `User ID` through an email change while others do not, so we recommend that you follow these steps to avoid duplicate users in LangSmith:

1. Remove the user from the organization (see [here](/langsmith/set-up-a-workspace#manage-users))
2. Change their email address in the IdP
3. Have them login to LangSmith again via SAML SSO - this will trigger the usual [JIT provisioning](#just-in-time-jit-provisioning) flow with their new email address

#### *How do I fix "405 method not allowed"?*

Ensure you're using the correct ACS URL: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)

#### *Can I use SCIM without SAML SSO?*

* **Cloud**: No, SAML SSO is required for SCIM in cloud deployments
* **Self-hosted**: Yes, SCIM works with OAuth with Client Secret authentication mode

#### *What happens if I have both JIT provisioning and SCIM enabled?*

JIT provisioning and SCIM can conflict with each other. We recommend disabling JIT provisioning before enabling SCIM to ensure consistent user provisioning behavior.

#### *How do I change a user's role or workspace access?*

Update the user's group membership in your IdP. The changes will be synchronized to LangSmith according to the [role precedence rules](#role-precedence).

#### *What happens when a user is removed from all groups?*

The user will be deprovisioned from your LangSmith organization according to your IdP's deprovisioning settings.

#### *Can I use custom group names?*

Yes. If your identity provider supports syncing alternate fields to the `displayName` group attribute, you may use an alternate attribute (like `description`) as the `displayName` in LangSmith and retain full customizability of the identity provider group name. Otherwise, groups must follow the specific naming convention described in the [Group Naming Convention](#group-naming-convention) section to properly map to LangSmith roles and workspaces.

#### *Why is my Okta integration not working?*

See Okta's troubleshooting guide here: [https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-group-push-troubleshoot.htm](https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-group-push-troubleshoot.htm).

### Do I need to use LangChain to use LangGraph? What's the difference?

No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.

### How is LangGraph different from other agent frameworks?

Other agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.

### Does LangGraph impact the performance of my app?

LangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.

### Is LangGraph open source? Is it free?

Yes. LangGraph is an MIT-licensed open-source library and is free to use.

### How are LangGraph and LangSmith different?

LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangSmith is a service for deploying and scaling agentic applications, with an opinionated API for building agent UXs, plus an integrated developer UI.

| Features            | LangGraph (open source)                                   | LangSmith                                                                                              |
| ------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| Description         | Stateful orchestration framework for agentic applications | Scalable infrastructure for deploying LangGraph applications                                           |
| SDKs                | Python and JavaScript                                     | Python and JavaScript                                                                                  |
| HTTP APIs           | None                                                      | Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant |
| Streaming           | Basic                                                     | Dedicated mode for token-by-token messages                                                             |
| Checkpointer        | Community contributed                                     | Supported out-of-the-box                                                                               |
| Persistence Layer   | Self-managed                                              | Managed Postgres with efficient storage                                                                |
| Deployment          | Self-managed                                              | • Cloud <br /> • Free self-hosted <br /> • Enterprise (paid self-hosted)                               |
| Scalability         | Self-managed                                              | Auto-scaling of task queues and servers                                                                |
| Fault-tolerance     | Self-managed                                              | Automated retries                                                                                      |
| Concurrency Control | Simple threading                                          | Supports double-texting                                                                                |
| Scheduling          | None                                                      | Cron scheduling                                                                                        |
| Monitoring          | None                                                      | Integrated with LangSmith for observability                                                            |
| IDE integration     | Studio                                                    | Studio                                                                                                 |

### Is LangSmith open source?

No. LangSmith is proprietary software.

There is a free, self-hosted version of LangSmith with access to basic features. The Cloud deployment option and the Self-Hosted deployment options are paid services. [Contact our sales team](https://www.langchain.com/contact-sales) to learn more.

For more information, see our [LangSmith pricing page](https://www.langchain.com/pricing).

### Does LangGraph work with LLMs that don't support tool calling?

Yes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.

### Does LangGraph work with OSS LLMs?

Yes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don't. But tool calling is not necessary (see [this section](#does-langgraph-work-with-llms-that-dont-support-tool-calling)) so you can totally use LangGraph with OSS LLMs.

### Can I use Studio without logging in to LangSmith?

Yes! You can use the [development version of LangGraph Server](/langsmith/local-server) to run the backend locally.
This will connect to the Studio frontend hosted as part of LangSmith.
If you set an environment variable of `LANGSMITH_TRACING=false`, then no traces will be sent to LangSmith.

### What does "nodes executed" mean for LangSmith usage?

**Nodes Executed** is the aggregate number of nodes in a LangGraph application that are called and completed successfully during an invocation of the application. If a node in the graph is not called during execution or ends in an error state, these nodes will not be counted. If a node is called and completes successfully multiple times, each occurrence will be counted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/faq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Find memories about food preferences

**URL:** llms-txt#find-memories-about-food-preferences

---

## Trace generator functions

**URL:** llms-txt#trace-generator-functions

**Contents:**
- Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

Source: https://docs.langchain.com/langsmith/trace-generator-functions

In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.

LangSmith's tracing functionality natively supports streamed outputs via `generator` functions. Below is an example.

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-generator-functions.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## List of standard content blocks

**URL:** llms-txt#list-of-standard-content-blocks

**Contents:**
  - Standard content blocks
  - Multimodal
  - Content block reference
- Use with chat models

human_message = HumanMessage(content_blocks=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image", "url": "https://example.com/image.jpg"},
])
python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {"type": "thinking", "thinking": "...", "signature": "WaUjzkyp..."},
            {"type": "text", "text": "..."},
        ],
        response_metadata={"model_provider": "anthropic"}
    )
    message.content_blocks
    
    [{'type': 'reasoning',
      'reasoning': '...',
      'extras': {'signature': 'WaUjzkyp...'}},
     {'type': 'text', 'text': '...'}]
    python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {
                "type": "reasoning",
                "id": "rs_abc123",
                "summary": [
                    {"type": "summary_text", "text": "summary 1"},
                    {"type": "summary_text", "text": "summary 2"},
                ],
            },
            {"type": "text", "text": "...", "id": "msg_abc123"},
        ],
        response_metadata={"model_provider": "openai"}
    )
    message.content_blocks
    
    [{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},
     {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},
     {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]
    python  theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-5-nano", output_version="v1")
  python Image input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "url": "https://example.com/path/to/image.jpg"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {
              "type": "image",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "image/jpeg",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "file_id": "file-abc123"},
      ]
  }
  python PDF document input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "url": "https://example.com/path/to/document.pdf"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {
              "type": "file",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "application/pdf",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "file_id": "file-abc123"},
      ]
  }
  python Audio input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {
              "type": "audio",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "audio/wav",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {"type": "audio", "file_id": "file-abc123"},
      ]
  }
  python Video input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {
              "type": "video",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "video/mp4",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {"type": "video", "file_id": "file-abc123"},
      ]
  }
  python  theme={null}
        {
            "type": "text",
            "text": "Hello world",
            "annotations": []
        }
        python  theme={null}
        {
            "type": "reasoning",
            "reasoning": "The user is asking about...",
            "extras": {"signature": "abc123"},
        }
        python  theme={null}
        {
            "type": "tool_call",
            "name": "search",
            "args": {"query": "weather"},
            "id": "call_123"
        }
        ```
      </Accordion>

<Accordion title="ToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming tool call fragments

<ParamField body="type" type="string" required>
          Always `"tool_call_chunk"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="id" type="string">
          Tool call identifier
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="InvalidToolCall" icon="triangle-exclamation">
        **Purpose:** Malformed calls, intended to catch JSON parsing errors.

<ParamField body="type" type="string" required>
          Always `"invalid_tool_call"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool that failed to be called
        </ParamField>

<ParamField body="args" type="object">
          Arguments to pass to the tool
        </ParamField>

<ParamField body="error" type="string">
          Description of what went wrong
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Server-Side Tool Execution" icon="server">
    <AccordionGroup>
      <Accordion title="ServerToolCall" icon="wrench">
        **Purpose:** Tool call that is executed server-side.

<ParamField body="type" type="string" required>
          Always `"server_tool_call"`
        </ParamField>

<ParamField body="id" type="string" required>
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string" required>
          The name of the tool to be called.
        </ParamField>

<ParamField body="args" type="string" required>
          Partial tool arguments (may be incomplete JSON)
        </ParamField>
      </Accordion>

<Accordion title="ServerToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming server-side tool call fragments

<ParamField body="type" type="string" required>
          Always `"server_tool_call_chunk"`
        </ParamField>

<ParamField body="id" type="string">
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="ServerToolResult" icon="box-open">
        **Purpose:** Search results

<ParamField body="type" type="string" required>
          Always `"server_tool_result"`
        </ParamField>

<ParamField body="tool_call_id" type="string" required>
          Identifier of the corresponding server tool call.
        </ParamField>

<ParamField body="id" type="string">
          Identifier associated with the server tool result.
        </ParamField>

<ParamField body="status" type="string" required>
          Execution status of the server-side tool. `"success"` or `"error"`.
        </ParamField>

<ParamField body="output">
          Output of the executed tool.
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Provider-Specific Blocks" icon="plug">
    <Accordion title="NonStandardContentBlock" icon="asterisk">
      **Purpose:** Provider-specific escape hatch

<ParamField body="type" type="string" required>
        Always `"non_standard"`
      </ParamField>

<ParamField body="value" type="object" required>
        Provider-specific data structure
      </ParamField>

**Usage:** For experimental or provider-unique features
    </Accordion>

Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.
  </Accordion>
</AccordionGroup>

<Tip>
  View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).
</Tip>

<Info>
  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.
</Info>

## Use with chat models

[Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.

Refer to the below guides to learn more:

* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory)
* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Specifying `content_blocks` when initializing a message will still populate message
  `content`, but provides a type-safe interface for doing so.
</Tip>

### Standard content blocks

LangChain provides a standard representation for message content that works across providers.

Message objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [ChatAnthropic](/oss/python/integrations/chat/anthropic) or [ChatOpenAI](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:

<Tabs>
  <Tab title="Anthropic">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="OpenAI">
```

Example 4 (unknown):
```unknown

```

---

## Configurable headers

**URL:** llms-txt#configurable-headers

**Contents:**
- Using within your graph

Source: https://docs.langchain.com/langsmith/configurable-headers

LangGraph allows runtime configuration to modify agent behavior and permissions dynamically. When using [LangSmith Deployment](/langsmith/deployment-quickstart), you can pass this configuration in the request body (`config`) or specific request headers. This enables adjustments based on user identity or other requests.

For privacy, control which headers are passed to the runtime configuration via the `http.configurable_headers` section in your [`langgraph.json`](/langsmith/application-structure#configuration-file) file.

Here's how to customize the included and excluded headers:

The `include` and `exclude` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.

Or by fetching from context (useful in tools and or within other nested functions).

You can even use this to dynamically compile the graph.

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The `include` and `exclude` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.
```

Example 2 (unknown):
```unknown
Or by fetching from context (useful in tools and or within other nested functions).
```

Example 3 (unknown):
```unknown
You can even use this to dynamically compile the graph.
```

---

## All integrations

**URL:** llms-txt#all-integrations

**Contents:**
- Top providers
- Chat Models
- LLMs
- Text Embedding Models
- Vector Stores
- Document loaders
  - File Loaders
  - Web Loaders
- Document Transformers
- Document Compressors

Source: https://docs.langchain.com/oss/javascript/integrations/providers/all_providers

Browse the complete collection of integrations available for JavaScript/TypeScript. LangChain.js offers hundreds of integrations across providers, tools, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Anthropic" href="/oss/javascript/integrations/providers/anthropic" icon="anthropic">
    Integrate with Anthropic's Claude models for advanced reasoning and conversation.
  </Card>

<Card title="AWS" href="/oss/javascript/integrations/providers/aws" icon="aws">
    Access AWS services and foundation models through comprehensive integrations.
  </Card>

<Card title="Google" href="/oss/javascript/integrations/providers/google" icon="google">
    Integrate with Google's AI services including Gemini and Vertex AI.
  </Card>

<Card title="Microsoft" href="/oss/javascript/integrations/providers/microsoft" icon="microsoft">
    Connect to Microsoft Azure services and AI platforms.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/providers/openai" icon="openai">
    Build with GPT models and OpenAI's comprehensive AI platform.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/chat/alibaba_tongyi">
    Alibaba's Tongyi language model for Chinese and multilingual applications.
  </Card>

<Card title="Anthropic" href="/oss/javascript/integrations/chat/anthropic" icon="anthropic">
    Claude models for advanced conversational AI and reasoning.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/chat/arcjet">
    Security-focused AI chat integration with built-in protections.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/chat/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/chat/baidu_qianfan">
    Baidu's Qianfan platform for Chinese language AI models.
  </Card>

<Card title="Baidu Wenxin" href="/oss/javascript/integrations/chat/baidu_wenxin">
    Baidu's Wenxin (ERNIE) models for natural language processing.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/chat/bedrock" icon="aws">
    Access foundation models through Amazon Bedrock's managed service.
  </Card>

<Card title="Bedrock Converse" href="/oss/javascript/integrations/chat/bedrock_converse" icon="aws">
    Unified Bedrock Converse API for multiple foundation models.
  </Card>

<Card title="Cerebras" href="/oss/javascript/integrations/chat/cerebras">
    Ultra-fast inference with Cerebras Systems' AI processors.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/chat/cloudflare_workersai">
    Run AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/chat/cohere" icon="cohere">
    Cohere's language models for text generation and understanding.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/chat/deep_infra">
    Access open-source models through Deep Infra's cloud platform.
  </Card>

<Card title="DeepSeek" href="/oss/javascript/integrations/chat/deepseek">
    DeepSeek's advanced reasoning and coding models.
  </Card>

<Card title="Fake LLM" href="/oss/javascript/integrations/chat/fake">
    Mock chat model for testing and development purposes.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/chat/fireworks" icon="fireworks">
    High-performance inference for open-source models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/chat/friendli">
    Optimized inference engine for efficient model serving.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/chat/google_generative_ai" icon="google">
    Google's Gemini models and generative AI capabilities.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/chat/google_vertex_ai" icon="google">
    Enterprise AI platform with Google Cloud's Vertex AI.
  </Card>

<Card title="Groq" href="/oss/javascript/integrations/chat/groq" icon="groq">
    Ultra-fast inference with Groq's specialized hardware.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/chat/ibm">
    IBM Watson AI models and enterprise solutions.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/chat/llama_cpp">
    Run local Llama models with llama.cpp backend.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/chat/minimax">
    Minimax's conversational AI models and services.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/chat/mistral" icon="mistral">
    Mistral's efficient and powerful language models.
  </Card>

<Card title="Moonshot" href="/oss/javascript/integrations/chat/moonshot">
    Moonshot's AI models for various language tasks.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/chat/ni_bittensor">
    Decentralized AI network through Bittensor protocol.
  </Card>

<Card title="Novita" href="/oss/javascript/integrations/chat/novita">
    Novita's AI models and cloud computing platform.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/chat/ollama" icon="ollama">
    Run local models with Ollama's lightweight inference engine.
  </Card>

<Card title="Ollama Functions" href="/oss/javascript/integrations/chat/ollama_functions" icon="ollama">
    Function calling capabilities with Ollama models.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/chat/openai" icon="openai">
    GPT models and OpenAI's comprehensive chat capabilities.
  </Card>

<Card title="Perplexity" href="/oss/javascript/integrations/chat/perplexity">
    Perplexity's search-augmented language models.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/chat/premai">
    PremAI's platform for AI model deployment and management.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/chat/prompt_layer_openai">
    OpenAI integration with PromptLayer's observability features.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/chat/tencent_hunyuan">
    Tencent's Hunyuan models for Chinese language processing.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/chat/togetherai" icon="together">
    Open-source models through Together AI's cloud platform.
  </Card>

<Card title="WebLLM" href="/oss/javascript/integrations/chat/web_llm">
    Run language models directly in web browsers.
  </Card>

<Card title="xAI" href="/oss/javascript/integrations/chat/xai">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/chat/yandex">
    Yandex's AI models and language processing services.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/chat/zhipuai">
    ZhipuAI's ChatGLM and other Chinese language models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI21" href="/oss/javascript/integrations/llms/ai21">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="Aleph Alpha" href="/oss/javascript/integrations/llms/aleph_alpha">
    European AI company's multilingual language models.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/llms/arcjet">
    Security-focused LLM integration with built-in protections.
  </Card>

<Card title="AWS SageMaker" href="/oss/javascript/integrations/llms/aws_sagemaker" icon="aws">
    Deploy models on Amazon SageMaker's ML platform.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/llms/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/llms/bedrock" icon="aws">
    Foundation models through Amazon Bedrock service.
  </Card>

<Card title="Chrome AI" href="/oss/javascript/integrations/llms/chrome_ai">
    Browser-based AI using Chrome's built-in capabilities.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/llms/cloudflare_workersai">
    AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/llms/cohere" icon="cohere">
    Cohere's language models for various NLP tasks.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/llms/deep_infra">
    Open-source models through Deep Infra's infrastructure.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/llms/fireworks" icon="fireworks">
    Fast inference for open-source language models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/llms/friendli">
    Optimized serving for efficient model inference.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/llms/google_vertex_ai" icon="google">
    Google Cloud's enterprise AI and ML platform.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/llms/gradient_ai">
    Private AI model training and deployment platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/llms/huggingface_inference">
    Access thousands of models via Hugging Face Inference API.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/llms/ibm">
    IBM Watson AI and language model services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/llms/jigsawstack">
    JigsawStack's AI infrastructure and model services.
  </Card>

<Card title="LayerUp Security" href="/oss/javascript/integrations/llms/layerup_security">
    Security-enhanced LLM integration with monitoring.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/llms/llama_cpp">
    Run Llama models locally with C++ implementation.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/llms/mistral" icon="mistral">
    Mistral's open-source and commercial language models.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/llms/ni_bittensor">
    Decentralized AI through Bittensor's peer-to-peer network.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/llms/ollama" icon="ollama">
    Local model serving with Ollama's simple interface.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/llms/openai" icon="openai">
    GPT models and OpenAI's language model APIs.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/llms/prompt_layer_openai">
    OpenAI with PromptLayer's logging and observability.
  </Card>

<Card title="Raycast" href="/oss/javascript/integrations/llms/raycast">
    AI integration for Raycast productivity tool.
  </Card>

<Card title="Replicate" href="/oss/javascript/integrations/llms/replicate">
    Run open-source models through Replicate's cloud platform.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/llms/together" icon="together">
    Fast inference for open-source models on Together's platform.
  </Card>

<Card title="Writer" href="/oss/javascript/integrations/llms/writer">
    Writer's enterprise AI platform for content generation.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/llms/yandex">
    Yandex's language models and AI services.
  </Card>
</Columns>

## Text Embedding Models

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/text_embedding/alibaba_tongyi">
    Alibaba's embedding models for multilingual text representation.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/text_embedding/azure_openai" icon="microsoft">
    OpenAI embeddings through Microsoft Azure platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/text_embedding/baidu_qianfan">
    Baidu's text embedding models for Chinese content.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/text_embedding/bedrock" icon="aws">
    Foundation model embeddings through Amazon Bedrock.
  </Card>

<Card title="ByteDance Doubao" href="/oss/javascript/integrations/text_embedding/bytedance_doubao">
    ByteDance's embedding models for content understanding.
  </Card>

<Card title="Cloudflare AI" href="/oss/javascript/integrations/text_embedding/cloudflare_ai">
    Text embeddings on Cloudflare's edge AI platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/text_embedding/cohere" icon="cohere">
    Cohere's multilingual embedding models.
  </Card>

<Card title="DeepInfra" href="/oss/javascript/integrations/text_embedding/deepinfra">
    Open-source embedding models via DeepInfra.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/text_embedding/fireworks" icon="fireworks">
    Fast embedding inference through Fireworks platform.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/text_embedding/google_generativeai" icon="google">
    Google's embedding models for text representation.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/text_embedding/google_vertex_ai" icon="google">
    Enterprise embedding models through Vertex AI.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/text_embedding/gradient_ai">
    Private embedding models with Gradient AI platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/text_embedding/hugging_face_inference">
    Thousands of embedding models via Hugging Face.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/text_embedding/ibm">
    IBM Watson embedding models and AI services.
  </Card>

<Card title="Jina" href="/oss/javascript/integrations/text_embedding/jina">
    Jina's neural search and embedding models.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/text_embedding/llama_cpp">
    Local embedding generation with llama.cpp.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/text_embedding/minimax">
    Minimax's text embedding and representation models.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/text_embedding/mistralai" icon="mistral">
    Mistral's efficient embedding models.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/text_embedding/mixedbread_ai">
    High-quality multilingual embedding models.
  </Card>

<Card title="Nomic" href="/oss/javascript/integrations/text_embedding/nomic">
    Nomic's open-source embedding models.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/text_embedding/ollama" icon="ollama">
    Local embedding models through Ollama.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/text_embedding/openai" icon="openai">
    OpenAI's text-embedding models for semantic search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/text_embedding/pinecone">
    Pinecone's embedding models and vector database.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/text_embedding/premai">
    PremAI's embedding models and AI platform.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/text_embedding/tencent_hunyuan">
    Tencent's embedding models for Chinese text.
  </Card>

<Card title="TensorFlow" href="/oss/javascript/integrations/text_embedding/tensorflow">
    TensorFlow-based embedding models and inference.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/text_embedding/togetherai" icon="together">
    Open-source embedding models on Together platform.
  </Card>

<Card title="Transformers" href="/oss/javascript/integrations/text_embedding/transformers">
    Local transformer-based embedding models.
  </Card>

<Card title="Voyage AI" href="/oss/javascript/integrations/text_embedding/voyageai">
    Voyage AI's domain-specific embedding models.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/text_embedding/zhipuai">
    ZhipuAI's Chinese language embedding models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AnalyticDB" href="/oss/javascript/integrations/vectorstores/analyticdb">
    Alibaba Cloud's AnalyticDB for vector storage and search.
  </Card>

<Card title="AstraDB" href="/oss/javascript/integrations/vectorstores/astradb">
    DataStax Astra DB vector database for scalable storage.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/vectorstores/azion-edgesql">
    Edge-based vector storage with Azion's EdgeSQL.
  </Card>

<Card title="Azure AI Search" href="/oss/javascript/integrations/vectorstores/azure_aisearch" icon="microsoft">
    Microsoft Azure's AI-powered search and vector storage.
  </Card>

<Card title="Azure Cosmos DB MongoDB" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_mongodb" icon="microsoft">
    Vector search in Azure Cosmos DB with MongoDB API.
  </Card>

<Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql" icon="microsoft">
    Vector storage in Azure Cosmos DB NoSQL API.
  </Card>

<Card title="Cassandra" href="/oss/javascript/integrations/vectorstores/cassandra">
    Apache Cassandra vector search capabilities.
  </Card>

<Card title="Chroma" href="/oss/javascript/integrations/vectorstores/chroma">
    Open-source embedding database for AI applications.
  </Card>

<Card title="ClickHouse" href="/oss/javascript/integrations/vectorstores/clickhouse">
    Fast columnar database with vector search support.
  </Card>

<Card title="CloseVector" href="/oss/javascript/integrations/vectorstores/closevector">
    High-performance vector database for similarity search.
  </Card>

<Card title="Cloudflare Vectorize" href="/oss/javascript/integrations/vectorstores/cloudflare_vectorize">
    Serverless vector database on Cloudflare's edge.
  </Card>

<Card title="Convex" href="/oss/javascript/integrations/vectorstores/convex">
    Full-stack platform with integrated vector storage.
  </Card>

<Card title="Couchbase" href="/oss/javascript/integrations/vectorstores/couchbase">
    NoSQL database with vector search capabilities.
  </Card>

<Card title="Elasticsearch" href="/oss/javascript/integrations/vectorstores/elasticsearch">
    Distributed search engine with vector search support.
  </Card>

<Card title="Faiss" href="/oss/javascript/integrations/vectorstores/faiss">
    Facebook's library for efficient similarity search.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/vectorstores/google_cloudsql_pg" icon="google">
    PostgreSQL with vector extensions on Google Cloud.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/vectorstores/googlevertexai" icon="google">
    Vector search through Google Vertex AI platform.
  </Card>

<Card title="SAP HANA Vector" href="/oss/javascript/integrations/vectorstores/hanavector">
    Enterprise vector database with SAP HANA.
  </Card>

<Card title="Hnswlib" href="/oss/javascript/integrations/vectorstores/hnswlib">
    Fast approximate nearest neighbor search library.
  </Card>

<Card title="LanceDB" href="/oss/javascript/integrations/vectorstores/lancedb">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LibSQL" href="/oss/javascript/integrations/vectorstores/libsql">
    SQLite-compatible database with vector extensions.
  </Card>

<Card title="MariaDB" href="/oss/javascript/integrations/vectorstores/mariadb">
    Open-source database with vector search capabilities.
  </Card>

<Card title="Memory Vector Store" href="/oss/javascript/integrations/vectorstores/memory">
    In-memory vector storage for development and testing.
  </Card>

<Card title="Milvus" href="/oss/javascript/integrations/vectorstores/milvus">
    Open-source vector database for AI applications.
  </Card>

<Card title="Momento Vector Index" href="/oss/javascript/integrations/vectorstores/momento_vector_index">
    Serverless vector indexing with Momento's platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/javascript/integrations/vectorstores/mongodb_atlas">
    Vector search in MongoDB Atlas cloud database.
  </Card>

<Card title="MyScale" href="/oss/javascript/integrations/vectorstores/myscale">
    SQL-compatible vector database for analytics.
  </Card>

<Card title="Neo4j Vector" href="/oss/javascript/integrations/vectorstores/neo4jvector">
    Graph database with integrated vector search.
  </Card>

<Card title="Neon" href="/oss/javascript/integrations/vectorstores/neon">
    Serverless PostgreSQL with vector extensions.
  </Card>

<Card title="OpenSearch" href="/oss/javascript/integrations/vectorstores/opensearch">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="PGVector" href="/oss/javascript/integrations/vectorstores/pgvector">
    PostgreSQL extension for vector similarity search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/vectorstores/pinecone">
    Managed vector database for machine learning applications.
  </Card>

<Card title="Prisma" href="/oss/javascript/integrations/vectorstores/prisma">
    Type-safe database client with vector support.
  </Card>

<Card title="Qdrant" href="/oss/javascript/integrations/vectorstores/qdrant">
    Open-source vector similarity search engine.
  </Card>

<Card title="Redis" href="/oss/javascript/integrations/vectorstores/redis">
    In-memory database with vector search capabilities.
  </Card>

<Card title="Rockset" href="/oss/javascript/integrations/vectorstores/rockset">
    Real-time analytics database with vector search.
  </Card>

<Card title="SingleStore" href="/oss/javascript/integrations/vectorstores/singlestore">
    Distributed database with built-in vector functions.
  </Card>

<Card title="Supabase" href="/oss/javascript/integrations/vectorstores/supabase">
    Open-source Firebase alternative with vector support.
  </Card>

<Card title="Tigris" href="/oss/javascript/integrations/vectorstores/tigris">
    Developer-focused database with vector search.
  </Card>

<Card title="Turbopuffer" href="/oss/javascript/integrations/vectorstores/turbopuffer">
    High-performance vector database for embeddings.
  </Card>

<Card title="TypeORM" href="/oss/javascript/integrations/vectorstores/typeorm">
    TypeScript ORM with vector database support.
  </Card>

<Card title="Typesense" href="/oss/javascript/integrations/vectorstores/typesense">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="Upstash Vector" href="/oss/javascript/integrations/vectorstores/upstash">
    Serverless vector database with Redis compatibility.
  </Card>

<Card title="USearch" href="/oss/javascript/integrations/vectorstores/usearch">
    Smaller and faster single-file vector search engine.
  </Card>

<Card title="Vectara" href="/oss/javascript/integrations/vectorstores/vectara">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vercel Postgres" href="/oss/javascript/integrations/vectorstores/vercel_postgres">
    PostgreSQL database with vector extensions on Vercel.
  </Card>

<Card title="Voy" href="/oss/javascript/integrations/vectorstores/voy">
    WebAssembly-based vector database for browsers.
  </Card>

<Card title="Weaviate" href="/oss/javascript/integrations/vectorstores/weaviate">
    Open-source vector database with GraphQL API.
  </Card>

<Card title="Xata" href="/oss/javascript/integrations/vectorstores/xata">
    Serverless database with built-in vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/vectorstores/zep_cloud">
    Long-term memory for AI assistants in the cloud.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/vectorstores/zep">
    Long-term memory for AI assistants and agents.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ChatGPT" href="/oss/javascript/integrations/document_loaders/file_loaders/chatgpt">
    Load and parse ChatGPT conversation exports.
  </Card>

<Card title="CSV" href="/oss/javascript/integrations/document_loaders/file_loaders/csv">
    Load data from CSV files with customizable parsing.
  </Card>

<Card title="Directory" href="/oss/javascript/integrations/document_loaders/file_loaders/directory">
    Recursively load documents from filesystem directories.
  </Card>

<Card title="DOCX" href="/oss/javascript/integrations/document_loaders/file_loaders/docx">
    Extract text and metadata from Microsoft Word documents.
  </Card>

<Card title="EPUB" href="/oss/javascript/integrations/document_loaders/file_loaders/epub">
    Load and parse EPUB e-book files.
  </Card>

<Card title="JSON" href="/oss/javascript/integrations/document_loaders/file_loaders/json">
    Load and parse JSON files with flexible structure handling.
  </Card>

<Card title="JSON Lines" href="/oss/javascript/integrations/document_loaders/file_loaders/jsonlines">
    Load newline-delimited JSON files.
  </Card>

<Card title="Multi-File" href="/oss/javascript/integrations/document_loaders/file_loaders/multi_file">
    Load multiple files of different types simultaneously.
  </Card>

<Card title="Notion Markdown" href="/oss/javascript/integrations/document_loaders/file_loaders/notion_markdown">
    Load Notion pages exported as Markdown.
  </Card>

<Card title="OpenAI Whisper Audio" href="/oss/javascript/integrations/document_loaders/file_loaders/openai_whisper_audio">
    Transcribe audio files using OpenAI's Whisper model.
  </Card>

<Card title="PDF" href="/oss/javascript/integrations/document_loaders/file_loaders/pdf">
    Extract text from PDF documents.
  </Card>

<Card title="PPTX" href="/oss/javascript/integrations/document_loaders/file_loaders/pptx">
    Load Microsoft PowerPoint presentations.
  </Card>

<Card title="Subtitles" href="/oss/javascript/integrations/document_loaders/file_loaders/subtitles">
    Load subtitle files (SRT, VTT formats).
  </Card>

<Card title="Text" href="/oss/javascript/integrations/document_loaders/file_loaders/text">
    Load plain text files with encoding detection.
  </Card>

<Card title="Unstructured" href="/oss/javascript/integrations/document_loaders/file_loaders/unstructured">
    Load various file formats using Unstructured.io.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Airtable" href="/oss/javascript/integrations/document_loaders/web_loaders/airtable">
    Load records from Airtable bases.
  </Card>

<Card title="Apify Dataset" href="/oss/javascript/integrations/document_loaders/web_loaders/apify_dataset">
    Load data from Apify web scraping datasets.
  </Card>

<Card title="AssemblyAI Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription">
    Transcribe audio using AssemblyAI's API.
  </Card>

<Card title="Azure Blob Storage Container" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container" icon="microsoft">
    Load files from Azure Blob Storage containers.
  </Card>

<Card title="Azure Blob Storage File" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file" icon="microsoft">
    Load individual files from Azure Blob Storage.
  </Card>

<Card title="Browserbase" href="/oss/javascript/integrations/document_loaders/web_loaders/browserbase">
    Load web content using Browserbase's cloud browsers.
  </Card>

<Card title="College Confidential" href="/oss/javascript/integrations/document_loaders/web_loaders/college_confidential">
    Scrape College Confidential forum content.
  </Card>

<Card title="Confluence" href="/oss/javascript/integrations/document_loaders/web_loaders/confluence">
    Load pages from Atlassian Confluence.
  </Card>

<Card title="Couchbase" href="/oss/javascript/integrations/document_loaders/web_loaders/couchbase">
    Load documents from Couchbase databases.
  </Card>

<Card title="Figma" href="/oss/javascript/integrations/document_loaders/web_loaders/figma">
    Load Figma design files and comments.
  </Card>

<Card title="Firecrawl" href="/oss/javascript/integrations/document_loaders/web_loaders/firecrawl">
    Crawl websites using Firecrawl's web scraping API.
  </Card>

<Card title="GitBook" href="/oss/javascript/integrations/document_loaders/web_loaders/gitbook">
    Load content from GitBook documentation sites.
  </Card>

<Card title="GitHub" href="/oss/javascript/integrations/document_loaders/web_loaders/github">
    Load files and repositories from GitHub.
  </Card>

<Card title="Google Cloud Storage" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloud_storage" icon="google">
    Load files from Google Cloud Storage buckets.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg" icon="google">
    Load data from Google Cloud SQL PostgreSQL databases.
  </Card>

<Card title="Hacker News" href="/oss/javascript/integrations/document_loaders/web_loaders/hn">
    Load posts and comments from Hacker News.
  </Card>

<Card title="IMSDb" href="/oss/javascript/integrations/document_loaders/web_loaders/imsdb">
    Load movie scripts from the Internet Movie Script Database.
  </Card>

<Card title="Jira" href="/oss/javascript/integrations/document_loaders/web_loaders/jira">
    Load issues and projects from Atlassian Jira.
  </Card>

<Card title="LangSmith" href="/oss/javascript/integrations/document_loaders/web_loaders/langsmith">
    Load runs and datasets from LangSmith.
  </Card>

<Card title="Notion API" href="/oss/javascript/integrations/document_loaders/web_loaders/notionapi">
    Load pages and databases from Notion.
  </Card>

<Card title="PDF (Web)" href="/oss/javascript/integrations/document_loaders/web_loaders/pdf">
    Load PDF files from web URLs.
  </Card>

<Card title="Recursive URL" href="/oss/javascript/integrations/document_loaders/web_loaders/recursive_url_loader">
    Recursively crawl and load web pages.
  </Card>

<Card title="S3" href="/oss/javascript/integrations/document_loaders/web_loaders/s3" icon="aws">
    Load files from Amazon S3 buckets.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/searchapi">
    Load search results using SearchAPI.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/serpapi">
    Load search results using SerpAPI.
  </Card>

<Card title="Sitemap" href="/oss/javascript/integrations/document_loaders/web_loaders/sitemap">
    Load URLs from website sitemaps.
  </Card>

<Card title="Sonix Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription">
    Transcribe audio using Sonix's transcription API.
  </Card>

<Card title="Sort.xyz Blockchain" href="/oss/javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain">
    Load blockchain data from Sort.xyz.
  </Card>

<Card title="Spider" href="/oss/javascript/integrations/document_loaders/web_loaders/spider">
    Fast web crawling using Spider API.
  </Card>

<Card title="Taskade" href="/oss/javascript/integrations/document_loaders/web_loaders/taskade">
    Load projects and tasks from Taskade.
  </Card>

<Card title="Web Cheerio" href="/oss/javascript/integrations/document_loaders/web_loaders/web_cheerio">
    Scrape web pages using Cheerio for server-side parsing.
  </Card>

<Card title="Web Playwright" href="/oss/javascript/integrations/document_loaders/web_loaders/web_playwright">
    Load dynamic web content using Playwright browser automation.
  </Card>

<Card title="Web Puppeteer" href="/oss/javascript/integrations/document_loaders/web_loaders/web_puppeteer">
    Scrape JavaScript-heavy websites using Puppeteer.
  </Card>

<Card title="YouTube" href="/oss/javascript/integrations/document_loaders/web_loaders/youtube">
    Load YouTube video transcripts and metadata.
  </Card>
</Columns>

## Document Transformers

<Columns cols={3}>
  <Card title="HTML to Text" href="/oss/javascript/integrations/document_transformers/html-to-text">
    Convert HTML content to clean, readable text.
  </Card>

<Card title="Mozilla Readability" href="/oss/javascript/integrations/document_transformers/mozilla_readability">
    Extract main content from web pages using Mozilla's Readability.
  </Card>

<Card title="OpenAI Metadata Tagger" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" icon="openai">
    Generate metadata tags for documents using OpenAI.
  </Card>
</Columns>

## Document Compressors

<Columns cols={3}>
  <Card title="Cohere Rerank" href="/oss/javascript/integrations/document_compressors/cohere_rerank" icon="cohere">
    Rerank documents using Cohere's reranking models.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/document_compressors/ibm">
    Document compression using IBM Watson AI services.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/document_compressors/mixedbread_ai">
    Rerank and compress documents using MixedBread AI.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI Plugin Tool" href="/oss/javascript/integrations/tools/aiplugin-tool">
    Execute OpenAI ChatGPT plugins as tools.
  </Card>

<Card title="Azure Dynamic Sessions" href="/oss/javascript/integrations/tools/azure_dynamic_sessions" icon="microsoft">
    Secure code execution in Azure Dynamic Sessions.
  </Card>

<Card title="Connery" href="/oss/javascript/integrations/tools/connery">
    Modular AI actions and integrations with Connery.
  </Card>

<Card title="Connery Toolkit" href="/oss/javascript/integrations/tools/connery_toolkit">
    Access Connery's toolkit of pre-built actions.
  </Card>

<Card title="DALL-E" href="/oss/javascript/integrations/tools/dalle" icon="openai">
    Generate images using OpenAI's DALL-E models.
  </Card>

<Card title="Decodo" href="/oss/javascript/integrations/tools/decodo">
    Code execution and analysis with Decodo.
  </Card>

<Card title="Discord" href="/oss/javascript/integrations/tools/discord_tool">
    Interact with Discord servers and channels.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/javascript/integrations/tools/duckduckgo_search">
    Privacy-focused web search with DuckDuckGo.
  </Card>

<Card title="Exa Search" href="/oss/javascript/integrations/tools/exa_search">
    AI-powered search engine for better results.
  </Card>

<Card title="Gmail" href="/oss/javascript/integrations/tools/gmail" icon="google">
    Read and send emails through Gmail API.
  </Card>

<Card title="Goat" href="/oss/javascript/integrations/tools/goat">
    Simple tool execution framework.
  </Card>

<Card title="Google Calendar" href="/oss/javascript/integrations/tools/google_calendar" icon="google">
    Manage events and schedules in Google Calendar.
  </Card>

<Card title="Google Places" href="/oss/javascript/integrations/tools/google_places" icon="google">
    Search for places using Google Places API.
  </Card>

<Card title="Google Routes" href="/oss/javascript/integrations/tools/google_routes" icon="google">
    Get directions and routing information.
  </Card>

<Card title="Google Scholar" href="/oss/javascript/integrations/tools/google_scholar" icon="google">
    Search academic papers and citations.
  </Card>

<Card title="Google Trends" href="/oss/javascript/integrations/tools/google_trends" icon="google">
    Analyze search trends and popularity data.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/tools/ibm">
    Access IBM Watson AI tools and services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/tools/jigsawstack">
    AI infrastructure tools from JigsawStack.
  </Card>

<Card title="JSON Tool" href="/oss/javascript/integrations/tools/json">
    Parse and manipulate JSON data structures.
  </Card>

<Card title="Lambda Agent" href="/oss/javascript/integrations/tools/lambda_agent" icon="aws">
    Execute code in AWS Lambda functions.
  </Card>

<Card title="MCP Toolbox" href="/oss/javascript/integrations/tools/mcp_toolbox">
    Model Context Protocol tools and utilities.
  </Card>

<Card title="OpenAPI" href="/oss/javascript/integrations/tools/openapi">
    Generate tools from OpenAPI specifications.
  </Card>

<Card title="Python Interpreter" href="/oss/javascript/integrations/tools/pyinterpreter">
    Execute Python code in a sandboxed environment.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/tools/searchapi">
    Web search capabilities through SearchAPI.
  </Card>

<Card title="SearXNG" href="/oss/javascript/integrations/tools/searxng">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/tools/serpapi">
    Google Search results through SerpAPI.
  </Card>

<Card title="Step Functions Agent" href="/oss/javascript/integrations/tools/sfn_agent" icon="aws">
    Execute AWS Step Functions workflows.
  </Card>

<Card title="SQL" href="/oss/javascript/integrations/tools/sql">
    Query databases using natural language.
  </Card>

<Card title="StackExchange" href="/oss/javascript/integrations/tools/stackexchange">
    Search Stack Overflow and other SE sites.
  </Card>

<Card title="Stagehand" href="/oss/javascript/integrations/tools/stagehand">
    Browser automation for web interactions.
  </Card>

<Card title="Tavily Crawl" href="/oss/javascript/integrations/tools/tavily_crawl">
    Web crawling capabilities with Tavily.
  </Card>

<Card title="Tavily Extract" href="/oss/javascript/integrations/tools/tavily_extract">
    Extract structured data from web pages.
  </Card>

<Card title="Tavily Map" href="/oss/javascript/integrations/tools/tavily_map">
    Map and visualize web crawling results.
  </Card>

<Card title="Tavily Search" href="/oss/javascript/integrations/tools/tavily_search">
    AI-optimized search for retrieval applications.
  </Card>

<Card title="Tavily Search Community" href="/oss/javascript/integrations/tools/tavily_search_community">
    Community-powered search through Tavily.
  </Card>

<Card title="Vector Store" href="/oss/javascript/integrations/tools/vectorstore">
    Query vector databases as tools.
  </Card>

<Card title="Web Browser" href="/oss/javascript/integrations/tools/webbrowser">
    Automated web browsing and interaction.
  </Card>

<Card title="Wikipedia" href="/oss/javascript/integrations/tools/wikipedia">
    Search and retrieve Wikipedia articles.
  </Card>

<Card title="Wolfram Alpha" href="/oss/javascript/integrations/tools/wolframalpha">
    Computational knowledge through Wolfram Alpha.
  </Card>

<Card title="Zapier Agent" href="/oss/javascript/integrations/tools/zapier_agent">
    Automate workflows using Zapier integrations.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ArXiv" href="/oss/javascript/integrations/retrievers/arxiv-retriever">
    Search and retrieve academic papers from ArXiv.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/retrievers/azion-edgesql">
    Edge-based document retrieval with Azion.
  </Card>

<Card title="Bedrock Knowledge Bases" href="/oss/javascript/integrations/retrievers/bedrock-knowledge-bases" icon="aws">
    Retrieve from Amazon Bedrock Knowledge Bases.
  </Card>

<Card title="BM25" href="/oss/javascript/integrations/retrievers/bm25">
    BM25 algorithm for keyword-based retrieval.
  </Card>

<Card title="Chaindesk" href="/oss/javascript/integrations/retrievers/chaindesk-retriever">
    Document retrieval using Chaindesk platform.
  </Card>

<Card title="ChatGPT Retriever Plugin" href="/oss/javascript/integrations/retrievers/chatgpt-retriever-plugin" icon="openai">
    Official ChatGPT retriever plugin integration.
  </Card>

<Card title="Dria" href="/oss/javascript/integrations/retrievers/dria">
    Decentralized knowledge retrieval with Dria.
  </Card>

<Card title="Exa" href="/oss/javascript/integrations/retrievers/exa">
    AI-powered web search and retrieval.
  </Card>

<Card title="HyDE" href="/oss/javascript/integrations/retrievers/hyde">
    Hypothetical Document Embeddings for better retrieval.
  </Card>

<Card title="Kendra" href="/oss/javascript/integrations/retrievers/kendra-retriever" icon="aws">
    Enterprise search with Amazon Kendra.
  </Card>

<Card title="Metal" href="/oss/javascript/integrations/retrievers/metal-retriever">
    Managed vector search with Metal.
  </Card>

<Card title="Supabase Hybrid" href="/oss/javascript/integrations/retrievers/supabase-hybrid">
    Hybrid search combining vector and keyword search.
  </Card>

<Card title="Tavily" href="/oss/javascript/integrations/retrievers/tavily">
    AI-optimized search for RAG applications.
  </Card>

<Card title="Time-Weighted" href="/oss/javascript/integrations/retrievers/time-weighted-retriever">
    Time-aware document retrieval and ranking.
  </Card>

<Card title="Vespa" href="/oss/javascript/integrations/retrievers/vespa-retriever">
    Big data serving engine for vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/retrievers/zep-cloud-retriever">
    Cloud-based long-term memory retrieval.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/retrievers/zep-retriever">
    Long-term memory and context retrieval.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Cassandra Storage" href="/oss/javascript/integrations/stores/cassandra_storage">
    Distributed key-value storage using Cassandra.
  </Card>

<Card title="File System" href="/oss/javascript/integrations/stores/file_system">
    Local file system storage for development.
  </Card>

<Card title="In-Memory" href="/oss/javascript/integrations/stores/in_memory">
    Fast in-memory storage for temporary data.
  </Card>

<Card title="IoRedis Storage" href="/oss/javascript/integrations/stores/ioredis_storage">
    Redis-based storage using IoRedis client.
  </Card>

<Card title="Upstash Redis Storage" href="/oss/javascript/integrations/stores/upstash_redis_storage">
    Serverless Redis storage with Upstash.
  </Card>

<Card title="Vercel KV Storage" href="/oss/javascript/integrations/stores/vercel_kv_storage">
    Key-value storage on Vercel's edge network.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" icon="microsoft">
    Cache LLM responses in Azure Cosmos DB.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Datadog Tracer" href="/oss/javascript/integrations/callbacks/datadog_tracer">
    Monitor and trace LangChain applications with Datadog.
  </Card>

<Card title="Upstash Rate Limit" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback">
    Rate limiting for AI applications using Upstash.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/all_providers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Thinking in LangGraph

**URL:** llms-txt#thinking-in-langgraph

**Contents:**
- Start with the process you want to automate
- Step 1: Map out your workflow as discrete steps
- Step 2: Identify what each step needs to do
  - LLM Steps
  - Data Steps
  - Action Steps
  - User Input Steps
- Step 3: Design your state
  - What belongs in state?
  - Keep state raw, format prompts on-demand

Source: https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph

Learn how to think about building agents with LangGraph by breaking down a customer support email agent into discrete steps

LangGraph can change how you think about the agents you build. When you build an agent with LangGraph, you will first break it apart into discrete steps called **nodes**. Then, you will describe the different decisions and transitions for each of your nodes. Finally, you will connect your nodes together through a shared **state** that each node can read from and write to. In this tutorial, we'll guide you through the thought process of building a customer support email agent with LangGraph.

## Start with the process you want to automate

Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:

* Read incoming customer emails
* Classify them by urgency and topic
* Search relevant documentation to answer questions
* Draft appropriate responses
* Escalate complex issues to human agents
* Schedule follow-ups when needed

Example scenarios to handle:

1. Simple product question: "How do I reset my password?"
2. Bug report: "The export feature crashes when I select PDF format"
3. Urgent billing issue: "I was charged twice for my subscription!"
4. Feature request: "Can you add dark mode to the mobile app?"
5. Complex technical issue: "Our API integration fails intermittently with 504 errors"

To implement an agent in LangGraph, you will usually follow the same five steps.

## Step 1: Map out your workflow as discrete steps

Start by identifying the distinct steps in your process. Each step will become a **node** (a function that does one specific thing). Then sketch how these steps connect to each other.

The arrows show possible paths, but the actual decision of which path to take happens inside each node.

Now that you've identified the components in your workflow, let's understand what each node needs to do:

* Read Email: Extract and parse the email content
* Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action
* Doc Search: Query your knowledge base for relevant information
* Bug Track: Create or update issue in tracking system
* Draft Reply: Generate an appropriate response
* Human Review: Escalate to human agent for approval or handling
* Send Reply: Dispatch the email response

<Tip>
  Notice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).
</Tip>

## Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

<CardGroup cols={2}>
  <Card title="LLM Steps" icon="brain" href="#llm-steps">
    Use when you need to understand, analyze, generate text, or make reasoning decisions
  </Card>

<Card title="Data Steps" icon="database" href="#data-steps">
    Use when you need to retrieve information from external sources
  </Card>

<Card title="Action Steps" icon="bolt" href="#action-steps">
    Use when you need to perform external actions
  </Card>

<Card title="User Input Steps" icon="user" href="#user-input-steps">
    Use when you need human intervention
  </Card>
</CardGroup>

When a step needs to understand, analyze, generate text, or make reasoning decisions:

<AccordionGroup>
  <Accordion title="Classify Intent Node">
    * Static context (prompt): Classification categories, urgency definitions, response format
    * Dynamic context (from state): Email content, sender information
    * Desired outcome: Structured classification that determines routing
  </Accordion>

<Accordion title="Draft Reply Node">
    * Static context (prompt): Tone guidelines, company policies, response templates
    * Dynamic context (from state): Classification results, search results, customer history
    * Desired outcome: Professional email response ready for review
  </Accordion>
</AccordionGroup>

When a step needs to retrieve information from external sources:

<AccordionGroup>
  <Accordion title="Document Search Node">
    * Parameters: Query built from intent and topic
    * Retry strategy: Yes, with exponential backoff for transient failures
    * Caching: Could cache common queries to reduce API calls
  </Accordion>

<Accordion title="Customer History Lookup">
    * Parameters: Customer email or ID from state
    * Retry strategy: Yes, but with fallback to basic info if unavailable
    * Caching: Yes, with time-to-live to balance freshness and performance
  </Accordion>
</AccordionGroup>

When a step needs to perform an external action:

<AccordionGroup>
  <Accordion title="Send Reply Node">
    * When to execute: After approval (human or automated)
    * Retry strategy: Yes, with exponential backoff for network issues
    * Should not cache: Each send is a unique action
  </Accordion>

<Accordion title="Bug Track Node">
    * When to execute: Always when intent is "bug"
    * Retry strategy: Yes, critical to not lose bug reports
    * Returns: Ticket ID to include in response
  </Accordion>
</AccordionGroup>

When a step needs human intervention:

<AccordionGroup>
  <Accordion title="Human Review Node">
    * Context for decision: Original email, draft response, urgency, classification
    * Expected input format: Approval boolean plus optional edited response
    * When triggered: High urgency, complex issues, or quality concerns
  </Accordion>
</AccordionGroup>

## Step 3: Design your state

State is the shared [memory](/oss/python/concepts/memory) accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### What belongs in state?

Ask yourself these questions about each piece of data:

<CardGroup cols={2}>
  <Card title="Include in State" icon="check">
    Does it need to persist across steps? If yes, it goes in state.
  </Card>

<Card title="Don't Store" icon="code">
    Can you derive it from other data? If yes, compute it when needed instead of storing it in state.
  </Card>
</CardGroup>

For our email agent, we need to track:

* The original email and sender info (can't reconstruct these)
* Classification results (needed by multiple downstream nodes)
* Search results and customer data (expensive to re-fetch)
* The draft response (needs to persist through review)
* Execution metadata (for debugging and recovery)

### Keep state raw, format prompts on-demand

<Tip>
  A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.
</Tip>

This separation means:

* Different nodes can format the same data differently for their needs
* You can change prompt templates without modifying your state schema
* Debugging is clearer - you see exactly what data each node received
* Your agent can evolve without breaking existing state

Let's define our state:

```python  theme={null}
from typing import TypedDict, Literal

**Examples:**

Example 1 (unknown):
```unknown
The arrows show possible paths, but the actual decision of which path to take happens inside each node.

Now that you've identified the components in your workflow, let's understand what each node needs to do:

* Read Email: Extract and parse the email content
* Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action
* Doc Search: Query your knowledge base for relevant information
* Bug Track: Create or update issue in tracking system
* Draft Reply: Generate an appropriate response
* Human Review: Escalate to human agent for approval or handling
* Send Reply: Dispatch the email response

<Tip>
  Notice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).
</Tip>

## Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

<CardGroup cols={2}>
  <Card title="LLM Steps" icon="brain" href="#llm-steps">
    Use when you need to understand, analyze, generate text, or make reasoning decisions
  </Card>

  <Card title="Data Steps" icon="database" href="#data-steps">
    Use when you need to retrieve information from external sources
  </Card>

  <Card title="Action Steps" icon="bolt" href="#action-steps">
    Use when you need to perform external actions
  </Card>

  <Card title="User Input Steps" icon="user" href="#user-input-steps">
    Use when you need human intervention
  </Card>
</CardGroup>

### LLM Steps

When a step needs to understand, analyze, generate text, or make reasoning decisions:

<AccordionGroup>
  <Accordion title="Classify Intent Node">
    * Static context (prompt): Classification categories, urgency definitions, response format
    * Dynamic context (from state): Email content, sender information
    * Desired outcome: Structured classification that determines routing
  </Accordion>

  <Accordion title="Draft Reply Node">
    * Static context (prompt): Tone guidelines, company policies, response templates
    * Dynamic context (from state): Classification results, search results, customer history
    * Desired outcome: Professional email response ready for review
  </Accordion>
</AccordionGroup>

### Data Steps

When a step needs to retrieve information from external sources:

<AccordionGroup>
  <Accordion title="Document Search Node">
    * Parameters: Query built from intent and topic
    * Retry strategy: Yes, with exponential backoff for transient failures
    * Caching: Could cache common queries to reduce API calls
  </Accordion>

  <Accordion title="Customer History Lookup">
    * Parameters: Customer email or ID from state
    * Retry strategy: Yes, but with fallback to basic info if unavailable
    * Caching: Yes, with time-to-live to balance freshness and performance
  </Accordion>
</AccordionGroup>

### Action Steps

When a step needs to perform an external action:

<AccordionGroup>
  <Accordion title="Send Reply Node">
    * When to execute: After approval (human or automated)
    * Retry strategy: Yes, with exponential backoff for network issues
    * Should not cache: Each send is a unique action
  </Accordion>

  <Accordion title="Bug Track Node">
    * When to execute: Always when intent is "bug"
    * Retry strategy: Yes, critical to not lose bug reports
    * Returns: Ticket ID to include in response
  </Accordion>
</AccordionGroup>

### User Input Steps

When a step needs human intervention:

<AccordionGroup>
  <Accordion title="Human Review Node">
    * Context for decision: Original email, draft response, urgency, classification
    * Expected input format: Approval boolean plus optional edited response
    * When triggered: High urgency, complex issues, or quality concerns
  </Accordion>
</AccordionGroup>

## Step 3: Design your state

State is the shared [memory](/oss/python/concepts/memory) accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### What belongs in state?

Ask yourself these questions about each piece of data:

<CardGroup cols={2}>
  <Card title="Include in State" icon="check">
    Does it need to persist across steps? If yes, it goes in state.
  </Card>

  <Card title="Don't Store" icon="code">
    Can you derive it from other data? If yes, compute it when needed instead of storing it in state.
  </Card>
</CardGroup>

For our email agent, we need to track:

* The original email and sender info (can't reconstruct these)
* Classification results (needed by multiple downstream nodes)
* Search results and customer data (expensive to re-fetch)
* The draft response (needs to persist through review)
* Execution metadata (for debugging and recovery)

### Keep state raw, format prompts on-demand

<Tip>
  A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.
</Tip>

This separation means:

* Different nodes can format the same data differently for their needs
* You can change prompt templates without modifying your state schema
* Debugging is clearer - you see exactly what data each node received
* Your agent can evolve without breaking existing state

Let's define our state:
```

---

## Set up online evaluators

**URL:** llms-txt#set-up-online-evaluators

**Contents:**
- View online evaluators
- Configure online evaluators
  - Configure a LLM-as-a-judge online evaluator
  - Configure a custom code evaluator
  - Video guide
- Configure multi-turn online evaluators
  - Prerequisites
  - Configuration
  - Limits
  - Troubleshooting

Source: https://docs.langchain.com/langsmith/online-evaluations

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* Running [online evaluations](/langsmith/evaluation-concepts#online-evaluation)
</Tip>

Online evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application—to identify issues, measure improvements, and ensure consistent quality over time.

There are two types of online evaluations supported in LangSmith:

* **[LLM-as-a-judge](/langsmith/evaluation-concepts#llm-as-judge)**: Use an LLM to evaluate traces as a scalable substitute for human-like judgment (e.g., toxicity, hallucinations, correctness). Supports two different levels of granularity:
  * **Run level**: Evaluate a single run.
  * [**Thread level**](/langsmith/online-evaluations#configure-multi-turn-online-evaluators): Evaluate all traces in a thread.
* **Custom Code**: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.

<Note>When an online evaluator runs on any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View online evaluators

Head to the **Tracing Projects** tab and select a tracing project. To view existing online evaluators for that project, click on the **Evaluators** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=471b55b0d23b6c54ea5044406f0c55f7" alt="View online evaluators" data-og-width="1350" width="1350" data-og-height="639" height="639" data-path="langsmith/images/view-evaluators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=141082993aba37d45550bfff9da502df 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=211cc6c5359e00ab23f0cf55bd67fd93 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcdae1f3bce28bfcdd91059e43f9e1be 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=93b239efbd10f6ab5013e91b08384df6 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b6e496bee86cfb221cccde72366f83bb 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=79817ff090124e1ec7b5f25eb2ddd978 2500w" />

## Configure online evaluators

#### 1. Navigate to online evaluators

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Evaluator**. Select the evaluator you want to configure.

#### 2. Name your evaluator

#### 3. Create a filter

For example, you may want to apply specific evaluators based on:

* Runs where a [user left feedback](/langsmith/attach-user-feedback) indicating the response was unsatisfactory.
* Runs that invoke a specific tool call. See [filtering for tool calls](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) for more information.
* Runs that match a particular piece of metadata (e.g. if you log traces with a `plan_type` and only want to run evaluations on traces from your enterprise customers). See [adding metadata to your traces](/langsmith/add-metadata-tags) for more information.

Filters on evaluators work the same way as when you're filtering traces in a project. For more information on filters, you can refer to [this guide](./filter-traces-in-application).

<Tip>
  It's often helpful to inspect runs as you're creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.
</Tip>

#### 4. (Optional) Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.

In order to track progress of the backfill, you can view logs for your evaluator by heading to the **Evaluators** tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to [automation rule logs](./rules#view-logs-for-your-automations).

* Add an evaluator name
* Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.
* Select **Apply Evaluator**

#### 6. Select evaluator type

* Configuring [LLM-as-a-judge evaluators](/langsmith/online-evaluations#configure-a-llm-as-a-judge-online-evaluator)
* Configuring [custom code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator)

### Configure a LLM-as-a-judge online evaluator

View this guide to configure an [LLM-as-a-judge evaluator](/langsmith/llm-as-judge?mode=ui#pre-built-evaluators-1).

### Configure a custom code evaluator

Select **custom code** evaluator.

#### Write your evaluation function

<Note>
  **Custom code evaluators restrictions.**

**Allowed Libraries**: You can import all standard library functions, as well as the following public packages:

**Network Access**: You cannot access the internet from a custom code evaluator.
</Note>

Custom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.

In the UI, you will see a panel that lets you write your code inline, with some starter code:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf7b75691edb3afaa10652a79813e581" alt="" data-og-width="2910" width="2910" data-og-height="902" height="902" data-path="langsmith/images/online-eval-custom-code.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acf6e6f3be5751c93a7287971fb18907 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5df847b9d1f8171120853834d5d12f38 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=17d028d3e0709087a8be7e31343f0ab0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=668fd653028ebe056fed1e3963d3dc8e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=62465285a785c577d0b0537c5ad307ca 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7c342ba749987066e8ae05129e6a4fb7 2500w" />

Custom code evaluators take in one argument:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the sampled run to evaluate.

They return a single value:

* Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.

In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:

#### Test and save your evaluation function

Before saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.

Once you **Save**, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).

If you prefer a video tutorial, check out the [Online Evaluations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Configure multi-turn online evaluators

Multi-turn online evaluators allow you to evaluate entire conversations between a human and an agent — not just individual exchanges. They measure end-to-end interaction quality across all turns in a thread.

You can use multi-turn evaluations to measure:

1. Semantic Intent: What the user was trying to do.
2. Semantic Outcome: What actually happened, did the task succeed.
3. Trajectory: How the conversation unfolded, including trajectory of tool calls.

<Note> Running multi-turn online evals will auto-upgrade each trace within a thread to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

* Your tracing project must be using [threads](/langsmith/threads).
* The top-level inputs and outputs of each trace in a thread must have a `messages` key that contains a list of messages. We support messages in [LangChain](/langsmith/log-llm-trace#messages-format), [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create), and [Anthropic Messages](https://docs.claude.com/en/api/messages) formats.
  * If the top-level inputs and outputs of each trace only contain the latest message in the conversation, LangSmith will automatically combine messages across turns into a thread.
  * If the top-level inputs and outputs of each trace contain the full conversation history, LangSmith will use that directly.

<Note>
  If your traces don't follow the format above, thread level evaluators won't work. You’ll need to update how you trace to LangSmith to ensure each trace’s top-level inputs and outputs contain a list of `messages`.

Please refer to the [troubleshooting](#troubleshooting) section for more information.
</Note>

1. Navigate to the **Tracing Projects** tab and select a tracing project.
2. Click **+ New** in the top right corner of the tracing project page >  **New Evaluator** > **Evaluate a multi-turn thread**.
3. **Name your evaluator**.
4. **Apply filters or a sampling rate**. <br />
   Use filters or sampling to control evaluator cost. For example, evaluate only threads under *N* turns or sample 10% of all threads.
5. **Configure an idle time**. <br />
   The first time you configure a thread level evaluator, you’ll define the idle time — the amount of time after the last trace in a thread before it’s considered complete and ready for evaluation. This value should reflect the expected length of user interactions in your app. It applies across all evaluators in the project.

<Tip>
  When first testing your evaluator, use a short idle time so you can see results quickly. Once validated, increase it to match the expected length of user interactions.
</Tip>

6. **Configure your model.**<br />
   Select the provider and model you want to use for your evaluator. Threads tend to get long, so you should use a model with a higher context window in order to avoid running into limits. For example, OpenAI's GPT-4.1 mini or Gemini 2.5 Flash are good options as they both have 1M+ token context windows.

7. **Configure your LLM-as-a-judge prompt.**<br />
   Define what you want to evaluate. This prompt will be used to evaluate the thread. You can also configure which parts of the `messages` list are passed to the evaluator to control the content it receives:
   * All messages: Send the full message list.
   * Human and AI pairs: Send only user and assistant messages (excluding system messages, tool calls, etc.).
   * First human and last AI: Send only the first user message and the last assistant reply.

8. **Set up your feedback configuration**.<br />
   Configure a name for the feedback key, the format for the feedback you want to collect and optionally enable reasoning on the feedback.

<Warning>
  We don't recommend using the same feedback key for a thread-level evaluator and a run-level evaluator as it can be hard to distinguish between the two.
</Warning>

8. **Save your evaluator.**

After saving, your evaluator will appear in the **Evaluators** tab. You can test it once the idle time has passed for any new threads created after saving.

These are the current limits for multi-turn online evaluators (subject to change). Please reach out if you are running into any of these limits.

* **Runs must be less than one week old**: When a thread becomes idle, only runs within the past 7 days are eligible for evaluation.
* **Maximum of 500 threads evaluated at once**: If you have more than 500 threads marked as idle in a five minute period, we will automatically sample beyond 500.
* **Maximum of 10 multi-turn online evaluators per workspace**

**Checking the status of your evaluator** <br />
You can check when your evaluator was last run by heading to the **Evaluators** tab within a tracing project and clicking the **Logs** button for the evaluator you created to view its run history.

**Inspect the data sent to the evaluator** <br />
Inspect the data sent to the evaluator by heading to the **Evaluators** tab within a tracing project, clicking on the evaluator you created and clicking the **Evaluator traces** tab.

In this tab, you can see the inputs passed into the LLM-as-a-judge evaluator. If your messages are not being passed in correctly, you will see blank values in the inputs. This can happen if your messages are not formatted in one of [the expected formats](/langsmith/online-evaluations#prerequisites).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/online-evaluations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
numpy (v2.2.2): "numpy"
  pandas (v1.5.2): "pandas"
  jsonschema (v4.21.1): "jsonschema"
  scipy (v1.14.1): "scipy"
  sklearn (v1.26.4): "scikit-learn"
```

Example 2 (unknown):
```unknown

```

---

## Invoke the graph with the initial state

**URL:** llms-txt#invoke-the-graph-with-the-initial-state

**Contents:**
  - Use Pydantic models for graph state

response = graph.invoke(
    {
        "a": "set at start",
    }
)

print()
print(f"Output of graph invocation: {response}")

Entered node `node_1`:
    ut: {'a': 'set at start'}.
    urned: {'private_data': 'set by node_1'}
Entered node `node_2`:
    ut: {'private_data': 'set by node_1'}.
    urned: {'a': 'set by node_2'}
Entered node `node_3`:
    ut: {'a': 'set by node_2'}.
    urned: {'a': 'set by node_3'}

Output of graph invocation: {'a': 'set by node_3'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Use Pydantic models for graph state

A [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) argument on initialization that specifies the "shape" of the state that the nodes in the graph can access and update.

In our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state_schema`, but [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects).

Here, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/) can be used for [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) to add run-time validation on **inputs**.

<Note>
  **Known Limitations**

  * Currently, the output of the graph will **NOT** be an instance of a pydantic model.
  * Run-time validation only occurs on inputs into nodes, not on the outputs.
  * The validation error trace from pydantic does not show which node the error arises in.
  * Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.
</Note>
```

---

## - /temp_notes.txt

**URL:** llms-txt#--/temp_notes.txt

---

## How to set up a JavaScript application

**URL:** llms-txt#how-to-set-up-a-javascript-application

**Contents:**
- Specify Dependencies
- Specify Environment Variables
- Define Graphs
- Create the API Config
- Next

Source: https://docs.langchain.com/langsmith/setup-javascript

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up a JavaScript application for deployment using `package.json` to specify project dependencies.

This walkthrough is based on [this repository](https://github.com/langchain-ai/langgraphjs-studio-starter), which you can play around with to learn more about how to set up your application for deployment.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:

When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:

Example file directory:

## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Here is an example `agent.ts`:

Example file directory:

## Create the API Config

Create a [configuration file](/langsmith/cli#configuration-file) called `langgraph.json`. See the [configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.

Example `langgraph.json` file:

Note that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).

<Info>
  **Configuration Location**
  The configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.
</Info>

After you setup your project and place it in a GitHub repository, it's time to [deploy your app](/langsmith/deployment-quickstart).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/setup-javascript.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:
```

Example 2 (unknown):
```unknown
When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## LangGraph Server changelog

**URL:** llms-txt#langgraph-server-changelog

**Contents:**
- v0.4.39
- v0.4.38
- v0.4.37
- v0.4.36
- v0.4.35
- v0.4.34
- v0.4.33
- v0.4.32
- v0.4.30
- v0.4.29

Source: https://docs.langchain.com/langsmith/langgraph-server-changelog

[LangGraph Server](/langsmith/langgraph-server) is an API platform for creating and managing agent-based applications. It provides built-in persistence, a task queue, and supports deploying, configuring, and running assistants (agentic workflows) at scale. This changelog documents all notable updates, features, and fixes to LangGraph Server releases.

<a id="2025-10-10" />

* Upgraded `hono` from version 4.7.6 to 4.9.7, addressing a security issue related to the `bodyLimit` middleware.
* Allowed customization of the base authentication URL to enhance flexibility.
* Pinned the 'ty' dependency to a stable version using 'uv' to prevent unexpected linting failures.

<a id="2025-10-08" />

* Replaced `LANGSMITH_API_KEY` with `LANGSMITH_CONTROL_PLANE_API_KEY` to support hybrid deployments requiring license verification.
* Introduced self-hosted log ingestion support, configurable via `SELF_HOSTED_LOGS_ENABLED` and `SELF_HOSTED_LOGS_ENDPOINT` environment variables.

<a id="2025-10-06" />

* Required create permissions for copying threads to ensure proper authorization.

<a id="2025-10-03" />

* Improved error handling and added a delay to the sweep loop for smoother operation during Redis downtime or cancellation errors.
* Updated the queue entrypoint to start the core-api gRPC server when `FF_USE_CORE_API` is enabled.
* Introduced checks for invalid configurations in assistant endpoints to ensure consistency with other endpoints.

<a id="2025-10-02" />

* Resolved a timezone issue in the core API, ensuring accurate time data retrieval.
* Introduced a new `middleware_order` setting to apply authentication middleware before custom middleware, allowing finer control over protected route configurations.
* Logged the Redis URL when errors occur during Redis client creation.
* Improved Go engine/runtime context propagation to ensure consistent execution flow.
* Removed the unnecessary `assistants.put` call from the executor entrypoint to streamline the process.

<a id="2025-10-01" />

* Blocked unauthorized users from updating thread TTL settings to enhance security.

<a id="2025-10-01" />

* Improved error handling for Redis locks by logging `LockNotOwnedError` and extending initial pool migration lock timeout to 60 seconds.
* Updated the BaseMessage schema to align with the latest langchain-core version and synchronized build dependencies for consistent local development.

<a id="2025-09-30" />

* Added a GO persistence layer to the API image, enabling GRPC server operation with PostgreSQL support and enhancing configurability.
* Set the status to error when a timeout occurs to improve error handling.

<a id="2025-09-30" />

* Added support for context when using `stream_mode="events"` and included new tests for this functionality.
* Added support for overriding the server port using `$LANGGRAPH_SERVER_PORT` and removed an unnecessary Dockerfile `ARG` for cleaner configuration.
* Applied authorization filters to all table references in thread delete CTE to enhance security.
* Introduced self-hosted metrics ingestion capability, allowing metrics to be sent to an OTLP collector every minute when the corresponding environment variables are set.
* Ensured that the `set_latest` function properly updates the name and description of the version.

<a id="2025-09-26" />

* Ensured proper cleanup of redis pubsub connections in all scenarios.

<a id="2025-09-25" />

* Added a format parameter to the queue metrics server for enhanced customization.
* Corrected `MOUNT_PREFIX` environment variable usage in CLI for consistency with documentation and to prevent confusion.
* Added a feature to log warnings when messages are dropped due to no subscribers, controllable via a feature flag.
* Added support for Bookworm and Bullseye distributions in Node images.
* Consolidated executor definitions by moving them from the `langgraph-go` repository, improving manageability and updating the checkpointer setup method for server migrations.
* Ensured correct response headers are sent for a2a, improving compatibility and communication.
* Consolidated PostgreSQL checkpoint implementation, added CI testing for the `/core` directory, fixed RemoteStore test errors, and enhanced the Store implementation with transactions.
* Added PostgreSQL migrations to the queue server to prevent errors from graphs being added before migrations are performed.

<a id="2025-09-23" />

* Replaced `coredis` with `redis-py` to improve connection handling and reliability under high traffic loads.

<a id="2025-09-22-v0.4.24" />

* Added functionality to return full message history for A2A calls in accordance with the A2A spec.
* Added a `LANGGRAPH_SERVER_HOST` environment variable to Dockerfiles to support custom host settings for dual stack mode.

<a id="2025-09-22" />

* Use a faster message codec for redis streaming.

<a id="2025-09-19" />

* Ported long-stream handling to the run stream, join, and cancel endpoints for improved stream management.

<a id="2025-09-18" />

* Added A2A streaming functionality and enhanced testing with the A2A SDK.
* Added Prometheus metrics to track language usage in graphs, middleware, and authentication for improved insights.
* Fixed bugs in Open Source Software related to message conversion for chunks.
* Removed await from pubsub subscribes to reduce flakiness in cluster tests and added retries in the shutdown suite to enhance API stability.

<a id="2025-09-11" />

* Optimized Pubsub initialization to prevent overhead and address subscription timing issues, ensuring smoother run execution.

<a id="2025-09-11" />

* Removed warnings from psycopg by addressing function checks introduced in version 3.2.10.

<a id="2025-09-11" />

* Filtered out logs with mount prefix to reduce noise in logging output.

<a id="2025-09-10" />

* Added support for implicit thread creation in a2a to streamline operations.
* Improved error serialization and emission in distributed runtime streams, enabling more comprehensive testing.

<a id="2025-09-09" />

* Monitored queue status in the health endpoint to ensure correct behavior when PostgreSQL fails to initialize.
* Addressed an issue with unequal swept ID lengths to improve log clarity.
* Enhanced streaming outputs by avoiding re-serialization of DR payloads, using msgpack byte inspection for json-like parsing.

<a id="2025-09-04" />

* Ensured metrics are returned even when experiencing database connection issues.
* Optimized update streams to prevent unnecessary data transmission.
* Upgraded `hono` from version 4.9.2 to 4.9.6 in the `storage_postgres/langgraph-api-server` for improved URL path parsing security.
* Added retries and an in-memory cache for LangSmith access calls to improve resilience against single failures.

<a id="2025-09-04" />

* Added support for TTL (time-to-live) in thread updates.

<a id="2025-09-04" />

* In distributed runtime, update serde logic for final checkpoint -> thread setting.

<a id="2025-09-02" />

* Added support for filtering search results by IDs in the search endpoint for more precise queries.
* Included configurable headers for assistant endpoints to enhance request customization.
* Implemented a simple A2A endpoint with support for agent card retrieval, task creation, and task management.

<a id="2025-08-30" />

* Stopped the inclusion of x-api-key to enhance security.

<a id="2025-08-29" />

* Fixed a race condition when joining streams, preventing duplicate start events.

<a id="2025-08-29" />

* Ensured the checkpointer starts and stops correctly before and after the queue to improve shutdown and startup efficiency.
* Resolved an issue where workers were being prematurely cancelled when the queue was cancelled.
* Prevented queue termination by adding a fallback for cases when Redis fails to wake a worker.

<a id="2025-08-28" />

* Set the custom auth thread\_id to None for stateless runs to prevent conflicts.
* Improved Redis signaling in the Go runtime by adding a wakeup worker and Redis lock implementation, and updated sweep logic.

<a id="2025-08-27" />

* Added stream mode to thread stream for improved data processing.
* Added a durability parameter to runs for improved data persistence.

<a id="2025-08-27" />

* Ensured pubsub is initialized before creating a run to prevent errors from missing messages.

<a id="2025-08-25" />

* Emitted attempt messages correctly within the thread stream.
* Reduced cluster conflicts by using only the thread ID for hashing in cluster mapping, prioritizing efficiency with stream\_thread\_cache.
* Introduced a stream endpoint for threads to track all outputs across sequentially executed runs.
* Made the filter query builder in PostgreSQL more robust against malformed expressions and improved validation to prevent potential security risks.

<a id="2025-08-25" />

* Added custom Prometheus metrics for Redis/PG connection pools and switched the queue server to Uvicorn/Starlette for improved monitoring.
* Restored Wolfi image build by correcting shell command formatting and added a Makefile target for testing with nginx.

<a id="2025-08-22" />

* Added timeouts to specific Redis calls to prevent workers from being left active.
* Updated the Golang runtime and added pytest skips for unsupported functionalities, including initial support for passing store to node and message streaming.
* Introduced a reverse proxy setup for serving combined Python and Node.js graphs, with nginx handling server routing, to facilitate a Postgres/Redis backend for the Node.js API server.

<a id="2025-08-21" />

* Added a statement timeout to the pool to prevent long-running queries.

<a id="2025-08-21" />

* Set a default 15-minute statement timeout and implemented monitoring for long-running queries to ensure system efficiency.
* Stop propagating run configurable values to the thread configuration, because this can cause issues on subsequent runs if you are specifying a checkpoint\_id. This is a **slight breaking change** in behavior, since the thread value will no longer automatically reflect the unioned configuration of the most recent run. We believe this behavior is more intuitive, however.
* Enhanced compatibility with older worker versions by handling event data in channel names within ops.py.

<a id="2025-08-20" />

* Fixed an unbound local error and improved logging for thread interruptions or errors, along with type updates.

<a id="2025-08-20" />

* Added enhanced logging to aid in debugging metaview issues.
* Upgraded executor and runtime to the latest version for improved performance and stability.

<a id="2025-08-19" />

* Ensured async coroutines are properly awaited to prevent potential runtime errors.

<a id="2025-08-18" />

* Enhanced search functionality to improve performance by allowing users to select specific columns for query results.

<a id="2025-08-18" />

* Added count endpoints for crons, threads, and assistants to enhance data tracking (#1132).
* Improved SSH functionality for better reliability and stability.
* Updated @langchain/langgraph-api to version 0.0.59 to fix an invalid state schema issue.

<a id="2025-08-15" />

* Added Go language images to enhance project compatibility and functionality.
* Printed internal PIDs for JS workers to facilitate process inspection via SIGUSR1 signal.
* Resolved a `run_pkey` error that occurred when attempting to insert duplicate runs.
* Added `ty run` command and switched to using uuid7 for generating run IDs.
* Implemented the initial Golang runtime to expand language support.

<a id="2025-08-14" />

* Added support for `object agent spec` with descriptions in JS.

<a id="2025-08-13" />

* Added a feature flag (FF\_RICH\_THREADS=false) to disable thread updates on run creation, reducing lock contention and simplifying thread status handling.
* Utilized existing connections for `aput` and `apwrite` operations to improve performance.
* Improved error handling for decoding issues to enhance data processing reliability.
* Excluded headers from logs to improve security while maintaining runtime functionality.
* Fixed an error that prevented mapping slots to a single node.
* Added debug logs to track node execution in JS deployments for improved issue diagnosis.
* Changed the default multitask strategy to enqueue, improving throughput by eliminating the need to fetch inflight runs during new run insertions.
* Optimized database operations for `Runs.next` and `Runs.sweep` to reduce redundant queries and improve efficiency.
* Improved run creation speed by skipping unnecessary inflight runs queries.

<a id="2025-08-11" />

* Stopped passing internal LGP fields to context to prevent breaking type checks.
* Exposed content-location headers to ensure correct resumability behavior in the API.

<a id="2025-08-08" />

* Ensured synchronized updates between `configurable` and `context` in assistants, preventing setup errors and supporting smoother version transitions.

<a id="2025-08-08" />

* Excluded unrequested stream modes from the resumable stream to optimize functionality.

<a id="2025-08-08" />

* Made access logger headers configurable to enhance logging flexibility.
* Debounced the Runs.stats function to reduce the frequency of expensive calls and improve performance.
* Introduced debouncing for sweepers to enhance performance and efficiency (#1147).
* Acquired a lock for TTL sweeping to prevent database spamming during scale-out operations.

<a id="2025-08-06" />

* Updated tracing context replicas to use the new format, ensuring compatibility.

<a id="2025-08-06" />

* Added an entrypoint to the queue replica for improved deployment management.

<a id="2025-08-06" />

* Utilized persisted interrupt status in `join` to ensure correct handling of user's interrupt state after completion.

<a id="2025-08-06" />

* Consolidated events to a single channel to prevent race conditions and optimize startup performance.
* Ensured custom lifespans are invoked on queue workers for proper setup, and added tests.

<a id="2025-08-04" />

* Restored the original streaming behavior of runs, ensuring consistent inclusion of interrupt events based on `stream_mode` settings.
* Optimized `Runs.next` query to reduce average execution time from \~14.43ms to \~2.42ms, improving performance.
* Added support for stream mode "tasks" and "checkpoints", normalized the UI namespace, and upgraded `@langchain/langgraph-api` for enhanced functionality.

<a id="2025-07-31" />

* Added a composite index on threads for faster searches with owner-based authentication and updated the default sort order to `updated_at` for improved query performance.

<a id="2025-07-31" />

* Reduced the default number of history checkpoints from 10 to 1 to optimize performance.

<a id="2025-07-31" />

* Optimized cache re-use to enhance application performance and efficiency.

<a id="2025-07-30" />

* Improved thread search pagination by updating response headers with `X-Pagination-Total` and `X-Pagination-Next` for better navigation.

<a id="2025-07-30" />

* Ensured sync logging methods are awaited and added a linter to prevent future occurrences.
* Fixed an issue where JavaScript tasks were not being populated correctly for JS graphs.

<a id="2025-07-29" />

* Fixed JS graph streaming failure by starting the heartbeat as soon as the connection opens.

<a id="2025-07-29" />

* Added interrupts as default values for join operations while preserving stream behavior.

<a id="2025-07-28" />

* Fixed an issue where config schema was missing when `config_type` was not set, ensuring more reliable configurations.

<a id="2025-07-28" />

* Prepared for LangGraph v0.6 compatibility with new context API support and bug fixes.

<a id="2025-07-27" />

* Implemented caching for authentication processes to enhance performance and efficiency.
* Optimized database performance by merging count and select queries.

<a id="2025-07-27" />

* Made log streams resumable, enhancing reliability and improving user experience when reconnecting.

<a id="2025-07-27" />

* Added a heapdump endpoint to save memory heap information to a file.

<a id="2025-07-25" />

* Used the correct metadata endpoint to resolve issues with data retrieval.

<a id="2025-07-24" />

* Captured interrupt events in the wait method to preserve previous behavior from langgraph 0.5.0.
* Added support for SDK structlog in the JavaScript environment for enhanced logging capabilities.

<a id="2025-07-24" />

* Corrected the metadata endpoint for self-hosted deployments.

<a id="2025-07-22" />

* Improved license check by adding an in-memory cache and handling Redis connection errors more effectively.
* Reloaded assistants to preserve manually created ones while discarding those removed from the configuration file.
* Reverted changes to ensure the UI namespace for gen UI is a valid JavaScript property name.
* Ensured that the UI namespace for generated UI is a valid JavaScript property name, improving API compliance.
* Enhanced error handling to return a 422 status code for unprocessable entity requests.

<a id="2025-07-19" />

* Added context to langgraph nodes to improve log filtering and trace visibility.

<a id="2025-07-19" />

* Improved interoperability with the ckpt ingestion worker on the main loop to prevent task scheduling issues.
* Delayed queue worker startup until after migrations are completed to prevent premature execution.
* Enhanced thread state error handling by adding specific metadata and improved response codes for better clarity when state updates fail during creation.
* Exposed the interrupt ID when retrieving the thread state to improve API transparency.

<a id="2025-07-17" />

* Added a fallback mechanism for configurable header patterns to handle exclude/include settings more effectively.

<a id="2025-07-17" />

* Avoided setting the future if it is already done to prevent redundant operations.
* Resolved compatibility errors in CI by switching from `typing.TypedDict` to `typing_extensions.TypedDict` for Python versions below 3.12.

<a id="2025-07-16" />

* Improved performance by omitting pending sends for langgraph versions 0.5 and above.
* Improved server startup logs to provide clearer warnings when the DD\_API\_KEY environment variable is set.

<a id="2025-07-16" />

* Removed the GIN index for run metadata to improve performance.

<a id="2025-07-16" />

* Enabled copying functionality for blobs and checkpoints, improving data management flexibility.

<a id="2025-07-16" />

* Reduced writes to the `checkpoint_blobs` table by inlining small values (null, numeric, str, etc.). This means we don't need to store extra values for channels that haven't been updated.

<a id="2025-07-16" />

* Improve checkpoint writes via node-local background queueing.

<a id="2025-07-15" />

* Decoupled checkpoint writing from thread/run state by removing foreign keys and updated logger to prevent timeout-related failures.

<a id="2025-07-14" />

* Removed the foreign key constraint for `thread` in the `run` table to simplify database schema.

<a id="2025-07-14" />

* Added more detailed logs for Redis worker signaling to improve debugging.

<a id="2025-07-11" />

* Honored tool descriptions in the `/mcp` endpoint to align with expected functionality.

<a id="2025-07-10" />

* Added support for the `on_disconnect` field to `runs/wait` and included disconnect logs for better debugging.

<a id="2025-07-09" />

* Removed unnecessary status updates to streamline thread handling and updated version to 0.2.84.

<a id="2025-07-09" />

* Reduced the default time-to-live for resumable streams to 2 minutes.
* Enhanced data submission logic to send data to both Beacon and LangSmith instance based on license configuration.
* Enabled submission of self-hosted data to a LangSmith instance when the endpoint is configured.

<a id="2025-07-03" />

* Addressed a race condition in background runs by implementing a lock using join, ensuring reliable execution across CTEs.

<a id="2025-07-03" />

* Optimized run streams by reducing initial wait time to improve responsiveness for older or non-existent runs.

<a id="2025-07-03" />

* Corrected parameter passing in the `logger.ainfo()` API call to resolve a TypeError.

<a id="2025-07-02" />

* Fixed a JsonDecodeError in checkpointing with remote graph by correcting JSON serialization to handle trailing slashes properly.
* Introduced a configuration flag to disable webhooks globally across all routes.

<a id="2025-07-02" />

* Added timeout retries to webhook calls to improve reliability.
* Added HTTP request metrics, including a request count and latency histogram, for enhanced monitoring capabilities.

<a id="2025-07-02" />

* Added HTTP metrics to improve performance monitoring.
* Changed the Redis cache delimiter to reduce conflicts with subgraph message names and updated caching behavior.

<a id="2025-07-01" />

* Updated Redis cache delimiter to prevent conflicts with subgraph messages.

<a id="2025-06-30" />

* Scheduled webhooks in an isolated loop to ensure thread-safe operations and prevent errors with PYTHONASYNCIODEBUG=1.

<a id="2025-06-27" />

* Fixed an infinite frame loop issue and removed the dict\_parser due to structlog's unexpected behavior.
* Throw a 409 error on deadlock occurrence during run cancellations to handle lock conflicts gracefully.

<a id="2025-06-27" />

* Ensured compatibility with future langgraph versions.
* Implemented a 409 response status to handle deadlock issues during cancellation.

<a id="2025-06-26" />

* Improved logging for better clarity and detail regarding log types.

<a id="2025-06-26" />

* Improved error handling to better distinguish and log TimeoutErrors caused by users from internal run timeouts.

<a id="2025-06-26" />

* Added sorting and pagination to the crons API and updated schema definitions for improved accuracy.

<a id="2025-06-26" />

* Fixed a 404 error when creating multiple runs with the same thread\_id using `on_not_exist="create"`.

<a id="2025-06-25" />

* Ensured that only fields from `assistant_versions` are returned when necessary.
* Ensured consistent data types for in-memory and PostgreSQL users, improving internal authentication handling.

<a id="2025-06-24" />

* Added descriptions to version entries for better clarity.

<a id="2025-06-23" />

* Improved user handling for custom authentication in the JS Studio.
* Added Prometheus-format run statistics to the metrics endpoint for better monitoring.
* Added run statistics in Prometheus format to the metrics endpoint.

<a id="2025-06-20" />

* Set a maximum idle time for Redis connections to prevent unnecessary open connections.

<a id="2025-06-20" />

* Enhanced error logging to include traceback details for dictionary operations.
* Added a `/metrics` endpoint to expose queue worker metrics for monitoring.

<a id="2025-06-18" />

* Removed CancelledError from retriable exceptions to allow local interrupts while maintaining retriability for workers.
* Introduced middleware to gracefully shut down the server after completing in-flight requests upon receiving a SIGINT.
* Reduced metadata stored in checkpoint to only include necessary information.
* Improved error handling in join runs to return error details when present.

<a id="2025-06-17" />

* Improved application stability by adding a handler for SIGTERM signals.

<a id="2025-06-17" />

* Improved the handling of cancellations in the queue entrypoint.
* Improved cancellation handling in the queue entry point.

<a id="2025-06-16" />

* Enhanced error message for LuaLock timeout during license validation.
* Fixed the \$contains filter in custom auth by requiring an explicit ::text cast and updated tests accordingly.
* Ensured project and tenant IDs are formatted as UUIDs for consistency.

<a id="2025-06-13" />

* Resolved a timing issue to ensure the queue starts only after the graph is registered.
* Improved performance by setting thread and run status in a single query and enhanced error handling during checkpoint writes.
* Reduced the default background grace period to 3 minutes.

<a id="2025-06-12" />

* Now logging expected graphs when one is omitted to improve traceability.
* Implemented a time-to-live (TTL) feature for resumable streams.
* Improved query efficiency and consistency by adding a unique index and optimizing row locking.

<a id="2025-06-12" />

* Handled `CancelledError` by marking tasks as ready to retry, improving error management in worker processes.
* Added LG API version and request ID to metadata and logs for better tracking.
* Added LG API version and request ID to metadata and logs to improve traceability.
* Improved database performance by creating indexes concurrently.
* Ensured postgres write is committed only after the Redis running marker is set to prevent race conditions.
* Enhanced query efficiency and reliability by adding a unique index on thread\_id/running, optimizing row locks, and ensuring deterministic run selection.
* Resolved a race condition by ensuring Postgres updates only occur after the Redis running marker is set.

<a id="2025-06-07" />

* Introduced a new connection for each operation while preserving transaction characteristics in Threads state `update()` and `bulk()` commands.

<a id="2025-06-05" />

* Enhanced streaming feature by incorporating tracing contexts.
* Removed an unnecessary query from the Crons.search function.
* Resolved connection reuse issue when scheduling next run for multiple cron jobs.
* Removed an unnecessary query in the Crons.search function to improve efficiency.
* Resolved an issue with scheduling the next cron run by improving connection reuse.

<a id="2025-06-04" />

* Enhanced the worker logic to exit the pipeline before continuing when the Redis message limit is reached.
* Introduced a ceiling for Redis message size with an option to skip messages larger than 128 MB for improved performance.
* Ensured the pipeline always closes properly to prevent resource leaks.

<a id="2025-06-04" />

* Improved performance by omitting logs in metadata calls and ensuring output schema compliance in value streaming.
* Ensured the connection is properly closed after use.
* Aligned output format to strictly adhere to the specified schema.
* Stopped sending internal logs in metadata requests to improve privacy.

<a id="2025-06-04" />

* Added timestamps to track the start and end of a request's run.
* Added tracer information to the configuration settings.
* Added support for streaming with tracing contexts.

<a id="2025-06-03" />

* Added locking mechanism to prevent errors in pipelined executions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-server-changelog.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to evaluate a graph

**URL:** llms-txt#how-to-evaluate-a-graph

**Contents:**
- End-to-end evaluations
  - Define a graph

Source: https://docs.langchain.com/langsmith/evaluate-graph

<Info>
  [langgraph](https://langchain-ai.github.io/langgraph/)
</Info>

`langgraph` is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating `langgraph` graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to `evaluate()` / `aevaluate()`. For evaluation techniques and best practices when building agents head to the [langgraph docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation).

## End-to-end evaluations

The most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.

Lets construct a simple ReACT agent to start:

```python  theme={null}
from typing import Annotated, Literal, TypedDict
from langchain.chat_models import init_chat_model
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

class State(TypedDict):
    # Messages have the type "list". The 'add_messages' function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]

---

## This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler

**URL:** llms-txt#this-takes-precedenceover-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler

@auth.on.threads.create_run
async def on_run_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create_run.value
):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    # Inherit thread's access control
    return {"owner": ctx.user.identity}

---

## LangSmith ingests JSON format events

**URL:** llms-txt#langsmith-ingests-json-format-events

export OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=http/json

---

## cache store

**URL:** llms-txt#cache-store

agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

---

## Define nodes

**URL:** llms-txt#define-nodes

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

---

## Build the state graph

**URL:** llms-txt#build-the-state-graph

builder = StateGraph(OverallState)
builder.add_node(node)  # node_1 is the first node
builder.add_edge(START, "node")  # Start the graph with node_1
builder.add_edge("node", END)  # End the graph after node_1
graph = builder.compile()

---

## this is the graph making function that will decide which graph to

**URL:** llms-txt#this-is-the-graph-making-function-that-will-decide-which-graph-to

---

## First invocation

**URL:** llms-txt#first-invocation

agent.invoke(HumanMessage(content="I live in Sydney, Australia."))

---

## Create clients with different sampling rates

**URL:** llms-txt#create-clients-with-different-sampling-rates

client_1 = Client(tracing_sampling_rate=0.5)  # 50% sampling
client_2 = Client(tracing_sampling_rate=0.25)  # 25% sampling
client_no_trace = Client(tracing_sampling_rate=0.0)  # No tracing

---

## - /memories/user_preferences.txt

**URL:** llms-txt#--/memories/user_preferences.txt

---

## Augment the LLM with schema for structured output

**URL:** llms-txt#augment-the-llm-with-schema-for-structured-output

structured_llm = llm.with_structured_output(SearchQuery)

---

## Composite evaluators

**URL:** llms-txt#composite-evaluators

**Contents:**
- Create a composite evaluator using the UI
  - 1. Navigate to the tracing project or dataset
  - 2. Configure the composite evaluator
  - 3. View composite evaluator results
- Create composite feedback with the SDK
  - 1. Configure evaluators on a dataset
  - 2. Create composite feedback

Source: https://docs.langchain.com/langsmith/composite-evaluators

*Composite evaluators* are a way to combine multiple evaluator scores into a single [score](/langsmith/evaluation-concepts#evaluator-outputs). This is useful when you want to evaluate multiple aspects of your application and combine the results into a single result.

## Create a composite evaluator using the UI

You can create composite evaluators on a [tracing project](/langsmith/observability-concepts#projects) (for [online evaluations](/langsmith/evaluation-concepts#online-evaluation)) or a [dataset](/langsmith/evaluation-concepts#datasets) (for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation)). With composite evaluators in the UI, you can compute a weighted average or weighted sum of multiple evaluator scores, with configurable weights.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=b3859ada8b576ebeaf5399ff15359b10" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="756" width="756" data-og-height="594" height="594" data-path="langsmith/images/create_composite_evaluator-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=9bab5ad812328acdd6ffe858f487262b 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=4637a2dc732f945d98b0214023266180 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=c3e7b24dde21ed45f481b7a513ecc256 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=1310a99e2a8b37d68d78f794b8ce6606 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=6beb89dcc6ec734b2ad012bc46c58821 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ba7fd7ba48a3e46d8701b6f64bb68f66 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ac13f4d2d4a5e3b67285284150b7d592" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="761" width="761" data-og-height="585" height="585" data-path="langsmith/images/create_composite_evaluator-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=bfc19d802f0327a579d90e519441cf9a 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=23ab26db75e25795c17abf07e487ba5d 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=7ce9597b62f3e68b2dc1afa5f17f0e8c 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ee7058d60185a820fe23decf003bd2c1 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=cff38ad541c55d6834edfa67f5650818 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=0f85093799a489eff72dae01ed5b6d94 2500w" />
</div>

### 1. Navigate to the tracing project or dataset

To start configuring a composite evaluator, navigate to the **Tracing Projects** or **Dataset & Experiments** tab and select a project or dataset.

* From within a tracing project: **+ New** > **Evaluator** > **Composite score**
* From within a dataset: **+ Evaluator** > **Composite score**

### 2. Configure the composite evaluator

1. Name your evaluator.
2. Select an aggregation method, either **Average** or **Sum**.
   * **Average**: ∑(weight\*score) / ∑(weight).
   * **Sum**: ∑(weight\*score).
3. Add the feedback keys you want to include in the composite score.
4. Add the weights for the feedback keys. By default, the weights are equal for each feedback key. Adjust the weights to increase or decrease the importance of specific feedback keys in the final score.
5. Click **Create** to save the evaluator.

<Tip> If you need to adjust the weights for the composite scores, they can be updated after the evaluator is created. The resulting scores will be updated for all runs that have the evaluator configured. </Tip>

### 3. View composite evaluator results

Composite scores are attached to a run as **feedback**, similarly to feedback from a single evaluator. How you can view them depends on where the evaluation was run:

**On a tracing project**:

* Composite scores appear as feedback on runs.
* [Filter for runs](/langsmith/filter-traces-in-application) with a composite score, or where the composite score meets a certain threshold.
* [Create a chart](/langsmith/dashboards#custom-dashboards) to visualize trends in the composite score over time.

* View the composite scores in the experiments tab. You can also filter and sort experiments based on the average composite score of their runs.
* Click into an experiment to view the composite score for each run.

<Note> If any of the constituent evaluators are not configured on the run, the composite score will not be calculated for that run. </Note>

## Create composite feedback with the SDK

This guide describes setting up an evaluation that uses multiple evaluators and combines their scores with a custom aggregation function.

<Note> Requires langsmith>=0.4.29 </Note>

### 1. Configure evaluators on a dataset

Start by configuring your evaluators. In this example, the application generates a tweet from a blog introduction and uses three evaluators — summary, tone, and formatting — to assess the output.

If you already have your own dataset with evaluators configured, you can skip this step.

<Accordion title="Configure evaluators on a dataset.">
  
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
  
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/composite-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
```

---

## Set up hybrid LangSmith

**URL:** llms-txt#set-up-hybrid-langsmith

**Contents:**
- Prerequisites
- Kubernetes
  - Prerequisites
  - Setup

Source: https://docs.langchain.com/langsmith/deploy-hybrid

<Info>
  **Important**
  The Hybrid deployment option requires an [Enterprise](https://langchain.com/pricing) plan.
</Info>

The [**hybrid**](/langsmith/hybrid) model lets you run the [data plane](/langsmith/data-plane)—your LangGraph Server deployments and agent workloads—in your own cloud, while LangChain hosts and manages the [control plane](/langsmith/control-plane) (the LangSmith UI and orchestration). This setup gives you the flexibility of self-hosting your runtime environments with the convenience of a managed LangSmith instance.

The following steps describe how to connect your self-hosted data plane to the managed LangSmith control plane.

1. Use the [LangGraph CLI](/langsmith/cli) to [test your application locally](/langsmith/local-server).
2. Use the [LangGraph CLI](/langsmith/cli) to build a Docker image (i.e. `langgraph build`) and push it to a registry your Kubernetes cluster or Amazon ECS cluster has access to.

1. `KEDA` is installed on your cluster.
   
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress).
3. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
4. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their LangGraph Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-platform-operator`: This operator handles changes to your LangSmith CRDs.
4. Configure your `langgraph-dataplane-values.yaml` file.
   
   * `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to LangGraph Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage LangGraph Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new LangGraph Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage LangGraph Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the LangGraph Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the LangGraph Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for LangGraph Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.enabled`: There can only be 1 instance of the `langgraph-platform-operator` deployed in a Kubernetes namespace. Set this to `false` if there is already an instance of `langgraph-platform-operator` deployed in the current Kubernetes namespace.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
   
6. If successful, you will see three services start up in your namespace.
   
7. Create a deployment from the [control plane UI](/langsmith/control-plane#control-plane-ui).
   1. Select the desired listener from the list of `Compute IDs` in the dropdown menu.
   2. Select the Kubernetes namespace to deploy to.
   3. Fill out all other required fields and select `Submit` in the top-right of the panel.
   4. The deployment will be deployed on the Kubernetes cluster where the listener is deployed and in the Kubernetes namespace specified in step 7b.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-hybrid.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress).
3. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
4. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

### Setup

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their LangGraph Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-platform-operator`: This operator handles changes to your LangSmith CRDs.
4. Configure your `langgraph-dataplane-values.yaml` file.
```

Example 2 (unknown):
```unknown
* `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to LangGraph Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage LangGraph Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new LangGraph Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage LangGraph Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the LangGraph Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the LangGraph Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for LangGraph Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.enabled`: There can only be 1 instance of the `langgraph-platform-operator` deployed in a Kubernetes namespace. Set this to `false` if there is already an instance of `langgraph-platform-operator` deployed in the current Kubernetes namespace.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
```

Example 3 (unknown):
```unknown
6. If successful, you will see three services start up in your namespace.
```

---

## Verify the variables are set

**URL:** llms-txt#verify-the-variables-are-set

**Contents:**
  - Initial export

echo "Customer ID: $CUSTOMER_ID"
echo "Customer Name: $CUSTOMER_NAME"
bash  theme={null}
curl -s $LANGSMITH_URL/api/v1/info
export CUSTOMER_ID="<id>"
export CUSTOMER_NAME="<name>"
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can then use these environment variables in your export scripts or other commands.

If you don't have `jq`, run these commands to set the environment variables based on the curl output:
```

Example 2 (unknown):
```unknown
### Initial export

These scripts export usage data to a CSV for reporting to LangChain. They additionally track the export by assigning a backfill ID and timestamp.

To export LangSmith trace usage:
```

---

## Error troubleshooting

**URL:** llms-txt#error-troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/common-errors

This page contains guides around resolving common errors you may find while building with LangGraph.
Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

* [`GRAPH_RECURSION_LIMIT`](/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT)
* [`INVALID_CONCURRENT_GRAPH_UPDATE`](/oss/python/langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE)
* [`INVALID_GRAPH_NODE_RETURN_VALUE`](/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE)
* [`MULTIPLE_SUBGRAPHS`](/oss/python/langgraph/errors/MULTIPLE_SUBGRAPHS)
* [`INVALID_CHAT_HISTORY`](/oss/python/langgraph/errors/INVALID_CHAT_HISTORY)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/common-errors.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## The sky is typically blue

**URL:** llms-txt#the-sky-is-typically-blue

---

## We use LCEL declarative syntax here.

**URL:** llms-txt#we-use-lcel-declarative-syntax-here.

---

## Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

runs = requests.post(
    f"https://api.smith.langchain.com/api/v1/runs/query",
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]},
    json={
        "session": experiment_ids,
        "is_root": True, # Only fetch root runs (spans) which contain the end outputs
        "select": ["id", "reference_example_id", "outputs"],
    }
).json()
runs = runs["runs"]
for run in runs:
    example_id = run["reference_example_id"]
    example_id_to_runs_map[example_id].append(run)

for example_id, runs in example_id_to_runs_map.items():
    print(f"Example ID: {example_id}")
    # Preferentially rank the outputs, in this case we will always prefer the first output
    # In reality, you can use an LLM to rank the outputs
    feedback_group_id = uuid4()

# Post a feedback score for each run, with the first run being the preferred one
    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post
    # We'll use the feedback group ID to associate the feedback scores with the same group
    for i, run in enumerate(runs):
        print(f"Run ID: {run['id']}")
        feedback = {
            "score": 1 if i == 0 else 0,
            "run_id": str(run["id"]),
            "key": "ranked_preference",
            "feedback_group_id": str(feedback_group_id),
            "comparative_experiment_id": comparative_experiment_id,
        }
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/feedback",
            json=feedback,
            headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
        )
        resp.raise_for_status()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evals-api-only.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## where message_chunk is the token streamed by the LLM and metadata is a dictionary

**URL:** llms-txt#where-message_chunk-is-the-token-streamed-by-the-llm-and-metadata-is-a-dictionary

---

## Load all documents

**URL:** llms-txt#load-all-documents

documents = loader.load()

---

## maxReplicas: 10

**URL:** llms-txt#maxreplicas:-10

---

## 'outputs' will come from your target function.

**URL:** llms-txt#'outputs'-will-come-from-your-target-function.

**Contents:**
- Example: Single LLM call
- Example: Non-LLM component
- Example: Application or agent

def evaluator_one(inputs: dict, outputs: dict) -> bool:
    return outputs["foo"] == 2

def evaluator_two(inputs: dict, outputs: dict) -> bool:
    return len(outputs["bar"]) < 3

client = Client()
results = client.evaluate(
    dummy_target,  # <-- target function
    data="your-dataset-name",
    evaluators=[evaluator_one, evaluator_two],
    ...
)
python Python theme={null}
  from langsmith import wrappers
  from openai import OpenAI

# Optionally wrap the OpenAI client to automatically
  # trace all model calls.
  oai_client = wrappers.wrap_openai(OpenAI())

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a 'messages' key.
    # You can update to match your dataset schema.
    messages = inputs["messages"]
    response = oai_client.chat.completions.create(
        messages=messages,
        model="gpt-4o-mini",
    )
    return {"answer": response.choices[0].message.content}
  typescript TypeScript theme={null}
  import OpenAI from 'openai';
  import { wrapOpenAI } from "langsmith/wrappers";

const client = wrapOpenAI(new OpenAI());

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const response = await client.chat.completions.create({
        messages: messages,
        model: 'gpt-4o-mini',
    });
    return { answer: response.choices[0].message.content };
  }
  python Python (LangChain) theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-4o-mini")

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    response = model.invoke(messages)
    return {"answer": response.content}
  typescript TypeScript (LangChain) theme={null}
  import { ChatOpenAI } from '@langchain/openai';

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const model = new ChatOpenAI({ model: "gpt-4o-mini" });
    const response = await model.invoke(messages);
    return {"answer": response.content};
  }
  python Python theme={null}
  from langsmith import traceable

# Optionally decorate with '@traceable' to trace all invocations of this function.
  @traceable
  def calculator_tool(operation: str, number1: float, number2: float) -> str:
    if operation == "add":
        return str(number1 + number2)
    elif operation == "subtract":
        return str(number1 - number2)
    elif operation == "multiply":
        return str(number1 * number2)
    elif operation == "divide":
        return str(number1 / number2)
    else:
        raise ValueError(f"Unrecognized operation: {operation}.")

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys.
    operation = inputs["operation"]
    number1 = inputs["num1"]
    number2 = inputs["num2"]
    result = calculator_tool(operation, number1, number2)
    return {"result": result}
  typescript TypeScript theme={null}
  import { traceable } from "langsmith/traceable";

// Optionally wrap in 'traceable' to trace all invocations of this function.
  const calculatorTool = traceable(async ({ operation, number1, number2 }) => {
  // Functions must return strings
  if (operation === "add") {
    return (number1 + number2).toString();
  } else if (operation === "subtract") {
    return (number1 - number2).toString();
  } else if (operation === "multiply") {
    return (number1 * number2).toString();
  } else if (operation === "divide") {
    return (number1 / number2).toString();
  } else {
    throw new Error("Invalid operation.");
  }
  });

// This is the function you will evaluate.
  const target = async (inputs) => {
  // This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys
  const result = await calculatorTool.invoke({
    operation: inputs.operation,
    number1: inputs.num1,
    number2: inputs.num2,
  });
  return { result };
  }
  python Python theme={null}
  from my_agent import agent

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    # Replace `invoke` with whatever you use to call your agent
    response = agent.invoke({"messages": messages})
    # This assumes your agent output is in the right format
    return response
  typescript TypeScript theme={null}
  import { agent } from 'my_agent';

// This is the function you will evaluate.
  const target = async(inputs) => {
  // This assumes your dataset has inputs with a `messages` key
  const messages = inputs.messages;
  // Replace `invoke` with whatever you use to call your agent
  const response = await agent.invoke({ messages });
  // This assumes your agent output is in the right format
  return response;
  }
  python  theme={null}
  from my_agent import agent
  from langsmith import Client
  client = Client()
  client.evaluate(agent, ...)
  ```
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/define-target-function.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Check>
  `evaluate()` will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.
</Check>

## Example: Single LLM call

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## minReplicas: 3

**URL:** llms-txt#minreplicas:-3

**Contents:**
- Ensure your Redis cache is at least 200 GB
  - High reads, low writes <a name="high-reads-low-writes" />

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

clickhouse:
  statefulSet:
    persistence:
      # This may depend on your configured TTL (see config section).
      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.
      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.
    resources:
      requests:
        cpu: "10"
        memory: "32Gi"
      limits:
        cpu: "16"
        memory: "48Gi"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true

frontend:
  deployment:
    replicas: 2

queue:
  deployment:
    replicas: 6 # OR enable autoscaling to this level (example below)

**Examples:**

Example 1 (unknown):
```unknown
### High reads, low writes <a name="high-reads-low-writes" />

You have a relatively low scale of trace ingestions, but many frontend users querying traces and/or have scripts that hit the `/runs/query` or `/runs/<run-id>` endpoints frequently.

**For this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency.** See our [external ClickHouse doc](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster) for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.

For this, we recommend a configuration like this:
```

---

## Workflow execution configuration with a unique thread identifier

**URL:** llms-txt#workflow-execution-configuration-with-a-unique-thread-identifier

config = {
    "configurable": {
        "thread_id": "1"  # Unique identifier to track workflow execution
    }
}

---

## {"type": "image", "base64": "...", "mime_type": "image/jpeg"},

**URL:** llms-txt#{"type":-"image",-"base64":-"...",-"mime_type":-"image/jpeg"},

---

## Create a new experiment using the /sessions endpoint

**URL:** llms-txt#create-a-new-experiment-using-the-/sessions-endpoint

---

## Run support queries against ClickHouse

**URL:** llms-txt#run-support-queries-against-clickhouse

**Contents:**
  - Prerequisites
  - Running the query script

Source: https://docs.langchain.com/langsmith/script-running-ch-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).

This command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `ch_get_query_exceptions.sql` input file in the `support_queries/clickhouse` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_ch.sh)

### Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag `--output path/to/file.csv`

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-ch-support-queries.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

---

## Integration packages

**URL:** llms-txt#integration-packages

**Contents:**
- Popular providers
- All providers

Source: https://docs.langchain.com/oss/python/integrations/providers/overview

{/* Do not manually edit */}

LangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

<Columns cols={3}>
  <Card title="Chat models" icon="message" href="/oss/python/integrations/chat" arrow />

<Card title="Embedding models" icon="layer-group" href="/oss/python/integrations/text_embedding" arrow />

<Card title="Tools and toolkits" icon="screwdriver-wrench" href="/oss/python/integrations/tools" arrow />
</Columns>

To see a full list of integrations by component type, refer to the categories in the sidebar.

| Provider                                                            | Package API reference                                                                                                 | Downloads                                                                                                                                                                                                                         | Latest version                                                                                                                                                                                                                                           | <Tooltip tip="Whether an equivalent version exists in the TypeScript version of LangChain. Click the checkmark to visit the respective package.">JS/TS support</Tooltip> |
| :------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI](/oss/python/integrations/providers/openai/)                | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain_openai/)                           | <a href="https://pypi.org/project/langchain-openai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-openai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-openai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-openai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |
| [Google (Vertex AI)](/oss/python/integrations/providers/google)     | [`langchain-google-vertexai`](https://reference.langchain.com/python/integrations/langchain_google_vertexai/)         | <a href="https://pypi.org/project/langchain-google-vertexai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-vertexai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>         | <a href="https://pypi.org/project/langchain-google-vertexai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-vertexai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>         | [✅](https://www.npmjs.com/package/@langchain/google-vertexai)                                                                                                            |
| [AWS](/oss/python/integrations/providers/aws/)                      | [`langchain-aws`](https://reference.langchain.com/python/integrations/langchain_aws/)                                 | <a href="https://pypi.org/project/langchain-aws/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-aws/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-aws/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-aws?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/aws)                                                                                                                        |
| [Anthropic (Claude)](/oss/python/integrations/providers/anthropic/) | [`langchain-anthropic`](https://reference.langchain.com/python/integrations/langchain_anthropic/)                     | <a href="https://pypi.org/project/langchain-anthropic/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-anthropic/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-anthropic/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/anthropic)                                                                                                                  |
| [Google (GenAI)](/oss/python/integrations/providers/google)         | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai/)               | <a href="https://pypi.org/project/langchain-google-genai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-genai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>               | <a href="https://pypi.org/project/langchain-google-genai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-genai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>               | [✅](https://www.npmjs.com/package/@langchain/google-genai)                                                                                                               |
| [Groq](/oss/python/integrations/providers/groq/)                    | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq/)                               | <a href="https://pypi.org/project/langchain-groq/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-groq/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                               | <a href="https://pypi.org/project/langchain-groq/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-groq?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                               | [✅](https://www.npmjs.com/package/@langchain/groq)                                                                                                                       |
| [Ollama](/oss/python/integrations/providers/ollama/)                | [`langchain-ollama`](https://reference.langchain.com/python/integrations/langchain_ollama/)                           | <a href="https://pypi.org/project/langchain-ollama/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-ollama/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-ollama/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-ollama?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/ollama)                                                                                                                     |
| [Chroma](/oss/python/integrations/providers/chroma/)                | [`langchain-chroma`](https://reference.langchain.com/python/integrations/langchain_chroma/)                           | <a href="https://pypi.org/project/langchain-chroma/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-chroma/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-chroma/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-chroma?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Huggingface](/oss/python/integrations/providers/huggingface/)      | [`langchain-huggingface`](https://reference.langchain.com/python/integrations/langchain_huggingface/)                 | <a href="https://pypi.org/project/langchain-huggingface/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-huggingface/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                 | <a href="https://pypi.org/project/langchain-huggingface/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-huggingface?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                 | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Cohere](/oss/python/integrations/providers/cohere/)                | [`langchain-cohere`](https://reference.langchain.com/python/integrations/langchain_cohere/)                           | <a href="https://pypi.org/project/langchain-cohere/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-cohere/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-cohere/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-cohere?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/cohere)                                                                                                                     |
| [Postgres](/oss/python/integrations/providers/pgvector)             | [`langchain-postgres`](https://reference.langchain.com/python/integrations/langchain_postgres/)                       | <a href="https://pypi.org/project/langchain-postgres/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-postgres/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-postgres/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-postgres?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Pinecone](/oss/python/integrations/providers/pinecone/)            | [`langchain-pinecone`](https://reference.langchain.com/python/integrations/langchain_pinecone/)                       | <a href="https://pypi.org/project/langchain-pinecone/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-pinecone/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-pinecone/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-pinecone?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/pinecone)                                                                                                                   |
| [MistralAI](/oss/python/integrations/providers/mistralai/)          | [`langchain-mistralai`](https://reference.langchain.com/python/integrations/langchain_mistralai/)                     | <a href="https://pypi.org/project/langchain-mistralai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-mistralai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-mistralai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-mistralai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/mistralai)                                                                                                                  |
| [Fireworks](/oss/python/integrations/providers/fireworks/)          | [`langchain-fireworks`](https://reference.langchain.com/python/integrations/langchain_fireworks/)                     | <a href="https://pypi.org/project/langchain-fireworks/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-fireworks/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-fireworks/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-fireworks?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Databricks](/oss/python/integrations/providers/databricks/)        | [`databricks-langchain`](https://pypi.org/project/databricks-langchain/)                                              | <a href="https://pypi.org/project/databricks-langchain/" target="_blank"><img src="https://static.pepy.tech/badge/databricks-langchain/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                   | <a href="https://pypi.org/project/databricks-langchain/" target="_blank"><img src="https://img.shields.io/pypi/v/databricks-langchain?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [MongoDB](/oss/python/integrations/providers/mongodb_atlas)         | [`langchain-mongodb`](https://reference.langchain.com/python/integrations/langchain_mongodb/)                         | <a href="https://pypi.org/project/langchain-mongodb/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-mongodb/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-mongodb/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-mongodb?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/mongodb)                                                                                                                    |
| [Perplexity](/oss/python/integrations/providers/perplexity/)        | [`langchain-perplexity`](https://reference.langchain.com/python/integrations/langchain_perplexity/)                   | <a href="https://pypi.org/project/langchain-perplexity/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-perplexity/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                   | <a href="https://pypi.org/project/langchain-perplexity/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-perplexity?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [IBM](/oss/python/integrations/providers/ibm/)                      | [`langchain-ibm`](https://reference.langchain.com/python/integrations/langchain_ibm/)                                 | <a href="https://pypi.org/project/langchain-ibm/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-ibm/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-ibm/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-ibm?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/ibm)                                                                                                                        |
| [Deepseek](/oss/python/integrations/providers/deepseek/)            | [`langchain-deepseek`](https://reference.langchain.com/python/integrations/langchain_deepseek/)                       | <a href="https://pypi.org/project/langchain-deepseek/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-deepseek/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-deepseek/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-deepseek?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/deepseek)                                                                                                                   |
| [Nvidia AI Endpoints](/oss/python/integrations/providers/nvidia)    | [`langchain-nvidia-ai-endpoints`](https://reference.langchain.com/python/integrations/langchain_nvidia_ai_endpoints/) | <a href="https://pypi.org/project/langchain-nvidia-ai-endpoints/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-nvidia-ai-endpoints/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a> | <a href="https://pypi.org/project/langchain-nvidia-ai-endpoints/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-nvidia-ai-endpoints?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a> | ❌                                                                                                                                                                        |
| [Tavily](/oss/python/integrations/providers/tavily/)                | [`langchain-tavily`](https://pypi.org/project/langchain-tavily/)                                                      | <a href="https://pypi.org/project/langchain-tavily/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-tavily/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-tavily/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-tavily?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/tavily)                                                                                                                     |
| [Milvus](/oss/python/integrations/providers/milvus/)                | [`langchain-milvus`](https://reference.langchain.com/python/integrations/langchain_milvus/)                           | <a href="https://pypi.org/project/langchain-milvus/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-milvus/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-milvus/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-milvus?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Qdrant](/oss/python/integrations/providers/qdrant/)                | [`langchain-qdrant`](https://reference.langchain.com/python/integrations/langchain_qdrant/)                           | <a href="https://pypi.org/project/langchain-qdrant/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-qdrant/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-qdrant/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-qdrant?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/qdrant)                                                                                                                     |
| [Elasticsearch](/oss/python/integrations/providers/elasticsearch/)  | [`langchain-elasticsearch`](https://reference.langchain.com/python/integrations/langchain_elasticsearch/)             | <a href="https://pypi.org/project/langchain-elasticsearch/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-elasticsearch/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>             | <a href="https://pypi.org/project/langchain-elasticsearch/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-elasticsearch?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>             | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [DataStax Astra DB](/oss/python/integrations/providers/astradb/)    | [`langchain-astradb`](https://reference.langchain.com/python/integrations/langchain_astradb/)                         | <a href="https://pypi.org/project/langchain-astradb/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-astradb/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-astradb/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-astradb?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Together](/oss/python/integrations/providers/together/)            | [`langchain-together`](https://reference.langchain.com/python/integrations/langchain_together/)                       | <a href="https://pypi.org/project/langchain-together/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-together/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-together/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-together?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [LiteLLM](/oss/python/integrations/providers/litellm/)              | [`langchain-litellm`](https://pypi.org/project/langchain-litellm/)                                                    | <a href="https://pypi.org/project/langchain-litellm/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-litellm/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-litellm/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-litellm?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | ❌                                                                                                                                                                        |
| [xAI (Grok)](/oss/python/integrations/providers/xai/)               | [`langchain-xai`](https://reference.langchain.com/python/integrations/langchain_xai/)                                 | <a href="https://pypi.org/project/langchain-xai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-xai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-xai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-xai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/xai)                                                                                                                        |
| [Redis](/oss/python/integrations/providers/redis/)                  | [`langchain-redis`](https://reference.langchain.com/python/integrations/langchain_redis/)                             | <a href="https://pypi.org/project/langchain-redis/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-redis/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                             | <a href="https://pypi.org/project/langchain-redis/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-redis?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                             | [✅](https://www.npmjs.com/package/@langchain/redis)                                                                                                                      |
| [Azure AI](/oss/python/integrations/providers/azure_ai)             | [`langchain-azure-ai`](https://reference.langchain.com/python/integrations/langchain_azure_ai/)                       | <a href="https://pypi.org/project/langchain-azure-ai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-azure-ai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-azure-ai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-azure-ai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |
| [MCP Toolbox (Google)](/oss/python/integrations/providers/toolbox/) | [`toolbox-langchain`](https://pypi.org/project/toolbox-langchain/)                                                    | <a href="https://pypi.org/project/toolbox-langchain/" target="_blank"><img src="https://static.pepy.tech/badge/toolbox-langchain/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/toolbox-langchain/" target="_blank"><img src="https://img.shields.io/pypi/v/toolbox-langchain?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | ❌                                                                                                                                                                        |
| [Google (Community)](/oss/python/integrations/providers/google)     | [`langchain-google-community`](https://reference.langchain.com/python/integrations/langchain_google_community/)       | <a href="https://pypi.org/project/langchain-google-community/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-community/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>       | <a href="https://pypi.org/project/langchain-google-community/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-community?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>       | ❌                                                                                                                                                                        |
| [Unstructured](/oss/python/integrations/providers/unstructured/)    | [`langchain-unstructured`](https://reference.langchain.com/python/integrations/langchain_unstructured/)               | <a href="https://pypi.org/project/langchain-unstructured/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-unstructured/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>               | <a href="https://pypi.org/project/langchain-unstructured/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-unstructured?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>               | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |

[See all providers](/oss/python/integrations/providers/all_providers) or search for a provider using the search field.

<Info>
  If you'd like to contribute an integration, see [our contributing guide](/oss/python/contributing).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## In your auth handler:

**URL:** llms-txt#in-your-auth-handler:

**Contents:**
- Supported Resources
- Next Steps

@auth.authenticate
async def authenticate(headers: dict) -> Auth.types.MinimalUserDict:
    ...
    return {
        "identity": "user-123",
        "is_authenticated": True,
        "permissions": ["threads:write", "threads:read"]  # Define permissions in auth
    }

def _default(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

@auth.on.threads.create
async def create_thread(ctx: Auth.types.AuthContext, value: dict):
    if "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)

@auth.on.threads.read
async def rbac_create(ctx: Auth.types.AuthContext, value: dict):
    if "threads:read" not in ctx.permissions and "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)
python  theme={null}
  @auth.on.threads.create
  async def on_thread_create(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.create.value  # Specific type for thread creation
  ):
  ...

@auth.on.threads
  async def on_threads(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.value  # Union type of all thread actions
  ):
  ...

@auth.on
  async def on_all(
  ctx: Auth.types.AuthContext,
  value: dict  # Union type of all possible actions
  ):
  ...
  ```

More specific handlers provide better type hints since they handle fewer action types.
</Tip>

<a id="supported-actions" />

#### Supported actions and types

Here are all the supported action handlers:

| Resource       | Handler                       | Description                | Value Type                                                                                                                                   |
| -------------- | ----------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| **Threads**    | `@auth.on.threads.create`     | Thread creation            | [`ThreadsCreate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.ThreadsCreate)       |
|                | `@auth.on.threads.read`       | Thread retrieval           | [`ThreadsRead`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.ThreadsRead)           |
|                | `@auth.on.threads.update`     | Thread updates             | [`ThreadsUpdate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.ThreadsUpdate)       |
|                | `@auth.on.threads.delete`     | Thread deletion            | [`ThreadsDelete`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.ThreadsDelete)       |
|                | `@auth.on.threads.search`     | Listing threads            | [`ThreadsSearch`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.ThreadsSearch)       |
|                | `@auth.on.threads.create_run` | Creating or updating a run | [`RunsCreate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.RunsCreate)             |
| **Assistants** | `@auth.on.assistants.create`  | Assistant creation         | [`AssistantsCreate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AssistantsCreate) |
|                | `@auth.on.assistants.read`    | Assistant retrieval        | [`AssistantsRead`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AssistantsRead)     |
|                | `@auth.on.assistants.update`  | Assistant updates          | [`AssistantsUpdate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AssistantsUpdate) |
|                | `@auth.on.assistants.delete`  | Assistant deletion         | [`AssistantsDelete`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AssistantsDelete) |
|                | `@auth.on.assistants.search`  | Listing assistants         | [`AssistantsSearch`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AssistantsSearch) |
| **Crons**      | `@auth.on.crons.create`       | Cron job creation          | [`CronsCreate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.CronsCreate)           |
|                | `@auth.on.crons.read`         | Cron job retrieval         | [`CronsRead`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.CronsRead)               |
|                | `@auth.on.crons.update`       | Cron job updates           | [`CronsUpdate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.CronsUpdate)           |
|                | `@auth.on.crons.delete`       | Cron job deletion          | [`CronsDelete`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.CronsDelete)           |
|                | `@auth.on.crons.search`       | Listing cron jobs          | [`CronsSearch`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.CronsSearch)           |

<Note>
  "About Runs"

Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread's handlers.
  There is a specific `create_run` handler for creating new runs because it had more arguments that you can view in the handler.
</Note>

For implementation details:

* Check out the introductory tutorial on [setting up authentication](/langsmith/set-up-custom-auth)
* See the how-to guide on implementing a [custom auth handlers](/langsmith/custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Supported Resources

LangGraph provides three levels of authorization handlers, from most general to most specific:

1. **Global Handler** (`@auth.on`): Matches all resources and actions
2. **Resource Handler** (e.g., `@auth.on.threads`, `@auth.on.assistants`, `@auth.on.crons`): Matches all actions for a specific resource
3. **Action Handler** (e.g., `@auth.on.threads.create`, `@auth.on.threads.read`): Matches a specific action on a specific resource

The most specific matching handler will be used. For example, `@auth.on.threads.create` takes precedence over `@auth.on.threads` for thread creation.
If a more specific handler is registered, the more general handler will not be called for that resource and action.

<Tip>
  "Type Safety"
  Each handler has type hints available for its `value` parameter at `Auth.types.on.<resource>.<action>.value`. For example:
```

---

## Evaluate a chatbot

**URL:** llms-txt#evaluate-a-chatbot

**Contents:**
- Setup
- Create a dataset

Source: https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial

In this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.

At a high level, in this tutorial we will:

* *Create an initial golden dataset to measure performance*
* *Define metrics to use to measure performance*
* *Run evaluations on a few different prompts or models*
* *Compare results manually*
* *Track results over time*
* *Set up automated testing to run in CI/CD*

For more information on the evaluation workflows LangSmith supports, check out the [how-to guides](/langsmith/evaluation), or see the reference docs for [evaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) and its asynchronous [aevaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) counterpart.

Lots to cover, let's dive in!

First install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:

And set environment variables to enable LangSmith tracing:

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And set environment variables to enable LangSmith tracing:
```

Example 3 (unknown):
```unknown
## Create a dataset

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!
```

---

## Evaluate a RAG application

**URL:** llms-txt#evaluate-a-rag-application

**Contents:**
- Overview
- Setup
  - Environment
  - Application
- Dataset
- Evaluators
  - Correctness: Response vs reference answer
  - Relevance: Response vs input
  - Groundedness: Response vs retrieved docs
  - Retrieval relevance: Retrieved docs vs input

Source: https://docs.langchain.com/langsmith/evaluate-rag-tutorial

<Info>
  [RAG evaluation](/langsmith/evaluation-concepts#retrieval-augmented-generation-rag) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [LLM-as-judge evaluators](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.

This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:

1. How to create test datasets
2. How to run your RAG application on those datasets
3. How to measure your application's performance using different evaluation metrics

A typical RAG evaluation workflow consists of three main steps:

1. Creating a dataset with questions and their expected answers

2. Running your RAG application on those questions

3. Using evaluators to measure how well your application performed, looking at factors like:

* Answer relevance
   * Answer accuracy
   * Retrieval quality

For this tutorial, we'll create and evaluate a bot that answers questions about a few of [Lilian Weng's](https://lilianweng.github.io/) insightful blog posts.

First, let's set our environment variables:

And install the dependencies we'll need:

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

We can now define the generative pipeline.

Now that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.

One way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:

1. **Correctness**: Response vs reference answer

* `Goal`: Measure "*how similar/correct is the RAG chain answer, relative to a ground-truth answer*"
* `Mode`: Requires a ground truth (reference) answer supplied through a dataset
* `Evaluator`: Use LLM-as-judge to assess answer correctness.

2. **Relevance**: Response vs input

* `Goal`: Measure "*how well does the generated response address the initial user input*"
* `Mode`: Does not require reference answer, because it will compare the answer to the input question
* `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.

3. **Groundedness**: Response vs retrieved docs

* `Goal`: Measure "*to what extent does the generated response agree with the retrieved context*"
* `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.

4. **Retrieval relevance**: Retrieved docs vs input

* `Goal`: Measure "*how relevant are my retrieved results for this query*"
* `Mode`: Does not require reference answer, because it will compare the question to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess relevance

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6f303c2a284296b42e7d2d2e658f5171" alt="" data-og-width="1252" width="1252" data-og-height="547" height="547" data-path="langsmith/images/rag-eval-overview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5531116cdd61ca9e8ea6fcd760643db 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8f83816ac849c05dd8d3dee4c9670729 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8656d8f8af7ffa7f2684376cf2f70874 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4299367332fbefd15e938aefc23ca6fe 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bc19645279f119031a5cb8ca32f2f7d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=185b7c4ba4f4127f780d5fa17b96c752 2500w" />

### Correctness: Response vs reference answer

### Relevance: Response vs input

The flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.

### Groundedness: Response vs retrieved docs

Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or "grounded in") the retrieved documents.

### Retrieval relevance: Retrieved docs vs input

We can now kick off our evaluation job with all of our different evaluators.

You can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)

<Accordion title="Here's a consolidated script with all the above code:">
  <CodeGroup>

</CodeGroup>
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-rag-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And install the dependencies we'll need:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Application

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

<CodeGroup>
```

---

## The Secret Life of Socks in the Dryer

**URL:** llms-txt#the-secret-life-of-socks-in-the-dryer

**Contents:**
  - 2. Identify a checkpoint

I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.

My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 2. Identify a checkpoint
```

---

## node_2 accepts private data from node_1, whereas

**URL:** llms-txt#node_2-accepts-private-data-from-node_1,-whereas

---

## Define edges

**URL:** llms-txt#define-edges

**Contents:**
  - Impose a recursion limit
- Async
- Combine control flow and state updates with `Command`

def route(state: State) -> Literal["b", END]:
    if len(state["aggregate"]) < 7:
        return "b"
    else:
        return END

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Node A sees []
Node B sees ['A']
Node A sees ['A', 'B']
Node B sees ['A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B']
Node B sees ['A', 'B', 'A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B', 'A', 'B']
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke({"aggregate": []}, {"recursion_limit": 4})
except GraphRecursionError:
    print("Recursion Error")

Node A sees []
Node B sees ['A']
Node C sees ['A', 'B']
Node D sees ['A', 'B']
Node A sees ['A', 'B', 'C', 'D']
Recursion Error
python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END
  from langgraph.managed.is_last_step import RemainingSteps

class State(TypedDict):
      aggregate: Annotated[list, operator.add]
      remaining_steps: RemainingSteps

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if state["remaining_steps"] <= 2:
          return END
      else:
          return "b"

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "a")
  graph = builder.compile()

# Test it out
  result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  print(result)
  
  Node A sees []
  Node B sees ['A']
  Node A sees ['A', 'B']
  {'aggregate': ['A', 'B', 'A']}
  python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END

class State(TypedDict):
      aggregate: Annotated[list, operator.add]

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

def c(state: State):
      print(f'Node C sees {state["aggregate"]}')
      return {"aggregate": ["C"]}

def d(state: State):
      print(f'Node D sees {state["aggregate"]}')
      return {"aggregate": ["D"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)
  builder.add_node(c)
  builder.add_node(d)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if len(state["aggregate"]) < 7:
          return "b"
      else:
          return END

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "c")
  builder.add_edge("b", "d")
  builder.add_edge(["c", "d"], "a")
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  result = graph.invoke({"aggregate": []})
  
  Node A sees []
  Node B sees ['A']
  Node D sees ['A', 'B']
  Node C sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Node B sees ['A', 'B', 'C', 'D', 'A']
  Node D sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node C sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']
  python  theme={null}
  from langgraph.errors import GraphRecursionError

try:
      result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  except GraphRecursionError:
      print("Recursion Error")
  
  Node A sees []
  Node B sees ['A']
  Node C sees ['A', 'B']
  Node D sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Recursion Error
  shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("openai:gpt-4.1")
      python Model Class theme={null}
      import os
      from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(model="gpt-4.1")
      shell  theme={null}
    pip install -U "langchain[anthropic]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("anthropic:claude-sonnet-4-5")
      python Model Class theme={null}
      import os
      from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = ChatAnthropic(model="claude-sonnet-4-5")
      shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
          "azure_openai:gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
      )
      python Model Class theme={null}
      import os
      from langchain_openai import AzureChatOpenAI

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = AzureChatOpenAI(
          model="gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
      )
      shell  theme={null}
    pip install -U "langchain[google-genai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")
      python Model Class theme={null}
      import os
      from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "..."

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")
      shell  theme={null}
    pip install -U "langchain[aws]"
    python init_chat_model theme={null}
      from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

model = init_chat_model(
          "anthropic.claude-3-5-sonnet-20240620-v1:0",
          model_provider="bedrock_converse",
      )
      python Model Class theme={null}
      from langchain_aws import ChatBedrock

model = ChatBedrock(model="anthropic.claude-3-5-sonnet-20240620-v1:0")
      python  theme={null}
from langchain.chat_models import init_chat_model
from langgraph.graph import MessagesState, StateGraph

async def node(state: MessagesState):  # [!code highlight]
    new_message = await llm.ainvoke(state["messages"])  # [!code highlight]
    return {"messages": [new_message]}

builder = StateGraph(MessagesState).add_node(node).set_entry_point("node")
graph = builder.compile()

input_message = {"role": "user", "content": "Hello"}
result = await graph.ainvoke({"messages": [input_message]})  # [!code highlight]
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
import random
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e1b99e7efe45b1fdc5836d590d5fbbc3" alt="Simple loop graph" data-og-width="188" width="188" data-og-height="249" height="249" data-path="oss/images/graph_api_image_7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a443c1ddc2f6a4e7c73f4482c7d63912 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f65d82d8aaeb024beb5da1aa2948bcdb 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b95f4df2fb69f28779a1d8dd113409d0 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=bdb4011d05756c10a1c7b5dea683fdb7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=dde791caa4279a6248b59b70df99dd2c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e4d568719f1761ff3a3d2ea9175241d8 2500w" />

This architecture is similar to a [ReAct agent](/oss/python/langgraph/workflows-agents) in which node `"a"` is a tool-calling model, and node `"b"` represents the tools.

In our `route` conditional edge, we specify that we should end after the `"aggregate"` list in the state passes a threshold length.

Invoking the graph, we see that we alternate between nodes `"a"` and `"b"` before terminating once we reach the termination condition.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Impose a recursion limit

In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/python/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/python/langgraph/graph-api#graphs). We can then catch and handle this exception:
```

---

## Stateless runs

**URL:** llms-txt#stateless-runs

**Contents:**
- Setup
- Stateless streaming
- Waiting for stateless results

Source: https://docs.langchain.com/langsmith/stateless-runs

Most of the time, you provide a `thread_id` to your client when you run your graph in order to keep track of prior runs through the persistent state implemented in LangSmith Deployment. However, if you don't need to persist the runs you don't need to use the built-in persistent state and can create stateless runs.

First, let's setup our client:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Waiting for stateless results

In addition to streaming, you can also wait for a stateless result by using the `.wait` function like follows:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/stateless-runs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

---

## Create a prompt

**URL:** llms-txt#create-a-prompt

**Contents:**
- Compose your prompt
  - Template format
  - Add a template variable
  - Structured output
  - Tools
- Run the prompt
- Save your prompt
- View your prompts
- Add metadata

Source: https://docs.langchain.com/langsmith/create-a-prompt

Navigate to the  in the left-hand sidebar or from the application homepage.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2bede4ae9332bdf43ae20580d5bb957d" alt="" data-og-width="1747" width="1747" data-og-height="1285" height="1285" data-path="langsmith/images/empty-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20602d3b2e7b4219a8dc3612fee194b7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=141b0ab234c54970f4e86f24ed13a954 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1fa1d0cb9075f4fbdcae487ea4348116 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bb3583d9357a597754396ee94e52c0da 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c21b3e9881e95a0b954d578fa1d8aa47 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3dcdde6a3fd4f3365059e181f53f68a0 2500w" />

## Compose your prompt

On the left is an editable view of the prompt.

The prompt is made up of messages, each of which has a "role" - including `system`, `human`, and `ai`.

The default template format is `f-string`, but you can change the prompt template format to `mustache` by clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats [here](/langsmith/prompt-engineering-concepts#f-string-vs-mustache).
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=417fa567135babb46d2bf080b7eb44f0" alt="" data-og-width="938" width="938" data-og-height="352" height="352" data-path="langsmith/images/template-format.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5bbcf83ad9e251ea9fdeb6c0f7dd49eb 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02776bb45c6ffe0df98775886e75eaeb 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63b67b6afe893f7944ee2fb8b76bbaf 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9df27878d7eaf60e953b9438fcf3f8c4 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a8b2e1b3fd0977e5b6beb984119bc6fd 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d30b1d76394200f9473f0b8fb08d2a5e 2500w" />

### Add a template variable

The power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:

1. Add `{{variable_name}}` to your prompt (with one curly brace on each side for `f-string` and two for `mustache`). <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b250a57ef0e0a40a56822af750d52810" alt="" data-og-width="726" width="726" data-og-height="169" height="169" data-path="langsmith/images/prompt-with-variable.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ea82252d99c7ee63c15d1c1036db8c55 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc56d047cf631b1e66ecfd09ab4c03a5 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7cc8cb861e9361445dcee545cbac84b7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7ed018754ebb8dad675a75c4aa638cb 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d360aac28b60689789e8d3274103611 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b9b98555c36f929004d234cc7fadaea5 2500w" />

2. Highlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. <img src="https://mintlify.s3.us-west-1.amazonaws.com/langchain-5e9cc07a/langsmith/images/convert-to-variable.gif" alt="" />

When we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=35674518c53e340d02719cbb7b5fd782" alt="" data-og-width="775" width="775" data-og-height="134" height="134" data-path="langsmith/images/prompt-inputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=59775af2fccde924b7ad7657db2b4656 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df9b24f1eb306578590ee772720ced08 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df549771817326a72a9387d0133ea590 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4bc399d02630e4873ae34afbbb60057e 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=78917ac5c4c938d926a5fe420d4d2780 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=89368aa76e0764d3f3fcb90623b2f28b 2500w" />

### Structured output

Adding an output schema to your prompt will get output in a structured format. Learn more about structured output [here](/langsmith/prompt-engineering-concepts#structured-output). <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=51cdef35a620c225896dbf2f3ab07528" alt="" data-og-width="814" width="814" data-og-height="574" height="574" data-path="langsmith/images/structured-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7af3797e6106f2c7858cc67dac7cfe60 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7a3b8bb3d32eda103213856e4e05e74c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e973d61622e18ab678631c734c4b16a8 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63196b1604baef88db3f56375458005 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e672a8f8af25d414d61592a1b22205c0 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d57196b5940203b66a31196ea464243 2500w" />

You can also add a tool by clicking the `+ Tool` button at the bottom of the prompt editor. See [here](/langsmith/use-tools) for more information on how to use tools.

Click "Start" to run the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9dec20b94326e5c8b11775b56eca55b4" alt="" data-og-width="1525" width="1525" data-og-height="766" height="766" data-path="langsmith/images/create-a-prompt-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10cdcbb52db991d37478cdd199f51baf 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=14986d2ef1ad2aafd69d5005413604c8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20cc523bb2ffa9756d95908b0b434d91 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5bdcb8118c13d9e9710ccc663058f47c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f6760ed7f81ce2d9c7907c8539d6122 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c47787a81ad1ffed842e630ec80a362 2500w" />

To save your prompt, click the "Save" button, name your prompt, and decide if you want it to be "private" or "public". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.

The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. <img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=180e2c79fb9d1ee8d7869fc279e2d94a" alt="" data-og-width="465" width="465" data-og-height="306" height="306" data-path="langsmith/images/save-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2a2ea17f2ffb787ce2bbbfc88302636e 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7120851531e79e13d8717ba14eb64483 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6b7d47e08120d2329fa923be8c19a612 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b6a4ed33c82c54f093ad4f07db1b7a1c 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5746af64213ab0bc08d7315a895a0d28 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=470e4700c6ed64348f25ae23a8d462c5 2500w" />

<Check>
  The first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.
</Check>

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=56dbd809c50fb2abc816c73c599f0baf" alt="" data-og-width="575" width="575" data-og-height="357" height="357" data-path="langsmith/images/public-handle.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e87d4538d4413b9ce13aed26f43e075 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=df539980aec2f06214ee13668d271e38 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0b19f257c2d9eb78b88c0df6e7920d8d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06c1efd1463437913cbdd6e19274f354 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=648394098abe45d6d916ee1fe3f9066b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=844bf001cfa48ba449c873d228cc5fa3 2500w" />

You've just created your first prompt! View a table of your prompts in the prompts tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9f8f5567bb93a0add181a51531474796" alt="" data-og-width="1508" width="1508" data-og-height="309" height="309" data-path="langsmith/images/prompt-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6421b33ff02f9af3f994e665fbcddf96 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bf3d973fd239d93c0c4b0dd45e2e7129 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d2d60a1b18eb7baaeaa5a04fa67d4ac7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9d11895d724a60910231494927f86e24 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=52afb51d257f64c5e59ad77e3a3cb67a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=15e2b24c349a9f21729b572fdb23734f 2500w" />

To add metadata to your prompt, click the prompt and then click the "Edit" pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e54ca6c0c8283b027f5848f79d1cf064" alt="" data-og-width="1167" width="1167" data-og-height="1067" height="1067" data-path="langsmith/images/pencil.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0336448912188bb85366b0c49556d207 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6456cdae3e5743b770ca530748efd21b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f16e9adaa3e2d3063623317344fb7d7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=12e5f2c3a2cc2f91d66d467cff910809 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ce6cb1b3011638d9bec990c1e8131837 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b61eb5331a47c9e6998261799729fdbf 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=febb5f53b2f917cf3e5a2ff2566eaef4" alt="" data-og-width="1508" width="1508" data-og-height="1084" height="1084" data-path="langsmith/images/edit-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=440242ac2e20e092c50810ca84d37286 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46ad1078bdd4342cc49ecd1a70837149 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f4998ff77574085ec2eb2fdc8c755fdc 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2065392afa627416297cc8dc5a10780d 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bda747e312cbabdd107a7b54b4e1c03e 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=846eaa58fed8f3a14aa76dbb610a0e41 2500w" />

---

## Store persists embeddings to the local filesystem

**URL:** llms-txt#store-persists-embeddings-to-the-local-filesystem

---

## Security policy

**URL:** llms-txt#security-policy

**Contents:**
- Best practices
- Reporting OSS vulnerabilities
  - In-scope targets
  - Out-of-scope targets
- Reporting LangSmith vulnerabilities
  - Other security concerns

Source: https://docs.langchain.com/oss/python/security-policy

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

When building such applications developers should remember to follow good security practices:

* [**Limit permissions**](https://en.wikipedia.org/wiki/Principle_of_least_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.
* **Anticipate potential misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data.
* [**Defense in depth**](https://en.wikipedia.org/wiki/Defense_in_depth_\(computing\)): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

* Data corruption or loss.
* Unauthorized access to confidential information.
* Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

* A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
* A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
* A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you're building applications that access external resources like file systems, APIs
or databases, consider speaking with your company's security team to determine how to best
design and secure your applications.

## Reporting OSS vulnerabilities

LangChain is partnered with [huntr by Protect AI](https://huntr.com/) to provide
a bounty program for our open source projects.

Please report security vulnerabilities associated with the LangChain
open source projects at [huntr](https://huntr.com/bounties/disclose/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain\&validSearch=true).

Before reporting a vulnerability, please review:

1. [In-scope targets](#in-scope-targets) and [out-of-scope targets](#out-of-scope-targets).
2. The [langchain-ai/langchain](https://python.langchain.com/docs/contributing/repo_structure) monorepo structure.
3. The [best practices](#best-practices) above to
   understand what we consider to be a security vulnerability vs. developer
   responsibility.

The following packages and repositories are eligible for bug bounties:

* langchain-core
* langchain (see exceptions)
* langchain-community (see exceptions)
* langgraph
* langserve

### Out-of-scope targets

All out of scope targets defined by huntr as well as:

* **langchain-experimental**: This repository is for experimental code and is not
  eligible for bug bounties (see [package warning](https://pypi.org/project/langchain-experimental/)), bug reports to it will be marked as interesting or waste of
  time and published with no bounty attached.
* **tools**: Tools in either langchain or langchain-community are not eligible for bug
  bounties. This includes the following directories
  * libs/langchain/langchain/tools
  * libs/community/langchain\_community/tools
  * Please review the [best practices](#best-practices)
    for more details, but generally tools interact with the real world. Developers are
    expected to understand the security implications of their code and are responsible
    for the security of their tools.
* Code documented with security notices. This will be decided on a case by
  case basis, but likely will not be eligible for a bounty as the code is already
  documented with guidelines for developers that should be followed for making their
  application secure.
* Any LangSmith related repositories or APIs (see [Reporting LangSmith vulnerabilities](#reporting-langsmith-vulnerabilities)).

## Reporting LangSmith vulnerabilities

Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`.

* LangSmith site: [https://smith.langchain.com](https://smith.langchain.com)
* SDK client: [https://github.com/langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk)

### Other security concerns

For any other security concerns, please contact us at `security@langchain.dev`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/security-policy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to run evaluations with Vitest/Jest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-vitest/jest-(beta)

**Contents:**
- Setup
  - Vitest
  - Jest
- Define and run evals
- Trace feedback
- Running multiple examples against a test case
- Log outputs
- Trace intermediate calls
- Focusing or skipping tests
- Configuring test suites

Source: https://docs.langchain.com/langsmith/vitest-jest

LangSmith provides integrations with Vitest and Jest that allow JavaScript and TypeScript developers define their datasets and evaluate using familiar syntax.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=94fd2a6f61c9dc386002fadbab7024a8" alt="Jest/Vitest reporter output" data-og-width="2200" width="2200" data-og-height="564" height="564" data-path="langsmith/images/jest-vitest-reporter-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf56669ba6d6ab79ed6237424f163fa7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=af88b4a6c4d31520b783336f311f56fc 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=32ee63fc2b8923236850f9b2a1fb1775 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e1f396ae3e1b68e358efc599f700e0c3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=77b9a394525825d5f9395cdb22a5c8b4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2d22bf302eedc50c70280ce2d8bc7d79 2500w" />

Compared to the `evaluate()` evaluation flow, this is useful when:

* Each example requires different evaluation logic
* You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
* You want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems

<Info>
  Requires JS/TS SDK version `langsmith>=0.3.1`.
</Info>

<Warning>
  The Vitest/Jest integrations are in beta and are subject to change in upcoming releases.
</Warning>

<Info>
  The Python SDK has an analogous [pytest integration](/langsmith/pytest).
</Info>

Set up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard `*.test.ts` files) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with `.eval.ts`.

This ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

Then create a separate `ls.vitest.config.ts` file with the following base config:

* `include` ensures that only files ending with some variation of `eval.ts` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"environment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Vitest with the config you just created:

Note that the above script disables Vitest's default watch mode for running evals since many evaluators may include longer running LLM calls.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<Info>
  The setup instructions below are for basic JS files and CJS. To add support for TypeScript and ESM, see Jest's official docs or use [Vitest](#vitest).
</Info>

Then create a separate config file named `ls.jest.config.cjs`:

* `testMatch` ensures that only files ending with some variation of `eval.js` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"testEnvironment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Jest with the config you just created:

## Define and run evals

You can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:

* You should import `describe` and `test` from the `langsmith/jest` or `langsmith/vitest` entrypoint
* You must wrap your test cases in a `describe` block
* When declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs

Try it out by creating a file named `sql.eval.ts` (or `sql.eval.js` if you are using Jest without TypeScript) and pasting the below contents into it:

You can think of each `ls.test()` case as corresponding to a dataset example, and `ls.describe()` as defining a LangSmith dataset. If you have LangSmith [tracing environment variables](/#3-set-up-your-environment) set when you run the test suite, the SDK does the following:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) with the same name as the name passed to `ls.describe()` in LangSmith if it does not exist
* creates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist
* creates a new [experiment](/langsmith/evaluation-concepts#experiment) with one result for each test case
* collects the pass/fail rate under the `pass` feedback key for each test case

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the `ls.logOutputs()` or return from the test function as "actual" result values from your app for the experiment.

Create a `.env` file with your `OPENAI_API_KEY` and LangSmith credentials if you don't already have one:

Now use the `eval` script we set up in the previous step to run the test:

And your declared test should run!

Once it finishes, if you've set your LangSmith environment variables, you should see a link directing you to an experiment created in LangSmith alongside the test results.

Here's what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cde688950dd2fc454a8514b02ed7268" alt="Experiment" data-og-width="2752" width="2752" data-og-height="902" height="902" data-path="langsmith/images/simple-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e583f4ee7179018b026ce9c037a05702 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3d87bf9ced639e2deb375f0638b1912e 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e1f92efbbffa880300575043180eb107 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=90a9e603ea1b613b6a95f4a686cb954b 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=335d4ba669f1a75d6c8171ce2d7cbb99 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9330a3bc998e80851ce3a162272b037d 2500w" />

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with either `ls.logFeedback()` or `wrapEvaluator()`. To do so, try the following as your `sql.eval.ts` file (or `sql.eval.js` if you are using Jest without TypeScript):

Note the use of `ls.wrapEvaluator()` around the `myEvaluator` function. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches `{ key: string; score: number | boolean }`. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the `correctness` feedback key.

You can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.

## Running multiple examples against a test case

You can run the same test case over multiple examples and parameterize your tests using `ls.test.each()`. This is useful when you want to evaluate your app the same way against different inputs:

If you have tracking enabled, each example in the local dataset will be synced to the one created in LangSmith.

Every time we run a test we're syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can use `ls.logOutputs()` like this:

The logged outputs will appear in your reporter summary and in LangSmith.

You can also directly return a value from your test function:

However keep in mind if you do this that if your test fails to complete due to a failed assertion or other error, your output will not appear.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Focusing or skipping tests

You can chain the Vitest/Jest `.skip` and `.only` methods on `ls.test()` and `ls.describe()`:

## Configuring test suites

You can configure test suites with values like metadata or a custom client by passing an extra argument to `ls.describe()` for the full suite or by passing a `config` field into `ls.test()` for individual tests:

The test suite will also automatically extract environment variables from `process.env.ENVIRONMENT`, `process.env.NODE_ENV` and `process.env.LANGSMITH_ENVIRONMENT` and set them as metadata on created experiments. You can then filter experiments by metadata in LangSmith's UI.

See [the API refs](https://docs.smith.langchain.com/reference/js/functions/vitest.describe) for a full list of configuration options.

If you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/vitest-jest.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Resume with approval decision

**URL:** llms-txt#resume-with-approval-decision

**Contents:**
  - Decision types
- Execution lifecycle
- Custom HITL logic

agent.invoke(
    Command( # [!code highlight]
        resume={"decisions": [{"type": "approve"}]}  # or "edit", "reject" [!code highlight]
    ), # [!code highlight]
    config=config # Same thread ID to resume the paused conversation
)
python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "approve",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "edit",
                        # Edited action with tool name and args
                        "edited_action": {
                            # Tool name to call.
                            # Will usually be the same as the original action.
                            "name": "new_tool_name",
                            # Arguments to pass to the tool.
                            "args": {"key1": "new_value", "key2": "original_value"},
                        }
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "reject",
                        # An explanation about why the action was rejected
                        "message": "No, this is wrong because ..., instead do this ...",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    {
        "decisions": [
            {"type": "approve"},
            {
                "type": "edit",
                "edited_action": {
                    "name": "tool_name",
                    "args": {"param": "new_value"}
                }
            },
            {
                "type": "reject",
                "message": "This action is not allowed"
            }
        ]
    }
    ```
  </Tab>
</Tabs>

## Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution.

For more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction.

Review the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Decision types

<Tabs>
  <Tab title="✅ approve">
    Use `approve` to approve the tool call as-is and execute it without changes.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="✏️ edit">
    Use `edit` to modify the tool call before execution.
    Provide the edited action with the new tool name and arguments.
```

Example 3 (unknown):
```unknown
<Tip>
      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
    </Tip>
  </Tab>

  <Tab title="❌ reject">
    Use `reject` to reject the tool call and provide feedback instead of execution.
```

Example 4 (unknown):
```unknown
The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.

    ***

    ### Multiple decisions

    When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:
```

---

## Example data format

**URL:** llms-txt#example-data-format

Source: https://docs.langchain.com/langsmith/example-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on evaluation](/langsmith/evaluation-concepts)
</Check>

LangSmith stores examples in datasets as follows:

| Field Name          | Type     | Description                                                                                          |
| ------------------- | -------- | ---------------------------------------------------------------------------------------------------- |
| **id**              | UUID     | Unique identifier for the example.                                                                   |
| **name**            | string   | The name of the example.                                                                             |
| **created\_at**     | datetime | The time this example was created                                                                    |
| **modified\_at**    | datetime | The last time this example was modified                                                              |
| **inputs**          | object   | A map of inputs for the example.                                                                     |
| **outputs**         | object   | A map or set of outputs generated by the run.                                                        |
| **dataset\_id**     | UUID     | The dataset the example belongs to                                                                   |
| **source\_run\_id** | UUID     | If this example was created from a LangSmith [`Run`](/langsmith/run-data-format), the ID of said run |
| **metadata**        | object   | A map of additional, user or SDK defined information that can be stored on an example.               |

To learn more about how examples are used in evaluation, read our how-to guide on [evaluating LLM applications](/langsmith/evaluate-llm-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/example-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Update these values as needed to connect to your replicated clickhouse cluster.

**URL:** llms-txt#update-these-values-as-needed-to-connect-to-your-replicated-clickhouse-cluster.

clickhouse:
  external:
    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory.
    enabled: true
    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local
    port: "8123"
    nativePort: "9000"
    user: "default"
    password: "password"
    database: "default"
    cluster: "replicated"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
```

<Note>
  Ensure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a `Running` state. Pods stuck in `Pending` may indicate that you are reaching node pool limits or need larger nodes.

Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-scale.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create a thread and chat

**URL:** llms-txt#create-a-thread-and-chat

**Contents:**
- Next steps

thread = await client.threads.create()
print(f"✅ Created thread as Alice: {thread['thread_id']}")

response = await client.runs.create(
    thread_id=thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hello!"}]},
)
print("✅ Bot responded:")
print(response)
```

1. Without a valid token, we can't access the bot
2. With a valid token, we can create threads and chat

Congratulations! You've built a chatbot that only lets "authenticated" users access it. While this system doesn't (yet) implement a production-ready security scheme, we've learned the basic mechanics of how to control access to our bot. In the next tutorial, we'll learn how to give each user their own private conversations.

Now that you can control who accesses your bot, you might want to:

1. Continue the tutorial by going to [Make conversations private](/langsmith/resource-auth) to learn about resource authorization.
2. Read more about [authentication concepts](/langsmith/auth).
3. Check out the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref) for more authentication details.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-custom-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ✅ Production - data persists

**URL:** llms-txt#✅-production---data-persists

**Contents:**
- Listing files

from langgraph.store.postgres import PostgresStore
store = PostgresStore(connection_string=os.environ["DATABASE_URL"])
python  theme={null}
agent.invoke({
    "messages": [{"role": "user", "content": "List all files"}]
})

**Examples:**

Example 1 (unknown):
```unknown
## Listing files

The `ls` tool shows files from both filesystems:
```

---

## Self-host LangSmith on Kubernetes

**URL:** llms-txt#self-host-langsmith-on-kubernetes

**Contents:**
- Prerequisites
  - Databases
  - Kubernetes cluster requirements
- Configure your Helm Charts:
- Deploying to Kubernetes:
- Validate your deployment:
- Using LangSmith

Source: https://docs.langchain.com/langsmith/kubernetes

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This page describes how to set up the **LangSmith** (observability, tracing, and evaluation) in a Kubernetes cluster. You'll use Helm to install LangSmith and its dependencies.

<Note>
  This guide installs the base [LangSmith platform](/langsmith/self-hosted#langsmith) which includes observability and evaluation, but **not** the deployment management features. Review the [self-hosted options](/langsmith/self-hosted) if you're unsure which you need.
</Note>

After completing this page, you'll have:

* ✅ **LangSmith UI and APIs**: for observability, tracing, and evaluation.
* ✅ **Backend services**: (queue, playground, ACE).
* ✅ **Datastores**: (PostgreSQL, Redis, ClickHouse, optional blob storage).
* ❌ **Deployment management**: To add deployment capabilities, complete this guide first, then follow [Self-host LangSmith with deployment](/langsmith/deploy-self-hosted-full-platform).

We've successfully tested LangSmith on the following Kubernetes distributions:

* Google Kubernetes Engine (GKE)
* Amazon Elastic Kubernetes Service (EKS)
* Azure Kubernetes Service (AKS)
* OpenShift (4.14+)
* Minikube and Kind (for development purposes)

<Note>
  We have several Terraform modules the help in the provisioning of resources for LangSmith. You can find those in our [public Terraform repo](https://github.com/langchain-ai/terraform).

Supported cloud providers include:

* [AWS terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/aws)
  * [Azure terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/azure)

You can click on the links above to see the documentation for each module. These modules are designed to help you quickly set up the necessary infrastructure for LangSmith, including Kubernetes clusters, storage, and networking.
</Note>

Ensure you have the following tools/items ready. Some items are marked optional:

1. LangSmith License Key

1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

3. JWT Secret (Optional but used for basic auth)

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

1. Recommended: At least 16 vCPUs, 64GB Memory available

* You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

* To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

You can verify this by running:

The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:

<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

2. At a minimum, you will need to set the following configuration options (using basic auth):

You will also need to specify connection details for any external databases you are using.

## Deploying to Kubernetes:

1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)

1. Run `kubectl get pods`

Output should look something like:

<Note>
     If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace>` flag.
   </Note>

2. Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts)

3. Find the latest version of the chart. You can find the available versions in the [Helm Chart repository](https://github.com/langchain-ai/helm/releases).

* We generally recommend using the latest version.
   * You can also run `helm search repo langchain/langsmith --versions` to see the available versions. The output will look something like this:

4. Run `helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug`

* Replace `<namespace>` with the namespace you want to deploy LangSmith to.
   * Replace `<version>` with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.

Once the `helm install` command runs and finishes successfully, you should see output similar to this:

This may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services.

5. Run `kubectl get pods` Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):

## Validate your deployment:

1. Run `kubectl get services`

Output should look something like:

2. Curl the external ip of the `langsmith-frontend` service:

3. Visit the external ip for the `langsmith-frontend` service on your browser

The LangSmith UI should be visible/operational

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt="" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the `langsmith_config.yaml` file.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith with [Single Sign-On](/langsmith/self-host-sso) to secure your LangSmith instance
* Connect LangSmith to external Postgres and Redis instances
* Set up [Blob Storage](/langsmith/self-host-blob-storage) for storing large files

Review our [configuration section](/langsmith/self-hosted) for more information on how to configure these options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/kubernetes.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
3. JWT Secret (Optional but used for basic auth)

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
### Databases

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

   1. Recommended: At least 16 vCPUs, 64GB Memory available

      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

   2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

      * To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

      You can verify this by running:
```

Example 3 (unknown):
```unknown
The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:
```

Example 4 (unknown):
```unknown
<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

      Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

2. Helm

   1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

2. At a minimum, you will need to set the following configuration options (using basic auth):
```

---

## Keep our resource authorization from the previous tutorial

**URL:** llms-txt#keep-our-resource-authorization-from-the-previous-tutorial

**Contents:**
- 4. Test authentication flow

@auth.on
async def add_owner(ctx, value):
    """Make resources private to their creator using resource metadata."""
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
python  theme={null}
import os
import httpx
from getpass import getpass
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The most important change is that we're now validating tokens with a real authentication server. Our authentication handler has the private key for our Supabase project, which we can use to validate the user's token and extract their information.

## 4. Test authentication flow

Let's test out the new authentication flow. You can run the following code in a file or notebook. You will need to provide:

* A valid email address
* A Supabase project URL (from [above](#setup-auth-provider))
* A Supabase anon **public key** (also from [above](#setup-auth-provider))
```

---

## LangGraph CLI

**URL:** llms-txt#langgraph-cli

**Contents:**
- Installation
  - Quick commands
- Configuration file
  - Examples
- Commands
  - `dev`
  - `build`
  - `up`
  - `dockerfile`

Source: https://docs.langchain.com/langsmith/cli

**LangGraph CLI** is a command-line tool for building and running the [LangGraph API server](/langsmith/langgraph-server) locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.

1. Ensure Docker is installed (e.g., `docker --version`).

3. Verify the install

| Command                               | What it does                                                                                                                         |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| [`langgraph dev`](#dev)               | Starts a lightweight local dev server (no Docker required), ideal for rapid testing.                                                 |
| [`langgraph build`](#build)           | Builds a Docker image of your LangGraph API server for deployment.                                                                   |
| [`langgraph dockerfile`](#dockerfile) | Emits a Dockerfile derived from your config for custom builds.                                                                       |
| [`langgraph up`](#up)                 | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

For JS, use `npx @langchain/langgraph-cli <command>` (or `langgraphjs` if installed globally).

## Configuration file

The LangGraph CLI requires a JSON configuration file that follows this [schema](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/libs/cli/schemas/schema.json). It contains the following properties:

<Note>The LangGraph CLI defaults to using the configuration file <strong>langgraph.json</strong> in the current directory.</Note>

<Tabs>
  <Tab title="Python">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | ---------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`dependencies`</span>     | **Required**. Array of dependencies for LangSmith API server. Dependencies can be one of the following: <ul><li>A single period (`"."`), which will look for local Python packages.</li><li>The directory path where `pyproject.toml`, `setup.py` or `requirements.txt` is located.<br />For example, if `requirements.txt` is located in the root of the project directory, specify `"./"`. If it's located in a subdirectory called `local_package`, specify `"./local_package"`. Do not specify the string `"requirements.txt"` itself.</li><li>A Python package name.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./your_package/your_file.py:variable`, where `variable` is an instance of `langgraph.graph.state.CompiledStateGraph`</li><li>`./your_package/your_file.py:make_graph`, where `make_graph` is a function that takes a config dictionary (`langchain_core.runnables.RunnableConfig`) and returns an instance of `langgraph.graph.state.StateGraph` or `langgraph.graph.state.CompiledStateGraph`. See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul>                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`auth`</span>             | *(Added in v0.0.11)* Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See [authentication guide](/langsmith/auth) for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`base_image`</span>       | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See [https://hub.docker.com/r/langchain/langgraph-server/tags](https://hub.docker.com/r/langchain/langgraph-server/tags) for more details. (added in `langgraph-cli==0.2.8`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`image_distro`</span>     | Optional. Linux distribution for the base image. Must be one of `"debian"`, `"wolfi"`, `"bookworm"`, or `"bullseye"`. If omitted, defaults to `"debian"`. Available in `langgraph-cli>=0.2.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                                                                                                                                                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`ui`</span>               | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    | <span style={{ whiteSpace: "nowrap" }}>`python_version`</span>   | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_config_file`</span>  | Path to `pip` config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_installer`</span>    | *(Added in v0.3)* Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`keep_pkg_tools`</span>   | *(Added in v0.3.4)* Optional. Control whether to retain Python packaging tools (`pip`, `setuptools`, `wheel`) in the final image. Accepted values: <ul><li><code>true</code> : Keep all three tools (skip uninstall).</li><li><code>false</code> / omitted : Uninstall all three tools (default behaviour).</li><li><code>list\[str]</code> : Names of tools <strong>to retain</strong>. Each value must be one of "pip", "setuptools", "wheel".</li></ul>. By default, all three tools are uninstalled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Contains a `ttl` field which is an object with the following keys: <ul><li>`strategy`: How to handle expired checkpoints (e.g., `"delete"`).</li><li>`sweep_interval_minutes`: How often to check for expired checkpoints (integer).</li><li>`default_ttl`: Default time-to-live for checkpoints in **minutes** (integer); applied to newly created checkpoints/threads only (existing data is unchanged). Defines how long checkpoints are kept before the specified strategy is applied.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration with the following fields: <ul><li>`app`: Path to custom Starlette/FastAPI app (e.g., `"./src/agent/webapp.py:app"`). See [custom routes guide](/langsmith/custom-routes).</li><li>`cors`: CORS configuration with fields for `allow_origins`, `allow_methods`, `allow_headers`, etc.</li><li>`configurable_headers`: Define which request headers to exclude or include as a run's configurable values.</li><li>`disable_assistants`: Disable `/assistants` routes</li><li>`disable_mcp`: Disable `/mcp` routes</li><li>`disable_meta`: Disable `/ok`, `/info`, `/metrics`, and `/docs` routes</li><li>`disable_runs`: Disable `/runs` routes</li><li>`disable_store`: Disable `/store` routes</li><li>`disable_threads`: Disable `/threads` routes</li><li>`disable_ui`: Disable `/ui` routes</li><li>`disable_webhooks`: Disable webhooks calls on run completion in all routes</li><li>`mount_prefix`: Prefix for mounted routes (e.g., "/my-deployment/api")</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/langgraph-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
  </Tab>

<Tab title="JS">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./src/graph.ts:variable`, where `variable` is an instance of [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph)</li><li>`./src/graph.ts:makeGraph`, where `makeGraph` is a function that takes a config dictionary (`LangGraphRunnableConfig`) and returns an instance of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph). See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Contains a `ttl` field which is an object with the following keys: <ul><li>`strategy`: How to handle expired checkpoints (e.g., `"delete"`).</li><li>`sweep_interval_minutes`: How often to check for expired checkpoints (integer).</li><li>`default_ttl`: Default time-to-live for checkpoints in **minutes** (integer); applied to newly created checkpoints/threads only (existing data is unchanged). Defines how long checkpoints are kept before the specified strategy is applied.</li></ul>                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/langgraph-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    #### Basic Configuration

#### Using Wolfi Base Images

You can specify the Linux distribution for your base image using the `image_distro` field. Valid options are `debian`, `wolfi`, `bookworm`, or `bullseye`. Wolfi is the recommended option as it provides smaller and more secure images. This is available in `langgraph-cli>=0.2.11`.

#### Adding semantic search to the store

All deployments come with a DB-backed BaseStore. Adding an "index" configuration to your `langgraph.json` will enable [semantic search](/langsmith/semantic-search) within the BaseStore of your deployment.

The `index.fields` configuration determines which parts of your documents to embed:

* If omitted or set to `["$"]`, the entire document will be embedded
    * To embed specific fields, use JSON path notation: `["metadata.title", "content.text"]`
    * Documents missing specified fields will still be stored but won't have embeddings for those fields
    * You can still override which fields to embed on a specific item at `put` time using the `index` parameter

<Note>
      **Common model dimensions**

* `openai:text-embedding-3-large`: 3072
      * `openai:text-embedding-3-small`: 1536
      * `openai:text-embedding-ada-002`: 1536
      * `cohere:embed-english-v3.0`: 1024
      * `cohere:embed-english-light-v3.0`: 384
      * `cohere:embed-multilingual-v3.0`: 1024
      * `cohere:embed-multilingual-light-v3.0`: 384
    </Note>

#### Semantic search with a custom embedding function

If you want to use semantic search with a custom embedding function, you can pass a path to a custom embedding function:

The `embed` field in store configuration can reference a custom function that takes a list of strings and returns a list of embeddings. Example implementation:

#### Adding custom authentication

See the [authentication conceptual guide](/langsmith/auth) for details, and the [setting up custom authentication](/langsmith/set-up-custom-auth) guide for a practical walk through of the process.

#### Configuring Store Item Time-to-Live

You can configure default data expiration for items/memories in the BaseStore using the `store.ttl` key. This determines how long items are retained after they are last accessed (with reads potentially refreshing the timer based on `refresh_on_read`). Note that these defaults can be overwritten on a per-call basis by modifying the corresponding arguments in `get`, `search`, etc.

The `ttl` configuration is an object containing optional fields:

* `refresh_on_read`: If `true` (the default), accessing an item via `get` or `search` resets its expiration timer. Set to `false` to only refresh TTL on writes (`put`).
    * `default_ttl`: The default lifespan of an item in **minutes**. Applies only to newly created items; existing items are not modified. If not set, items do not expire by default.
    * `sweep_interval_minutes`: How frequently (in minutes) the system should run a background process to delete expired items. If not set, sweeping does not occur automatically.

Here is an example enabling a 7-day TTL (10080 minutes), refreshing on reads, and sweeping every hour:

#### Configuring Checkpoint Time-to-Live

You can configure the time-to-live (TTL) for checkpoints using the `checkpointer` key. This determines how long checkpoint data is retained before being automatically handled according to the specified strategy (e.g., deletion). The `ttl` configuration is an object containing:

* `strategy`: The action to take on expired checkpoints (currently `"delete"` is the only accepted option).
    * `sweep_interval_minutes`: How frequently (in minutes) the system checks for expired checkpoints.
    * `default_ttl`: The default lifespan of a checkpoint in **minutes**. Applies only to checkpoints/threads created after deployment; existing data is not modified.

Here's an example setting a default TTL of 30 days (43200 minutes):

In this example, checkpoints older than 30 days will be deleted, and the check runs every 10 minutes.

<a id="api-version" />

#### Pinning API Version

You can pin the API version of the LangGraph server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tab title="JS">
    #### Basic Configuration

<a id="api-version" />

#### Pinning API Version

You can pin the API version of the LangGraph server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tabs>
  <Tab title="Python">
    The base command for the LangGraph CLI is `langgraph`.

<Tab title="JS">
    The base command for the LangGraph.js CLI is `langgraphjs`.

We recommend using `npx` to always use the latest version of the CLI.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Run LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

<Note>Currently, the CLI only supports Python >= 3.11.</Note>

This command requires the "inmem" extra to be installed:

| Option                        | Default          | Description                                                                                                                                                                  |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                                          |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                                   |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                                   |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                                          |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                                     |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                               |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                                           |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                                |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                             |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code (added in `0.2.6`)                                                                                  |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                                |
  </Tab>

<Tab title="JS">
    Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

| Option                        | Default          | Description                                                                                                                                                      |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                              |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                       |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                       |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                              |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                         |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                   |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                               |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                    |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                 |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code                                                                                         |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                    |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Build LangSmith API server Docker image.

| Option                                | Default          | Description                                                                                                                                             |
    | ------------------------------------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`                     |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64`                                         |
    | `-t, --tag TEXT`                      |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                                                          |
    | `--pull / --no-pull`                  | `--pull`         | Build with latest remote Docker image. Use `--no-pull` for running the LangSmith API server with locally built images.                                  |
    | `-c, --config FILE`                   | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                                                                    |
    | `--build-command TEXT`<sup>\*</sup>   |                  | Build command to run. Runs from the directory where your `langgraph.json` file lives. Example: `langgraph build --build-command "yarn run turbo build"` |
    | `--install-command TEXT`<sup>\*</sup> |                  | Install command to run. Runs from the directory where you call `langgraph build` from. Example: `langgraph build --install-command "yarn install"`      |
    | `--help`                              |                  | Display command documentation.                                                                                                                          |

<sup>\*</sup>Only supported for JS deployments, will have no impact on Python deployments.
  </Tab>

<Tab title="JS">
    Build LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`   |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64` |
    | `-t, --tag TEXT`    |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                  |
    | `--no-pull`         |                  | Use locally built images. Defaults to `false` to build with latest remote Docker image.                         |
    | `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                            |
    | `--help`            |                  | Display command documentation.                                                                                  |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                       | Default                   | Description                                                                                                             |
    | ---------------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
    | `--wait`                     |                           | Wait for services to start before returning. Implies --detach                                                           |
    | `--base-image TEXT`          | `langchain/langgraph-api` | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                            |
    | `--image TEXT`               |                           | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly.           |
    | `--postgres-uri TEXT`        | Local database            | Postgres URI to use for the database.                                                                                   |
    | `--watch`                    |                           | Restart on file changes                                                                                                 |
    | `--debugger-base-url TEXT`   | `http://127.0.0.1:[PORT]` | URL used by the debugger to access LangGraph API.                                                                       |
    | `--debugger-port INTEGER`    |                           | Pull the debugger image locally and serve the UI on specified port                                                      |
    | `--verbose`                  |                           | Show more output from the server logs.                                                                                  |
    | `-c, --config FILE`          | `langgraph.json`          | Path to configuration file declaring dependencies, graphs and environment variables.                                    |
    | `-d, --docker-compose FILE`  |                           | Path to docker-compose.yml file with additional services to launch.                                                     |
    | `-p, --port INTEGER`         | `8123`                    | Port to expose. Example: `langgraph up --port 8000`                                                                     |
    | `--pull / --no-pull`         | `pull`                    | Pull latest images. Use `--no-pull` for running the server with locally-built images. Example: `langgraph up --no-pull` |
    | `--recreate / --no-recreate` | `no-recreate`             | Recreate containers even if their configuration and image haven't changed                                               |
    | `--help`                     |                           | Display command documentation.                                                                                          |
  </Tab>

<Tab title="JS">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                                                                    | Default                                                                 | Description                                                                                                   |
    | ------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`--wait`</span>                    |                                                                         | Wait for services to start before returning. Implies --detach                                                 |
    | <span style={{ whiteSpace: "nowrap" }}>`--base-image TEXT`</span>         | <span style={{ whiteSpace: "nowrap" }}>`langchain/langgraph-api`</span> | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                  |
    | <span style={{ whiteSpace: "nowrap" }}>`--image TEXT`</span>              |                                                                         | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. |
    | <span style={{ whiteSpace: "nowrap" }}>`--postgres-uri TEXT`</span>       | Local database                                                          | Postgres URI to use for the database.                                                                         |
    | <span style={{ whiteSpace: "nowrap" }}>`--watch`</span>                   |                                                                         | Restart on file changes                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`-c, --config FILE`</span>         | `langgraph.json`                                                        | Path to configuration file declaring dependencies, graphs and environment variables.                          |
    | <span style={{ whiteSpace: "nowrap" }}>`-d, --docker-compose FILE`</span> |                                                                         | Path to docker-compose.yml file with additional services to launch.                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`-p, --port INTEGER`</span>        | `8123`                                                                  | Port to expose. Example: `langgraph up --port 8000`                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`--no-pull`</span>                 |                                                                         | Use locally built images. Defaults to `false` to build with latest remote Docker image.                       |
    | <span style={{ whiteSpace: "nowrap" }}>`--recreate`</span>                |                                                                         | Recreate containers even if their configuration and image haven't changed                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`--help`</span>                    |                                                                         | Display command documentation.                                                                                |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `langgraph dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>

<Tab title="JS">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `npx @langchain/langgraph-cli dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cli.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

3. Verify the install

   <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ChatOpenAI

**URL:** llms-txt#chatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/openai

[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.

This guide will help you getting started with ChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).

<Note>
  **Chat Completions API compatibility**

`ChatOpenAI` is fully compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see [instructions](/oss/javascript/integrations/chat#chat-completions-api).
</Note>

<Info>
  **OpenAI models hosted on Azure**

Note that certain OpenAI models can also be accessed via the [Microsoft Azure platform](https://azure.microsoft.com/en-us/products/ai-foundry/models/openai/). To use the Azure OpenAI service use the [`AzureChatOpenAI`](/oss/javascript/integrations/chat/azure_chat_openai/) integration.
</Info>

### Integration details

| Class                                                                               | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/openai) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :----------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                     ✅                                    | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

To access OpenAI chat models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [OpenAI's website](https://platform.openai.com/) to sign up for OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## model_1 is tagged with "joke"

**URL:** llms-txt#model_1-is-tagged-with-"joke"

model_1 = init_chat_model(model="openai:gpt-4o-mini", tags=['joke'])

---

## Node 3 only has access to the overall state (no access to private data from node_1)

**URL:** llms-txt#node-3-only-has-access-to-the-overall-state-(no-access-to-private-data-from-node_1)

def node_3(state: OverallState) -> OverallState:
    output = {"a": "set by node_3"}
    print(f"Entered node `node_3`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## Test

**URL:** llms-txt#test

**Contents:**
- Prerequisites
- Getting started
- Testing individual nodes and edges
- Partial execution

Source: https://docs.langchain.com/oss/python/langgraph/test

After you've prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.

Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out [this section](/oss/python/langchain/test/) that uses LangChain's built-in [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) instead.

First, make sure you have [`pytest`](https://docs.pytest.org/) installed:

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:

## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/python/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) will suffice for testing).
2. Call your agent's [`update_state`](/oss/python/langgraph/use-time-travel) method with an [`as_node`](/oss/python/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interrupt_after` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/test.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Getting started

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:
```

Example 2 (unknown):
```unknown
## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:
```

Example 3 (unknown):
```unknown
## Partial execution

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/python/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) will suffice for testing).
2. Call your agent's [`update_state`](/oss/python/langgraph/use-time-travel) method with an [`as_node`](/oss/python/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interrupt_after` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:
```

---

## Authentication & access control

**URL:** llms-txt#authentication-&-access-control

**Contents:**
- Core Concepts
  - Authentication vs Authorization
- Default Security Models
  - LangSmith
  - Self-Hosted
- System Architecture
- Authentication
  - Agent authentication
  - Agent authentication with MCP
- Authorization

Source: https://docs.langchain.com/langsmith/auth

LangSmith provides a flexible authentication and authorization system that can integrate with most authentication schemes.

### Authentication vs Authorization

While often used interchangeably, these terms represent distinct security concepts:

* [**Authentication**](#authentication) ("AuthN") verifies *who* you are. This runs as middleware for every request.
* [**Authorization**](#authorization) ("AuthZ") determines *what you can do*. This validates the user's privileges and roles on a per-resource basis.

In LangSmith, authentication is handled by your [`@auth.authenticate`](/langsmith/smith-python-sdk#langgraph_sdk.auth.Auth.authenticate) handler, and authorization is handled by your [`@auth.on`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.on) handlers.

## Default Security Models

LangSmith provides different security defaults:

* Uses LangSmith API keys by default
* Requires valid API key in `x-api-key` header
* Can be customized with your auth handler

<Note>
  **Custom auth**
  Custom auth **is supported** for all plans in LangSmith.
</Note>

* No default authentication
* Complete flexibility to implement your security model
* You control all aspects of authentication and authorization

## System Architecture

A typical authentication setup involves three main components:

1. **Authentication Provider** (Identity Provider/IdP)

* A dedicated service that manages user identities and credentials
* Handles user registration, login, password resets, etc.
* Issues tokens (JWT, session tokens, etc.) after successful authentication
* Examples: Auth0, Supabase Auth, Okta, or your own auth server

2. **LangGraph Backend** (Resource Server)

* Your LangGraph application that contains business logic and protected resources
* Validates tokens with the auth provider
* Enforces access control based on user identity and permissions
* Doesn't store user credentials directly

3. **Client Application** (Frontend)

* Web app, mobile app, or API client
* Collects time-sensitive user credentials and sends to auth provider
* Receives tokens from auth provider
* Includes these tokens in requests to LangGraph backend

Here's how these components typically interact:

Your [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.on) handlers implement step 7.

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid

The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

* request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.

After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

After authentication, LangGraph calls your [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.

<a id="resource-specific-handlers" />

### Resource-Specific Handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Your [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.on) handlers implement step 7.

## Authentication

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid
```

Example 2 (unknown):
```unknown
The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

  * request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

  In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.
```

Example 3 (unknown):
```unknown
After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

## Authorization

After authentication, LangGraph calls your [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.
```

Example 4 (unknown):
```unknown
<a id="resource-specific-handlers" />

### Resource-Specific Handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.Auth.on) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>
```

---

## Bob tries to access Alice's thread

**URL:** llms-txt#bob-tries-to-access-alice's-thread

try:
    await bob.threads.get(alice_thread["thread_id"])
    print("❌ Bob shouldn't see Alice's thread!")
except Exception as e:
    print("✅ Bob correctly denied access:", e)

---

## Define other parameters

**URL:** llms-txt#define-other-parameters

val = 42
text = "Hello, world!"

---

## Set up feedback criteria

**URL:** llms-txt#set-up-feedback-criteria

**Contents:**
- Continuous feedback
- Categorical feedback

Source: https://docs.langchain.com/langsmith/set-up-feedback-criteria

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.

To set up a new feedback criteria, follow [this link](https://smith.langchain.com/settings/workspaces/feedbacks) to view all existing tags for your workspace, then click **New Tag**.

## Continuous feedback

For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=44798176648f0a65e873fddecc90d43d" alt="" data-og-width="350" width="350" data-og-height="529" height="529" data-path="langsmith/images/cont-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4181c432230e33e7b6a7839e64729efa 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d11740b1d6f782cb551b6c8e7af92b50 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c966a6835ae5e2aaf3a320cf9bb71c74 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d6ea2414a3f698cb66cd5f336f4bac51 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=442f373f36d5e8aa7dcf0e9685eb29f5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e8d44fd982322dd6865a25af561bc24 2500w" />

## Categorical feedback

For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.
Both the category label and the score will be logged as feedback in `value` and `score` fields, respectively.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ec5030c3ba55b1fb12d60bca91719f7" alt="" data-og-width="470" width="470" data-og-height="465" height="465" data-path="langsmith/images/cat-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a11c14d2e7361e9aebc7d5997944f4c8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5e1dcf94730da4f7664092c4582410a 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c18cd85595c56d9cf795fc07902db38 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ba75e7685d5cbd1918789301333038e2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=311246a21107c2f66b130b720ec67121 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc625d7b4686fb291f4bf795f8bd0f6e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-feedback-criteria.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create the dataset

**URL:** llms-txt#create-the-dataset

ls_client = Client()
dataset_name = "attachment-test-dataset"
dataset = ls_client.create_dataset(
  dataset_name=dataset_name,
  description="Test dataset for evals with publicly available attachments",
)

inputs = {
  "audio_question": "What is in this audio clip?",
  "image_question": "What is in this image?",
}

outputs = {
  "audio_answer": "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
  "image_answer": "A mug with a blanket over it.",
}

---

## Transient file (lost after thread ends)

**URL:** llms-txt#transient-file-(lost-after-thread-ends)

agent.invoke({
    "messages": [{"role": "user", "content": "Write draft to /draft.txt"}]
})

---

## Create parent run

**URL:** llms-txt#create-parent-run

parent_run_id = uuid4()
post_run(parent_run_id, "Chat Pipeline", "chain", {"question": question})

---

## Configure webhook notifications for rules

**URL:** llms-txt#configure-webhook-notifications-for-rules

**Contents:**
- Webhook payload
- Security
  - Webhook custom HTTP headers
  - Webhook Delivery
- Example with Modal
  - Setup
  - Secrets
  - Service

Source: https://docs.langchain.com/langsmith/webhooks

When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da310e976aa8824071d65b8fb44b9123" alt="" data-og-width="872" width="872" data-og-height="991" height="991" data-path="langsmith/images/webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6f30f7cd2de82b0ccb826d257b933f12 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0fce81ff2661e8944ebfb781a07017fe 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5799cf7458a15ac99579ba273d0b875e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aa958b944f848f3a64ce068a64bc8433 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a3e94463ac9d8fa498accb27124785ab 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a5db18db5cf7097fdba6d6b7162ba6d 2500w" />

The payload we send to your webhook endpoint contains:

* `"rule_id"`: this is the ID of the automation that sent this payload
* `"start_time"` and `"end_time"`: these are the time boundaries where we found matching runs
* `"runs"`: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.
* `"feedback_stats"`: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.

<Note>
  **fetching from S3 URLs**

Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

To finish setting up your account, run the command:

and follow the instructions

Next, you will need to set up some secrets in Modal.

First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in *Modal* to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets [here](https://modal.com/docs/guide/secrets).
For this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.

We can also set up a LangSmith secret - luckily there is already an integration template for this!

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0c3209b59cb36273d82fb44383efa1d5" alt="LangSmith Modal Template" data-og-width="1229" width="1229" data-og-height="779" height="779" data-path="langsmith/images/modal-langsmith-secret.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2ff70b647c04bb6a45a08de537b4d22 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=02181b882935f45339d31f48adeed1e9 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=52119f475cca739369cebb71bfefafae 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aa8a1fc73b2e0b7f27c3186732e3bde9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=91d36950d22b86f7ad790a61957cbad7 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=efcdfe7d54ca30079b147e00a0d9e934 2500w" />

After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on:

```python  theme={null}
from fastapi import HTTPException, status, Request, Query
from modal import Secret, Stub, web_endpoint, Image

stub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))

@stub.function(
    secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")]
)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  **fetching from S3 URLs**

  Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

  The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:
```

Example 2 (unknown):
```unknown
## Security

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

An example would be
```

Example 3 (unknown):
```unknown
### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

### Webhook Delivery

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

### Setup

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure webhook notifications for LangSmith alerts

**URL:** llms-txt#configure-webhook-notifications-for-langsmith-alerts

**Contents:**
- Overview
- Prerequisites
- Integration Configuration
  - Step 1: Prepare Your Receiving Endpoint
  - Step 2: Configure Webhook Parameters
  - Step 3: Test the Webhook
- Troubleshooting
- Security Considerations
- Sending alerts to Slack using a webhook
  - Prerequisites

Source: https://docs.langchain.com/langsmith/alerts-webhook

This guide details the process for setting up webhook notifications for [LangSmith alerts](/langsmith/alerts). Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following [this guide](./alerts). Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.

* An endpoint that can receive HTTP POST requests
* Appropriate authentication credentials for your receiving service (if required)

## Integration Configuration

### Step 1: Prepare Your Receiving Endpoint

Before configuring the webhook in LangSmith, ensure your receiving endpoint:

* Accepts HTTP POST requests
* Can process JSON payloads
* Is accessible from external services
* Has appropriate authentication mechanisms (if required)

Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.

### Step 2: Configure Webhook Parameters

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fecb6275ad3d576a864d1c6a2771c847" alt="Webhook Setup" data-og-width="754" width="754" data-og-height="523" height="523" data-path="langsmith/images/webhook-setup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ef03d3ab887113e73dbdc1097076d103 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a25fcaedcbed92c9c3f2e2bddd8d88bd 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4785471ce1e58f3c48ce19b7be3889c5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8c7dd40aeb5635cdf4ddf207d0dfe7c7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aff125529b9db8fbf861999e70bcdb26 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e9de7c4f0dcc440d734f4f3d09d2abf4 2500w" />

In the notification section of your alert complete the webhook configuration with the following parameters:

* **URL**: The complete URL of your receiving endpoint
  * Example: `https://api.example.com/incident-webhook`

* **Headers**: JSON Key-value pairs sent with the webhook request

* Common headers include:

* `Authorization`: For authentication tokens
    * `Content-Type`: Usually set to `application/json` (default)
    * `X-Source`: To identify the source as LangSmith

* If no headers, then simply use `{}`

* **Request Body Template**: Customize the JSON payload sent to your endpoint

* Default: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:

* `project_name`: Name of the triggered alert
    * `alert_rule_id`: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.
    * `alert_rule_name`: The name of the alert rule.
    * `alert_rule_type`: The type of alert (as of 04/01/2025 all alerts are of type `threshold`).
    * `alert_rule_attribute`: The attribute associated with the alert rule - `error_count`, `feedback_score` or `latency`.
    * `triggered_metric_value`: The value of the metric at the time the threshold was triggered.
    * `triggered_threshold`: The threshold that triggered the alert.
    * `timestamp`: The timestamp that triggered the alert.

### Step 3: Test the Webhook

Click **Send Test Alert** to send the webhook notification to ensure the notification works as intended.

If webhook notifications aren't being delivered:

* Verify the webhook URL is correct and accessible
* Ensure any authentication headers are properly formatted
* Check that your receiving endpoint accepts POST requests
* Examine your endpoint's logs for received but rejected requests
* Verify your custom payload template is valid JSON format

## Security Considerations

* Use HTTPS for your webhook endpoints
* Implement authentication for your webhook endpoint
* Consider adding a shared secret in your headers to verify webhook sources
* Validate incoming webhook requests before processing them

## Sending alerts to Slack using a webhook

Here is an example for configuring LangSmith alerts to send notifications to Slack channels using the [`chat.postMessage`](https://api.slack.com/methods/chat.postMessage) API.

* Access to a Slack workspace
* A LangSmith project to set up alerts
* Permissions to create Slack applications

### Step 1: Create a Slack App

1. Visit the [Slack API Applications page](https://api.slack.com/apps)
2. Click **Create New App**
3. Select **From scratch**
4. Provide an **App Name** (e.g., "LangSmith Alerts")
5. Select the workspace where you want to install the app
6. Click **Create App**

### Step 2: Configure Bot Permissions

1. In the left sidebar of your Slack app configuration, click **OAuth & Permissions**

2. Scroll down to **Bot Token Scopes** under **Scopes** and click **Add an OAuth Scope**

3. Add the following scopes:

* `chat:write` (Send messages as the app)
   * `chat:write.public` (Send messages to channels the app isn't in)
   * `channels:read` (View basic channel information)

### Step 3: Install the App to Your Workspace

1. Scroll up to the top of the **OAuth & Permissions** page
2. Click **Install to Workspace**
3. Review the permissions and click **Allow**
4. Copy the **Bot User OAuth Token** that appears (begins with `xoxb-`)

### Step 4: Configure the Webhook Alert in LangSmith

1. In LangSmith, navigate to your project
2. Select **Alerts → Create Alert**
3. Define your alert metrics and conditions
4. In the notification section, select **Webhook**
5. Configure the webhook with the following settings:

> **Note:** Replace `xoxb-your-token-here` with your actual Bot User OAuth Token

**Request Body Template**

**NOTE:** Fill in the `channel_id`, `alert_name`, `project_name` and `project_url` when creating the alert. You can find your `project_url` in the browser's URL bar. Copy the portion up to but not including any query parameters.

6. Click **Save** to activate the webhook configuration

### Step 5: Test the Integration

1. In the LangSmith alert configuration, click **Test Alert**
2. Check your specified Slack channel for the test notification
3. Verify that the message contains the expected alert information

### (Optional) Step 6: Link to the Alert Preview in the Request Body

After creating an alert, you can optionally link to its preview in the webhook's request body.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=286ebb8f90bafbdcacf9a0602aaf749c" alt="Alert Preview Pane" data-og-width="832" width="832" data-og-height="773" height="773" data-path="langsmith/images/alert-preview-pane.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=20a409a30bff44a1a8bb1b79a6a2216b 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=414bb4719617bd23452273c73327d601 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc7bc7aaee65f7f4afac42102047ad2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=491244ac56f6f4bcbb64419b267df0fe 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d47f5ba127c3f61e3cb7498f8b7568fe 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a70706db839b2d211024116ba19acef 2500w" />

1. Save your alert
2. Find your saved alert in the alerts table and click it
3. Copy the dsiplayed URL
4. Click "Edit Alert"
5. Replace the existing project URL with the copied alert preview URL

## Additional Resources

* [LangSmith Alerts Documentation](/langsmith/alerts)
* [Slack chat.postMessage API Documentation](https://api.slack.com/methods/chat.postMessage)
* [Slack Block Kit Builder](https://app.slack.com/block-kit-builder/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts-webhook.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Headers**
```

Example 2 (unknown):
```unknown
> **Note:** Replace `xoxb-your-token-here` with your actual Bot User OAuth Token

**Request Body Template**
```

---

## Create a thread as user 1

**URL:** llms-txt#create-a-thread-as-user-1

thread = await user1_client.threads.create()
print(f"✅ User 1 created thread: {thread['thread_id']}")

---

## The states are returned in reverse chronological order.

**URL:** llms-txt#the-states-are-returned-in-reverse-chronological-order.

states = list(graph.get_state_history(config))

for state in states:
    print(state.next)
    print(state.config["configurable"]["checkpoint_id"])
    print()

()
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

('write_joke',)
1f02ac4a-ce2a-6494-8001-cb2e2d651227

('generate_topic',)
1f02ac4a-a4e0-630d-8000-b73c254ba748

('__start__',)
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown

```

---

## maxReplicas: 16

**URL:** llms-txt#maxreplicas:-16

---

## 1. Specify config schema

**URL:** llms-txt#1.-specify-config-schema

class ContextSchema(TypedDict):
    my_runtime_value: str

---

## LangSmith control plane

**URL:** llms-txt#langsmith-control-plane

**Contents:**
- Control plane UI
- Control plane API
  - Integrations
  - Deployments
  - Revisions
  - Listeners
- Control Plane Features
  - Deployment Types
  - Database provisioning
  - Asynchronous deployment

Source: https://docs.langchain.com/langsmith/control-plane

The *control plane* is the part of LangSmith that manages deployments. It includes the control plane UI, where users create and update [LangGraph Servers](/langsmith/langgraph-server), and the control plane APIs, which support the UI and provide programmatic access.

When you make an update through the control plane, the update is stored in control plane state. The [data plane](/langsmith/data-plane) “listener” polls for these updates by calling the control plane APIs.

From the control plane UI, you can:

* View a list of outstanding deployments.
* View details of an individual deployment.
* Create a new deployment.
* Update a deployment.
* Update environment variables for a deployment.
* View build and server logs of a deployment.
* View deployment metrics such as CPU and memory usage.
* Delete a deployment.

The Control plane UI is embedded in [LangSmith](https://docs.smith.langchain.com).

This section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](/langsmith/api-ref-control-plane) for more details.

An integration is an abstraction for a `git` repository provider (e.g. GitHub). It contains all of the required metadata needed to connect with and deploy from a `git` repository.

A deployment is an instance of a LangGraph Server. A single deployment can have many revisions.

A revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.

A listener is an instance of a ["listener" application](/langsmith/data-plane#”listener”-application). A listener contains metadata about the application (e.g. version) and metadata about the compute infrastructure where it can deploy to (e.g. Kubernetes namespaces).

The listener data model only applies for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.

## Control Plane Features

This section describes various features of the control plane.

For simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.

| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |
| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |
| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |
| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |

CPU and memory resources are per replica.

<Warning>
  **Immutable Deployment Type**
  Once a deployment is created, the deployment type cannot be changed.
</Warning>

<Info>
  **Self-Hosted Deployment**
  Resources for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments can be fully customized. Deployment types are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

`Production` type deployments are suitable for "production" workloads. For example, select `Production` for customer-facing applications in the critical path.

Resources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact [support@langchain.dev](mailto:support@langchain.dev) to request an increase in resources.

`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for "production" workloads.

<Danger>
  **Preemptible Compute Infrastructure**
  `Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...

* Redis connection timeouts/errors
  * Postgres connection timeouts/errors
  * Failed or retrying background runs

This behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, LangGraph Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.

`Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.
</Danger>

Database disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](/langsmith/configure-ttl) should be configured to manage disk usage. Contact [support@langchain.dev](mailto:support@langchain.dev) to request an increase in resources.

### Database provisioning

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](/oss/python/langgraph/persistence#memory-store) for the deployment.

When implementing a LangGraph application, a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.

There is no direct access to the database. All access to the database occurs through the [LangGraph Server](/langsmith/langgraph-server).

The database is never deleted until the deployment itself is deleted.

<Info>
  A custom Postgres instance can be configured for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

### Asynchronous deployment

Infrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.

* When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.
* When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.
* The deployment process for each revision contains a build step, which can take up to a few minutes.

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to achieve asynchronous deployments.

After a deployment is ready, the control plane monitors the deployment and records various metrics, such as:

* CPU and memory usage of the deployment.
* Number of container restarts.
* Number of replicas (this will increase with [autoscaling](/langsmith/data-plane#autoscaling)).
* [PostgreSQL](/langsmith/data-plane#postgres) CPU, memory usage, and disk usage.
* [LangGraph Server queue](/langsmith/langgraph-server#persistence-and-task-queue) pending/active run count.
* [LangGraph Server API](/langsmith/langgraph-server) success response count, error response count, and latency.

These metrics are displayed as charts in the Control Plane UI.

### LangSmith integration

A [LangSmith](/langsmith/home) tracing project is automatically created for each deployment. The tracing project has the same name as the deployment. When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.

When a deployment is deleted, the traces and the tracing project are not deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/control-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Configure Semantic Kernel

**URL:** llms-txt#configure-semantic-kernel

kernel = Kernel()
kernel.add_service(OpenAIChatCompletion())

---

## Node that *uses* the instructions

**URL:** llms-txt#node-that-*uses*-the-instructions

def call_model(state: State, store: BaseStore):
    namespace = ("agent_instructions", )
    instructions = store.get(namespace, key="agent_a")[0]
    # Application logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"])
    ...

---

## Alice creates an assistant

**URL:** llms-txt#alice-creates-an-assistant

alice_assistant = await alice.assistants.create()
print(f"✅ Alice created assistant: {alice_assistant['assistant_id']}")

---

## Configure LangSmith OpenTelemetry export (no OTEL env vars or headers needed)

**URL:** llms-txt#configure-langsmith-opentelemetry-export-(no-otel-env-vars-or-headers-needed)

**Contents:**
- Advanced configuration
  - Use OpenTelemetry Collector for fan-out
  - Distributed tracing with LangChain and OpenTelemetry

configure(project_name="adk-otel-demo")

async def main():
    agent = LlmAgent(
        name="travel_assistant",
        model="gemini-2.5-flash-lite",
        instruction="You are a helpful travel assistant.",
    )

session_service = InMemorySessionService()
    runner = Runner(app_name="travel_app", agent=agent, session_service=session_service)

user_id = "user_123"
    session_id = "session_abc"
    await session_service.create_session(app_name="travel_app", user_id=user_id, session_id=session_id)

new_message = types.Content(parts=[types.Part(text="Hi! Recommend a weekend trip to Paris.")], role="user")

for event in runner.run(user_id=user_id, session_id=session_id, new_message=new_message):
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
yaml  theme={null}
   receivers:
     otlp:
       protocols:
         grpc:
           endpoint: 0.0.0.0:4317
         http:
           endpoint: 0.0.0.0:4318

processors:
     batch:

exporters:
     otlphttp/langsmith:
       endpoint: https://api.smith.langchain.com/otel/v1/traces
       headers:
         x-api-key: ${env:LANGSMITH_API_KEY}
         Langsmith-Project: my_project
     otlphttp/other_provider:
       endpoint: https://otel.your-provider.com/v1/traces
       headers:
         api-key: ${env:OTHER_PROVIDER_API_KEY}

service:
     pipelines:
       traces:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlphttp/langsmith, otlphttp/other_provider]
   python  theme={null}
   import os
   from opentelemetry import trace
   from opentelemetry.sdk.trace import TracerProvider
   from opentelemetry.sdk.trace.export import BatchSpanProcessor
   from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
   from langchain_openai import ChatOpenAI
   from langchain_core.prompts import ChatPromptTemplate

# Point to your local OpenTelemetry Collector
   otlp_exporter = OTLPSpanExporter(
       endpoint="http://localhost:4318/v1/traces"
   )
   provider = TracerProvider()
   processor = BatchSpanProcessor(otlp_exporter)
   provider.add_span_processor(processor)
   trace.set_tracer_provider(provider)

# Set environment variables for LangChain
   os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
   os.environ["LANGSMITH_TRACING"] = "true"

# Create and run a LangChain application
   prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
   model = ChatOpenAI()
   chain = prompt | model
   result = chain.invoke({"topic": "programming"})
   print(result.content)
   python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.propagate import inject, extract
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
import requests
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set OTEL environment variables or exporters. `configure()` wires them for LangSmith automatically; instrumentors (like `GoogleADKInstrumentor`) create the spans.
</Note>

5. View the trace in your LangSmith dashboard ([example](https://smith.langchain.com/public/106f5bed-edca-4357-91a5-80089252c9ed/r)).

## Advanced configuration

### Use OpenTelemetry Collector for fan-out

For more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.

1. [Install the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/) for your environment.

2. Create a configuration file (e.g., `otel-collector-config.yaml`) that exports to multiple destinations:
```

Example 2 (unknown):
```unknown
3. Configure your application to send to the collector:
```

Example 3 (unknown):
```unknown
This approach offers several advantages:

* Centralized configuration for all your telemetry destinations
* Reduced overhead in your application code
* Better scalability and resilience
* Ability to add or remove destinations without changing application code

### Distributed tracing with LangChain and OpenTelemetry

Distributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry's context propagation capabilities ensure that traces remain connected across service boundaries.

#### Context propagation in distributed tracing

In distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:

* **Trace ID**: A unique identifier for the entire trace
* **Span ID**: A unique identifier for the current span
* **Sampling Decision**: Indicates whether this trace should be sampled

#### Set up distributed tracing with LangChain

To enable distributed tracing across multiple services:
```

---

## Delete workspaces

**URL:** llms-txt#delete-workspaces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single workspace

Source: https://docs.langchain.com/langsmith/script-delete-a-workspace

<Note>
  Deleting a workspace is supported **nativley in LangSmith Self-Hosted v0.10**. View [instructions for deleting a workspace](/langsmith/set-up-a-workspace#delete-a-workspace).

Follow the guide below for Self-Hosted versions before v0.10.
</Note>

The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.

This command using the Workspace ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete a workspace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_workspace.sh)

### Running the deletion script for a single workspace

Run the following command to run the workspace removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see workspace is deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-a-workspace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## The "Auth" object is a container that LangGraph will use to mark our authentication function

**URL:** llms-txt#the-"auth"-object-is-a-container-that-langgraph-will-use-to-mark-our-authentication-function

---

## this is supported

**URL:** llms-txt#this-is-supported

{"messages": [HumanMessage(content="message")]}

---

## Pick a dataset id. In this case, we are using the dataset we created above.

**URL:** llms-txt#pick-a-dataset-id.-in-this-case,-we-are-using-the-dataset-we-created-above.

---

## Configure LangSmith for scale

**URL:** llms-txt#configure-langsmith-for-scale

**Contents:**
- Summary
- Trace ingestion (write path)
- Trace querying (read path)
- Example LangSmith configurations for scale
  - Low reads, low writes <a name="low-reads-low-writes" />
  - Low reads, high writes <a name="low-reads-high-writes" />

Source: https://docs.langchain.com/langsmith/self-host-scale

A self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.

For example configurations, refer to [Example LangSmith configurations for scale](#example-langsmith-configurations-for-scale).

The table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):

|                                                                                                                                                                  | **[Low / low](#low-reads-low-writes)**              | **[Low / high](#low-reads-high-writes)**            | **[High / low](#high-reads-low-writes)**                                                                                                                                                                                                 | [Medium / medium](#medium-reads-medium-writes)      | [High / high](#high-reads-high-writes)                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <Tooltip tip="Number of users actively viewing traces on the frontend">Concurrent frontend users</Tooltip>                                                       | 5                                                   | 5                                                   | 50                                                                                                                                                                                                                                       | 20                                                  | 50                                                                                                                                                                                                                                       |
| <Tooltip tip="Number of traces being ingested via SDKs or API endpoints">Traces submitted per second</Tooltip>                                                   | 10                                                  | 1000                                                | 10                                                                                                                                                                                                                                       | 100                                                 | 1000                                                                                                                                                                                                                                     |
| **Frontend replicas**                                                                                                                                            | 1 (default)                                         | 4                                                   | 2                                                                                                                                                                                                                                        | 2                                                   | 4                                                                                                                                                                                                                                        |
| **Platform backend replicas**                                                                                                                                    | 3 (default)                                         | 20                                                  | 3 (default)                                                                                                                                                                                                                              | 3 (default)                                         | 20                                                                                                                                                                                                                                       |
| **Queue replicas**                                                                                                                                               | 3 (default)                                         | 160                                                 | 6                                                                                                                                                                                                                                        | 10                                                  | 160                                                                                                                                                                                                                                      |
| **Backend replicas**                                                                                                                                             | 2 (default)                                         | 5                                                   | 40                                                                                                                                                                                                                                       | 16                                                  | 50                                                                                                                                                                                                                                       |
| **Redis resources**                                                                                                                                              | 8 Gi (default)                                      | 200 Gi external                                     | 8 Gi (default)                                                                                                                                                                                                                           | 13Gi external                                       | 200 Gi external                                                                                                                                                                                                                          |
| **ClickHouse resources**                                                                                                                                         | 4 CPU<br />16 Gi (default)                          | 10 CPU<br />32Gi memory                             | 8 CPU<br />16 Gi per replica                                                                                                                                                                                                             | 16 CPU<br />24Gi memory                             | 14 CPU<br />24 Gi per replica                                                                                                                                                                                                            |
| **ClickHouse setup**                                                                                                                                             | Single instance                                     | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> |
| <Tooltip tip="We recommend using an external instance and enabling autoexpansion for the disk to handle growing data requirements.">Postgres resources</Tooltip> | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      |
| **Blob storage**                                                                                                                                                 | Disabled                                            | Enabled                                             | Enabled                                                                                                                                                                                                                                  | Enabled                                             | Enabled                                                                                                                                                                                                                                  |

Below we go into more details about the read and write paths as well as provide a `values.yaml` snippet for you to start with for your self-hosted LangSmith instance.

## Trace ingestion (write path)

Common usage that put load on the write path:

* Ingesting traces via the Python or JavaScript LangSmith SDK
* Ingesting traces via the `@traceable` wrapper
* Submitting traces via the `/runs/multipart` endpoint

Services that play a large role in trace ingestion:

* Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue
* Redis cache: Used to queue traces that need to be persisted
* Queue service: Persists traces for querying
* ClickHouse: Persistent storage used for traces

When scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:

* Give ClickHouse more resources (CPU and memory) if it is approaching resource limits.
* Increase the number of platform-backend pods if ingest requests are taking long to respond.
* Increase queue service pod replicas if traces are not being processed from Redis fast enough.
* Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.

## Trace querying (read path)

Common usage that puts load on the read path:

* Users on the frontend looking at tracing projects or individual traces
* Scripts used to query for trace info
* Hitting either the `/runs/query` or `/runs/<run-id>` api endpoints

Services that play a large role in querying traces:

* Backend service: Receives the request and submits a query to ClickHouse to then respond to the request
* ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.

When scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:

* Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.
* Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.
* Move to a [replicated ClickHouse cluster](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster). Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).

For more precise guidance on how this translates to helm chart values, refer to the examples the following [section](#example-langsmith-configurations-for-scale). If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.

## Example LangSmith configurations for scale

Below we provide some example LangSmith configurations based on expected read and write loads.

For read load (trace querying):

* Low means roughly 5 users looking at traces at a time (about 10 requests per second)
* Medium means roughly 20 users looking at traces at a time (about 40 requests per second)
* High means roughly 50 users looking at traces at a time (about 100 requests per second)

For write load (trace ingestion):

* Low means up to 10 traces submitted per second
* Medium means up to 100 traces submitted per second
* High means up to 1000 traces submitted per second

<Note>
  The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.
</Note>

### Low reads, low writes <a name="low-reads-low-writes" />

The default LangSmith configuration will handle this load. No custom resource configuration is needed here.

### Low reads, high writes <a name="low-reads-high-writes" />

You have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.

For this, we recommend a configuration like this:

```yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true
  settings:
    redisRunsExpirySeconds: "3600"

---

## Section 2: Full Observability Stack

**URL:** llms-txt#section-2:-full-observability-stack

**Contents:**
- Prerequisites
  - 1. Compute Resources
  - 2. Cert-Manager
  - 3. OpenTelemetry Operator
- Installation
- Post-Installation
  - Enable Logs and Traces in LangSmith
- Grafana Usage

<Warning>
  **This is not a production observability stack. Use this to gain quick insight into logs, metrics and traces for your deployment. This is only made to handle a few dozen GB of data per day.**
</Warning>

This section will show you how to deploy the end-to-end observability stack for LangSmith, using the [Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith-observability).

This chart is built around the open-source LGTM Stack from Grafana. It consists of:

* [Loki](https://grafana.com/docs/loki/latest/) for logs.
* [Mimir](https://grafana.com/docs/mimir/latest/) for metrics + alerting.
* [Tempo](https://grafana.com/docs/tempo/latest/) for traces.
* [Grafana](https://grafana.com/docs/grafana/latest/) for monitoring UI.

As well as [OpenTelemetry Collectors](https://opentelemetry.io/docs/collector/) for gathering the telemetry data.

### 1. Compute Resources

The resource requests and limits for each part of the stack can be modified in the helm chart. Here are the current allocations (request/limit):

* Loki: `2vCPU/3vCPU + 2Gi/4Gi`
* Mimir: `1vCPU/2vCPU + 2Gi/4Gi`
* Tempo: `1vCPU/2vCPU + 4Gi/6Gi`

Make sure you have those resources allocated before bringing up the helm chart, or modify the resource values in your helm configuration file.

The helm chart uses the OpenTelemetry Operator to provision collectors. The operator require that you have [cert-manager](https://cert-manager.io/docs/installation/) installed in your Kubernetes cluster.

If you do not have it installed, you can run the following commands:

### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.

<Info>
  1. To get `${LANGSMITH_OTEL_CRD_NAME}`, you can run `kubectl get opentelemetrycollectors -n ${LANGSMITH_OBS_NAMESPACE}` and select the name of the one with MODE = `sidecar`
  2. To get `${GATEWAY_COLLECTOR_SERVICE_NAME}` name, run `kubectl get services -n ${LANGSMITH_OBS_NAMESPACE}` and select the one with Ports 4317/4318 AND a ClusterIP set. It should be something like `langsmith-observability-collector-gateway-collector`
</Info>

Now run `helm upgrade langsmith langchain/langsmith --values langsmith_config.yaml -n <langsmith-namespace> --wait --debug`

Once upgraded, if you run `kubectl get pods -n <langsmith-namespace>` you should see the following (note the 2/2 for sidecar collectors):

Once everything is installed, do the following: to get your Grafana password:

Then port-forward into the `langsmith-observability-grafana` container at port 3000, and open your browser as `localhost:3000`. Use the username `admin` and the password from the secret above to log into Grafana.

Once in Grafana, you can use the UI to monitor logs, metrics and traces. Grafana also comes pre-packaged with sets of dashboards for monitoring the main components of your deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee47243826737bab23944e01536dec71" alt="LangSmith Grafana Dashboards" data-og-width="1715" width="1715" data-og-height="1073" height="1073" data-path="langsmith/images/langsmith-grafana-dashboards.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=11e0d71897053d012e929ca49533d6dd 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0e674be629c1c1e795e1b6e686ca28e2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb3bcdc4fa39e82fe9f3370834c7f7fa 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=80c401f8794ac0671207e1a260aac25b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f5baa041ab9e5c39b7d8a62e2bfe1e5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7620f91021593db1d87c92fc43d8fe2b 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-stack.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:
```

Example 2 (unknown):
```unknown
## Installation

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:
```

Example 3 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

Example 4 (unknown):
```unknown
## Post-Installation

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.
```

---

## This compiles it into a LangChain Runnable,

**URL:** llms-txt#this-compiles-it-into-a-langchain-runnable,

---

## Delete traces

**URL:** llms-txt#delete-traces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single trace
  - Running the deletion script for a multiple traces from a file with one trace ID per line

Source: https://docs.langchain.com/langsmith/script-delete-traces

The LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedback table themselves.

This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `delete_trace_by_id` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to delete a trace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_trace_by_id.sh)

### Running the deletion script for a single trace

Run the following command to run the trace deletion script using a single trace ID:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see all the specified traces have been removed.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

Example 2 (unknown):
```unknown
If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:
```

Example 3 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility

**URL:** llms-txt#note:-utilities-might-be-shared,-e.g.,-googlefinanceapiwrapper-was-listed,-verify-correct-utility

---

## See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**URL:** llms-txt#see-trace:-https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**Contents:**
- Ensure all traces are submitted before exiting
  - Using the LangSmith SDK
  - Using LangChain

MyClass(13).combine(29)
python Python theme={null}
  from langsmith import Client

@traceable(client=client)
  async def my_traced_func():
    # Your code here...
    pass

try:
    await my_traced_func()
  finally:
    await client.flush()
  typescript TypeScript theme={null}
  import { Client } from "langsmith";

const langsmithClient = new Client({});

const myTracedFunc = traceable(async () => {
    // Your code here...
  },{ client: langsmithClient });

try {
    await myTracedFunc();
  } finally {
    await langsmithClient.flush();
  }
  ```
</CodeGroup>

If you are using LangChain, please refer to our [LangChain tracing guide](/langsmith/trace-with-langchain#ensure-all-traces-are-submitted-before-exiting).

If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Ensure all traces are submitted before exiting

LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.

### Using the LangSmith SDK

If you are using the LangSmith SDK standalone, you can use the `flush` method before exit:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Invoke the augmented LLM

**URL:** llms-txt#invoke-the-augmented-llm

output = structured_llm.invoke("How does Calcium CT score relate to high cholesterol?")

---

## Assistant creation

**URL:** llms-txt#assistant-creation

**Contents:**
  - Filter Operations
- Common Access Patterns
  - Single-Owner Resources
  - Permission-based Access

@auth.on.assistants.create
async def on_assistant_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.assistants.create.value
):
    if "assistants:create" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
python  theme={null}
@auth.on
async def owner_only(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a `thread` would match the `on_thread_create` handler but NOT the `reject_unhandled_requests` handler. A request to `update` a thread, however would be handled by the global handler, since we don't have a more specific handler for that resource and action.

<a id="filter-operations" />

### Filter Operations

Authorization handlers can return `None`, a boolean, or a filter dictionary.

* `None` and `True` mean "authorize access to all underling resources"
* `False` means "deny access to all underling resources (raises a 403 exception)"
* A metadata filter dictionary will restrict access to resources

A filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:

* The default value is a shorthand for exact match, or "\$eq", below. For example, `{"owner": user_id}` will include only resources with metadata containing `{"owner": user_id}`
* `$eq`: Exact match (e.g., `{"owner": {"$eq": user_id}}`) - this is equivalent to the shorthand above, `{"owner": user_id}`
* `$contains`: List membership (e.g., `{"allowed_users": {"$contains": user_id}}`) or list containment (e.g., `{"allowed_users": {"$contains": [user_id_1, user_id_2]}}`). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type.

A dictionary with multiple keys is treated using a logical `AND` filter. For example, `{"owner": org_id, "allowed_users": {"$contains": user_id}}` will only match resources with metadata whose "owner" is `org_id` and whose "allowed\_users" list contains `user_id`.
See the reference [here](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.FilterType) for more information.

## Common Access Patterns

Here are some typical authorization patterns:

### Single-Owner Resources

This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It's useful for common single-user use cases like regular chatbot-style apps.
```

Example 2 (unknown):
```unknown
### Permission-based Access

This pattern lets you control access based on **permissions**. It's useful if you want certain roles to have broader or more restricted access to resources.
```

---

## Log retriever traces

**URL:** llms-txt#log-retriever-traces

Source: https://docs.langchain.com/langsmith/log-retriever-trace

<Note>
  Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.
</Note>

Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.

1. Annotate the retriever step with `run_type="retriever"`.

2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:

* `page_content`: The text of the document.
   * `type`: This should always be "Document".
   * `metadata`: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.

The following code snippets show how to log a retrieval steps in Python and TypeScript.

The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=786c74f63e4c94d35535aa46ac9f38f4" alt="" data-og-width="1614" width="1614" data-og-height="736" height="736" data-path="langsmith/images/retriever-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c2e2720a208cc2402e869e214609ec21 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8949f5484519221d159191e9437a862f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7f376f3ca4575ffc08eb7fc6bdc0db4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06ac1c1dfa24dab91376f2d641fe9aa2 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5046d388f4583d270740adfbd1e58539 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aa3aae9c839dd00ca450cfeb7b285371 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-retriever-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Bulk Exporting Trace Data

**URL:** llms-txt#bulk-exporting-trace-data

**Contents:**
- Destinations
- Exporting Data
  - Destinations - Providing a S3 bucket
  - Preparing the Destination
  - Create an export job
  - Scheduled exports
- Monitoring the Export Job
  - Monitor Export Status
  - List Runs for an Export
  - List All Exports

Source: https://docs.langchain.com/langsmith/data-export

<Info>
  **Plan restrictions apply**

Please note that the Data Export functionality is only supported for [LangSmith Plus or Enterprise tiers](https://www.langchain.com/pricing-langsmith).
</Info>

LangSmith's bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the
data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.

An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.
Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.
Bulk exports also have a runtime timeout of 24 hours.

Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in
[Parquet](https://parquet.apache.org/docs/overview/) columnar format. This format will allow you to easily import the data into
other systems. The data export will contain equivalent data fields as the [Run data format](/langsmith/run-data-format).

### Destinations - Providing a S3 bucket

To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.

The following information is needed for the export:

* **Bucket Name**: The name of the S3 bucket where the data will be exported to.
* **Prefix**: The root prefix within the bucket where the data will be exported to.
* **S3 Region**: The region of the bucket - this is needed for AWS S3 buckets.
* **Endpoint URL**: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.
* **Access Key**: The access key for the S3 bucket.
* **Secret Key**: The secret key for the S3 bucket.

We support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.

### Preparing the Destination

<Note>
  **For self-hosted and EU region deployments**

Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.
  For the EU region, use `eu.api.smith.langchain.com`.
</Note>

<Note>
  **Permissions required**

Both the `backend` and `queue` services require write access to the destination bucket:

* The `backend` service attempts to write a test file to the destination bucket when the export destination is created.
    It will delete the test file if it has permission to do so (delete access is optional).
  * The `queue` service is responsible for bulk export execution and uploading the files to the bucket.
</Note>

The following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.
Note that credentials will be stored securely in an encrypted form in our system.

Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.

#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:

See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:

<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

### Scheduled exports

<Note>
  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)
</Note>

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:

* `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.
* For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.
  Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.
* `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.
* Scheduled exports can be stopped by [cancelling the export](#stop-an-export).
  * Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.
  * If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -
    canceling the source bulk export **does not** cancel the spawned bulk exports.
* Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.

If a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:

| Export | Start Time           | End Time             | Runs At              |
| ------ | -------------------- | -------------------- | -------------------- |
| 1      | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |
| 2      | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |
| 3      | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |

## Monitoring the Export Job

### Monitor Export Status

To monitor the status of an export job, use the following cURL command:

Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.

### List Runs for an Export

An export is typically broken up into multiple runs which correspond to a specific date partition to export.
To list all runs associated with a specific export, use the following cURL command:

This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.

To retrieve a list of all export jobs, use the following cURL command:

This command returns a list of all export jobs along with their current statuses and creation timestamps.

To stop an existing export, use the following cURL command:

Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,
you will need to create a new export job instead.

## Partitioning Scheme

Data will be exported into your bucket into the follow Hive partitioned format:

## Importing Data into other systems

Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:

To import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also
[Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).

You can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).

You can COPY data from S3 or Parquet into Amazon Redshift by following the [AWS COPY command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).

You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:

See [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.

You can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).

### Debugging Destination Errors

The destinations API endpoint will validate that the destination and credentials are valid and that write access is
is present for the bucket.

If you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/)
to test the connectivity to the bucket. You should be able to write a file with the CLI using the same
data that you supplied to the destinations API above.

```bash  theme={null}
aws configure

**Examples:**

Example 1 (unknown):
```unknown
Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

#### AWS S3 bucket

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.
```

Example 2 (unknown):
```unknown
#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:
```

Example 3 (unknown):
```unknown
See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:
```

Example 4 (unknown):
```unknown
<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

### Scheduled exports

<Note>
  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)
</Note>

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:
```

---

## Store with specific fields to embed

**URL:** llms-txt#store-with-specific-fields-to-embed

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "food_preference": "I love Italian cuisine",
        "context": "Discussing dinner plans"
    },
    index=["food_preference"]  # Only embed "food_preferences" field
)

---

## maxReplicas: 20

**URL:** llms-txt#maxreplicas:-20

---

## maxReplicas: 4

**URL:** llms-txt#maxreplicas:-4

---

## Simple text invocation

**URL:** llms-txt#simple-text-invocation

result = llm.invoke("Sing a ballad of LangChain.")
print(result.content)

---

## Evaluation Concepts

**URL:** llms-txt#evaluation-concepts

**Contents:**
- Datasets
  - Examples
  - Dataset curation
  - Splits
  - Versions
- Evaluators
  - Human
  - Heuristic
  - LLM-as-judge
  - Pairwise

Source: https://docs.langchain.com/langsmith/evaluation-concepts

LangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:

* [**Datasets**:](/langsmith/evaluation-concepts#datasets) Collections of test inputs and reference outputs.
* [**Evaluators**](/langsmith/evaluation-concepts#evaluators): Functions for scoring outputs. These can be [online evaluators](/langsmith/evaluation-concepts#online-evaluation) that run on traces in real time or [offline evaluators](/langsmith/evaluation-concepts#offline-evaluation) that run on a dataset.

A dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=be2adaa8535dbb253bc0f199895da2e1" alt="Dataset" data-og-width="1279" width="1279" data-og-height="495" height="495" data-path="langsmith/images/dataset-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b86cc15fe64ec6fdac0a14472b84c1e0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b388730259225fbf466b9a8682303330 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bf8930b4598fbc88212e6e3dde2d0950 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10220cebf437b481fd655d8e1ddca492 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=16e3d46679352d87e6358acc155d0bd9 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8b987fee69c354d78e7182a685ac38ed 2500w" />

Each example consists of:

* **Inputs**: a dictionary of input variables to pass to your application.
* **Reference outputs** (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.
* **Metadata** (optional): a dictionary of additional information that can be used to create filtered views of a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0c25674bd30e502e3034b754b8649d66" alt="Example" data-og-width="1281" width="1281" data-og-height="406" height="406" data-path="langsmith/images/example-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f49d9e298b397039ae97266ad8eda6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=eb14858aac0f28b26cb1e6ee9ca920f4 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ae707f4b153d7c6e77215012018483c7 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9791ddf51677e2e4f046f32e46304e3d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=02b7df4950f45c8d4460b0b0cfad88c0 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=db7340730c6025b03bcb96339fdc980e 2500w" />

There are various ways to build datasets for evaluation, including:

#### Manually curated examples

This is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what "good" responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.

#### Historical traces

Once you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they're, well, the most realistic!

If you're getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:

* **User feedback**: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.
* **Heuristics**: You can also use other heuristics to identify "interesting" datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.
* **LLM feedback**: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly.

Once you have a few examples, you can try to artificially generate some more. It's generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.

When setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.

Learn how to [create and manage dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).

Datasets are [versioned](/langsmith/manage-datasets#version-a-dataset) such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also [tag versions](/langsmith/manage-datasets#tag-a-version) of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history.

You can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn't accidentally break your CI pipelines.

Evaluators are functions that score how well your application performs on a particular example.

#### Evaluator inputs

Evaluators receive these inputs:

* [Example](/langsmith/evaluation-concepts#examples): The example(s) from your [Dataset](/langsmith/evaluation-concepts#datasets). Contains inputs, (reference) outputs, and metadata.
* [Run](/langsmith/observability-concepts#runs): The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.

#### Evaluator outputs

An evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:

* `key`: The name of the metric.
* `score` | `value`: The value of the metric. Use `score` if it's a numerical metric and `value` if it's categorical.
* `comment` (optional): The reasoning or additional string information justifying the score.

#### Defining evaluators

There are a number of ways to define and run evaluators:

* **Custom code**: Define [custom evaluators](/langsmith/code-evaluator) as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.
* **Built-in evaluators**: LangSmith has a number of built-in evaluators that you can configure and run via the UI.

You can run evaluators using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)), via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground), or by configuring [Rules](/langsmith/rules) to automatically run them on particular tracing projects or datasets.

#### Evaluation techniques

There are a few high-level approaches to LLM evaluation:

Human evaluation is [often a great starting point for evaluation](https://hamel.dev/blog/posts/evals/#looking-at-your-traces). LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).

LangSmith's [annotation queues](/langsmith/evaluation-concepts#annotation-queues) make it easy to get human feedback on your application's outputs.

Heuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot's response isn't empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.

LLM-as-judge evaluators use LLMs to score the application's output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference).

With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.

Learn about [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

Pairwise evaluators allow you to compare the outputs of two versions of an application. Think [LMSYS Chatbot Arena](https://chat.lmsys.org/) - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic ("which response is longer"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).

**When should you use pairwise evaluation?**

Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

Each time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see [how to analyze experiment results](/langsmith/analyze-an-experiment).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89c78822157136d0e28e9a110dbdbfd5" alt="Experiment view" data-og-width="1633" width="1633" data-og-height="942" height="942" data-path="langsmith/images/experiment-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f911d77a92d1fb020d6ddea937bd224e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=11c80e9c9aad7df87ea97a540170809a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5b3722ed4fa19b7fcd7d73454dfd342a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d5e419a79137652aeac9139473414a 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a79633866510d7f8ea1d92fa6f58138 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=53012b35d0f323587d5956b3a800818b 2500w" />

Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can [compare multiple experiments in a comparison view](/langsmith/compare-experiment-results).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

## Experiment configuration

LangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.

Running an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.

Repetitions can be configured by passing the `num_repetitions` argument to `evaluate` / `aevaluate` ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.

To learn more about running repetitions on experiments, read the [how-to-guide](/langsmith/repetition).

By passing the `max_concurrency` argument to `evaluate` / `aevaluate`, you can specify the concurrency of your experiment. The `max_concurrency` argument has slightly different semantics depending on whether you are using `evaluate` or `aevaluate`.

The `max_concurrency` argument to `evaluate` specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.

The `max_concurrency` argument to `aevaluate` is fairly similar to `evaluate`, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. `aevaluate` works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The `max_concurrency` argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.

Lastly, you can also cache the API calls made in your experiment by setting the `LANGSMITH_TEST_CACHE` to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up.

Human feedback is often the most valuable feedback you can gather on your application. With [annotation queues](/langsmith/annotation-queues) you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a [dataset](/langsmith/evaluation-concepts#datasets) for future evaluations. While you can always [annotate runs inline](/langsmith/annotate-traces-inline), annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.

Learn more about [annotation queues and human feedback](/langsmith/annotation-queues).

## Offline evaluation

Evaluating an application on a dataset is what we call "offline" evaluation. It is offline because we're evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application's outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.

You can run offline evaluations client-side using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)). You can run them server-side via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground) or by configuring [automations](/langsmith/rules) to run certain evaluators on every new experiment against a specific dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=879e4ee3616cecd7cff39879cfc6ec7b" alt="Offline" data-og-width="1581" width="1581" data-og-height="477" height="477" data-path="langsmith/images/offline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba01953933bebf30c6dc5d8112a3b3db 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3424f1efa82db871cba04c9a4bcac188 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ad0eca755f778a844465976b00a3efb6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cddc86584ad7d9e82a60fc219cff886b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e8cf5a07175523921ee1595e36ea1d73 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0f94c55d22273e06d6cde301b4a0a3f3 2500w" />

Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q\&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made.

Unit tests are used in software development to verify the correctness of individual system components. [Unit tests in the context of LLMs are often rule-based assertions](https://hamel.dev/blog/posts/evals/#level-1-unit-tests) on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.

Unit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).

Regression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.

LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

Backtesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.

This is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.

### Pairwise evaluation

For some tasks [it is easier](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/) for a human or LLM grader to determine if "version A is better than B" than to assign an absolute score to either A or B. Pairwise evaluations are just this — a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine "Which of these two summaries is more clear and concise?" than to give an absolute score like "Give this summary a score of 1-10 in terms of clarity and concision."

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

Evaluating a deployed application's outputs in (roughly) realtime is what we call "online" evaluation. In this case there is no dataset involved and no possibility of reference outputs — we're running evaluators on real inputs and real outputs as they're produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation.

Online evaluators are generally intended to be run server-side. LangSmith has built-in [LLM-as-judge evaluators](/langsmith/llm-as-judge) that you can configure, or you can define custom code evaluators that are also run within LangSmith.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d6c1b932e5487c4c01d84ae4f984240" alt="Online" data-og-width="1474" width="1474" data-og-height="521" height="521" data-path="langsmith/images/online.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd658bcfa1357196dd87ab6263a4896d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=371e394c68e91e93efe1c80fb85d5484 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c65ce1c6487a959ce54bffcf8155bb8 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b8e1815f6df4419f65294d2a658bc9f0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f737a486fdb5232e8db121a760075dd8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0ad2d3d938b8048c21f23f0052904940 2500w" />

### Evaluations vs testing

Testing and evaluation are very similar and overlapping concepts that often get confused.

**An evaluation measures performance according to a metric(s).** Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they're often used to compare two systems against each other rather than to assert something about an individual system.

**Testing asserts correctness.** A system can only be deployed if it passes all tests.

Evaluation metrics can be *turned into* tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics.

It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.

You can also choose to write evaluations using standard software testing tools like `pytest` or `vitest/jest` out of convenience.

### Using `pytest` and `Vitest/Jest`

The LangSmith SDKs come with integrations for [pytest](/langsmith/pytest) and [`Vitest/Jest`](/langsmith/vitest-jest). These make it easy to:

* Track test results in LangSmith
* Write evaluations as tests

Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.

Writing evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow.

Using testing tools is also helpful when you want to *both* evaluate your system's outputs *and* assert some basic things about them.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## minReplicas: 8

**URL:** llms-txt#minreplicas:-8

**Contents:**
- Note that we are actively working on improving performance of this service to reduce the number of replicas.

## Note that we are actively working on improving performance of this service to reduce the number of replicas.
queue:
  deployment:
    replicas: 160 # OR enable autoscaling to this level (example below)

---

## >    Interrupt(

**URL:** llms-txt#>----interrupt(

---

## Instrument CrewAI and OpenAI

**URL:** llms-txt#instrument-crewai-and-openai

CrewAIInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Deploy

**URL:** llms-txt#deploy

**Contents:**
- Prerequisites
- Deploy your agent
  - 1. Create a repository on GitHub
  - 2. Deploy to LangSmith
  - 3. Test your application in Studio
  - 4. Get the API URL for your deployment
  - 5. Test the API

Source: https://docs.langchain.com/oss/python/langgraph/deploy

LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is **purpose-built for stateful, long-running agents**, so you can go from repo to reliable cloud deployment in minutes.

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

### 1. Create a repository on GitHub

Your application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langgraph/studio#setup-local-langgraph-server). Then, push your code to the repository.

### 2. Deploy to LangSmith

<Steps>
  <Step title="Navigate to LangSmith Deployments">
    Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select **Deployments**.
  </Step>

<Step title="Create new deployment">
    Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
  </Step>

<Step title="Link repository">
    If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.
  </Step>

<Step title="Deploy repository">
    Select your application's repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.
  </Step>
</Steps>

### 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### 4. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python">
    1. Install LangGraph Python:

2. Send a message to the agent:

<Tab title="Rest API">
    
  </Tab>
</Tabs>

<Tip>
  LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Hosting overview](/langsmith/hosting).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/deploy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the agent:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Rest API">
```

---

## ttl_period_seconds:

**URL:** llms-txt#ttl_period_seconds:

---

## How to handle model rate limits

**URL:** llms-txt#how-to-handle-model-rate-limits

**Contents:**
- Using `langchain` RateLimiters (Python only)
- Retrying with exponential backoff
- Limiting max\_concurrency

Source: https://docs.langchain.com/langsmith/rate-limiting

A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.

## Using `langchain` RateLimiters (Python only)

If you're using `langchain` Python ChatModels in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.

See the [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

See the `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting max\_concurrency

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See the `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting max\_concurrency

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The private data is only shared between node_1 and node_2

**URL:** llms-txt#the-private-data-is-only-shared-between-node_1-and-node_2

def node_1(state: OverallState) -> Node1Output:
    output = {"private_data": "set by node_1"}
    print(f"Entered node `node_1`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## Include multimodal content in a prompt

**URL:** llms-txt#include-multimodal-content-in-a-prompt

**Contents:**
- Inline content
- Template variables
- Populate the template variable
- Run an evaluation

Source: https://docs.langchain.com/langsmith/multimodal-content

Some applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, you'll want to include multimodal content in your prompt and test the model's ability to answer questions about the content.

The LangSmith Playground supports two methods for incorporating multimodal content in your prompts:

1. Inline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the model's responses.

2. Template variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:

* Test how the model handles different inputs
   * Create reusable prompts that work with varying content

<Note>
  Not all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.
</Note>

Click the file icon in the message where you want to add multimodal content. Under the `Upload content` tab, you can upload a file and include it inline in the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b93d2a6731d26d3ff58a3d0d6d909159" alt="" data-og-width="410" width="410" data-og-height="339" height="339" data-path="langsmith/images/upload-inline-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ae329136b8de8d5f4b12ecab49651063 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f1644a04d5a33e803a15bc9794ecb528 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=275ffe46010c6b24012ed049a17a59fb 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9591d26a3b27c0462f7674f84c2a525c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f4439dc91e005750232b40efbb19954d 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1c429f8f78846a14e0c9a379b9a24355 2500w" />

## Template variables

Click the file icon in the message where you want to add multimodal content. Under the `Template variables` tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e69596dd9fe7d16252c7054bf9efcdf0" alt="" data-og-width="391" width="391" data-og-height="303" height="303" data-path="langsmith/images/template-variable-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1efb9ed1c9a4a64be7174b90ffbfe664 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1375989ffa599f506d308441e46907f9 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50c62b520314c18e433f77952ff79a2 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e38e8283352a13b54e96a381a2449396 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331ff1a4664debd8d7b08a6ad645d9c6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ffa740fa2a3f4f52173cd073843d697f 2500w" />

## Populate the template variable

Once you've added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the `+` button to upload or select content that will be used to populate the template variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5983e91ca9f596918c9068f8d7450d8d" alt="" data-og-width="1466" width="1466" data-og-height="482" height="482" data-path="langsmith/images/manual-prompt-multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ccb36221eb97f54196de13df4f9749e7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b4554ac3156e805655eba6afd2ce771 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3e85646401f9aa31e1d79257344c158 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9727ce9dec69152f6e49e3d8ff575300 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f73957416f51c78a0e29c449f64ff1e2 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ab8421f512b312b56ac515b970d337f4 2500w" />

After testing out your prompt manually, you can [run an evaluation](/langsmith/evaluate-with-attachments?mode=ui) to see how the prompt performs over a golden dataset of examples.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multimodal-content.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to evaluate an LLM application

**URL:** llms-txt#how-to-evaluate-an-llm-application

**Contents:**
- Define an application
- Create or select a dataset
- Define an evaluator
- Run the evaluation
- Explore the results[​](#explore-the-results "Direct link to Explore the results")
- Reference code[​](#reference-code "Direct link to Reference code")
- Related[​](#related "Direct link to Related")

Source: https://docs.langchain.com/langsmith/evaluate-llm-application

This guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets)
</Info>

In this guide we'll go over how to evaluate an application using the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method in the LangSmith SDK.

<Check>
  For larger evaluation jobs in Python we recommend using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), the asynchronous version of [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on [running an evaluation asynchronously](/langsmith/evaluation-async).

In JS/TS evaluate() is already asynchronous so no separate method is needed.

It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.
</Check>

## Define an application

First we need an application to evaluate. Let's create a simple toxicity classifier for this example.

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

## Run the evaluation

We'll use the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) methods to run the evaluation.

The key arguments are:

* a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each [Example](/langsmith/example-data-format) is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.
* `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
* `evaluators` - a list of evaluators to score the outputs of the function

Python: Requires `langsmith>=0.3.13`

## Explore the results[​](#explore-the-results "Direct link to Explore the results")

Each invocation of `evaluate()` creates an [Experiment](/langsmith/evaluation-concepts#experiments) which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.

*If you've annotated your code for tracing, you can open the trace of each row in a side panel view.*

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-experiment.gif?s=252d96dbd2100a691f1d3b61716fde38" alt="" data-og-width="1132" width="1132" data-og-height="720" height="720" data-path="langsmith/images/view-experiment.gif" data-optimize="true" data-opv="3" />

## Reference code[​](#reference-code "Direct link to Reference code")

<Accordion title="Click to see a consolidated code snippet">
  <CodeGroup>

</CodeGroup>
</Accordion>

## Related[​](#related "Direct link to Related")

* [Run an evaluation asynchronously](/langsmith/evaluation-async)
* [Run an evaluation via the REST API](/langsmith/run-evals-api-only)
* [Run an evaluation from the prompt playground](/langsmith/run-evaluation-from-prompt-playground)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-llm-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

<CodeGroup>
```

---

## Instrument Google ADK directly

**URL:** llms-txt#instrument-google-adk-directly

**Contents:**
  - 3. Create and run your ADK agent

GoogleADKInstrumentor().instrument()
python  theme={null}
import asyncio
from langsmith.integrations.otel import configure
from google.adk import Runner
from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.genai import types

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set any OpenTelemetry environment variables or configure exporters manually—`configure()` handles everything automatically.
</Note>

### 3. Create and run your ADK agent

Once configured, your Google ADK application will automatically send traces to LangSmith:

This example includes a minimal app that sets up an agent, session, and runner, then sends a message and streams events.
```

---

## How to define an LLM-as-a-judge evaluator

**URL:** llms-txt#how-to-define-an-llm-as-a-judge-evaluator

**Contents:**
- SDK
  - Pre-built evaluators
  - Create your own LLM-as-a-judge evaluator

Source: https://docs.langchain.com/langsmith/llm-as-judge

<Info>
  * [LLM-as-a-judge evaluator](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.

This guide shows you how to define an LLM-as-a-judge evaluator for [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](/langsmith/online-evaluations#configure-llm-as-judge-evaluators).

### Pre-built evaluators

Pre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](/langsmith/prebuilt-evaluators) for how to use pre-built evaluators with LangSmith.

### Create your own LLM-as-a-judge evaluator

For complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).

Requires `langsmith>=0.2.0`

```python  theme={null}
from langsmith import evaluate, traceable, wrappers, Client
from openai import OpenAI

---

## Model for performing extraction.

**URL:** llms-txt#model-for-performing-extraction.

info_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    PurchaseInformation, method="json_schema", include_raw=True
)

---

## LLM-as-judge output schema

**URL:** llms-txt#llm-as-judge-output-schema

class Grade(TypedDict):
    """Compare the expected and actual answers and grade the actual answer."""
    reasoning: Annotated[str, ..., "Explain your reasoning for whether the actual response is correct or not."]
    is_correct: Annotated[bool, ..., "True if the student response is mostly or exactly correct, otherwise False."]

---

## Long-term files:

**URL:** llms-txt#long-term-files:

---

## AzureOpenAIEmbeddings

**URL:** llms-txt#azureopenaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/azure_openai

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

This will help you get started with AzureOpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).

<Info>
  **Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.**

If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.
</Info>

### Integration details

| Class                                                                                                     | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/azure_openai/) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureOpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                             ✅                                             | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access Azure OpenAI embedding models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.

You'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## Query traces (SDK)

**URL:** llms-txt#query-traces-(sdk)

**Contents:**
- Use filter arguments
  - List all runs in a project
  - List LLM and Chat runs in the last 24 hours
  - List root runs in a project
  - List runs without errors
  - List runs by run ID
- Use filter query language
  - List all root runs in a conversational thread
  - List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1
  - List runs with "star\_rating" key whose score is greater than 4

Source: https://docs.langchain.com/langsmith/export-traces

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Run (span) data format](/langsmith/run-data-format)
  * <RegionalUrl type="api" suffix="/redoc" text="LangSmith API Reference" />
  * [LangSmith trace query syntax](/langsmith/trace-query-syntax)
</Tip>

<Note>
  **If you are looking to export a large volume of traces, we recommend that you use the [Bulk Data Export](./data-export) functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.**
</Note>

The recommended way to query runs (the span data in LangSmith traces) is to use the `list_runs` method in the SDK or `/runs/query` endpoint in the API.

LangSmith stores traces in a simple format that is specified in the [Run (span) data format](/langsmith/run-data-format).

## Use filter arguments

For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the [filter arguments reference](/langsmith/trace-query-syntax#filter-arguments).

<Warning>
  **Prerequisites**

Initialize the client before running the below code snippets.
</Warning>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

### List LLM and Chat runs in the last 24 hours

### List root runs in a project

Root runs are runs that have no parents. These are assigned a value of `True` for `is_root`. You can use this to filter for root runs.

### List runs without errors

### List runs by run ID

<Warning>
  **Ignores Other Arguments**

If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.
</Warning>

If you have a list of run IDs, you can list them directly:

## Use filter query language

For more complex queries, you can use the query language described in the [filter query language reference](/langsmith/trace-query-syntax#filter-query-language).

### List all root runs in a conversational thread

This is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our [how-to guide on setting up threads](./threads).
Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: `session_id`, `conversation_id`, or `thread_id`. The session ID is also known as the tracing project ID. The following query matches on any of them.

### List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1

### List runs with "star\_rating" key whose score is greater than 4

### List runs that took longer than 5 seconds to complete

### List all runs that have "error" not equal to null

### List all runs where start\_time is greater than a specific timestamp

### List all runs that contain the string "substring"

### List all runs that are tagged with the git hash "2aa1cf4"

### List all runs that started after a specific timestamp and either have "error" not equal to null or a "Correctness" feedback score equal to 0

### Complex query: List all runs where tags include "experimental" or "beta" and latency is greater than 2 seconds

### Search trace trees by full text

You can use the `search()` function without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.

### Check for presence of metadata

If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.

### Check for environment details in metadata

A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:

### Check for conversation ID in metadata

Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.

### Negative filtering on key-value pairs

You can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.

### Combine multiple filters

If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here's how you can search for runs named "ChatOpenAI" that also have a specific `conversation_id` in their metadata:

List all runs named "RetrieveDocs" whose root run has a "user\_score" feedback of 1 and any run in the full trace is named "ExpandQuery".

This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.

### Advanced: export flattened trace view with child tool usage

The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces.

This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.

To optimize the query, the example:

1. Selects only the necessary fields when querying tool runs to reduce query time.
2. Fetches root runs in batches while processing tool runs concurrently.

<CodeGroup>
  
</CodeGroup>

### Advanced: export retriever IO for traces with feedback

This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.

<CodeGroup>
  
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### List LLM and Chat runs in the last 24 hours

<CodeGroup>
```

---

## if configured with a subdomain / path prefix:

**URL:** llms-txt#if-configured-with-a-subdomain-/-path-prefix:

**Contents:**
  - Process the API response with jq

curl http://<langsmith_url/prefix/api/v1/info
json  theme={null}
{
  "version": "0.11.4",
  "license_expiration_time": "2026-08-18T19:14:34Z",
  "customer_info": {
    "customer_id": "<id>",
    "customer_name": "<name>"
  }
}
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will return a JSON response containing your customer information:
```

Example 2 (unknown):
```unknown
Extract the `customer_id` and `customer_name` from this response to use as input for the export scripts.

### Process the API response with jq

You can use [jq](https://jqlang.org/download) to parse the JSON response and set bash variables for use in your scripts:
```

---

## The resume payload becomes the return value of interrupt() inside the node

**URL:** llms-txt#the-resume-payload-becomes-the-return-value-of-interrupt()-inside-the-node

**Contents:**
- Common patterns
  - Approve or reject

graph.invoke(Command(resume=True), config=config)
python  theme={null}
from typing import Literal
from langgraph.types import interrupt, Command

def approval_node(state: State) -> Command[Literal["proceed", "cancel"]]:
    # Pause execution; payload shows up under result["__interrupt__"]
    is_approved = interrupt({
        "question": "Do you want to proceed with this action?",
        "details": state["action_details"]
    })

# Route based on the response
    if is_approved:
        return Command(goto="proceed")  # Runs after the resume payload is provided
    else:
        return Command(goto="cancel")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Key points about resuming:**

* You must use the **same thread ID** when resuming that was used when the interrupt occurred
* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call
* The node restarts from the beginning of the node where the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called when resumed, so any code before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) runs again
* You can pass any JSON-serializable value as the resume value

## Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

* <Icon icon="check-circle" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)
* <Icon icon="pencil" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing
* <Icon icon="wrench" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution
* <Icon icon="shield-check" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input

### Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.
```

Example 2 (unknown):
```unknown
When you resume the graph, pass `true` to approve or `false` to reject:
```

---

## LangSmith Python SDK

**URL:** llms-txt#langsmith-python-sdk

Source: https://docs.langchain.com/langsmith/smith-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-python-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to implement generative user interfaces with LangGraph

**URL:** llms-txt#how-to-implement-generative-user-interfaces-with-langgraph

**Contents:**
- Tutorial
  - 1. Define and configure UI components
  - 2. Send the UI components in your graph
  - 3. Handle UI elements in your React application
- How-to guides
  - Provide custom components on the client side
  - Show loading UI when components are loading
  - Customise the namespace of UI components.
  - Access and interact with the thread state from the UI component
  - Pass additional context to the client components

Source: https://docs.langchain.com/langsmith/generative-ui-react

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [LangGraph Server](/langsmith/langgraph-server)
  * [`useStream()` React Hook](/langsmith/use-stream-react)
</Info>

Generative user interfaces (Generative UI) allows agents to go beyond text and generate rich user interfaces. This enables creating more interactive and context-aware applications where the UI adapts based on the conversation flow and AI responses.

<img src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=105943c6c28853fad0a9bc3b4af3a999" alt="Agent Chat showing a prompt about booking/lodging and a generated set of hotel listing cards (images, titles, prices, locations) rendered inline as UI components." data-og-width="1814" width="1814" data-og-height="898" height="898" data-path="langsmith/images/generative-ui-sample.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0fd526a7132d33ab6f72002d68a66dec 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0c9ffe86700a7b8404f1fdf51b906aa1 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=50652e58566db8171ead4aef57d78fa6 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a764d790719e8233313fabe4cee93958 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a02d8d6ecace7eee6df55e3a391c09e2 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b0709ca94bd9533f5ef5a80da1d60bf6 2500w" />

LangSmith supports colocating your React components with your graph code. This allows you to focus on building specific UI components for your graph while easily plugging into existing chat interfaces such as [Agent Chat](https://agentchat.vercel.app) and loading the code only when actually needed.

### 1. Define and configure UI components

First, create your first UI component. For each component you need to provide an unique identifier that will be used to reference the component in your graph code.

Next, define your UI components in your `langgraph.json` configuration:

The `ui` section points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see [Customise the namespace of UI components](#customise-the-namespace-of-ui-components) for more details.

LangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the `LoadExternalComponent` component. Some dependencies such as `react` and `react-dom` will be automatically excluded from the bundle.

CSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as `shadcn/ui` in your UI components.

<Tabs>
  <Tab title="src/agent/ui.tsx">
    
  </Tab>

<Tab title="src/agent/styles.css">
    
  </Tab>
</Tabs>

### 2. Send the UI components in your graph

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    Use the `typedUi` utility to emit UI elements from your agent nodes:

### 3. Handle UI elements in your React application

On the client side, you can use `useStream()` and `LoadExternalComponent` to display the UI elements.

Behind the scenes, `LoadExternalComponent` will fetch the JS and CSS for the UI components from LangSmith and render them in a shadow DOM, thus ensuring style isolation from the rest of your application.

### Provide custom components on the client side

If you already have the components loaded in your client application, you can provide a map of such components to be rendered directly without fetching the UI code from LangSmith.

### Show loading UI when components are loading

You can provide a fallback UI to be rendered when the components are loading.

### Customise the namespace of UI components.

By default `LoadExternalComponent` will use the `assistantId` from `useStream()` hook to fetch the code for UI components. You can customise this by providing a `namespace` prop to the `LoadExternalComponent` component.

<Tabs>
  <Tab title="src/app/page.tsx">
    
  </Tab>

<Tab title="langgraph.json">
    
  </Tab>
</Tabs>

### Access and interact with the thread state from the UI component

You can access the thread state inside the UI component by using the `useStreamContext` hook.

### Pass additional context to the client components

You can pass additional context to the client components by providing a `meta` prop to the `LoadExternalComponent` component.

Then, you can access the `meta` prop in the UI component by using the `useStreamContext` hook.

### Streaming UI messages from the server

You can stream UI messages before the node execution is finished by using the `onCustomEvent` callback of the `useStream()` hook. This is especially useful when updating the UI component as the LLM is generating the response.

Then you can push updates to the UI component by calling `ui.push()` / `push_ui_message()` with the same ID as the UI message you wish to update.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>

<Tab title="ui.tsx">
    
  </Tab>
</Tabs>

### Remove UI messages from state

Similar to how messages can be removed from the state by appending a RemoveMessage you can remove an UI message from the state by calling `remove_ui_message` / `ui.delete` with the ID of the UI message.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

* [JS/TS SDK Reference](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/generative-ui-react.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Next, define your UI components in your `langgraph.json` configuration:
```

Example 2 (unknown):
```unknown
The `ui` section points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see [Customise the namespace of UI components](#customise-the-namespace-of-ui-components) for more details.

LangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the `LoadExternalComponent` component. Some dependencies such as `react` and `react-dom` will be automatically excluded from the bundle.

CSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as `shadcn/ui` in your UI components.

<Tabs>
  <Tab title="src/agent/ui.tsx">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="src/agent/styles.css">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

### 2. Send the UI components in your graph

<Tabs>
  <Tab title="Python">
```

---

## client.py

**URL:** llms-txt#client.py

from langsmith.run_helpers import get_current_run_tree, traceable
import httpx

@traceable
async def my_client_function():
    headers = {}
    async with httpx.AsyncClient(base_url="...") as client:
        if run_tree := get_current_run_tree():
            # add langsmith-id to headers
            headers.update(run_tree.to_headers())
        return await client.post("/my-route", headers=headers)
python  theme={null}
from langsmith import traceable
from langsmith.middleware import TracingMiddleware
from fastapi import FastAPI, Request

app = FastAPI()  # Or Flask, Django, or any other framework
app.add_middleware(TracingMiddleware)

@traceable
async def some_function():
    ...

@app.post("/my-route")
async def fake_route(request: Request):
    return await some_function()
python  theme={null}
from starlette.applications import Starlette
from starlette.middleware import Middleware
from langsmith.middleware import TracingMiddleware

routes = ...
middleware = [
    Middleware(TracingMiddleware),
]
app = Starlette(..., middleware=middleware)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Then the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith's `TracingMiddleware`.

<Info>
  The `TracingMiddleware` class was added in `langsmith==0.1.133`.
</Info>

Example using FastAPI:
```

Example 2 (unknown):
```unknown
Or in Starlette:
```

Example 3 (unknown):
```unknown
If you are using other server frameworks, you can always "receive" the distributed trace by passing the headers in through `langsmith_extra`:
```

---

## Self-hosted LangSmith

**URL:** llms-txt#self-hosted-langsmith

**Contents:**
- LangSmith
  - Services
  - Storage services
  - Setup methods
  - Setup guides
- LangSmith with Deployment
  - Requirements
  - Supported compute platforms
  - Setup guide
- Standalone Server

Source: https://docs.langchain.com/langsmith/self-hosted

<Note>
  **Important**<br />
  Self-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to [Pricing](https://www.langchain.com/pricing). [Contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Note>

LangSmith supports different self-hosted configurations depending on your scale, security, and infrastructure needs. This page provides an overview of the supported self-hosted models:

* **[LangSmith](#langsmith)**: Deploy an instance of the LangSmith application that includes observability, tracing, and evaluations in the UI and API. Best for teams who want self-hosted monitoring and evaluation without deploying agents.
* **[LangSmith with deployment](#langsmith-with-deployment)**: Deploy a *graph* to LangGraph Server via the control plane. The control plane and data plane form the full LangSmith platform, providing UI and API management for running and monitoring agents. This includes observability, evaluation, and deployment management.
* **[Standalone server](#standalone-server)**: Deploy a LangGraph Server directly without the control plane UI. Ideal for lightweight setups running one or a few agents as independent services, with full control over scaling and integration.

| Model                         | Includes                                                                                                                                                                                                 | Best for                                                                                                                                                                               | Methods                                                                                                                                               |
| ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **LangSmith**                 | <ul><li>LangSmith app (UI + API)</li><li>Backend services (queue, playground, ACE)</li><li>Datastores: PostgreSQL, Redis, ClickHouse, optional blob storage</li></ul>                                    | <ul><li>Teams who need self-hosted observability, tracing, and evaluation</li><li>Running the LangSmith app without deploying agents/graphs</li></ul>                                  | <ul><li>Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li></ul>                                                                    |
| **LangSmith with deployment** | <ul><li>Everything from LangSmith</li><li>Control plane (deployments UI, revision management, Studio)</li><li>Data plane (LangGraph Server pods)</li><li>Kubernetes operator for orchestration</li></ul> | <ul><li>Enterprise teams needing a private LangChain Cloud</li><li>Centralized UI/API for managing multiple agents/graphs</li><li>Integrated observability and orchestration</li></ul> | <ul><li>Kubernetes with Helm (required)</li><li>Runs on EKS, GKE, AKS, or self-managed clusters</li></ul>                                             |
| **Standalone server**         | <ul><li>LangGraph Server container(s)</li><li>Requires PostgreSQL + Redis (shared or dedicated)</li><li>Optional LangSmith integration for tracing</li></ul>                                             | <ul><li>Lightweight deployments of one or a few agents</li><li>Integrating LangGraph Servers as microservices</li><li>Teams preferring to manage scaling & CI/CD themselves</li></ul>  | <ul><li>Docker / Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li><li>Any container runtime or VM (ECS, EC2, ACI, etc.)</li></ul> |

<Note>
  For a guide on deployment, refer to:

* [How to deploy the Self-Hosted Full Platform](/langsmith/deploy-self-hosted-full-platform)
  * [How to deploy the Self-Hosted Standalone Server](/langsmith/deploy-standalone-server)

Supported compute platforms: [Kubernetes](https://kubernetes.io/) (for Control Plane), any compute platform (for Standalone Server Only)
</Note>

Deploy an instance of the LangSmith application that includes observability, tracing, and evaluations in the UI and API—but **without** the ability to deploy agents through the control plane.

* LangSmith frontend UI
* LangSmith backend API
* LangSmith Platform backend
* LangSmith Playground
* LangSmith queue
* LangSmith ACE (Arbitrary Code Execution) backend

**Storage services:**

* ClickHouse (traces and feedback data)
* PostgreSQL (operational data)
* Redis (queuing and caching)
* Blob storage (optional, but recommended for production)

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />

To access the LangSmith UI and send API requests, you will need to expose the [LangSmith frontend](#langsmith-frontend) service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.

| Service                                                                                                        | Description                                                                                                                                                                                                                                                                                                                                              |
| -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <a id="langsmith-frontend" /> **LangSmith frontend**                                                           | The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.                                                                                                                                                |
| <a id="langsmith-backend" /> **LangSmith backend**                                                             | The backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.                                                                                                      |
| <a id="langsmith-queue" /> **LangSmith queue**                                                                 | The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database. |
| <a id="langsmith-platform-backend" /> **LangSmith platform backend**                                           | The platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.                                                                                                                                                                                                                      |
| <a id="langsmith-playground" /> **LangSmith playground**                                                       | The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.                                                                                                                                                         |
| <a id="langsmith-ace-arbitrary-code-execution-backend" /> **LangSmith ACE (Arbitrary Code Execution) backend** | The ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.                                                                                                                                                                                                |

<Note>
  LangSmith will bundle all storage services by default. You can configure it to use external versions of all storage services. In a production setting, we **strongly recommend using external storage services**.
</Note>

| Service                                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| <a id="clickhouse" /> **ClickHouse**     | [ClickHouse](https://clickhouse.com/docs/en/intro) is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).<br /><br />LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).                                                                                                                                                                                      |
| <a id="postgresql" /> **PostgreSQL**     | [PostgreSQL](https://www.postgresql.org/about/) is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.<br /><br />LangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).                                                        |
| <a id="redis" /> **Redis**               | [Redis](https://github.com/redis/redis) is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.<br /><br />LangSmith uses Redis to back queuing and caching operations.                                                                                                                                                                                                  |
| <a id="blob-storage" /> **Blob storage** | LangSmith supports several blob storage providers, including [AWS S3](https://aws.amazon.com/s3/), [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/), and [Google Cloud Storage](https://cloud.google.com/storage).<br /><br />LangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments. |

* **Docker Compose** (development/testing only)
* **Kubernetes + Helm** (recommended for production)

* [Install on Kubernetes](/langsmith/kubernetes) (production)
* [Install with Docker](/langsmith/docker) (development only)

## LangSmith with Deployment

**LangSmith with deployment** builds on top of the [LangSmith](#langsmith) option. Enabling deployment is ideal for enterprise teams who want a centralized, UI-driven platform to deploy and manage multiple agents and graphs, with all infrastructure, data, and orchestration fully under their control.

This includes everything from [LangSmith](#langsmith), plus:

| Component                                                                                                    | Responsibilities                                                                                                                                        | Where it runs | Who manages it |
| ------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | -------------- |
| <Tooltip tip="The LangSmith UI and APIs for managing deployments.">Control plane</Tooltip>                   | <ul><li>UI for creating deployments & revisions</li><li>APIs for deployment management</li></ul>                                                        | Your cloud    | You            |
| <Tooltip tip="The runtime environment where your LangGraph Servers and agents execute.">Data plane</Tooltip> | <ul><li>Operator/listener to reconcile deployments</li><li>LangGraph Servers (agents/graphs)</li><li>Backing services (Postgres, Redis, etc.)</li></ul> | Your cloud    | You            |

You run both the control plane and the data plane entirely within your own infrastructure. You are responsible for provisioning and managing all components.

<Note>
  Learn more about the [control plane](/langsmith/control-plane) and [data plane](/langsmith/data-plane) architecture concepts.
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=78861d689cf446190d0c1e80ed1a65dd" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=28f2db3fb8d323baa1ed925fa39a0f82 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=724b17ae55c6fc19c8ced2d8e081bcb3 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=46d965e47f66ece91077b1dfdf56196b 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d723ddff8b6942e8fcf15fe5523102b0 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=06932f510e15f96376c67c790dcb31e3 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c482dbbe51709c394194bb3ea7842c09 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f5253bab42b0b351e1d13489fea4311b" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ba56323cd1e66109847f8adea173063c 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6e529069bd427fde7c940ab944f0a1ba 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ee4ed77d09b804dc2de32af0ddedbfbe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ce52832ec7c18f9dc374546651545c42 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=8c8781ac82576d68faa8fc11c4b47a83 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a0b4f5a8a31a23136d03160744305700 2500w" />

1. You must already have a [self-hosted LangSmith instance](#langsmith) installed in your cloud
2. Kubernetes cluster (required for control plane and data plane)
3. Use `langgraph-cli` or [Studio](/langsmith/studio) to test your graph locally
4. Build a Docker image with `langgraph build`
5. Deploy your LangGraph Server via the LangSmith control plane UI or through your container tooling of choice
6. All agents are deployed as Kubernetes services behind the ingress configured for your LangSmith instance

### Supported compute platforms

* **Kubernetes**: LangSmith with deployment supports running control plane and data plane infrastructure on any Kubernetes cluster.

<Tip>
  If you would like to enable this on your LangSmith instance, please follow the [Self-Hosted Full Platform deployment guide](/langsmith/deploy-self-hosted-full-platform).
</Tip>

The **Standalone server** option is the most lightweight and flexible way to run LangSmith. Unlike the other models, you only manage a simplified <Tooltip tip="The runtime environment where your LangGraph Servers and agents execute.">data plane</Tooltip> made up of LangGraph Servers and their required backing services (PostgreSQL, Redis, etc.).

| Component         | Responsibilities                                                  | Where it runs | Who manages it |
| ----------------- | ----------------------------------------------------------------- | ------------- | -------------- |
| **Control plane** | n/a                                                               | n/a           | n/a            |
| **Data plane**    | <ul><li>LangGraph Servers</li><li>Postgres, Redis, etc.</li></ul> | Your cloud    | You            |

This option gives you full control over scaling, deployment, and CI/CD pipelines, while still allowing optional integration with LangSmith for tracing and evaluation.

<Warning>
  Do not run standalone servers in serverless environments. Scale-to-zero may cause task loss and scaling up will not work reliably.
</Warning>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=db67e2add4cf039b1ce2324fa1c1f244" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e28c7f91a45d2608e74febe474066b27 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=6956b33b01bd4fa69269d890d09fa571 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=9e4727b09b6c88780787c6d6ff7bd490 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e380af553cd4dd0ddec93a44aee6ce09 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=5a0da7b4a6b3da6b5ac6fe804fe219b4 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=11f1fbd4fdbb59dd77ef3ebde76422f3 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=57ede6682332db867f1900200f675a5f" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=d38e2ee8deee27d3bbd2d465854bc850 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=19f7f77f74f6c11c1b40c0169796cbe3 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=68547f892a250fc426ce9f0dad79a80e 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=66b5b9d43d2d08cae16755d9f510d418 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=1c045709ec08809225b084d8c850cdc2 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=49c858d75788a600cdc558f78fa2dc41 2500w" />

1. Define and test your graph locally using the `langgraph-cli` or [Studio](/langsmith/studio)
2. Package your agent as a Docker image
3. Deploy the LangGraph Server to your compute platform of choice (Kubernetes, Docker, VM)
4. Optionally, configure LangSmith API keys and endpoints so the server reports traces and evaluations back to LangSmith (self-hosted or SaaS)

### Supported compute platforms

* **Kubernetes**: Use the LangSmith Helm chart to run LangGraph Servers in a Kubernetes cluster. This is the recommended option for production-grade deployments.

* **Docker**: Run in any Docker-supported compute platform (local dev machine, VM, ECS, etc.). This is best suited for development or small-scale workloads.

<Tip>
  To set up a [LangGraph Server](/langsmith/langgraph-server), see the [how-to guide](/langsmith/deploy-standalone-server).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-hosted.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## >          ],

**URL:** llms-txt#>----------],

---

## Ollama

**URL:** llms-txt#ollama

**Contents:**
- Model interfaces

Source: https://docs.langchain.com/oss/python/integrations/providers/ollama

All LangChain integrations with [Ollama](https://ollama.com/).

Ollama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.

For a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).

<Columns cols={2}>
  <Card title="ChatOllama" href="/oss/python/integrations/chat/ollama" cta="Get started" icon="message" arrow>
    Ollama chat models.
  </Card>

<Card title="OllamaLLM" href="/oss/python/integrations/llms/ollama" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Ollama text completion models.
  </Card>

<Card title="OllamaEmbeddings" href="/oss/python/integrations/text_embedding/ollama" cta="Get started" icon="microsoft" arrow>
    Ollama embedding models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/ollama.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Export LangSmith telemetry to your observability backend

**URL:** llms-txt#export-langsmith-telemetry-to-your-observability-backend

Source: https://docs.langchain.com/langsmith/export-backend

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

Self-Hosted LangSmith instances produce telemetry data in the form of logs, metrics and traces. This section will show you how to access and export that data to an observability collector or backend.

This section assumes that you have monitoring infrastructure set up already, or you will set up this infrastructure and want to know how to configure LangSmith to collect data from it.

Infrastructure refers to:

* Collectors, such as [OpenTelemetry](https://opentelemetry.io/docs/collector/), [FluentBit](https://docs.fluentbit.io/manual) or [Prometheus](https://prometheus.io/).
* Observability backends, such as [Datadog](https://www.datadoghq.com/) or the [Grafana](https://grafana.com/) ecosystem.

---

## How to integrate LangGraph into your React application

**URL:** llms-txt#how-to-integrate-langgraph-into-your-react-application

**Contents:**
- Installation
- Example
- Customizing Your UI
  - Loading States
  - Resume a stream after page refresh
  - Thread Management
  - Messages Handling
  - Interrupts
  - Branching
  - Optimistic Updates

Source: https://docs.langchain.com/langsmith/use-stream-react

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [LangGraph Server](/langsmith/langgraph-server)
</Info>

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) React hook provides a seamless way to integrate LangGraph into your React applications. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great chat experiences.

* Messages streaming: Handle a stream of message chunks to form a complete message
* Automatic state management for messages, interrupts, loading states, and errors
* Conversation branching: Create alternate conversation paths from any point in the chat history
* UI-agnostic design: bring your own components and styling

Let's explore how to use [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) in your React application.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) provides a solid foundation for creating bespoke chat experiences. For pre-built chat components and interfaces, we also recommend checking out [CopilotKit](https://docs.copilotkit.ai/coagents/quickstart/langgraph) and [assistant-ui](https://www.assistant-ui.com/docs/runtimes/langgraph).

## Customizing Your UI

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button

### Resume a stream after page refresh

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.

By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).

You can also manually manage the resuming process by using the run callbacks to persist the run metadata and the `joinStream` function to resume the stream. Make sure to pass `streamResumable: true` when creating the run; otherwise some events might be lost.

### Thread Management

Keep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:

We recommend storing the `threadId` in your URL's query parameters to let users resume conversations after page refreshes.

### Messages Handling

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will keep track of the message chunks received from the server and concatenate them together to form a complete message. The completed message chunks can be retrieved via the `messages` property.

By default, the `messagesKey` is set to `messages`, where it will append the new messages chunks to `values["messages"]`. If you store messages in a different key, you can change the value of `messagesKey`.

Under the hood, the [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will use the `streamMode: "messages-tuple"` to receive a stream of messages (i.e. individual LLM tokens) from any LangChain chat model invocations inside your graph nodes. Learn more about messages streaming in the [streaming](/langsmith/streaming#messages) guide.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook exposes the `interrupt` property, which will be filled with the last interrupt from the thread. You can use interrupts to:

* Render a confirmation UI before executing a node
* Wait for human input, allowing agent to ask the user with clarifying questions

Learn more about interrupts in the [How to handle interrupts](/oss/python/langgraph/interrupts#pause-using-interrupt) guide.

For each message, you can use `getMessagesMetadata()` to get the first checkpoint from which the message has been first seen. You can then create a new run from the checkpoint preceding the first seen checkpoint to create a new branch in a thread.

A branch can be created in following ways:

1. Edit a previous user message.
2. Request a regeneration of a previous assistant message.

For advanced use cases you can use the `experimental_branchTree` property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs.

### Optimistic Updates

You can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request.

### Cached Thread Display

Use the `initialValues` option to display cached thread data immediately while the history is being loaded from the server. This improves user experience by showing cached data instantly when navigating to existing threads.

### Optimistic Thread Creation

Use the `threadId` option in `submit` function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support.

You can also optionally specify types for different scenarios, such as:

* `ConfigurableType`: Type for the `config.configurable` property (default: `Record<string, unknown>`)
* `InterruptType`: Type for the interrupt value - i.e. contents of `interrupt(...)` function (default: `unknown`)
* `CustomEventType`: Type for the custom events (default: `unknown`)
* `UpdateType`: Type for the submit function (default: `Partial<State>`)

If you're using LangGraph.js, you can also reuse your graph's annotation types. However, make sure to only import the types of the annotation schema in order to avoid importing the entire LangGraph.js runtime (i.e. via `import type { ... }` directive).

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook provides several callback options to help you respond to different events:

* `onError`: Called when an error occurs.
* `onFinish`: Called when the stream is finished.
* `onUpdateEvent`: Called when an update event is received.
* `onCustomEvent`: Called when a custom event is received. See the [streaming](/oss/python/langgraph/streaming#stream-custom-data) guide to learn how to stream custom events.
* `onMetadataEvent`: Called when a metadata event is received, which contains the Run ID and Thread ID.

* [JS/TS SDK Reference](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-stream-react.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

Example 2 (unknown):
```unknown
## Customizing Your UI

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

### Loading States

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button
```

Example 3 (unknown):
```unknown
### Resume a stream after page refresh

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.
```

Example 4 (unknown):
```unknown
By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).
```

---

## Create a documentation generator

**URL:** llms-txt#create-a-documentation-generator

**Contents:**
- Advanced usage
  - Custom metadata and tags

doc_prompt = """
Generate comprehensive documentation for the following function:

Include:
- Purpose and functionality
- Parameters and return values
- Usage examples
- Any important notes
"""

doc_template_config = PromptTemplateConfig(
    template=doc_prompt,
    name="doc_generator",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="function_code", description="The function code to document", is_required=True),
    ],
)

doc_generator = kernel.add_function(
    function_name="generateDocs",
    plugin_name="documentationPlugin",
    prompt_template_config=doc_template_config,
)

async def main():
    # Example code to analyze
    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    """

# Analyze the code
    analysis_result = await kernel.invoke(code_analyzer, code=sample_code)
    print("Code Analysis:")
    print(analysis_result)
    print("\n" + "="*50 + "\n")

# Generate documentation
    doc_result = await kernel.invoke(doc_generator, function_code=sample_code)
    print("Generated Documentation:")
    print(doc_result)

return {"analysis": str(analysis_result), "documentation": str(doc_result)}

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes:
```

---

## Set the project name to whichever project you'd like to be testing against

**URL:** llms-txt#set-the-project-name-to-whichever-project-you'd-like-to-be-testing-against

project_name = "Tweet Writing Task"
os.environ["LANGSMITH_PROJECT"] = project_name
os.environ["LANGSMITH_TRACING"] = "true"

if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass("YOUR API KEY")

---

## Prompt engineering

**URL:** llms-txt#prompt-engineering

Source: https://docs.langchain.com/langsmith/prompt-engineering

The following sections help you create, manage, and optimize your prompts:

<Columns cols={3}>
  <Card title="Review core concepts" icon="circle-info" href="/langsmith/prompt-engineering-concepts" arrow="true">
    Read definitions and key terminology for prompt engineering in LangSmith.
  </Card>

<Card title="Create and update prompts" icon="pen-to-square" href="/langsmith/create-a-prompt" arrow="true">
    Build prompts via the UI or SDK, configure settings, use tools, add multimodal inputs, and connect model providers.
  </Card>

<Card title="Manage prompts" icon="tags" href="/langsmith/manage-prompts" arrow="true">
    Organize with tags, commit changes, trigger webhooks, and share through the public prompt hub.
  </Card>

<Card title="Explore the prompt hub" icon="folder-tree" href="/langsmith/manage-prompts#public-prompt-hub" arrow="true">
    Browse and manage prompt tags and discover community prompts from the LangChain Hub.
  </Card>

<Card title="Open the prompt playground" icon="vial" href="/langsmith/prompt-engineering-concepts#prompt-playground" arrow="true">
    Test and experiment with prompts using custom endpoints and model configurations.
  </Card>

<Card title="Follow tutorials" icon="book-open" href="/langsmith/optimize-classifier" arrow="true">
    Learn step-by-step techniques, like optimizing classifiers and advanced prompt engineering.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Deployment

**URL:** llms-txt#langsmith-deployment

**Contents:**
- What you'll learn
  - Related

Source: https://docs.langchain.com/langsmith/deployments

<Callout icon="rocket" color="#4F46E5" iconType="regular">
  **Start here if you're building or operating agent applications.** This section is about deploying **your application**. If you need to set up LangSmith infrastructure, the [Hosting section](/langsmith/hosting) covers infrastructure options (Cloud, Hybrid, Self-hosted) and setup guides for Hybrid and Self-hosted deployments.
</Callout>

This section covers how to package, build, and deploy your *agents* and applications as [LangGraph Servers](/langsmith/langgraph-server).

A typical deployment workflow consists of the following steps:

1. **[Test locally](/langsmith/local-server)**: Run your application on a local server.
2. **[Choose hosting](/langsmith/hosting)**: Select Cloud, Hybrid, or Self-hosted.
3. **Deploy your app**: Push code or build images to your chosen environment.
4. **[Monitor & manage](/langsmith/observability)**: Track traces, alerts, and dashboards.

<Info>
  **Prerequisites:** Before deploying applications, you need a LangSmith instance to deploy to. Choose a [hosting option](/langsmith/hosting) first:

* [**Cloud**](/langsmith/cloud): Fully managed with automated CI/CD from a Git repository.
  * [**Hybrid**](/langsmith/hybrid): Enterprise option for data residency requirements.
  * [**Self-hosted**](/langsmith/self-hosted): Full control and data isolation.
</Info>

* Configure your [app for deployment](/langsmith/application-structure) (dependencies, [project setup](/langsmith/setup-app-requirements-txt), and [monorepo support](/langsmith/monorepo-support)).
* Build, deploy, and update [LangGraph Servers](/langsmith/langgraph-server).
* Secure your deployments with [authentication and access control](/langsmith/auth).
* Customize your server runtime ([lifespan hooks](/langsmith/custom-lifespan), [middleware](/langsmith/custom-middleware), and [routes](/langsmith/custom-routes)).
* Debug, observe, and troubleshoot deployed agents using the [Studio UI](/langsmith/studio).

<Columns cols={1}>
  <Card title="Get started with deployment" icon="robot" href="/langsmith/application-structure" cta="Configure your app">
    Package, build, and deploy your agents and graphs to LangGraph Server.
  </Card>
</Columns>

* [LangGraph Server](/langsmith/langgraph-server)
* [Application structure](/langsmith/application-structure)
* [Local server testing](/langsmith/local-server)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deployments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Logs: [OTel Example](/langsmith/langsmith-collector#logs)

**URL:** llms-txt#logs:-[otel-example](/langsmith/langsmith-collector#logs)

All services that are part of the LangSmith self-hosted deployment write logs to their node's filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.

* **OpenTelemetry**: [File Log Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
* **FluentBit**: [Tail Input](https://docs.fluentbit.io/manual/pipeline/inputs/tail)
* **Datadog**: [Kubernetes Log Collection](https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator)

---

## The trace produced will have its metadata present, but the inputs and outputs will be anonymized

**URL:** llms-txt#the-trace-produced-will-have-its-metadata-present,-but-the-inputs-and-outputs-will-be-anonymized

response_with_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
  langsmith_extra={"client": langsmith_client},
)

---

## How to evaluate an application's intermediate steps

**URL:** llms-txt#how-to-evaluate-an-application's-intermediate-steps

**Contents:**
- 1. Define your LLM pipeline
- 2. Create a dataset and examples to evaluate the pipeline
- 3. Define your custom evaluators
- 4. Evaluate the pipeline
- Related

Source: https://docs.langchain.com/langsmith/evaluate-on-intermediate-steps

While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.

For example, for retrieval-augmented generation (RAG), you might want to

1. Evaluate the retrieval step to ensure that the correct documents are retrieved w\.r.t the input query.
2. Evaluate the generation step to ensure that the correct answer is generated w\.r.t the retrieved documents.

In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.

In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the `run`/`rootRun` argument, which is a `Run` object that contains the intermediate steps of your pipeline.

## 1. Define your LLM pipeline

The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.

Requires `langsmith>=0.3.13`

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

## 3. Define your custom evaluators

As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w\.r.t the input query and another that evaluates the hallucination of the generated answer w\.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with [`with_structured_output`](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) to define the evaluator for hallucination.

The key here is that the evaluator function should traverse the `run` / `rootRun` argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.

Example uses `langchain` for convenience, this is not required.

## 4. Evaluate the pipeline

Finally, we'll run `evaluate` with the custom evaluators defined above.

The experiment will contain the results of the evaluation, including the scores and comments from the evaluators: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e926744573c6b9757ba22ff245a3da2c" alt="evaluation_intermediate_experiment.png" data-og-width="2446" width="2446" data-og-height="1244" height="1244" data-path="langsmith/images/evaluation-intermediate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7b6e321b15a06b2adc7f1cacb8e07a35 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c677007bcc1e2af4b3767d6b44fcb327 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a39153399b6721b7c51693f5a59cf2b0 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1132228eba6761a724ae98d85fcf536c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5d74785384737df0cf67145b397b1934 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bef00e4bdc12289d9f1e4b77ed8489cf 2500w" />

* [Evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-on-intermediate-steps.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

---

## Model initialization

**URL:** llms-txt#model-initialization

**Contents:**
  - `langchain-classic`
- Migration guide
- Reporting issues
- Additional resources
- See also

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
bash pip theme={null}
  pip install langchain-classic
  bash uv theme={null}
  uv add langchain-classic
  python  theme={null}
from langchain import ...  # [!code --]
from langchain_classic import ...  # [!code ++]

from langchain.chains import ...  # [!code --]
from langchain_classic.chains import ...  # [!code ++]

from langchain.retrievers import ...  # [!code --]
from langchain_classic.retrievers import ...  # [!code ++]

from langchain import hub  # [!code --]
from langchain_classic import hub  # [!code ++]
```

See our [migration guide](/oss/python/migrate/langchain-v1) for help updating your code to LangChain v1.

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchain/issues) using the `'v1'` [label](https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangChain 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Middleware Guide" icon="puzzle-piece" href="https://blog.langchain.com/agent-middleware/">
    Deep dive into middleware
  </Card>

<Card title="Agents Documentation" icon="book" href="/oss/python/langchain/agents" arrow>
    Full agent documentation
  </Card>

<Card title="Message Content" icon="message" href="/oss/python/langchain/messages#message-content" arrow>
    New content blocks API
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/python/migrate/langchain-v1" arrow>
    How to migrate to LangChain v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langchain">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/python/versioning) - Understanding version numbers
* [Release policy](/oss/python/release-policy) - Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### `langchain-classic`

Legacy functionality has moved to [`langchain-classic`](https://pypi.org/project/langchain-classic) to keep the core packages lean and focused.

**What's in `langchain-classic`:**

* Legacy chains and chain implementations
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* [`langchain-community`](https://pypi.org/project/langchain-community) exports
* Other deprecated functionality

If you use any of this functionality, install [`langchain-classic`](https://pypi.org/project/langchain-classic):

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Then update your imports:
```

---

## Interact with your self-hosted instance of LangSmith

**URL:** llms-txt#interact-with-your-self-hosted-instance-of-langsmith

**Contents:**
  - Configuring the application you want to use with LangSmith
  - Self-Signed Certificates

Source: https://docs.langchain.com/langsmith/self-host-usage

This guide will walk you through the process of using your self-hosted instance of LangSmith.

<Info>
  This guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the [kubernetes deployment guide](/langsmith/kubernetes) or the [docker deployment guide](/langsmith/docker).
</Info>

### Configuring the application you want to use with LangSmith

LangSmith has a single API for interacting with both the hub and the LangSmith backend.

1. Once you have deployed your instance, you can access the LangSmith UI at `http(s)://<host>`.
2. The LangSmith API will be available at `http(s)://<host>/api/v1`
3. The LangSmith Control Plane will be available at `http(s)://<host>/api-host`

To use the API of your instance, you will need to set the following environment variables in your application:

You can also configure these variables directly in the LangSmith SDK client:

After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:

```python  theme={null}
import truststore
truststore.inject_into_ssl()

**Examples:**

Example 1 (unknown):
```unknown
You can also configure these variables directly in the LangSmith SDK client:
```

Example 2 (unknown):
```unknown
After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:
```

---

## The trace produced will not have anonymized inputs and outputs

**URL:** llms-txt#the-trace-produced-will-not-have-anonymized-inputs-and-outputs

response_without_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
)
```

The anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cea182d95ef02e614a6f1bbd7e3a2657" alt="Anonymized run" data-og-width="3180" width="3180" data-og-height="1616" height="1616" data-path="langsmith/images/aws-comprehend-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d3c5a665e2ee726ad6dacf89ade8daea 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=77bccbc4ba3bcde3bd771866e44ce535 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=527a6563672cb66d28bf7ae3272c0c5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=108afc60434f26addae7525049850aac 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0216a9846c6e7ff041bcccdb23ce98a 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cd4119b7e81420f54db86cad58db5426 2500w" />

The non-anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec61e5c8d78268b5b34b6b9c184871cc" alt="Non-anonymized run" data-og-width="3180" width="3180" data-og-height="1648" height="1648" data-path="langsmith/images/aws-comprehend-not-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=34ad34770c55fcea58caee9dfa7f856e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=089c86d993f3dc8a5998bfc2adc8a75d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7fcd25c38a94d366762cf74629dd07c7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b82a670a0e25bc656475cccea5d1a50d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c422ed47282a844783713e5dc291a7b0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a5a62a5bd72b2cf49c9495d14d3059fa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/mask-inputs-outputs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Add custom authentication

**URL:** llms-txt#add-custom-authentication

**Contents:**
- Add custom authentication to your deployment
- Enable agent authentication
  - Authorizing a user for Studio

Source: https://docs.langchain.com/langsmith/custom-auth

This guide shows you how to add custom authentication to your LangSmith application. The steps on this page apply to both [cloud](/langsmith/cloud) and [self-hosted](/langsmith/self-hosted) deployments. It does not apply to isolated usage of the [LangGraph open source library](/oss/python/langgraph/overview) in your own custom server.

## Add custom authentication to your deployment

To leverage custom authentication and access user-level metadata in your deployments, set up custom authentication to automatically populate the `config["configurable"]["langgraph_auth_user"]` object through a custom authentication handler. You can then access this object in your graph with the `langgraph_auth_user` key to [allow an agent to perform authenticated actions on behalf of the user](#enable-agent-authentication).

1. Implement authentication:

<Note>
     Without a custom `@auth.authenticate` handler, LangGraph sees only the API-key owner (usually the developer), so requests aren’t scoped to individual end-users. To propagate custom tokens, you must implement your own handler.
   </Note>

* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:

3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

<Tabs>
     <Tab title="Python Client">
       
     </Tab>

<Tab title="Python RemoteGraph">
       
     </Tab>

<Tab title="JavaScript Client">
       
     </Tab>

<Tab title="JavaScript RemoteGraph">
       
     </Tab>

<Tab title="CURL">
       
     </Tab>
   </Tabs>

For more details on RemoteGraph, refer to the [Use RemoteGraph](/langsmith/use-remote-graph) guide.

## Enable agent authentication

After [authentication](#add-custom-authentication-to-your-deployment), the platform creates a special configuration object (`config`) that is passed to LangSmith deployment. This object contains information about the current user, including any custom fields you return from your `@auth.authenticate` handler.

To allow an agent to perform authenticated actions on behalf of the user, access this object in your graph with the `langgraph_auth_user` key:

<Note>
  Fetch user credentials from a secure secret store. Storing secrets in graph state is not recommended.
</Note>

### Authorizing a user for Studio

By default, if you add custom authorization on your resources, this will also apply to interactions made from [Studio](/langsmith/studio). If you want, you can handle logged-in Studio users differently by checking [is\_studio\_user()](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.StudioUser).

<Note>
  `is_studio_user` was added in version 0.1.73 of the langgraph-sdk. If you're on an older version, you can still check whether `isinstance(ctx.user, StudioUser)`.
</Note>

```python  theme={null}
from langgraph_sdk.auth import is_studio_user, Auth
auth = Auth()

**Examples:**

Example 1 (unknown):
```unknown
* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:
```

Example 2 (unknown):
```unknown
3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

   <Tabs>
     <Tab title="Python Client">
```

Example 3 (unknown):
```unknown
</Tab>

     <Tab title="Python RemoteGraph">
```

Example 4 (unknown):
```unknown
</Tab>

     <Tab title="JavaScript Client">
```

---

## Section 1: Prometheus Exporters

**URL:** llms-txt#section-1:-prometheus-exporters

Use this section if you would like to only deploy metrics exporters for the components in your self hosted deployment, which you can then scrape using your telemetry. If you would like a full observability stack deployed for you, go to the [End-to-End Deployment Section](/langsmith/observability-stack#prerequisites).

The helm chart provides a set of Prometheus exporters to expose metrics from [Redis](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-redis-exporter), [Postgres](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-postgres-exporter), [Nginx](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-nginx-exporter), and [Kube state metrics](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics).

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/metric-exporters-only.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

This will allow you to scrape metrics at the following service endpoints:

* Postgres: `langsmith-observability-postgres-exporter:9187/metrics`
* Redis: `langsmith-observability-redis-exporter:9121/metrics`
* Nginx: `langsmith-observability-nginx-exporter:9113/metrics`
* KubeStateMetrics: `langsmith-observability-kube-state-metrics:8080/metrics`

You should see the following if the installation went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

**Examples:**

Example 1 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

---

## Self-host LangSmith with Docker

**URL:** llms-txt#self-host-langsmith-with-docker

**Contents:**
- Prerequisites
- Running via Docker Compose
  - 1. Fetch the LangSmith `docker-compose.yml` file
  - 2. Configure environment variables
  - 3. Start server
  - Validate your deployment:
  - Checking the logs
  - Stopping the server
- Using LangSmith

Source: https://docs.langchain.com/langsmith/docker

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This guide provides instructions for running the **LangSmith platform** locally using Docker for development and testing purposes.

<Warning>
  **For development/testing only**. Do not use Docker Compose for production. For production deployments, use [Kubernetes](/langsmith/kubernetes).
</Warning>

<Note>
  This page describes how to install the base [LangSmith platform](/langsmith/self-hosted#langsmith) for local testing. It does **not** include deployment management features. For more details, review the [self-hosted options](/langsmith/self-hosted).
</Note>

Note that Docker Compose is limited to local development environments only and does not extend support to container services such as AWS Elastic Container Service, Azure Container Instances, and Google Cloud Run.

1. Ensure Docker is installed and running on your system. You can verify this by running:

If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

Start the LangSmith application by executing the following command in your terminal:

You can also run the server in the background by running:

### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:

2. Visit the exposed port of the `cli-langchain-frontend-1` container on your browser

The LangSmith UI should be visible/operational at `http://localhost:1980`

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt=".langsmith_ui.png" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

### Checking the logs

If, at any point, you want to check if the server is running and see the logs, run

### Stopping the server

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith for [oauth authentication](/langsmith/self-host-sso) or [basic authentication](/langsmith/self-host-basic-auth) to secure your LangSmith instance
* Secure access to your Docker environment to limit access to only the LangSmith frontend and API
* Connect LangSmith to secured Postgres and Redis instances

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/docker.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

   1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

3. Api Key Salt

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

### 3. Start server

Start the LangSmith application by executing the following command in your terminal:
```

Example 3 (unknown):
```unknown
You can also run the server in the background by running:
```

Example 4 (unknown):
```unknown
### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:
```

---

## Double texting

**URL:** llms-txt#double-texting

**Contents:**
- Reject
- Enqueue
- Interrupt
- Rollback

Source: https://docs.langchain.com/langsmith/double-texting

<Info>
  **Prerequisites**

* [LangGraph Server](/langsmith/langgraph-server)
</Info>

Many times users might interact with your graph in unintended ways.
For instance, a user may send one message and before the graph has finished running send a second message.
More generally, users may invoke the graph a second time before the first run has finished.
We call this "double texting".

<Note>
  Double texting is a feature of LangSmith Deployment. It is not available in the [LangGraph open source framework](/oss/python/langgraph/overview).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=1cae1e8cd4920872e7992460b081f76d" alt="Double-text strategies across first vs. second run: Reject keeps only the first; Enqueue runs the second afterward; Interrupt halts the first to run the second; Rollback reverts the first and reruns with the second." data-og-width="1886" width="1886" data-og-height="648" height="648" data-path="langsmith/images/double-texting.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=280&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=67fc4d3817141da00d0f0e0b5c6de093 280w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=560&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2c9cf620db602c51a7e3804cb0815058 560w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=840&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=7e16e946f3c616476fd99b40aa731a3c 840w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1100&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=98032b626677cf73744a3922112abda4 1100w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1650&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2537ca1524e871001cd454f41dca6597 1650w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=2500&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=d37e7e8cd1c4a5bad8cc399d0b878382 2500w" />

This option rejects any additional incoming runs while a current run is in progress and prevents concurrent execution or double texting.

For configuring the reject double text option, refer to the [how-to guide](/langsmith/reject-concurrent).

This option allows the current run to finish before processing any new input. Incoming requests are queued and executed sequentially once prior runs complete.

For configuring the enqueue double text option, refer to the [how-to guide](/langsmith/enqueue-concurrent).

This option halts the current execution and preserves the progress made up to the interruption point. The new user input is then inserted, and execution continues from that state.

When using this option, your graph must account for potential edge cases. For example, a tool call may have been initiated but not yet completed at the time of interruption. In these cases, handling or removing partial tool calls may be necessary to avoid unresolved operations.

For configuring the interrupt double text option, refer to the [how-to guide](/langsmith/interrupt-concurrent).

This option halts the current execution and reverts all progress—including the initial run input—before processing the new user input. The new input is treated as a fresh run, starting from the initial state.

For configuring the rollback double text option, refer to the [how-to guide](/langsmith/rollback-concurrent).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/double-texting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## it will take precedence for any "create" actions on the "threads" resources

**URL:** llms-txt#it-will-take-precedence-for-any-"create"-actions-on-the-"threads"-resources

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Reject if the user does not have write access
    if "write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## Document API authentication in OpenAPI

**URL:** llms-txt#document-api-authentication-in-openapi

**Contents:**
- Default Schema
- Custom Security Schema
- Testing

Source: https://docs.langchain.com/langsmith/openapi-security

This guide shows how to customize the OpenAPI security schema for your LangSmith API documentation. A well-documented security schema helps API consumers understand how to authenticate with your API and even enables automatic client generation. See the [Authentication & Access Control conceptual guide](/langsmith/auth) for more details about LangGraph's authentication system.

<Note>
  **Implementation vs Documentation**
  This guide only covers how to document your security requirements in OpenAPI. To implement the actual authentication logic, see [How to add custom authentication](/langsmith/custom-auth).
</Note>

This guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.

The default security scheme varies by deployment type:

<Tabs>
  <Tab title="LangSmith" />
</Tabs>

By default, LangSmith requires a LangSmith API key in the `x-api-key` header:

When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
    
  </Tab>

<Tab title="API Key">
    
  </Tab>
</Tabs>

After updating your configuration:

1. Deploy your application
2. Visit `/docs` to see the updated OpenAPI documentation
3. Try out the endpoints using credentials from your authentication server (make sure you've implemented the authentication logic first)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/openapi-security.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="API Key">
```

---

## Compile the graph with the checkpointer and store

**URL:** llms-txt#compile-the-graph-with-the-checkpointer-and-store

graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we'll use to namespace our memories to this particular user as we showed above.
```

---

## Connect to an external PostgreSQL database

**URL:** llms-txt#connect-to-an-external-postgresql-database

**Contents:**
- Requirements
- Connection String
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-postgres

LangSmith uses a PostgreSQL database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal PostgreSQL database. However, you can configure LangSmith to use an external PostgreSQL database (). By configuring an external PostgreSQL database, you can more easily manage backups, scaling, and other operational tasks for your database.

* A provisioned PostgreSQL database that your LangSmith instance will have network access to. We recommend using a managed PostgreSQL service like:

* [Amazon RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html)
  * [Google Cloud SQL](https://cloud.google.com/curated-resources/cloud-sql#section-1)
  * [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/products/postgresql#features)

* Note: We only officially support PostgreSQL versions >= 14.

* A user with admin access to the PostgreSQL database. This user will be used to create the necessary tables, indexes, and schemas.

* This user will also need to have the ability to create extensions in the database. We use/will try to install the btree\_gin, btree\_gist, pgcrypto, citext, ltree, and pg\_trgm extensions.

* If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.

* Support for pgbouncer and other connection poolers is community-based. Community members have reported that pgbouncer has worked with `pool_mode` = `session` and a suitable setting for `ignore_startup_parameters` (as of writing, `search_path` and `lock_timeout` need to be ignored). Care is needed to avoid polluting connection pools; some level of PostgreSQL expertise is advisable. LangChain Inc currently does not have roadmap plans for formal test coverage or commercial support of pgbouncer or amazon rds proxy or any other poolers, but the community is welcome to discuss and collaborate on support through GitHub issues.

* By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your PostgreSQL instance and scaling up as needed.

You will need to provide a connection string to your PostgreSQL database. This connection string should include the following information:

* Host
* Port
* Database
* Username
* Password(Make sure to url encode this if there are any special characters)
* URL params

This will take the form of:

An example connection string might look like:

Without url parameters, the connection string would look like:

With your connection string in hand, you can configure your LangSmith instance to use an external PostgreSQL database. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external PostgreSQL database.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-postgres.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
username:password@host:port/database?<url_params>
```

Example 2 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase?sslmode=disable
```

Example 3 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase
```

Example 4 (unknown):
```unknown

```

---

## To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums

**URL:** llms-txt#to-ensure-this,-we'll-create-vectorstore-indexes-for-all-of-the-artists,-tracks-and-albums

---

## Streaming API

**URL:** llms-txt#streaming-api

**Contents:**
- Basic usage
  - Supported stream modes
  - Stream multiple modes
- Stream graph state
  - Stream Mode: `updates`
  - Stream Mode: `values`
- Subgraphs
- Debugging
- LLM tokens
  - Filter LLM tokens

Source: https://docs.langchain.com/langsmith/streaming

[LangGraph SDK](/langsmith/langgraph-python-sdk) allows you to [stream outputs](/oss/python/langgraph/streaming/) from the [LangGraph Server API](/langsmith/server-api-ref).

<Note>
  LangGraph SDK and LangGraph Server are a part of [LangSmith](/langsmith/home).
</Note>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

Create a streaming run:

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the LangGraph API server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running LangGraph API server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
         2\. Set `stream_mode="updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="JavaScript">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
      2. Set `streamMode: "updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

### Supported stream modes

| Mode                             | Description                                                                                                                                                                         | LangGraph Library Method                                                                                      |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| [`values`](#stream-graph-state)  | Stream the full graph state after each [super-step](/langsmith/graph-rebuild#graphs).                                                                                               | `.stream()` / `.astream()` with [`stream_mode="values"`](/oss/python/langgraph/streaming#stream-graph-state)  |
| [`updates`](#stream-graph-state) | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. | `.stream()` / `.astream()` with [`stream_mode="updates"`](/oss/python/langgraph/streaming#stream-graph-state) |
| [`messages-tuple`](#messages)    | Streams LLM tokens and metadata for the graph node where the LLM is invoked (useful for chat apps).                                                                                 | `.stream()` / `.astream()` with [`stream_mode="messages"`](/oss/python/langgraph/streaming#messages)          |
| [`debug`](#debug)                | Streams as much information as possible throughout the execution of the graph.                                                                                                      | `.stream()` / `.astream()` with [`stream_mode="debug"`](/oss/python/langgraph/streaming#stream-graph-state)   |
| [`custom`](#stream-custom-data)  | Streams custom data from inside your graph                                                                                                                                          | `.stream()` / `.astream()` with [`stream_mode="custom"`](/oss/python/langgraph/streaming#stream-custom-data)  |
| [`events`](#stream-events)       | Stream all events (including the state of the graph); mainly useful when migrating large LCEL apps.                                                                                 | `.astream_events()`                                                                                           |

### Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Accordion title="Example graph">
  
</Accordion>

<Note>
  **Stateful runs**
  Examples below assume that you want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB and have created a thread. To create a thread:

<Tabs>
    <Tab title="Python">
      
    </Tab>

<Tab title="JavaScript">
      
    </Tab>

<Tab title="cURL">
      
    </Tab>
  </Tabs>

If you don't need to persist the outputs of a run, you can pass `None` instead of `thread_id` when streaming.
</Note>

### Stream Mode: `updates`

Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Stream Mode: `values`

Use this to stream the **full state** of the graph after each step.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.

<Accordion title="Extended example: streaming from subgraphs">
  This is an example graph you can run in the LangGraph API server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running LangGraph API server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.
    </Tab>

<Tab title="JavaScript">

1. Set `streamSubgraphs: true` to stream outputs from subgraphs.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

Use the `messages-tuple` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages-tuple` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

<Accordion title="Example graph">

1. Note that the message events are emitted even when the LLM is run using `invoke` rather than `stream`.
</Accordion>

<Tabs>
  <Tab title="Python">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="JavaScript">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Filter LLM tokens

* To filter the streamed tokens by LLM invocation, you can [associate `tags` with LLM invocations](/oss/python/langgraph/streaming#filter-by-llm-invocation).
* To stream tokens only from specific nodes, use `stream_mode="messages"` and [filter the outputs by the `langgraph_node` field](/oss/python/langgraph/streaming#filter-by-node) in the streamed metadata.

## Stream custom data

To send **custom user-defined data**:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To stream all events, including the state of the graph:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

If you don't want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB, you can create a stateless run without creating a thread:

<Tabs>
  <Tab title="Python">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="JavaScript">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

LangSmith allows you to join an active [background run](/langsmith/background-run) and stream outputs from it. To do so, you can use [LangGraph SDK's](/langsmith/langgraph-python-sdk) `client.runs.join_stream` method:

<Tabs>
  <Tab title="Python">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="JavaScript">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

<Warning>
  **Outputs not buffered**
  When you use `.join_stream`, output is not buffered, so any output produced before joining will not be received.
</Warning>

For API usage and implementation, refer to the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/thread-runs/POST/threads/\{thread_id}/runs/stream).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/streaming.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 3 (unknown):
```unknown
Create a streaming run:
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the LangGraph API server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.
```

---

## Persistence

**URL:** llms-txt#persistence

**Contents:**
- Threads
- Checkpoints
  - Get state

Source: https://docs.langchain.com/oss/python/langgraph/persistence

LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=966566aaae853ed4d240c2d0d067467c" alt="Checkpoints" data-og-width="2316" width="2316" data-og-height="748" height="748" data-path="oss/images/checkpoints.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bb8525bfcd22b3903b3209aa7497f47 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e8d07fc2899b9a13c7b00eb9b259c3c9 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=46a2f9ed3b131a7c78700711e8c314d6 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c339bd49757810dad226e1846f066c94 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8333dfdb9d766363f251132f2dfa08a1 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=33ba13937eed043ba4a7a87b36d3046f 2500w" />

<Info>
  **LangGraph API handles checkpointing automatically**
  When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.
</Info>

A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of [runs](/langsmith/assistants#execution). When a run is executed, the [state](/oss/python/langgraph/graph-api#state) of the underlying graph of the assistant will be persisted to the thread.

When invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config.

A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/) for more details.

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/python/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:

After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed
* Checkpoint with the outputs of `node_a` `{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed
* Checkpoint with the outputs of `node_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/) for more details.

## Checkpoints

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/python/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:
```

Example 2 (unknown):
```unknown
After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed
* Checkpoint with the outputs of `node_a` `{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed
* Checkpoint with the outputs of `node_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.

### Get state

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.
```

---

## Text splitters

**URL:** llms-txt#text-splitters

**Contents:**
- Text structure-based
- Length-based
- Document structure-based
- Provider-specific

Source: https://docs.langchain.com/oss/python/integrations/splitters/index

**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.

There are several strategies for splitting documents, each with its own advantages.

<Tip>
  For most use cases, start with the [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive_text_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.
</Tip>

## Text structure-based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept:

* The [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.
* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
* This process continues down to the word level if necessary.

**Available text splitters**:

* [Recursively split text](/oss/python/integrations/splitters/recursive_text_splitter)

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's CharacterTextSplitter with token-based splitting:

**Available text splitters**:

* [Split by tokens](/oss/python/integrations/splitters/split_by_token)
* [Split by characters](/oss/python/integrations/splitters/character_text_splitter)

## Document structure-based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:

* Preserves the logical organization of the document
* Maintains context within each chunk
* Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:

* Markdown: Split based on headers (e.g., #, ##, ###)
* HTML: Split using tags
* JSON: Split by object or array elements
* Code: Split by functions, classes, or logical blocks

**Available text splitters**:

* [Split Markdown](/oss/python/integrations/splitters/markdown_header_metadata_splitter)
* [Split JSON](/oss/python/integrations/splitters/recursive_json_splitter)
* [Split code](/oss/python/integrations/splitters/code_splitter)
* [Split HTML](/oss/python/integrations/splitters/split_html)

<Columns cols={3}>
  <Card title="WRITER" icon="link" href="/oss/python/integrations/splitters/writer" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/integrations/splitters/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Available text splitters**:

* [Recursively split text](/oss/python/integrations/splitters/recursive_text_splitter)

## Length-based

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's CharacterTextSplitter with token-based splitting:
```

---

## Resume (use same config)

**URL:** llms-txt#resume-(use-same-config)

**Contents:**
  - Match decision order to actions
  - Tailor configurations by risk

result = agent.invoke(Command(resume={...}), config=config)
python  theme={null}
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

# Create one decision per action, in order
    decisions = []
    for action in action_requests:
        decision = get_user_decision(action)  # Your logic
        decisions.append(decision)

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
interrupt_on = {
    # High risk: full control (approve, edit, reject)
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},
    "send_email": {"allowed_decisions": ["approve", "edit", "reject"]},

# Medium risk: no editing allowed
    "write_file": {"allowed_decisions": ["approve", "reject"]},

# Low risk: no interrupts
    "read_file": False,
    "list_files": False,
}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Match decision order to actions

The decisions list must match the order of `action_requests`:
```

Example 2 (unknown):
```unknown
### Tailor configurations by risk

Configure different tools based on their risk level:
```

---

## The rest of your code

**URL:** llms-txt#the-rest-of-your-code

**Contents:**
- API reference

import langsmith
langsmith_client = langsmith.Client(
    api_key='<api_key>',
    api_url='http(s)://<host>/api/v1',
)
```

To access the API reference, navigate to `http://<host>/api/docs` in your browser.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-usage.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Case studies

**URL:** llms-txt#case-studies

Source: https://docs.langchain.com/oss/python/langgraph/case-studies

This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.

| Company                                                                                                                                 | Industry                             | Use case                                                      | Reference                                                                                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AirTop](https://www.airtop.ai/)                                                                                                        | Software & Technology (GenAI Native) | Browser automation for AI agents                              | [Case study, 2024](https://blog.langchain.dev/customers-airtop/)                                                                                                                                                                                                                                                                              |
| [AppFolio](https://www.appfolio.com/)                                                                                                   | Real Estate                          | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-appfolio/)                                                                                                                                                                                                                                                                            |
| [Athena Intelligence](https://www.athenaintel.com/)                                                                                     | Software & Technology (GenAI Native) | Research & summarization                                      | [Case study, 2024](https://blog.langchain.dev/customers-athena-intelligence/)                                                                                                                                                                                                                                                                 |
| [BlackRock](https://www.blackrock.com/)                                                                                                 | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/oyqeCHFM5U4?feature=shared)                                                                                                                                                                                                                                                                           |
| [Captide](https://www.captide.co/)                                                                                                      | Software & Technology (GenAI Native) | Data extraction                                               | [Case study, 2025](https://blog.langchain.dev/how-captide-is-redefining-equity-research-with-agentic-workflows-built-on-langgraph-and-langsmith/)                                                                                                                                                                                             |
| [Cisco CX](https://www.cisco.com/site/us/en/services/modern-data-center/index.html?CCID=cc005911\&DTID=eivtotr001480\&OID=srwsas032775) | Software & Technology                | Customer support                                              | [Interrupt Talk, 2025](https://youtu.be/gPhyPRtIMn0?feature=shared)                                                                                                                                                                                                                                                                           |
| [Cisco Outshift](https://outshift.cisco.com/)                                                                                           | Software & Technology                | DevOps                                                        | [Video story, 2025](https://www.youtube.com/watch?v=htcb-vGR_x0); [Case study, 2025](https://blog.langchain.com/cisco-outshift/); [Blog post, 2025](https://outshift.cisco.com/blog/build-react-agent-application-for-devops-tasks-using-rest-apis)                                                                                           |
| [Cisco TAC](https://www.cisco.com/c/en/us/support/index.html)                                                                           | Software & Technology                | Customer support                                              | [Video story, 2025](https://youtu.be/EAj0HBDGqaE?feature=shared)                                                                                                                                                                                                                                                                              |
| [City of Hope](https://www.cityofhope.org/)                                                                                             | Non-profit                           | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/9ABwtK2gIZU?feature=shared)                                                                                                                                                                                                                                                                              |
| [C.H. Robinson](https://www.chrobinson.com/en-us/)                                                                                      | Logistics                            | Automation                                                    | [Case study, 2025](https://blog.langchain.dev/customers-chrobinson/)                                                                                                                                                                                                                                                                          |
| [Definely](https://www.definely.com/)                                                                                                   | Legal                                | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.com/customers-definely/)                                                                                                                                                                                                                                                                            |
| [Docent Pro](https://docentpro.com/)                                                                                                    | Travel                               | GenAI embedded product experiences                            | [Case study, 2025](https://blog.langchain.com/customers-docentpro/)                                                                                                                                                                                                                                                                           |
| [Elastic](https://www.elastic.co/)                                                                                                      | Software & Technology                | Copilot for domain-specific task                              | [Blog post, 2025](https://www.elastic.co/blog/elastic-security-generative-ai-features)                                                                                                                                                                                                                                                        |
| [Exa](https://exa.ai/)                                                                                                                  | Software & Technology (GenAI Native) | Search                                                        | [Case study, 2025](https://blog.langchain.com/exa/)                                                                                                                                                                                                                                                                                           |
| [GitLab](https://about.gitlab.com/)                                                                                                     | Software & Technology                | Code generation                                               | [Duo workflow docs](https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/duo_workflow/)                                                                                                                                                                                                                             |
| [Harmonic](https://harmonic.ai/)                                                                                                        | Software & Technology                | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-harmonic/)                                                                                                                                                                                                                                                                            |
| [Inconvo](https://inconvo.ai/?ref=blog.langchain.dev)                                                                                   | Software & Technology                | Code generation                                               | [Case study, 2025](https://blog.langchain.dev/customers-inconvo/)                                                                                                                                                                                                                                                                             |
| [Infor](https://infor.com/)                                                                                                             | Software & Technology                | GenAI embedded product experiences; customer support; copilot | [Case study, 2025](https://blog.langchain.dev/customers-infor/)                                                                                                                                                                                                                                                                               |
| [J.P. Morgan](https://www.jpmorganchase.com/)                                                                                           | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/yMalr0jiOAc?feature=shared)                                                                                                                                                                                                                                                                           |
| [Klarna](https://www.klarna.com/)                                                                                                       | Fintech                              | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.dev/customers-klarna/)                                                                                                                                                                                                                                                                              |
| [Komodo Health](https://www.komodohealth.com/)                                                                                          | Healthcare                           | Copilot for domain-specific task                              | [Blog post](https://www.komodohealth.com/perspectives/new-gen-ai-assistant-empowers-the-enterprise/)                                                                                                                                                                                                                                          |
| [LinkedIn](https://www.linkedin.com/)                                                                                                   | Social Media                         | Code generation; Search & discovery                           | [Interrupt talk, 2025](https://youtu.be/NmblVxyBhi8?feature=shared); [Blog post, 2025](https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics); [Blog post, 2024](https://www.linkedin.com/blog/engineering/generative-ai/behind-the-platform-the-journey-to-create-the-linkedin-genai-application-tech-stack) |
| [Minimal](https://gominimal.ai/)                                                                                                        | E-commerce                           | Customer support                                              | [Case study, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                              |
| [Modern Treasury](https://www.moderntreasury.com/)                                                                                      | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/AwAiffXqaCU?feature=shared)                                                                                                                                                                                                                                                                              |
| [Monday](https://monday.com/)                                                                                                           | Software & Technology                | GenAI embedded product experiences                            | [Interrupt talk, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                          |
| [Morningstar](https://www.morningstar.com/)                                                                                             | Financial Services                   | Research & summarization                                      | [Video story, 2025](https://youtu.be/6LidoFXCJPs?feature=shared)                                                                                                                                                                                                                                                                              |
| [OpenRecovery](https://www.openrecovery.com/)                                                                                           | Healthcare                           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-openrecovery/)                                                                                                                                                                                                                                                                        |
| [Pigment](https://www.pigment.com/)                                                                                                     | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/5JVSO2KYOmE?feature=shared)                                                                                                                                                                                                                                                                              |
| [Prosper](https://www.prosper.com/)                                                                                                     | Fintech                              | Customer support                                              | [Video story, 2025](https://youtu.be/9RFNOYtkwsc?feature=shared)                                                                                                                                                                                                                                                                              |
| [Qodo](https://www.qodo.ai/)                                                                                                            | Software & Technology (GenAI Native) | Code generation                                               | [Blog post, 2025](https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/)                                                                                                                                                                                                                                                 |
| [Rakuten](https://www.rakuten.com/)                                                                                                     | E-commerce / Fintech                 | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/gD1LIjCkuA8?feature=shared); [Blog post, 2025](https://rakuten.today/blog/from-ai-hype-to-real-world-tools-rakuten-teams-up-with-langchain.html)                                                                                                                                                         |
| [Replit](https://replit.com/)                                                                                                           | Software & Technology                | Code generation                                               | [Blog post, 2024](https://blog.langchain.dev/customers-replit/); [Breakout agent story, 2024](https://www.langchain.com/breakoutagents/replit); [Fireside chat video, 2024](https://www.youtube.com/watch?v=ViykMqljjxU)                                                                                                                      |
| [Rexera](https://www.rexera.com/)                                                                                                       | Real Estate (GenAI Native)           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-rexera/)                                                                                                                                                                                                                                                                              |
| [Abu Dhabi Government](https://www.tamm.abudhabi/)                                                                                      | Government                           | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-abu-dhabi-government/)                                                                                                                                                                                                                                                                |
| [Tradestack](https://www.tradestack.uk/)                                                                                                | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-tradestack/)                                                                                                                                                                                                                                                                          |
| [Uber](https://www.uber.com/)                                                                                                           | Transportation                       | Developer productivity; Code generation                       | [Interrupt talk, 2025](https://youtu.be/Bugs0dVcNI8?feature=shared); [Presentation, 2024](https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/); [Video, 2024](https://www.youtube.com/watch?v=8rkA5vWUE4Y)                                                                            |
| [Unify](https://www.unifygtm.com/)                                                                                                      | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/pKk-LfhujwI?feature=shared); [Blog post, 2024](https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/)                                                                                                                                             |
| [Vizient](https://www.vizientinc.com/)                                                                                                  | Healthcare                           | Copilot for domain-specific task                              | [Video story, 2025](https://www.youtube.com/watch?v=vrjJ6NuyTWA); [Case study, 2025](https://blog.langchain.dev/p/3d2cd58c-13a5-4df9-bd84-7d54ed0ed82c/)                                                                                                                                                                                      |
| [Vodafone](https://www.vodafone.com/)                                                                                                   | Telecommunications                   | Code generation; internal search                              | [Case study, 2025](https://blog.langchain.dev/customers-vodafone/)                                                                                                                                                                                                                                                                            |
| [WebToon](https://www.webtoons.com/en/)                                                                                                 | Media & Entertainment                | Data extraction                                               | [Case study, 2025](https://blog.langchain.com/customers-webtoon/)                                                                                                                                                                                                                                                                             |
| [11x](https://www.11x.ai/)                                                                                                              | Software & Technology (GenAI Native) | Research & outreach                                           | [Interrupt talk, 2025](https://youtu.be/fegwPmaAPQk?feature=shared)                                                                                                                                                                                                                                                                           |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/case-studies.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## AzureChatOpenAI

**URL:** llms-txt#azurechatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/azure

Azure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.

This will help you getting started with AzureChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all AzureChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html).

### Integration details

| Class                                                                                         | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/azure_chat_openai) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview).

If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

You'll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance. Then, if using Node.js, you can set your credentials as environment variables:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## The above chain will be traced as a child run of the traceable function

**URL:** llms-txt#the-above-chain-will-be-traced-as-a-child-run-of-the-traceable-function

**Contents:**
- Interoperability between LangChain.JS and LangSmith SDK
  - Tracing LangChain objects inside `traceable` (JS only)
  - Tracing LangChain child runs via `traceable` / RunTree API (JS only)

@traceable(
    tags=["openai", "chat"],
    metadata={"foo": "bar"}
)
def invoke_runnnable(question, context):
    result = chain.invoke({"question": question, "context": context})
    return "The response is: " + result

invoke_runnnable("Can you summarize this morning's meetings?", "During this morning's meeting, we solved all world conflict.")
typescript  theme={null}
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { getLangchainCallbacks } from "langsmith/langchain";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. Please respond to the user's request only based on the given context.",
  ],
  ["user", "Question: {question}\nContext: {context}"],
]);

const model = new ChatOpenAI({ modelName: "gpt-4o-mini" });
const outputParser = new StringOutputParser();
const chain = prompt.pipe(model).pipe(outputParser);

const main = traceable(
  async (input: { question: string; context: string }) => {
    const callbacks = await getLangchainCallbacks();
    const response = await chain.invoke(input, { callbacks });
    return response;
  },
  { name: "main" }
);
typescript  theme={null}
import { traceable } from "langsmith/traceable";
import { RunnableLambda } from "@langchain/core/runnables";
import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
  name: "Child Run",
});

const parrot = new RunnableLambda({
  func: async (input: { text: string }, config?: RunnableConfig) => {
    return await tracedChild(input.text);
  },
});
typescript Traceable theme={null}
  import { traceable } from "langsmith/traceable";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
    name: "Child Run",
  });

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // Pass the config to existing traceable function
      await tracedChild(config, input.text);
      return input.text;
    },
  });
  typescript Run Tree theme={null}
  import { RunTree } from "langsmith/run_trees";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // create the RunTree from the RunnableConfig of the RunnableLambda
      const childRunTree = RunTree.fromRunnableConfig(config, {
        name: "Child Run",
      });

childRunTree.inputs = { input: input.text };
      await childRunTree.postRun();

childRunTree.outputs = { output: `Child Run: ${input.text}` };
      await childRunTree.patchRun();

return input.text;
    },
  });
  ```
</CodeGroup>

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
This will produce the following trace tree: <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=52c64fd784522c4b2d75886ae76f8c18" alt="" data-og-width="1334" width="1334" data-og-height="734" height="734" data-path="langsmith/images/trace-tree-python-interop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21a424e2326767bb66a6b5a207390bec 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=72f1854193fd30317d1b69d8de433d73 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34fa74aff75c11172b350d38319bf276 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4ebfb4252764af54033e62ad088f60b1 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2d783eecf72ba0680e78a0f122bd4411 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d977793e5281e5d255d962224bd70df 2500w" />

## Interoperability between LangChain.JS and LangSmith SDK

### Tracing LangChain objects inside `traceable` (JS only)

Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.

For older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.
```

Example 2 (unknown):
```unknown
### Tracing LangChain child runs via `traceable` / RunTree API (JS only)

<Note>
  We're working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:

  1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.
  2. It's discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.
  3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.
</Note>

In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.
```

Example 3 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7b117d3aa9b419fe2a314ec6d9cc7c16" alt="Trace Tree" data-og-width="2564" width="2564" data-og-height="1530" height="1530" data-path="langsmith/images/trace-tree-manual-tracing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=517f8a525908d5241c0d635726bf2da7 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2772298bff6569c12537d8b31cc90e78 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4b02aa33df7ef1d18a33bb36c3e2edfe 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e9d4021c62cf3bad95e01c8aa2895d44 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02ef49a98151ad9fc63c742241542d94 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=765dabfe056ce5f28b1b09ca7eb735d7 2500w" />

Alternatively, you can convert LangChain's [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or pass the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) as the first argument of `traceable`-wrapped function.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The default RetryPolicy is optimized for retrying specific network errors.

**URL:** llms-txt#the-default-retrypolicy-is-optimized-for-retrying-specific-network-errors.

**Contents:**
- Caching Tasks
- Resuming after an error

retry_policy = RetryPolicy(retry_on=ValueError)

@task(retry_policy=retry_policy)
def get_info():
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError('Failure')
    return "OK"

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer):
    return get_info().result()

config = {
    "configurable": {
        "thread_id": "1"
    }
}

main.invoke({'any_input': 'foobar'}, config=config)
pycon  theme={null}
'OK'
python  theme={null}
import time
from langgraph.cache.memory import InMemoryCache
from langgraph.func import entrypoint, task
from langgraph.types import CachePolicy

@task(cache_policy=CachePolicy(ttl=120))    # [!code highlight]
def slow_add(x: int) -> int:
    time.sleep(1)
    return x * 2

@entrypoint(cache=InMemoryCache())
def main(inputs: dict) -> dict[str, int]:
    result1 = slow_add(inputs["x"]).result()
    result2 = slow_add(inputs["x"]).result()
    return {"result1": result1, "result2": result2}

for chunk in main.stream({"x": 5}, stream_mode="updates"):
    print(chunk)

#> {'slow_add': 10}
#> {'slow_add': 10, '__metadata__': {'cached': True}}
#> {'main': {'result1': 10, 'result2': 10}}
python  theme={null}
import time
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import StreamWriter

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Caching Tasks
```

Example 3 (unknown):
```unknown
1. `ttl` is specified in seconds. The cache will be invalidated after this time.

## Resuming after an error
```

---

## Use webhooks

**URL:** llms-txt#use-webhooks

**Contents:**
- Supported endpoints
- Set up your assistant and thread
- Use a webhook with a graph run
- Webhook payload
- Secure webhooks
- Disable webhooks
- Test webhooks

Source: https://docs.langchain.com/langsmith/use-webhooks

Webhooks enable event-driven communication from your LangSmith application to external services. For example, you may want to issue an update to a separate service once an API call to LangSmith has finished running.

Many LangSmith endpoints accept a `webhook` parameter. If this parameter is specified by an endpoint that can accept POST requests, LangSmith will send a request at the completion of a run.

When working with LangSmith, you may want to use webhooks to receive updates after an API call completes. Webhooks are useful for triggering actions in your service once a run has finished processing. To implement this, you need to expose an endpoint that can accept `POST` requests and pass this endpoint as a `webhook` parameter in your API request.

Currently, the SDK does not provide built-in support for defining webhook endpoints, but you can specify them manually using API requests.

## Supported endpoints

The following API endpoints accept a `webhook` parameter:

| Operation            | HTTP Method | Endpoint                          |
| -------------------- | ----------- | --------------------------------- |
| Create Run           | `POST`      | `/thread/{thread_id}/runs`        |
| Create Thread Cron   | `POST`      | `/thread/{thread_id}/runs/crons`  |
| Stream Run           | `POST`      | `/thread/{thread_id}/runs/stream` |
| Wait Run             | `POST`      | `/thread/{thread_id}/runs/wait`   |
| Create Cron          | `POST`      | `/runs/crons`                     |
| Stream Run Stateless | `POST`      | `/runs/stream`                    |
| Wait Run Stateless   | `POST`      | `/runs/wait`                      |

In this guide, we’ll show how to trigger a webhook after streaming a run.

## Set up your assistant and thread

Before making API calls, set up your assistant and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

LangSmith sends webhook notifications in the format of a [Run](/langsmith/assistants#execution). See the [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/assistants) for details. The request payload includes run input, configuration, and other metadata in the `kwargs` field.

To ensure only authorized requests hit your webhook endpoint, consider adding a security token as a query parameter:

Your server should extract and validate this token before processing requests.

As of `langgraph-api>=0.2.78`, developers can disable webhooks in the `langgraph.json` file:

This feature is primarily intended for self-hosted deployments, where platform administrators or developers may prefer to disable webhooks to simplify their security posture—especially if they are not configuring firewall rules or other network controls. Disabling webhooks helps prevent untrusted payloads from being sent to internal endpoints.

For full configuration details, refer to the [configuration file reference](/langsmith/cli?h=disable_webhooks#configuration-file).

You can test your webhook using online services like:

* **[Beeceptor](https://beeceptor.com/)** – Quickly create a test endpoint and inspect incoming webhook payloads.
* **[Webhook.site](https://webhook.site/)** – View, debug, and log incoming webhook requests in real time.

These tools help you verify that LangSmith is correctly triggering and sending webhooks to your service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-webhooks.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Example response:
```

Example 4 (unknown):
```unknown
## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
```

---

## Run (span) data format

**URL:** llms-txt#run-(span)-data-format

Source: https://docs.langchain.com/langsmith/run-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and runs](/langsmith/observability-concepts)
</Check>

LangSmith stores and processes trace data in a simple format that is easy to export and import.

Many of these fields are optional or not important to know about but are included for completeness. The **bolded** fields are the most important ones to know about.

| Field Name                    | Type             | Description                                                                                                                   |
| ----------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **id**                        | UUID             | Unique identifier for the span.                                                                                               |
| **name**                      | string           | The name associated with the run.                                                                                             |
| **inputs**                    | object           | A map or set of inputs provided to the run.                                                                                   |
| **run\_type**                 | string           | Type of run, e.g., "llm", "chain", "tool".                                                                                    |
| **start\_time**               | datetime         | Start time of the run.                                                                                                        |
| **end\_time**                 | datetime         | End time of the run.                                                                                                          |
| **extra**                     | object           | Any extra information run.                                                                                                    |
| **error**                     | string           | Error message if the run encountered an error.                                                                                |
| **outputs**                   | object           | A map or set of outputs generated by the run.                                                                                 |
| **events**                    | array of objects | A list of event objects associated with the run. This is relevant for runs executed with streaming.                           |
| **tags**                      | array of strings | Tags or labels associated with the run.                                                                                       |
| **trace\_id**                 | UUID             | Unique identifier for the trace the run is a part of. This is also the `id` field of the root run of the trace                |
| **dotted\_order**             | string           | Ordering string, hierarchical. Format: `run_start_time`Z`run_uuid`.`child_run_start_time`Z`child_run_uuid`...                 |
| **status**                    | string           | Current status of the run execution, e.g., "error", "pending", "success"                                                      |
| **child\_run\_ids**           | array of UUIDs   | List of IDs for all child runs.                                                                                               |
| **direct\_child\_run\_ids**   | array of UUIDs   | List of IDs for direct children of this run.                                                                                  |
| **parent\_run\_ids**          | array of UUIDs   | List of IDs for all parent runs.                                                                                              |
| **feedback\_stats**           | object           | Aggregations of feedback statistics for this run                                                                              |
| **reference\_example\_id**    | UUID             | ID of a reference example associated with the run. This is usually only present for evaluation runs.                          |
| **total\_tokens**             | integer          | Total number of tokens processed by the run.                                                                                  |
| **prompt\_tokens**            | integer          | Number of tokens in the prompt of the run.                                                                                    |
| **completion\_tokens**        | integer          | Number of tokens in the completion of the run.                                                                                |
| **total\_cost**               | string           | Total cost associated with processing the run.                                                                                |
| **prompt\_cost**              | string           | Cost associated with the prompt part of the run.                                                                              |
| **completion\_cost**          | string           | Cost associated with the completion of the run.                                                                               |
| **first\_token\_time**        | datetime         | Time when the first token of a model output was generated. Only applies for runs with `run_type="llm"` and streaming enabled. |
| **session\_id**               | string           | Session identifier for the run, also known as the tracing project ID.                                                         |
| **in\_dataset**               | boolean          | Indicates whether the run is included in a dataset.                                                                           |
| **parent\_run\_id**           | UUID             | Unique identifier of the parent run.                                                                                          |
| execution\_order (deprecated) | integer          | The order in which this run was executed within the trace.                                                                    |
| serialized                    | object           | Serialized state of the object executing the run if applicable.                                                               |
| manifest\_id (deprecated)     | UUID             | Identifier for a manifest associated with the span.                                                                           |
| manifest\_s3\_id              | UUID             | S3 identifier for the manifest.                                                                                               |
| inputs\_s3\_urls              | object           | S3 URLs for the inputs.                                                                                                       |
| outputs\_s3\_urls             | object           | S3 URLs for the outputs.                                                                                                      |
| price\_model\_id              | UUID             | Identifier for the pricing model applied to the run.                                                                          |
| app\_path                     | string           | Application (UI) path for this run.                                                                                           |
| last\_queued\_at              | datetime         | Last time the span was queued.                                                                                                |
| share\_token                  | string           | Token for sharing access to the run's data.                                                                                   |

Here is an example of a JSON representation of a run in the above format:

#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:

If you print out the IDs at each stage, you may get the following:

Note a few invariants:

* The "id" is equal to the last 36 characters of the dotted order (the suffix after the final "Z"). See `0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6` for example in the grandchild.
* The "trace\_id" is equal to the first UUID in the dotted order (i.e., `dotted_order.split('.')[0].split('Z')[1]`)
* If "parent\_run\_id" exists, it is the penultimate UUID in the dotted order. See `a8024e23-5b82-47fd-970e-f6a5ba3f5097` in the grandchild, for an example.
* If you split the dotted\_order on the dots, each segment is formatted as (`<run_start_time>Z<run_id>`)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:
```

Example 2 (unknown):
```unknown
If you print out the IDs at each stage, you may get the following:
```

---

## minReplicas: 16

**URL:** llms-txt#minreplicas:-16

---

## ahead of time and use those to disambiguate the user input. E.g. if a user searches for

**URL:** llms-txt#ahead-of-time-and-use-those-to-disambiguate-the-user-input.-e.g.-if-a-user-searches-for

---

## Judge LLM

**URL:** llms-txt#judge-llm

grader_llm = init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(Grade, method="json_schema", strict=True)

---

## Contributing integrations

**URL:** llms-txt#contributing-integrations

Source: https://docs.langchain.com/oss/python/contributing/integrations-langgraph

<Icon icon="plug-circle-plus" size={32} />

**Integrations are a core component of LangGraph.**

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/integrations-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to improve your evaluator with few-shot examples

**URL:** llms-txt#how-to-improve-your-evaluator-with-few-shot-examples

**Contents:**
- How few-shot examples work
- Configure your evaluator
  - 1. Configure variable mapping
  - 2. Specify the number of few-shot examples to use
- Make corrections
- View your corrections dataset

Source: https://docs.langchain.com/langsmith/create-few-shot-evaluators

Using LLM-as-a-judge evaluators can be very helpful when you can't evaluate your system programmatically. However, their effectiveness depends on their quality and how well they align with human reviewer feedback. LangSmith provides the ability to improve the alignment of LLM-as-a-judge evaluator to human preferences using few-shot examples.

Human corrections are automatically inserted into your evaluator prompt using few-shot examples. Few-shot examples is a technique inspired by [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot) that guides the models output with a few high-quality examples.

This guide covers how to set up few-shot examples as part of your LLM-as-a-judge evaluator and apply corrections to feedback scores.

## How few-shot examples work

* Few-shot examples are added to your evaluator prompt using the `{{Few-shot examples}}` variable
* Creating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections
* At runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences

## Configure your evaluator

<Note>
  Few-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.
</Note>

Before enabling few-shot examples, set up your LLM-as-a-judge evaluator. If you haven't done this yet, follow the steps in the [LLM-as-a-judge evaluator guide](/langsmith/llm-as-judge).

### 1. Configure variable mapping

Each few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus a `few_shot_explanation` and a `score` variable which should have the same name as your feedback key.

For example, if your main prompt has variables `question` and `response`, and your evaluator outputs a `correctness` score, then your few-shot prompt should have the vartiables `question`, `response`, `few_shot_explanation`, and `correctness`.

### 2. Specify the number of few-shot examples to use

You may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.

<Info>
  [Audit evaluator scores](/langsmith/audit-evaluator-scores)
</Info>

As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you [make corrections to these scores](/langsmith/audit-evaluator-scores), you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the `few_shot_explanation` variable.

The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8c7bfcc6cc4ab86c18240c3cbf2ea44c" alt="Few-shot example" data-og-width="1572" width="1572" data-og-height="790" height="790" data-path="langsmith/images/few-shot-example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91f4e17fd853ba23c1b04934144dfa77 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e0a5e2a026e4166c341900dd49316f35 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=88aec2ef5c37c16c67e0eefecd3fbc0a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bfbbd35cf503ce2f3dbf743fab8fb75b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf0765bfeabc8d34ef49626dca0135ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f9d2cf9437ee0e160a511903bd88238a 2500w" />

Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!

## View your corrections dataset

In order to view your corrections dataset:

* **Online evaluators**: Select your run rule and click **Edit Rule**
* **Offline evaluators**: Select your evaluator and click **Edit Evaluator**

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=03453ef08f1c272d5d9aaf71d1fb7301" alt="Edit Evaluator" data-og-width="800" width="800" data-og-height="284" height="284" data-path="langsmith/images/edit-evaluator.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d495c791e6c8ae9d241085795d4b67b5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b1ddde8054744862494e4d3f02a460b0 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9838e073d1d7e61c6b79d8f35ba1a1b3 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67322be4166f4479a466427a9b270ca1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4cd490404d1616498fed810b3ce75a21 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c8a87ad332661a0b8b472dd34f1f4ab 2500w" />

Head to your dataset of corrections linked in the the **Improve evaluator accuracy using few-shot examples** section. You can view and update your few-shot examples in the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3215f3f24a08186fd76c6dbad18a3cf5" alt="View few-shot dataset" data-og-width="1470" width="1470" data-og-height="478" height="478" data-path="langsmith/images/view-few-shot-ds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ad702a532a8f083c71056baff4370f30 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d45ebd4263adc9c10598fad633167ca3 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fe060c8d000a41566949ff35d6c62135 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f735943da46a1e57328b86246f5da25f 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9861b9651d5d63a07662e7aa1bc68491 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6084c3697ffd582e30301540906a5698 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-few-shot-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Beta LangSmith Collector-Proxy

**URL:** llms-txt#beta-langsmith-collector-proxy

**Contents:**
- When to Use the Collector-Proxy
- Key Features
- Configuration
  - Project Configuration
  - Authentication
- Deployment (Docker)
- Usage
- Health & Scaling
- Horizontal Scaling
- Fork & Extend

Source: https://docs.langchain.com/langsmith/collector-proxy

<Note>
  This is a beta feature. The API may change in future releases.
</Note>

The LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.

## When to Use the Collector-Proxy

The Collector-Proxy is particularly valuable when:

* You're running multiple instances of your application in parallel and need to efficiently aggregate traces
* You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)
* You're using a language that doesn't have a native LangSmith SDK

* **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.
* **Compression** Uses zstd to minimize payload size.
* **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.
* **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.
* **Flexible Batching** Flush by span count or time interval.

Configure via environment variables:

| Variable             | Description                       | Default                           |
| -------------------- | --------------------------------- | --------------------------------- |
| `HTTP_PORT`          | Port to run the proxy server      | `4318`                            |
| `LANGSMITH_ENDPOINT` | LangSmith backend URL             | `https://api.smith.langchain.com` |
| `LANGSMITH_API_KEY`  | API key for LangSmith             | **Required** (env var or header)  |
| `LANGSMITH_PROJECT`  | Default tracing project           | Default project if not specified  |
| `BATCH_SIZE`         | Spans per upload batch            | `100`                             |
| `FLUSH_INTERVAL_MS`  | Flush interval in milliseconds    | `1000`                            |
| `MAX_BUFFER_BYTES`   | Max uncompressed buffer size      | `10485760` (10 MB)                |
| `MAX_BODY_BYTES`     | Max incoming request body size    | `209715200` (200 MB)              |
| `MAX_RETRIES`        | Retry attempts for failed uploads | `3`                               |
| `RETRY_BACKOFF_MS`   | Initial backoff in milliseconds   | `100`                             |

### Project Configuration

The Collector-Proxy supports LangSmith project configuration with the following priority:

1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used
2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable
3. If neither is set, it will trace to the `default` project.

The API key can be provided either:

* As an environment variable (`LANGSMITH_API_KEY`)
* In the request headers (`X-API-Key`)

## Deployment (Docker)

You can deploy the Collector-Proxy with Docker:

1. **Build the image**

2. **Run the container**

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:

* **Liveness**: `GET /live` → 200
* **Readiness**: `GET /ready` → 200

## Horizontal Scaling

To ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).

Fork the [Collector-Proxy repo on GitHub](https://github.com/langchain-ai/langsmith-collector-proxy) and implement your own converter:

* Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`
* Register the custom converter in `internal/translator/translator.go`

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/collector-proxy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. **Run the container**
```

Example 2 (unknown):
```unknown
## Usage

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:
```

Example 3 (unknown):
```unknown
Send a test trace:
```

---

## Alice can still create threads

**URL:** llms-txt#alice-can-still-create-threads

**Contents:**
- What's Next?

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")
bash  theme={null}
✅ Alice created thread: dcea5cd8-eb70-4a01-a4b6-643b14e8f754
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/dcea5cd8-eb70-4a01-a4b6-643b14e8f754'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 400f8d41-e946-429f-8f93-4fe395bc3eed
✅ Alice sees 1 thread
✅ Bob sees 1 thread
✅ Alice correctly denied access:
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500
✅ Alice correctly denied access to searching assistants:
```

Congratulations! You've built a chatbot where each user has their own private conversations. While this system uses simple token-based authentication, these authorization patterns will work with implementing any real authentication system. In the next tutorial, you'll replace your test users with real user accounts using OAuth2.

Now that you can control access to resources, you might want to:

1. Move on to [Connect an authentication provider](/langsmith/add-auth-server) to add real user accounts.
2. Read more about [authorization patterns](/langsmith/auth#authorization).
3. Check out the [API reference](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) for details about the interfaces and methods used in this tutorial.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/resource-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

---

## How to filter experiments in the UI

**URL:** llms-txt#how-to-filter-experiments-in-the-ui

**Contents:**
- Background: add metadata to your experiments
- Filter experiments in the UI

Source: https://docs.langchain.com/langsmith/filter-experiments-ui

LangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.

## Background: add metadata to your experiments

When you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.

In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:

## Filter experiments in the UI

In the UI, we see all experiments that have been run by default.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0cb5b29f405286dadb8b6491709eb789" alt="" data-og-width="2900" width="2900" data-og-height="1370" height="1370" data-path="langsmith/images/filter-all-experiments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45a66945b4ca67d1dc3c29890b5d5feb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1d8fdbdf637f750dcef092a49b45e0ae 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7177578edd6e6bcfef63d7a778ded723 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f0c6bdae3afabe2b179a74450adbc508 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8dd72a9e30b093a58819d79f40f69b65 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e65610094b48a0f81ac75a49be573382 2500w" />

If we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e3a0c42e4eb37cae68d367dec75d0df1" alt="" data-og-width="2910" width="2910" data-og-height="1130" height="1130" data-path="langsmith/images/filter-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=96ef35ffdaf371b2819da5a3f4bdf7aa 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b44c6d30e0e03560d26719facf1dfc0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a737dceb8e7a2c89b9d0dfc008352afd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cd3adf4b2d249bde3646810d3783b931 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7dc39864577acc20bb9229de313cbe33 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7d5392a8453b2f416f795834eb02cc6e 2500w" />

We can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c0e223bbe637a03a4c251896a5662f52" alt="" data-og-width="2912" width="2912" data-og-height="826" height="826" data-path="langsmith/images/filter-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=476871f1d9569046084b380b3fa50ba3 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c8f24775e8f736a2cfd6b1092106a200 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c014f713cb651fa61d13dc8fe11055e2 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f8c9ad41ba9ba73ee43df8c0c740b53e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4e8105316812331bf3d865c15028e3e7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45c2cd7461fde3670a814ae0915a3584 2500w" />

Finally, we can clear and reset filters. For example, if we see there is clear there's a winner with the `singleminded` prompt, we can change filtering settings to see if any other model providers' models work as well with it:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a30d214745fcddc022e3c715267dfd2" alt="" data-og-width="2904" width="2904" data-og-height="832" height="832" data-path="langsmith/images/filter-singleminded.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bcf8d6a876b76137cc5682308f9e2f74 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c1e77dc12de1b65183d26877104a384 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3aa8585e58f88e5e7eac344ae386808f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=613558759a127d7003d3ae653e83298b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ba32a223d7012904c0eb63fe7c1fdb7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c643c61125d409c706ea607b1504740c 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-experiments-ui.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## For production use, consider using a configuration file or environment variables

**URL:** llms-txt#for-production-use,-consider-using-a-configuration-file-or-environment-variables

api_url = "https://api.smith.langchain.com"
api_key = os.environ.get("LANGSMITH_API_KEY")

if not api_key:
    raise ValueError("LANGSMITH_API_KEY environment variable is not set")

---

## Implement a LangChain integration

**URL:** llms-txt#implement-a-langchain-integration

Source: https://docs.langchain.com/oss/python/contributing/implement-langchain

Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.

LangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more.

Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.

<Tabs>
  <Tab title="Chat Models">
    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.

<Warning>
      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.
    </Warning>
  </Tab>

<Tab title="Tools">
    Tools are used in 2 main ways:

1. To define an "input schema" or "args schema" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a "tool call", or parameters to call the tool with.
    2. To take a "tool call" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.

The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.

<Warning>
      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.
    </Warning>
  </Tab>

<Tab title="Retrievers">
    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.

<Warning>
      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.
    </Warning>
  </Tab>

<Tab title="Vector Stores">
    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.

See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.

<Warning>
      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.
    </Warning>
  </Tab>

<Tab title="Embeddings">
    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) class.

<Warning>
      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text_embedding) for details on how LangChain embedding models function.
    </Warning>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to compare experiment results

**URL:** llms-txt#how-to-compare-experiment-results

**Contents:**
- Open the comparison view
- Adjust the table display
- View regressions and improvements
- Update baseline experiment and metric
- Open a trace
- Expand detailed view
- View summary charts
- Use experiment metadata as chart labels

Source: https://docs.langchain.com/langsmith/compare-experiment-results

When you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different [*experiments*](/langsmith/evaluation-concepts#experiment).

LangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.

## Open the comparison view

1. To access the experiment comaprison view, navigate to the **Datasets & Experiments** page.
2. Select a dataset, which will open the **Experiments** tab.
3. Select two or more experiments abd then click **Compare**.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67d4d6068012e92b101f900595734977" alt="The Experiments view in the UI with 3 experiments selected and the Compare button highlighted." data-og-width="1626" width="1626" data-og-height="966" height="966" data-path="langsmith/images/compare-select.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e520c4ec316531d45a9f538c3f36f78 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0aed7d0b1cb5d70321ea536ce1decea9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71ed0d1faed09fe9bd845e8049327948 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b8ccc3f4aa28630b633021b18eb64dd0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6e8b473e44e88e18aa743d1f75b9f6b5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c0784778d3b730d9e8b34400c852462b 2500w" />

## Adjust the table display

You can toggle between different views by clicking **Full** or **Compact** at the top of the **Comparing Experiments** page.

Toggling **Full** will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on **Expand detailed view** to view the full content.

You can also select and hide individual feedback keys or individual metrics in the **Display** settings dropdown to isolate the information you need in the comparison view.

## View regressions and improvements

In the comparison view, runs that *regressed* on your specified feedback key against your baseline experiment will be highlighted in red, while runs that *improved* will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.

Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=14f3a9b65dec55c9e4a0f5688d9e8f43" alt="The comparison view comparing 2 experiments with the regressions and improvements highlighted in red and green respectively." data-og-width="1632" width="1632" data-og-height="739" height="739" data-path="langsmith/images/regression-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7256046a4a6c5d28f350dd8d26bb7e3 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78c6a2cb4baa784abeb3336d5bea94c4 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9772371f026939748db446fff60fe19b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d5639c88a1a0214a0955ffbcda23faf0 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e4f232fbef17ff53192d8ac6db3913e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d55c8e280732ba5d9e3866564af29de9 2500w" />

## Update baseline experiment and metric

In order to track regressions, you need to:

1. In the **Baseline** dropdown at the top of the comparison view, select a **Baseline experiment** against which to compare. By default, the newest experiment is selected as the baseline.
2. Select a  **Feedback key** (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.
3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3df57789664fd92aba18ea2f438934bd" alt="The Baseline dropdown highlighted with a selected experiment and feedback key of &#x22;hallucination&#x22;." data-og-width="1627" width="1627" data-og-height="898" height="898" data-path="langsmith/images/select-baseline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=173b5ead77441998fc19669be3290aff 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=46f3bd52bad1fe8fc11ea269b37b15f9 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7cf328e494a3b674ad60f8a082d6b44 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae7ec9831c8c36855d085f48a8d8e6b3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4ef21b8740df8f02f7579239e9a9792a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c87682dbb3438f9c59a837a740d56979 2500w" />

If the example you're evaluating is from an ingested [run](/langsmith/observability-concepts#runs), you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b5feaa0894a645f4642c7422de937c7d" alt="The View trace icon highlighted from an ingested run." data-og-width="1632" width="1632" data-og-height="662" height="662" data-path="langsmith/images/open-source-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=49a42505f2992ab6caeab6f1c52c8e81 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c05cf153d430c062d42d1bf18ef73d6a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=734ad1f9a66a03615b259305298480f0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3ad016ed31dc9c43eaa74cdffcc66a5e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=991be6dc57ad88ef21aaba9c90ea24fa 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c4110db6d5e60b4e8df840c409cb5c38 2500w" />

## Expand detailed view

From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ff7f02d5ba6ea89902b4de0e37967e7" alt="An example in the Comparing Experiments view of a expanded view of the repetitions." data-og-width="1643" width="1643" data-og-height="926" height="926" data-path="langsmith/images/expanded-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4803efab7ae4a4363145857941d50055 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98f67feac8ad53ddb18be06955ab2895 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=628927173d27260eb7f76d82e66ff2d4 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6de1f01362d8658d9f673e91deec77b6 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8f68caed39717af5314beb187c3f542f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=00fa65a0580a10973a91d506b7cdbc09 2500w" />

## View summary charts

View summary charts by clicking on the **Charts** tab at the top of the page.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ada485c324964c8df96caa566ad11b1d" alt="The Charts summary page with 8 summary charts for the comparison." data-og-width="1639" width="1639" data-og-height="1147" height="1147" data-path="langsmith/images/charts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e21f174643b26892cae4280962653263 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=40f98d83faf7c13a05a9b410787c9a75 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a49ddad4c2c02bc14a0d765dc7dc261 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8aec566c2fd07b7d6e12a3e639cf6660 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=716edd3ad3626831a88c6db07f0fa566 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08e8ea763e825413a3aff09c061593ca 2500w" />

## Use experiment metadata as chart labels

You can configure the x-axis labels for the charts based on [experiment metadata](/langsmith/filter-experiments-ui#background-add-metadata-to-your-experiments).

Select a metadata key in the **x-axis** dropdown to change the chart labels.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb175478369f9a7a2314e44f6becc9e4" alt="x-axis dropdown highlighted with a list of the metadata attached to the experiment." data-og-width="1637" width="1637" data-og-height="1141" height="1141" data-path="langsmith/images/metadata-in-charts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b34f45a5f0736b6b83a73a00f94212f6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7084f0126711a63d6f539904f4e9091f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dee53003f1c958d09f839d24cb54eb63 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7a0323fef549e2aef3fcf22295d40d19 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81c37649a07ffa594bad4fd57cc8fb32 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=70d700760e7ba9dd0edce54429d11f95 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-experiment-results.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Feedback data format

**URL:** llms-txt#feedback-data-format

Source: https://docs.langchain.com/langsmith/feedback-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
</Check>

**Feedback** is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues)
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application)
4. Generated by an [online evaluator](/langsmith/online-evaluations)

Feedback is stored in a simple format with the following fields:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

Here is an example JSON representation of a feedback record in the above format:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/feedback-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Interrupt concurrent

**URL:** llms-txt#interrupt-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/interrupt-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `interrupt` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to `interrupted`. Below is a quick example of using the `interrupt` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can start our two runs and join the second one until it has completed:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has partial data from the first run + data from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, interrupted run was interrupted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/interrupt-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Using standard tests

**URL:** llms-txt#using-standard-tests

Source: https://docs.langchain.com/oss/python/contributing/standard-tests-langgraph

**Standard tests ensure your integration works as expected.**

When creating either a custom class for yourself or to publish in a LangGraph integration, it is important to add standard tests to ensure it works as expected. This guide will show you how to add standard tests to each integration type.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/standard-tests-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Contributing to documentation

**URL:** llms-txt#contributing-to-documentation

**Contents:**
- Getting started
  - Quick edit: fix a typo
  - Full development IDE setup
- Documentation types
  - Conceptual guides
  - References
- Writing standard
  - Mintlify components
  - Page structure
  - Localization

Source: https://docs.langchain.com/oss/python/contributing/documentation

Accessible documentation is a vital part of LangChain. We welcome both documentation for new features/[integrations](/oss/python/contributing/publish-langchain#adding-documentation), as well as community improvements to existing docs.

<Note>
  We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please [open a new issue](https://github.com/langchain-ai/docs/issues).
</Note>

All documentation falls under one of four categories:

<CardGroup cols={2}>
  <Card title="Conceptual guides" icon="lightbulb" href="#conceptual-guides">
    Explanations that provide deeper understanding and insights
  </Card>

<Card title="References" icon="book" href="#references">
    Technical descriptions of APIs and implementation details
  </Card>

<Card title="Tutorials (Learn)" icon="graduation-cap" href="/oss/python/learn">
    Lessons that guide users through practical activities to build understanding
  </Card>

<Card title="How-to guides" icon="wrench">
    Task-oriented instructions for users who know what they want to accomplish
  </Card>
</CardGroup>

### Quick edit: fix a typo

For simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:

<Info>
  **Prerequisites:**

* A [GitHub](https://github.com/) account
  * Basic familiarity of the [fork-and-pull workflow](https://graphite.dev/guides/understanding-git-fork-pull-request-workflow) for contributing
</Info>

<Steps>
  <Step title="Find the page">
    Navigate to any documentation page, scroll to the bottom of the page, and click the link "Edit the source of this page on GitHub"
  </Step>

<Step title="Fork the repository">
    GitHub will prompt you to fork the repository to your account. Make sure to fork into your <Tooltip tip="If you clone to an organization, maintainers are unable to make edits.">personal account</Tooltip>
  </Step>

<Step title="Make your changes">
    Correct the typo directly in GitHub's web editor
  </Step>

<Step title="Commit your changes">
    Click `Commit changes...` and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an [extended description](https://www.gitkraken.com/learn/git/best-practices/git-commit-message#git-commit-message-structure)
  </Step>

<Step title="Create pull request">
    GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist, if present
  </Step>
</Steps>

<Note>
  Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers. Do not bump the PR unless you have new information to provide - maintainers will address it as their availability permits.
</Note>

### Full development IDE setup

For larger changes or ongoing contributions, it's important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting.

Please review the steps to set up your environment outlined in the docs repo [`README.md`](https://github.com/langchain-ai/docs?tab=readme-ov-file#contributing).

## Documentation types

<Note>
  Where applicable, all documentation must have translations in both Python and JavaScript/TypeScript. See [our localization guide](#localization) for details.
</Note>

### Conceptual guides

Conceptual guide cover core concepts abstractly, providing deep understanding.

<AccordionGroup>
  <Accordion title="Characteristics">
    * **Understanding-focused**: Explain why things work as they do
    * **Broad perspective**: Higher and wider view than other types
    * **Design-oriented**: Explain decisions and trade-offs
    * **Context-rich**: Use analogies and comparisons
  </Accordion>

<Accordion title="Tips">
    * Explain design decisions - *"why does concept X exist?"*
    * Use analogies and reference alternatives
    * Avoid blending in too much reference content
    * Link to related tutorials and how-to guides
    * Focus on the **"why"** rather than the "how"
  </Accordion>

<Accordion title="Examples">
    <Columns cols={3}>
      <Card title="Memory" href="/oss/python/concepts/memory" icon="brain" />

<Card title="Context" href="/oss/python/concepts/context" icon="file-lines" />
    </Columns>
  </Accordion>
</AccordionGroup>

Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.

<Card title="Python reference" href="https://reference.langchain.com/python/" icon="python" arrow />

A good reference should:

* Describe what exists (all parameters, options, return values)
* Be comprehensive and structured for easy lookup
* Serve as the authoritative source for technical details

<AccordionGroup>
  <Accordion title="Contributing to references">
    See the contributing guide for [Python reference docs](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md).
  </Accordion>

<Accordion title="LangChain reference best practices">
    * **Be consistent**; follow existing patterns for provider-specific documentation
    * Include both basic usage (code snippets) and common edge cases/failure modes
    * Note when features require specific versions
  </Accordion>

<Accordion title="When to create new reference documentation">
    * New integrations or providers need dedicated reference pages
    * Complex configuration options require detailed explanation
    * API changes introduce new parameters or behavior
    * Community frequently asks questions about specific functionality
  </Accordion>
</AccordionGroup>

<Warning>
  Reference documentation has different standards - see the [reference docs contributing guide](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md) for details.
</Warning>

### Mintlify components

Use appropriate [Mintlify components](https://mintlify.com/docs/text) to enhance readability:

<Tabs>
  <Tab title="Callouts">
    * `<Note>` for helpful supplementary information
    * `<Warning>` for important cautions and breaking changes
    * `<Tip>` for best practices and advice
    * `<Info>` for neutral contextual information
    * `<Check>` for success confirmations
  </Tab>

<Tab title="Structure">
    * `<Steps>` for sequential procedures
    * `<Tabs>` for platform-specific content
    * `<AccordionGroup>` and `<Accordion>` for progressive disclosure
    * `<CardGroup>` and `<Card>` for highlighting content
  </Tab>

<Tab title="Code">
    * `<CodeGroup>` for multiple language examples
    * Always specify language tags on code blocks
    * Titles for code blocks (e.g. `Success`, `Error Response`)
  </Tab>
</Tabs>

Every documentation page must begin with YAML frontmatter:

All documentation must be localized in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:

<Note>
  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.
</Note>

<Note>
  If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.
</Note>

### General guidelines

<AccordionGroup>
  <Accordion title="Avoid duplication">
    Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.
  </Accordion>

<Accordion title="Link frequently">
    Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.
  </Accordion>

<Accordion title="Be concise">
    Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.
  </Accordion>
</AccordionGroup>

### Accessibility requirements

Ensure documentation is accessible to all users:

* Structure content for easy scanning with headers and lists
* Use specific, actionable link text instead of "click here"
* Include descriptive alt text for all images and diagrams

### Testing and validation

Before submitting documentation:

<Steps>
  <Step title="Test all code">
    Run all code examples to ensure they work correctly
  </Step>

<Step title="Check formatting">
    
  </Step>

<Step title="Build locally">

Verify the build completes without errors
  </Step>

<Step title="Review links">
    Check that all internal links work correctly
  </Step>
</Steps>

## In-code documentation

### Language and style

<Info>
  Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) with complete type hints for all public functions.
</Info>

Follow these standards for all documentation:

* **Voice**: Use second person ("you") for instructions
* **Tense**: Use active voice and present tense
* **Clarity**: Write clear, direct language for technical audiences
* **Consistency**: Use consistent terminology throughout
* **Conciseness**: Keep sentences concise while providing necessary context

<Warning>
  Always test code examples before publishing. Never include real API keys or secrets.
</Warning>

Requirements for code examples:

<Steps>
  <Step title="Completeness">
    Include complete, runnable examples that users can copy and execute without errors
  </Step>

<Step title="Realism">
    Use realistic data instead of placeholder values like "foo" or "example"
  </Step>

<Step title="Error handling">
    Show proper error handling and edge case management
  </Step>

<Step title="Documentation">
    Add explanatory comments for complex logic
  </Step>
</Steps>

Example of a well-documented function:

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You now have everything you need to contribute high-quality documentation to LangChain! 🎤🦜
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/documentation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Localization

All documentation must be localized in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:
```

Example 2 (unknown):
```unknown
<Note>
  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.
</Note>

<Note>
  If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.
</Note>

***

## Quality standards

### General guidelines

<AccordionGroup>
  <Accordion title="Avoid duplication">
    Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.
  </Accordion>

  <Accordion title="Link frequently">
    Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.
  </Accordion>

  <Accordion title="Be concise">
    Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.
  </Accordion>
</AccordionGroup>

### Accessibility requirements

Ensure documentation is accessible to all users:

* Structure content for easy scanning with headers and lists
* Use specific, actionable link text instead of "click here"
* Include descriptive alt text for all images and diagrams

### Testing and validation

Before submitting documentation:

<Steps>
  <Step title="Test all code">
    Run all code examples to ensure they work correctly
  </Step>

  <Step title="Check formatting">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Build locally">
```

Example 4 (unknown):
```unknown
Verify the build completes without errors
  </Step>

  <Step title="Review links">
    Check that all internal links work correctly
  </Step>
</Steps>

***

## In-code documentation

### Language and style

<Info>
  Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) with complete type hints for all public functions.
</Info>

Follow these standards for all documentation:

* **Voice**: Use second person ("you") for instructions
* **Tense**: Use active voice and present tense
* **Clarity**: Write clear, direct language for technical audiences
* **Consistency**: Use consistent terminology throughout
* **Conciseness**: Keep sentences concise while providing necessary context

### Code examples

<Warning>
  Always test code examples before publishing. Never include real API keys or secrets.
</Warning>

Requirements for code examples:

<Steps>
  <Step title="Completeness">
    Include complete, runnable examples that users can copy and execute without errors
  </Step>

  <Step title="Realism">
    Use realistic data instead of placeholder values like "foo" or "example"
  </Step>

  <Step title="Error handling">
    Show proper error handling and edge case management
  </Step>

  <Step title="Documentation">
    Add explanatory comments for complex logic
  </Step>
</Steps>

Example of a well-documented function:
```

---

## Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**Contents:**
- Run a pairwise experiment

model_names = ("gpt-3.5-turbo", "gpt-4o-mini")
experiment_ids = []
for model_name in model_names:
    resp = requests.post(
        "https://api.smith.langchain.com/api/v1/sessions",
        json={
            "start_time": datetime.utcnow().isoformat(),
            "reference_dataset_id": str(dataset_id),
            "description": "An optional description for the experiment",
            "name": f"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}",  # A name for the experiment
            "extra": {
                "metadata": {"foo": "bar"},  # Optional metadata
            },
        },
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )

experiment = resp.json()
    experiment_ids.append(experiment["id"])

# Run completions on all examples
    for example in examples:
        run_completion_on_example(example, model_name, experiment["id"])

# Issue a patch request to "end" the experiment by updating the end_time
    requests.patch(
        f"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}",
        json={"end_time": datetime.utcnow().isoformat()},
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Run a pairwise experiment

Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.

For more information, check out [this guide](/langsmith/evaluate-pairwise).
```

---

## server.py

**URL:** llms-txt#server.py

import langsmith as ls
from fastapi import FastAPI, Request

@ls.traceable
async def my_application():
    ...

app = FastAPI()  # Or Flask, Django, or any other framework

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    # as well as optional metadata/tags in `baggage`
    with ls.tracing_context(parent=request.headers):
        return await my_application()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.
```

---

## How to audit evaluator scores

**URL:** llms-txt#how-to-audit-evaluator-scores

**Contents:**
- In the comparison view
- In the runs table
- In the SDK

Source: https://docs.langchain.com/langsmith/audit-evaluator-scores

LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.

## In the comparison view

In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the "edit" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under "Make correction". If you would like, you may also attach an explanation to your correction. This is useful if you are using a [few-shot evaluator](/langsmith/create-few-shot-evaluators) and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5b815b771c18f291a9ef1b7defb9feb3" alt="Audit Evaluator Comparison View" data-og-width="3426" width="3426" data-og-height="1878" height="1878" data-path="langsmith/images/corrections-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4840ceb8c340713fef6a7999c5d9c6cb 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=08b128d085701f17e20fdc6d314253a8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7d6300071894c9ff3f1fc80c6954c13d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ffe472be6a4b33d741782a1bc3269c60 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=454422187b095a4ad0ec3ad9074d4301 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=93b0585577eb98e6d76db3cba6868473 2500w" />

In the runs table, find the "Feedback" column and click on the feedback tag to bring up the feedback details. Again, click the "edit" icon on the right to bring up the corrections view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e64530681ac9125751af2383b67ba35" alt="Audit Evaluator Runs Table" data-og-width="1734" width="1734" data-og-height="1002" height="1002" data-path="langsmith/images/corrections-runs-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46a1a8328ad238d876d3b003a7ab836a 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=60183b8a46938ccfe97a694cb941e7e3 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7a2b5008a78d18d283e81eae9a8e23c0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e58f3c26472e5e78209927d662ab72c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eeb374392b18fa613e43564269cd8ff8 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=117673d7c311c45f0b954313a35efb32 2500w" />

Corrections can be made via the SDK's `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/audit-evaluator-scores.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Dataset transformations

**URL:** llms-txt#dataset-transformations

**Contents:**
- Transformation types
- Chat Model prebuilt schema
  - Compatibility
  - Enablement
  - Specs

Source: https://docs.langchain.com/langsmith/dataset-transformations

LangSmith allows you to attach transformations to fields in your dataset's schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.

Coupled with [LangSmith's prebuilt JSON schema types](/langsmith/dataset-json-types), these allow you to do easy preprocessing of your data before saving it into your datasets.

## Transformation types

| Transformation Type          | Target Types                                                              | Functionality                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| remove\_system\_messages     | Array\[Message]                                                           | Filters a list of messages to remove any system messages.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| convert\_to\_openai\_message | Message Array\[Message]                                                   | Converts any incoming data from LangChain's internal serialization format to OpenAI's standard message format using langchain's [convert\_to\_openai\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html). If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) run or traced run from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client)), and remove the original key containing the message. |
| convert\_to\_openai\_tool    | Array\[Tool] Only available on top level fields in the inputs dictionary. | Converts any incoming data into OpenAI standard tool formats here using langchain's [convert\_to\_openai\_tool](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html) Will extract tool definitions from a run's invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the `extra.invocation_params` field of the run rather than inputs.                                                                                                                                                                                                                                                                                                                             |
| remove\_extra\_fields        | Object                                                                    | Removes any field not defined in the schema for this target object.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

## Chat Model prebuilt schema

The main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.

To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:

* Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers' SDK for downstream evaluation and experimentation
* Extract any tools used by your LLM and add them to your example's input to be used for reproducability in downstream evaluation

<Check>
  Users who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.
</Check>

The LLM run collection schema is built to collect data from LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) runs or traced runs from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client).

Please reach out to [support@langchain.dev](mailto:support@langchain.dev) if you have an LLM run you are tracing that is not compatible and we can extend support.

If you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.

When adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.

For enablement on new datasets, see our [dataset management how-to guide](/langsmith/manage-datasets-in-application).

For the full API specs of the prebuilt schema, see the below sections:

And the transformations look as follows:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dataset-transformations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Output Schema
```

Example 2 (unknown):
```unknown
#### Transformations

And the transformations look as follows:
```

---

## Data purging for compliance

**URL:** llms-txt#data-purging-for-compliance

**Contents:**
- Data retention
- Trace deletes
  - Deletion timeline
  - Delete specific traces
  - Delete by metadata
- Example deletes
  - Deleting examples is a two-step process
  - Deletion types

Source: https://docs.langchain.com/langsmith/data-purging-compliance

This guide covers the various features available after data reaches LangSmith Cloud servers to help you achieve your privacy goals.

LangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels.

For detailed information about data retention configuration and management, please refer to the [Data Retention concepts](/langsmith/administration-overview#data-retention) documentation.

You can use the API to complete trace deletes. The API supports two methods for deleting traces:

1. **By trace IDs and session ID**: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)
2. **By metadata**: Delete traces across a workspace that match any of the specified metadata key-value pairs

For more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/run/operation/delete_runs_api_v1_runs_delete_post).

<Warning>
  All trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.
</Warning>

### Deletion timeline

Trace deletions are processed during non-peak usage times and are not instant, usually within a few hours. There is no confirmation of deletion - you'll need to query the data again to verify it has been removed.

### Delete specific traces

To delete specific traces by their trace IDs from a single session:

### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):

This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned

This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[DELETE /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete)

* Specify example IDs and add `"hard_delete": true` to the query params of the request

#### Soft delete (default)

* Creates tombstoned entries with NULL inputs/outputs in the dataset
* Preserves historical data and maintains dataset versioning
* Only affects the current version of the dataset

* Permanently removes inputs, outputs, and metadata from ALL dataset versions
* Complete data removal when compliance requires zero-out across all versions
* Add `"hard_delete": true` to the query parameters

For more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-purging-compliance.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):
```

Example 2 (unknown):
```unknown
This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

## Example deletes

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned
```

Example 3 (unknown):
```unknown
This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[DELETE /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete)

* Specify example IDs and add `"hard_delete": true` to the query params of the request
```

---

## Configure prompt settings

**URL:** llms-txt#configure-prompt-settings

**Contents:**
- Model configurations
  - Create saved configurations
  - Edit configurations
  - Delete configurations
  - Extra parameters
- Tool settings
- Prompt formatting

Source: https://docs.langchain.com/langsmith/managing-model-configurations

The LangSmith [playground](/langsmith/prompt-engineering-concepts#prompt-playground) enables you to control various settings for your prompts. The **Prompt Settings** window contains:

* [Model configuration](#model-configurations)
* [Tool settings](#tool-settings)
* [Prompt formatting](#prompt-formatting)

To access **Prompt Settings**:

1. Navigate to the **Playground** in the left sidebar.
2. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name, which will launch the **Prompt Settings** window.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=6c0f7d7012b1e5295fe545149f955e6b" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="886" width="886" data-og-height="689" height="689" data-path="langsmith/images/model-config-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=280&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=4e3b9ad92f6f14f4e0523bef50199318 280w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=560&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=e538eb740495a8afa8bfc552b13ae294 560w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=840&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=ebe73264e977153c869fd04d1552d09b 840w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1100&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=2eeb01882056046bc73cc019d674af7e 1100w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1650&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=8f28fe2fe8054cf0623fb9d17f91966f 1650w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=2500&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=cf9ad39be3623e73322d123699e73f19 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=2e9da272c3fc8f7ac958c6e6d1da85e3" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="881" width="881" data-og-height="732" height="732" data-path="langsmith/images/model-config-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=280&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=652fb75a4682cfc813743a1260764e59 280w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=560&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=02c980a8387f3d69a5870660b1668080 560w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=840&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ee633c06056fa7ad46ea58a179afa169 840w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1100&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=f62a35ed726b5f89c156a40c9ea76f2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1650&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=18114575db8e6c7ce928763ddcb88c12 1650w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=2500&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ab24dc4975def52db55c4896ead5b77c 2500w" />
   </div>

## Model configurations

Model configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider’s documentation (for example, [Anthropic](https://docs.claude.com/en/api/messages), [OpenAI](https://platform.openai.com/docs/api-reference/responses/create)).

### Create saved configurations

1. In the **Model Configurations** tab, adjust the model configuration as needed—you can select a [saved configuration to edit](#edit-configurations).
2. Click the **Save As** button in the top bar.
3. Enter a name and optional description for your configuration and confirm.
4. Now that you've saved the configuration, anyone in your organization's [workspace](/langsmith/administration-overview#workspaces) can access it. All saved configurations are available in the **Model Configuration** dropdown.
5. Once you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the **Set as default** <Icon icon="thumbtack" iconType="solid" /> icon next to the model name in the dropdown.

### Edit configurations

1. To rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.
2. Update the current configuration's parameters as needed and click the **Save** button at the top.

### Delete configurations

1. Select the configuration you want to remove.
2. Click the trash <Icon icon="trash" iconType="solid" /> icon to delete it.

The **Extra Parameters** field allows you to pass additional model parameters that aren't directly supported in the LangSmith interface. This is particularly useful in two scenarios:

1. When model providers release new parameters that haven't yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:

2. When troubleshooting parameter-related errors in the playground, such as:

If you receive an error about unnecessary parameters (which is more common when using [LangChain JS](/oss/python/langchain/overview) for run tracing), you can use this field to remove the extra parameters.

[*Tools*](/langsmith/prompt-engineering-concepts#tools) enable your LLM to perform tasks like searching the web, looking up information, and so on. In the **Tools Settings** tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:

* **Parallel Tool Calls**: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)
* **Tool Choice**: Select the tools that the model can access. For more details, refer to [Use tools in a prompt](/langsmith/use-tools).

The **Prompt Format** tab allows you to specify:

* The **Prompt type**. For details on chat and completion prompts, refer to [Prompt engineering](/langsmith/prompt-engineering-concepts#chat-vs-completion) concepts.
* The **Template format**. For details on prompt templating and using variables, refer to [F-string vs. mustache](/langsmith/prompt-engineering-concepts##f-string-vs-mustache).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/managing-model-configurations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. When troubleshooting parameter-related errors in the playground, such as:
```

---

## ... the rest is the same as before

**URL:** llms-txt#...-the-rest-is-the-same-as-before

---

## Add the nodes

**URL:** llms-txt#add-the-nodes

orchestrator_worker_builder.add_node("orchestrator", orchestrator)
orchestrator_worker_builder.add_node("llm_call", llm_call)
orchestrator_worker_builder.add_node("synthesizer", synthesizer)

---

## Node-style: logging before model calls

**URL:** llms-txt#node-style:-logging-before-model-calls

@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

---

## Invoke the graph with an input and print the result

**URL:** llms-txt#invoke-the-graph-with-an-input-and-print-the-result

**Contents:**
  - Pass private state between nodes

print(graph.invoke({"question": "hi"}))

{'answer': 'bye'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Notice that the output of invoke only includes the output schema.

### Pass private state between nodes

In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.

Below, we'll create an example sequential graph consisting of three nodes (node\_1, node\_2 and node\_3), where private data is passed between the first two steps (node\_1 and node\_2), while the third step (node\_3) only has access to the public overall state.
```

---

## String content

**URL:** llms-txt#string-content

human_message = HumanMessage("Hello, how are you?")

---

## >          'review_configs': [

**URL:** llms-txt#>----------'review_configs':-[

---

## Connect nodes in a sequence

**URL:** llms-txt#connect-nodes-in-a-sequence

---

## > [

**URL:** llms-txt#>-[

---

## Regions FAQ

**URL:** llms-txt#regions-faq

**Contents:**
- Legal and compliance
- Features
- Plans and pricing

Source: https://docs.langchain.com/langsmith/regions-faq

<Note>
  See the [cloud architecture reference](/langsmith/cloud#architecture) for additional details.
</Note>

## Legal and compliance

#### *What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?*

LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to [support@langchain.dev](mailto:support@langchain.dev). Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.

#### *My company isn't based in the EU, can I still have my data hosted there?*

Yes, you can host your LangSmith data in the EU instance independent of your location.

#### *Do you have a legal entity in the EU that we can contract with?*

We do not have a legal entity in the EU for customer contracting today.

#### *Do different legal terms apply if I choose the EU region?*

The terms are the same for the EU and US regions.

#### *How do I use the EU instance?*

Follow the instructions [here](/langsmith/create-account-api-key) to create an account and an API key (make sure to change the region to EU in the dropdown)

#### *Are there any functional differences between US and EU cloud-managed LangSmith?*

There may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.

#### *Can an organization have workspaces in different regions?*

LangSmith does not support this at the moment, but if you are interested, please contact [support@langchain.dev](mailto:support@langchain.dev) and share your use case.

#### *Can I connect an EU organization to a US organization and share billing?*

LangSmith does not support this at the moment, but if you are interested, please contact [support@langchain.dev](mailto:support@langchain.dev) and share your use case.

#### *What data will be stored in my selected region?*

See the [cloud architecture reference](/langsmith/cloud#architecture) for details.

#### *How can I see my organization's region?*

Check your URL - any organizations on [https://eu.smith.langchain.com](https://eu.smith.langchain.com) are in the EU, and any on [https://smith.langchain.com](https://smith.langchain.com) are in the US.

#### *Can I switch my organization from the US to EU or vice versa?*

We do not support migration between regions at this time, but if you are interested in this feature, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

#### *Is the EU region available on all LangSmith plans?*

Yes, you can sign up for the EU region on all plans including free plans.

#### *Is pricing different for the EU region compared to the US region?*

No, pricing is the same for the EU and US regions.

#### *What currency is used for payment if I use the EU region?*

All LangSmith plans are paid in USD.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/regions-faq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## over the generic handler for all actions on the "threads" resource

**URL:** llms-txt#over-the-generic-handler-for-all-actions-on-the-"threads"-resource

@auth.on.threads
async def on_thread(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## Connect to a custom model

**URL:** llms-txt#connect-to-a-custom-model

**Contents:**
- Deploy a custom model server
- Adding configurable fields
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-endpoint

The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via , an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.

## Deploy a custom model server

For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server [here](https://github.com/langchain-ai/langsmith-model-server) We highly recommend using the sample model server as a starting point.

Depending on your model is an instruct-style or chat-style model, you will need to implement either `custom_model.py` or `custom_chat_model.py` respectively.

## Adding configurable fields

It is often useful to configure your model with different parameters. These might include temperature, model\_name, max\_tokens, etc.

To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.

You can add configurable fields by implementing the `with_configurable_fields` function in the `config.py` file. You can

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the `ChatCustomModel` or the `CustomModel` provider for chat-style model or instruct-style models.

Enter the `URL`. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7a2889af5f55cc73661033837a50fad6" alt="ChatCustomModel in Playground" data-og-width="2816" width="2816" data-og-height="1676" height="1676" data-path="langsmith/images/playground-custom-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c6509706fee0c85205e039f6868a5ead 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=deafe903353d9bec02143ebd578d5599 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=928818d42fc58d83e1b5a04ecaa36630 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=552046bb4c04947154a2c8fa3457beca 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2735d4eed015cafa0861079133c5220c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f59ef79d897acce3ae4835ce949d61b6 2500w" />

If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the `with_configurable_fields`.

See how to store your model configuration for later use [here](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-endpoint.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ... Define the graph ...

**URL:** llms-txt#...-define-the-graph-...

**Contents:**
- Capabilities
  - Human-in-the-loop
  - Memory
  - Time Travel
  - Fault-tolerance

graph.compile(
    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))
)
python  theme={null}
import sqlite3

from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.sqlite import SqliteSaver

serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY
checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde)
python  theme={null}
from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.postgres import PostgresSaver

serde = EncryptedSerializer.from_pycryptodome_aes()
checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde)
checkpointer.setup()
```

When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing [`CipherProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer).

### Human-in-the-loop

First, checkpointers facilitate [human-in-the-loop workflows](/oss/python/langgraph/interrupts) workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](/oss/python/langgraph/interrupts) for examples.

Second, checkpointers allow for ["memory"](/oss/python/concepts/memory) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](/oss/python/langgraph/add-memory) for information on how to add and manage conversation memory using checkpointers.

Third, checkpointers allow for ["time travel"](/oss/python/langgraph/use-time-travel), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/persistence.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Encryption

Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) implementation. The easiest way to create an encrypted serializer is via [`from_pycryptodome_aes`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes), which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable (or accepts a `key` argument):
```

Example 2 (unknown):
```unknown

```

---

## >       }

**URL:** llms-txt#>-------}

---

## Get started with Studio

**URL:** llms-txt#get-started-with-studio

**Contents:**
- Deployed graphs
- Local development server
  - Prerequisites
  - Setup
  - (Optional) Attach a debugger
- Next steps

Source: https://docs.langchain.com/langsmith/quick-start-studio

[Studio](/langsmith/studio) in the [LangSmith Deployments UI](https://smith.langchain.com) supports connecting to two types of graphs:

* Graphs deployed on [cloud or self-hosted](#deployed-graphs).
* Graphs running locally with [LangGraph server](#local-development-server).

Studio is accessed in the [LangSmith UI](https://smith.langchain.com) from the **Deployments** navigation.

For applications that are [deployed](/langsmith/deployment-quickstart), you can access Studio as part of that deployment. To do so, navigate to the deployment in the UI and select **Studio**.

This will load Studio connected to your live deployment, allowing you to create, read, and update the [threads](/oss/python/langgraph/persistence#threads), [assistants](/langsmith/assistants), and [memory](/oss/python/concepts/memory) in that deployment.

## Local development server

To test your application locally using Studio:

* Follow the [local application quickstart](/langsmith/local-server) first.
* If you don't want data [traced](/langsmith/observability-concepts#traces) to LangSmith, set `LANGSMITH_TRACING=false` in your application's `.env` file. With tracing disabled, no data leaves your local server.

1. Install the [LangGraph CLI](/langsmith/cli):

<Warning>
     **Browser Compatibility**
     Safari blocks `localhost` connections to Studio. To work around this, run the command with `--tunnel` to access Studio via a secure tunnel.
   </Warning>

This will start the LangGraph Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this [reference](/langsmith/cli#dev) to learn about all the options for starting the API server.

You will see the following logs:

Once running, you will automatically be directed to Studio.

2. For a running server, access the Dbugger with one of the following:

1. Directly navigate to the following URL: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.
   2. Navigate to **Deployments** in the UI, click the **Studio** button on a deployment, enter `http://127.0.0.1:2024` and click **Connect**.

If running your server at a different host or port, update the `baseUrl` to match.

### (Optional) Attach a debugger

For step-by-step debugging with breakpoints and variable inspection, run the following:

Then attach your preferred debugger:

<Tabs>
  <Tab title="VS Code">
    Add this configuration to `launch.json`:

<Tab title="PyCharm">
    1. Go to Run → Edit Configurations
    2. Click + and select "Python Debug Server"
    3. Set IDE host name: `localhost`
    4. Set port: `5678` (or the port number you chose in the previous step)
    5. Click "OK" and start debugging
  </Tab>
</Tabs>

<Tip>
  For issues getting started, refer to the [troubleshooting guide](/langsmith/troubleshooting-studio).
</Tip>

For more information on how to run Studio, refer to the following guides:

* [Run application](/langsmith/use-studio#run-application)
* [Manage assistants](/langsmith/use-studio#manage-assistants)
* [Manage threads](/langsmith/use-studio#manage-threads)
* [Iterate on prompts](/langsmith/observability-studio)
* [Debug LangSmith traces](/langsmith/observability-studio#debug-langsmith-traces)
* [Add node to dataset](/langsmith/observability-studio#add-node-to-dataset)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/quick-start-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

   <Warning>
     **Browser Compatibility**
     Safari blocks `localhost` connections to Studio. To work around this, run the command with `--tunnel` to access Studio via a secure tunnel.
   </Warning>

   This will start the LangGraph Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this [reference](/langsmith/cli#dev) to learn about all the options for starting the API server.

   You will see the following logs:
```

Example 4 (unknown):
```unknown
Once running, you will automatically be directed to Studio.

2. For a running server, access the Dbugger with one of the following:

   1. Directly navigate to the following URL: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.
   2. Navigate to **Deployments** in the UI, click the **Studio** button on a deployment, enter `http://127.0.0.1:2024` and click **Connect**.

   If running your server at a different host or port, update the `baseUrl` to match.

### (Optional) Attach a debugger

For step-by-step debugging with breakpoints and variable inspection, run the following:

<CodeGroup>
```

---

## Building our graph

**URL:** llms-txt#building-our-graph

graph_builder = StateGraph(State)

graph_builder.add_node(gather_info)
graph_builder.add_node(refund)
graph_builder.add_node(lookup)

graph_builder.set_entry_point("gather_info")
graph_builder.add_edge("lookup", END)
graph_builder.add_edge("refund", END)

refund_graph = graph_builder.compile()

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our refund graph:
```

---

## LangSmith Studio

**URL:** llms-txt#langsmith-studio

**Contents:**
- Features
  - Graph mode
  - Chat mode
- Learn more
- Video guide

Source: https://docs.langchain.com/langsmith/studio

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [LangGraph Server](/langsmith/langgraph-server)
  * [LangGraph CLI](/langsmith/cli)
</Info>

Studio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with [tracing](/langsmith/observability-concepts), [evaluation](/langsmith/evaluation), and [prompt engineering](/langsmith/prompt-engineering).

Key features of Studio:

* Visualize your graph architecture
* [Run and interact with your agent](/langsmith/use-studio#run-application)
* [Manage assistants](/langsmith/use-studio#manage-assistants)
* [Manage threads](/langsmith/use-studio#manage-threads)
* [Iterate on prompts](/langsmith/observability-studio)
* [Run experiments over a dataset](/langsmith/observability-studio#run-experiments-over-a-dataset)
* Manage [long term memory](/oss/python/concepts/memory)
* Debug agent state via [time travel](/oss/python/langgraph/use-time-travel)

Studio works for graphs that are deployed on [LangSmith](/langsmith/deployment-quickstart) or for graphs that are running locally via the [LangGraph Server](/langsmith/local-server).

Studio supports two modes:

Graph mode exposes the full feature-set and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).

Chat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph's whose state includes or extends [`MessagesState`](/oss/python/langgraph/use-graph-api#messagesstate).

* See this guide on how to [get started](/langsmith/quick-start-studio) with Studio.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Mi1gSlHwZLM?si=oWCeHQ640zPHoLwn" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Nodes

**URL:** llms-txt#nodes

def orchestrator(state: State):
    """Orchestrator that generates a plan for the report"""

# Generate queries
    report_sections = planner.invoke(
        [
            SystemMessage(content="Generate a plan for the report."),
            HumanMessage(content=f"Here is the report topic: {state['topic']}"),
        ]
    )

return {"sections": report_sections.sections}

def llm_call(state: WorkerState):
    """Worker writes a section of the report"""

# Generate section
    section = llm.invoke(
        [
            SystemMessage(
                content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."
            ),
            HumanMessage(
                content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"
            ),
        ]
    )

# Write the updated section to completed sections
    return {"completed_sections": [section.content]}

def synthesizer(state: State):
    """Synthesize full report from sections"""

# List of completed sections
    completed_sections = state["completed_sections"]

# Format completed section to str to use as context for final sections
    completed_report_sections = "\n\n---\n\n".join(completed_sections)

return {"final_report": completed_report_sections}

---

## Indicate that a chat model supports image inputs

**URL:** llms-txt#indicate-that-a-chat-model-supports-image-inputs

**Contents:**
- Running tests

class TestChatParrotLinkStandard(ChatModelIntegrationTests):
    # ... other required properties

@property
    def supports_image_inputs(self) -> bool:
        return True  # (The default is False)
bash  theme={null}
make test
make integration_test
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You should organize tests in these subdirectories relative to the root of your package:

  * `tests/unit_tests` for unit tests
  * `tests/integration_tests` for integration tests
</Note>

To see the complete list of configurable capabilities and their defaults, visit the [API reference](https://reference.langchain.com/python/langchain_tests) for standard tests.

Here are some example implementations of standard tests from popular integrations:

<Tabs>
  <Tab title="Unit tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/unit_tests/chat_models/test_base_standard.py" arrow>Unit tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
    </Columns>
  </Tab>

  <Tab title="Integration tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/integration_tests/chat_models/test_base_standard.py" arrow>Integration tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
    </Columns>
  </Tab>
</Tabs>

***

## Running tests

If bootstrapping an integration from a template, a `Makefile` is provided that includes targets for running unit and integration tests:
```

Example 2 (unknown):
```unknown
Otherwise, if you follow the recommended directory structure, you can run tests with:
```

---

## Node for routing.

**URL:** llms-txt#node-for-routing.

async def intent_classifier(
    state: State,
) -> Command[Literal["refund_agent", "question_answering_agent"]]:
    response = router_llm.invoke(
        [{"role": "system", "content": route_instructions}, *state["messages"]]
    )
    return Command(goto=response["intent"] + "_agent")

---

## Reject Concurrent

**URL:** llms-txt#reject-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/reject-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `reject` option for double texting, which rejects the new run of the graph by throwing an error and continues with the original run until completion. Below is a quick example of using the `reject` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can run a thread and try to run a second one with the "reject" option, which should fail since we have already started a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can verify that the original thread finished executing:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/reject-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Generate a completion

**URL:** llms-txt#generate-a-completion

client = openai.Client()
chat_completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages
)

---

## Integrations

**URL:** llms-txt#integrations

Source: https://docs.langchain.com/oss/python/reference/integrations-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/integrations-python.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Subsequent calls use the cache

**URL:** llms-txt#subsequent-calls-use-the-cache

**Contents:**
- All embedding models

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"Second call took: {time.time() - tic:.2f} seconds")
```

In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/python/integrations/stores/) for options.

## All embedding models

<Columns cols={3}>
  <Card title="Aleph Alpha" icon="link" href="/oss/python/integrations/text_embedding/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Anyscale" icon="link" href="/oss/python/integrations/text_embedding/anyscale" arrow="true" cta="View guide" />

<Card title="Ascend" icon="link" href="/oss/python/integrations/text_embedding/ascend" arrow="true" cta="View guide" />

<Card title="AI/ML API" icon="link" href="/oss/python/integrations/text_embedding/aimlapi" arrow="true" cta="View guide" />

<Card title="AwaDB" icon="link" href="/oss/python/integrations/text_embedding/awadb" arrow="true" cta="View guide" />

<Card title="AzureOpenAI" icon="link" href="/oss/python/integrations/text_embedding/azure_openai" arrow="true" cta="View guide" />

<Card title="Baichuan Text Embeddings" icon="link" href="/oss/python/integrations/text_embedding/baichuan" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/python/integrations/text_embedding/baidu_qianfan_endpoint" arrow="true" cta="View guide" />

<Card title="Baseten" icon="link" href="/oss/python/integrations/text_embedding/baseten" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/python/integrations/text_embedding/bedrock" arrow="true" cta="View guide" />

<Card title="BGE on Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/bge_huggingface" arrow="true" cta="View guide" />

<Card title="Bookend AI" icon="link" href="/oss/python/integrations/text_embedding/bookend" arrow="true" cta="View guide" />

<Card title="Clarifai" icon="link" href="/oss/python/integrations/text_embedding/clarifai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/python/integrations/text_embedding/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Clova Embeddings" icon="link" href="/oss/python/integrations/text_embedding/clova" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/python/integrations/text_embedding/cohere" arrow="true" cta="View guide" />

<Card title="DashScope" icon="link" href="/oss/python/integrations/text_embedding/dashscope" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/text_embedding/databricks" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/python/integrations/text_embedding/deepinfra" arrow="true" cta="View guide" />

<Card title="EDEN AI" icon="link" href="/oss/python/integrations/text_embedding/edenai" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/text_embedding/elasticsearch" arrow="true" cta="View guide" />

<Card title="Embaas" icon="link" href="/oss/python/integrations/text_embedding/embaas" arrow="true" cta="View guide" />

<Card title="Fake Embeddings" icon="link" href="/oss/python/integrations/text_embedding/fake" arrow="true" cta="View guide" />

<Card title="FastEmbed by Qdrant" icon="link" href="/oss/python/integrations/text_embedding/fastembed" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/python/integrations/text_embedding/fireworks" arrow="true" cta="View guide" />

<Card title="Google Gemini" icon="link" href="/oss/python/integrations/text_embedding/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/python/integrations/text_embedding/google_vertex_ai_palm" arrow="true" cta="View guide" />

<Card title="GPT4All" icon="link" href="/oss/python/integrations/text_embedding/gpt4all" arrow="true" cta="View guide" />

<Card title="Gradient" icon="link" href="/oss/python/integrations/text_embedding/gradient" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/text_embedding/greennode" arrow="true" cta="View guide" />

<Card title="Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/huggingfacehub" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/text_embedding/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="Infinity" icon="link" href="/oss/python/integrations/text_embedding/infinity" arrow="true" cta="View guide" />

<Card title="Instruct Embeddings" icon="link" href="/oss/python/integrations/text_embedding/instruct_embeddings" arrow="true" cta="View guide" />

<Card title="IPEX-LLM CPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm" arrow="true" cta="View guide" />

<Card title="IPEX-LLM GPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm_gpu" arrow="true" cta="View guide" />

<Card title="Intel Extension for Transformers" icon="link" href="/oss/python/integrations/text_embedding/itrex" arrow="true" cta="View guide" />

<Card title="Jina" icon="link" href="/oss/python/integrations/text_embedding/jina" arrow="true" cta="View guide" />

<Card title="John Snow Labs" icon="link" href="/oss/python/integrations/text_embedding/johnsnowlabs_embedding" arrow="true" cta="View guide" />

<Card title="LASER" icon="link" href="/oss/python/integrations/text_embedding/laser" arrow="true" cta="View guide" />

<Card title="Lindorm" icon="link" href="/oss/python/integrations/text_embedding/lindorm" arrow="true" cta="View guide" />

<Card title="Llama.cpp" icon="link" href="/oss/python/integrations/text_embedding/llamacpp" arrow="true" cta="View guide" />

<Card title="LLMRails" icon="link" href="/oss/python/integrations/text_embedding/llm_rails" arrow="true" cta="View guide" />

<Card title="LocalAI" icon="link" href="/oss/python/integrations/text_embedding/localai" arrow="true" cta="View guide" />

<Card title="MiniMax" icon="link" href="/oss/python/integrations/text_embedding/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/python/integrations/text_embedding/mistralai" arrow="true" cta="View guide" />

<Card title="Model2Vec" icon="link" href="/oss/python/integrations/text_embedding/model2vec" arrow="true" cta="View guide" />

<Card title="ModelScope" icon="link" href="/oss/python/integrations/text_embedding/modelscope_embedding" arrow="true" cta="View guide" />

<Card title="MosaicML" icon="link" href="/oss/python/integrations/text_embedding/mosaicml" arrow="true" cta="View guide" />

<Card title="Naver" icon="link" href="/oss/python/integrations/text_embedding/naver" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/text_embedding/nebius" arrow="true" cta="View guide" />

<Card title="Netmind" icon="link" href="/oss/python/integrations/text_embedding/netmind" arrow="true" cta="View guide" />

<Card title="NLP Cloud" icon="link" href="/oss/python/integrations/text_embedding/nlp_cloud" arrow="true" cta="View guide" />

<Card title="Nomic" icon="link" href="/oss/python/integrations/text_embedding/nomic" arrow="true" cta="View guide" />

<Card title="NVIDIA NIMs" icon="link" href="/oss/python/integrations/text_embedding/nvidia_ai_endpoints" arrow="true" cta="View guide" />

<Card title="Oracle Cloud Infrastructure" icon="link" href="/oss/python/integrations/text_embedding/oci_generative_ai" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/python/integrations/text_embedding/ollama" arrow="true" cta="View guide" />

<Card title="OpenClip" icon="link" href="/oss/python/integrations/text_embedding/open_clip" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/python/integrations/text_embedding/openai" arrow="true" cta="View guide" />

<Card title="OpenVINO" icon="link" href="/oss/python/integrations/text_embedding/openvino" arrow="true" cta="View guide" />

<Card title="Optimum Intel" icon="link" href="/oss/python/integrations/text_embedding/optimum_intel" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/text_embedding/oracleai" arrow="true" cta="View guide" />

<Card title="OVHcloud" icon="link" href="/oss/python/integrations/text_embedding/ovhcloud" arrow="true" cta="View guide" />

<Card title="Pinecone Embeddings" icon="link" href="/oss/python/integrations/text_embedding/pinecone" arrow="true" cta="View guide" />

<Card title="PredictionGuard" icon="link" href="/oss/python/integrations/text_embedding/predictionguard" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/python/integrations/text_embedding/premai" arrow="true" cta="View guide" />

<Card title="SageMaker" icon="link" href="/oss/python/integrations/text_embedding/sagemaker-endpoint" arrow="true" cta="View guide" />

<Card title="SambaNovaCloud" icon="link" href="/oss/python/integrations/text_embedding/sambanova" arrow="true" cta="View guide" />

<Card title="SambaStudio" icon="link" href="/oss/python/integrations/text_embedding/sambastudio" arrow="true" cta="View guide" />

<Card title="Self Hosted" icon="link" href="/oss/python/integrations/text_embedding/self-hosted" arrow="true" cta="View guide" />

<Card title="Sentence Transformers" icon="link" href="/oss/python/integrations/text_embedding/sentence_transformers" arrow="true" cta="View guide" />

<Card title="Solar" icon="link" href="/oss/python/integrations/text_embedding/solar" arrow="true" cta="View guide" />

<Card title="SpaCy" icon="link" href="/oss/python/integrations/text_embedding/spacy_embedding" arrow="true" cta="View guide" />

<Card title="SparkLLM" icon="link" href="/oss/python/integrations/text_embedding/sparkllm" arrow="true" cta="View guide" />

<Card title="TensorFlow Hub" icon="link" href="/oss/python/integrations/text_embedding/tensorflowhub" arrow="true" cta="View guide" />

<Card title="Text Embeddings Inference" icon="link" href="/oss/python/integrations/text_embedding/text_embeddings_inference" arrow="true" cta="View guide" />

<Card title="TextEmbed" icon="link" href="/oss/python/integrations/text_embedding/textembed" arrow="true" cta="View guide" />

<Card title="Titan Takeoff" icon="link" href="/oss/python/integrations/text_embedding/titan_takeoff" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/python/integrations/text_embedding/together" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/text_embedding/upstage" arrow="true" cta="View guide" />

<Card title="Volc Engine" icon="link" href="/oss/python/integrations/text_embedding/volcengine" arrow="true" cta="View guide" />

<Card title="Voyage AI" icon="link" href="/oss/python/integrations/text_embedding/voyageai" arrow="true" cta="View guide" />

<Card title="Xinference" icon="link" href="/oss/python/integrations/text_embedding/xinference" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/python/integrations/text_embedding/yandex" arrow="true" cta="View guide" />

<Card title="ZhipuAI" icon="link" href="/oss/python/integrations/text_embedding/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set environment variables for external services

**URL:** llms-txt#set-environment-variables-for-external-services

**Contents:**
- Troubleshooting
  - Wrong API Endpoints

export POSTGRES_URI_CUSTOM="postgresql://user:pass@host:5432/db"
export REDIS_URI_CUSTOM="redis://host:6379/0"
```

See the [environment variables documentation](/langsmith/env-var#postgres-uri-custom) for more details.

### Wrong API Endpoints

If you're experiencing connection issues, verify you're using the correct endpoint format for your LangSmith instance. There are two different APIs with different endpoints:

#### LangSmith API (Traces, Ingestion, etc.)

For LangSmith API operations (traces, evaluations, datasets):

| Region | Endpoint                             |
| ------ | ------------------------------------ |
| US     | `https://api.smith.langchain.com`    |
| EU     | `https://eu.api.smith.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api` where `<langsmith-url>` is your self-hosted instance URL.

<Note>
  If you're setting the endpoint in the `LANGSMITH_ENDPOINT` environment variable, you need to add `/v1` at the end (e.g., `https://api.smith.langchain.com/v1` or `http(s)://<langsmith-url>/api/v1` if self-hosted).
</Note>

#### LangSmith Deployments API (Deployments)

For LangSmith Deployments operations (deployments, revisions):

| Region | Endpoint                            |
| ------ | ----------------------------------- |
| US     | `https://api.host.langchain.com`    |
| EU     | `https://eu.api.host.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api-host` where `<langsmith-url>` is your self-hosted instance URL.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cicd-pipeline-example.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ChatGoogleGenerativeAI

**URL:** llms-txt#chatgooglegenerativeai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/google_generative_ai

[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).

This will help you getting started with `ChatGoogleGenerativeAI` [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).

### Integration details

| Class                                                                                                             | Package                                                                                     | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_generative_ai) |                                                Downloads                                                |                                                Version                                               |
| :---------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ | :---: | :----------: | :------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [ChatGoogleGenerativeAI](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html) | [@langchain/google-genai](https://api.js.langchain.com/modules/langchain_google_genai.html) |   ❌   |       ✅      |                                            ✅                                           | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ✅      |      ✅      |                               ✅                               |                              ✅                              |                                ❌                               |

You can access Google's `gemini` and `gemini-vision` models, as well as other
generative models in LangChain through `ChatGoogleGenerativeAI` class in the
`@langchain/google-genai` integration package.

<Tip>
  You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations. Click [here](/oss/javascript/integrations/chat/google_vertex_ai) to read the docs.
</Tip>

Get an API key here: [https://ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup)

Then set the `GOOGLE_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## Test multi-turn conversations

**URL:** llms-txt#test-multi-turn-conversations

**Contents:**
- From an existing run
- From a dataset
- Manually
- Next Steps

Source: https://docs.langchain.com/langsmith/multiple-messages

This how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d468f069fe0ee6eac2e95c8942990aa" alt="" data-og-width="963" width="963" data-og-height="552" height="552" data-path="langsmith/images/multiturn-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee61b9e81315c0b78ca5de6edb93f303 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae0c7a1b4e68b46ef29469a2dc04610e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9ce886cc977da99e442bf0ffeb82b683 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4365db9abb545cf507c7d3333404ffa 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd3ff5634b263980b0f9b9d97756d2b8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c28271d3bc0988362e988a64c1f1b396 2500w" />

## From an existing run

First, ensure you have properly [traced](/langsmith/observability) a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-run.gif?s=b4918bc6c6fac9c71859d962495db053" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-run.gif" data-optimize="true" data-opv="3" />

You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.

Before starting, make sure you have [set up your dataset](/langsmith/manage-datasets-in-application). Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.

Once you have created your dataset, head to the playground and [load your dataset](/langsmith/manage-datasets-in-application#from-the-prompt-playground) to evaluate.

Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-dataset.gif?s=42e2f11a348f50a7d2a0c8b6630c57e9" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-dataset.gif" data-optimize="true" data-opv="3" />

When you run your prompt, the messages from each example will be added as a list in place of the 'Messages List' variable.

There are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual.gif?s=1278a29854a66ee3dec92cc6f5059da0" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual.gif" data-optimize="true" data-opv="3" />

This is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a 'Messages List' variable and add your multi-turn conversation there:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual-list.gif?s=12458af5558482bdfe40855c3117c02b" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual-list.gif" data-optimize="true" data-opv="3" />

This allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the `Messages List` variable, allowing you to reuse this prompt across various runs.

Now that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can [add evaluators](/langsmith/code-evaluator) to classify results.

You can also read [these how-to guides](/langsmith/create-a-prompt) to learn more about how to use the playground to run evaluations.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-messages.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Name of the experiment we want to create from the historical runs

**URL:** llms-txt#name-of-the-experiment-we-want-to-create-from-the-historical-runs

baseline_experiment_name = f"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}"

---

## the desired output.

**URL:** llms-txt#the-desired-output.

class UserIntent(TypedDict):
    """The user's current intent in the conversation"""

intent: Literal["refund", "question_answering"]

---

## Target function

**URL:** llms-txt#target-function

async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    result = await graph.ainvoke({"messages": [
        { "role": "user", "content": inputs['question']},
    ]}, config={"env": "test"})
    return {"response": result["followup"]}

---

## Use with chat models

**URL:** llms-txt#use-with-chat-models

**Contents:**
  - Text prompts
  - Message prompts
  - Dictionary format
- Message types
  - System Message
  - Human Message
  - AI Message

messages = [system_msg, human_msg]
response = model.invoke(messages)  # Returns AIMessage
python  theme={null}
response = model.invoke("Write a haiku about spring")
python  theme={null}
from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage("You are a poetry expert"),
    HumanMessage("Write a haiku about spring"),
    AIMessage("Cherry blossoms bloom...")
]
response = model.invoke(messages)
python  theme={null}
messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]
response = model.invoke(messages)
python Basic instructions theme={null}
system_msg = SystemMessage("You are a helpful coding assistant.")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Detailed persona theme={null}
from langchain.messages import SystemMessage, HumanMessage

system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Message object theme={null}
  response = model.invoke([
    HumanMessage("What is machine learning?")
  ])
  python String shortcut theme={null}
  # Using a string is a shortcut for a single HumanMessage
  response = model.invoke("What is machine learning?")
  python Add metadata theme={null}
human_msg = HumanMessage(
    content="Hello!",
    name="alice",  # Optional: identify different users
    id="msg_123",  # Optional: unique identifier for tracing
)
python  theme={null}
response = model.invoke("Explain AI")
print(type(response))  # <class 'langchain_core.messages.AIMessage'>
python  theme={null}
from langchain.messages import AIMessage, SystemMessage, HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
### Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.
```

Example 2 (unknown):
```unknown
**Use text prompts when:**

* You have a single, standalone request
* You don't need conversation history
* You want minimal code complexity

### Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.
```

Example 3 (unknown):
```unknown
**Use message prompts when:**

* Managing multi-turn conversations
* Working with multimodal content (images, audio, files)
* Including system instructions

### Dictionary format

You can also specify messages directly in OpenAI chat completions format.
```

Example 4 (unknown):
```unknown
## Message types

* <Icon icon="gear" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions
* <Icon icon="user" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model
* <Icon icon="robot" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata
* <Icon icon="wrench" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling)

### System Message

A [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.
```

---

## Analyze an experiment

**URL:** llms-txt#analyze-an-experiment

**Contents:**
- Analyze a single experiment
  - Open the experiment view
  - View experiment results
  - Group results by metadata
  - Repetitions
  - Compare to another experiment
- Download experiment results as a CSV
- Rename an experiment

Source: https://docs.langchain.com/langsmith/analyze-an-experiment

This page describes some of the essential tasks for working with [*experiments*](/langsmith/evaluation-concepts#experiment) in LangSmith:

* **[Analyze a single experiment](#analyze-a-single-experiment)**: View and interpret experiment results, customize columns, filter data, and compare runs.
* **[Download experiment results as a CSV](#how-to-download-experiment-results-as-a-csv)**: Export your experiment data for external analysis and sharing.
* **[Rename an experiment](#how-to-rename-an-experiment)**: Update experiment names in both the Playground and Experiments view.

## Analyze a single experiment

After running an experiment, you can use LangSmith's experiment view to analyze the results and draw insights about your experiment's performance.

### Open the experiment view

To open the experiment view, select the relevant [*dataset*](/langsmith/evaluation-concepts#datasets) from the **Dataset & Experiments** page and then select the experiment you want to view.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74207f0a2422f89fdc75b23f0a88c58f" alt="Open experiment view" data-og-width="1640" width="1640" data-og-height="899" height="899" data-path="langsmith/images/select-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa173f885a87adac0c9ced9b3d553876 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=011a588eeca2032ab40c3612345a0b4f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7baa325ce358d73c61dbab0cce54222b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2810aba5a4ce3f7f0098f167bff7a78f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78d38c66b183cee29fa66959f339954c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bf7c03ef6f6ca6e269053a984f29c3a 2500w" />

### View experiment results

#### Customize columns

By default, the experiment view shows the input, output, and reference output for each [example](/langsmith/evaluation-concepts#examples) in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.

You can customize the columns using the **Display** button to make it easier to interpret experiment results:

* **Break out fields from inputs, outputs, and reference outputs** into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.
* **Hide and reorder columns** to create focused views for analysis.
* **Control decimal precision on feedback scores**. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.
* **Set the Heat Map threshold** to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0203a449f0f7df70900735ba540d712" alt="Column heatmap configuration" data-og-width="1780" width="1780" data-og-height="1688" height="1688" data-path="langsmith/images/column-heat-map.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ac06a00a4d11c8455d3996e3b3cc7ea 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c2207a21fea1078e5002d0d96c8c989 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ea673195e58c5bae676a782cb03bbbaa 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac4708a7612a3f7e7f82db28ac3a7b91 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=da649bbe343e2fd5e87d1845d1b19944 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f06ba0a04ef257e2d69193021938e761 2500w" />

<Tip>
  You can set default configurations for an entire dataset or temporarily save settings just for yourself.
</Tip>

To sort or filter feedback scores, you can use the actions in the column headers.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=067490743d1229ae233f15e46236ed67" alt="Sort and filter" data-og-width="1633" width="1633" data-og-height="788" height="788" data-path="langsmith/images/sort-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a116c731329b3fc088b57eae1d2f41a4 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d4ab881725f50ca23dc2650aa5376efd 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62027a48b9fd97a335caf3bd7e99d0dc 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137a7384faf8c1958dd309d5cfeba998 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3ea99639ad38e02f997f66f504663b8a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2372af19c9bd6d45b420e870f7b77402 2500w" />

Depending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.

* The **Compact** view shows each run as a one-line row, for ease of comparing scores at a glance.
* The **Full** view shows the full output for each run for digging into the details of individual runs.
* The **Diff** view shows the text difference between the reference output and the output for each run.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb916d33cea2f344f3483b42d3670696" alt="Diff view" data-og-width="1638" width="1638" data-og-height="969" height="969" data-path="langsmith/images/diff-mode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d30d2281df2f18a7c45fcae5eb839ded 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=edcf8cc4a3aca2bc3498a7c8f97b31b1 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3d0b55249029b6d72a98fea716d4dae7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ef89e5bf58911f5362635cfefe8ead27 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=337796d8a22712dea4c848ba8bc0e94a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b1019a7ec0249a9b58da199421689bc 2500w" />

Hover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.

To view the entire tracing project, click on the **View Project** button in the top right of the header.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c94c0d2ecedf248c639c971bf29196e6" alt="View trace" data-og-width="1634" width="1634" data-og-height="835" height="835" data-path="langsmith/images/view-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=45822be0900e0aaf9a06b9ddf7a8d91c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c0bc6ecc7cb67144c7899b8345d6ccef 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=85387bf5a2dd30a8411759069d4b3bbc 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f5edba01b477e0c8a4f7a5e81123ffb7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0241b8036e5e409a7d365e39d8d72bf1 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9a7f0042a8319a0398427665d50347d3 2500w" />

#### View evaluator runs

For evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you're running a [LLM-as-a-judge evaluator](/langsmith/llm-as-judge), you can view the prompt used for the evaluator in this run. If your experiment has [repetitions](/langsmith/evaluation-concepts#repetitions), you can click on the aggregate average score to find links to all of the individual runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc8df7233285b0f5a4ca9b44c06fcb47" alt="View evaluator runs" data-og-width="1634" width="1634" data-og-height="831" height="831" data-path="langsmith/images/evaluator-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=368b84b566407689f0e1b69f7e2d1ec8 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9f7888c2163589263d5ed0827bf9b55a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a9cf41bd478fa7bb86b5594f4a0f163a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=644229f6557ca6b1b68df6be37b79cf9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=da59385aa685d9e2c163f18385ee201c 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=039cad7ccdd3c0ed5d5e0e30151ed916 2500w" />

### Group results by metadata

You can add metadata to examples to categorize and organize them. For example, if you're evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either [via the UI](/langsmith/manage-datasets-in-application#edit-example-metadata) or [via the SDK](/langsmith/manage-datasets-programmatically#update-single-example).

To analyze results by metadata, use the **Group by** dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.

<Info>
  You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.
</Info>

If you've run your experiment with [*repetitions*](/langsmith/evaluation-concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.

When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=60962de04e5533d7718ca60fa9c7dcce" alt="Repetitions" data-og-width="1636" width="1636" data-og-height="959" height="959" data-path="langsmith/images/repetitions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8be83801a53f2544883faf173bc16ef1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7a924559be193efcc2c77dba3fea1231 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=25cbd580d06bda48419b83401c268c2d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9da3908c81d1c8fd44dde6d3ec7dfe1d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=775af0be371e662bea7ba7e29c2f21fd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d593460688be852a64638f092cba9f3 2500w" />

### Compare to another experiment

In the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see [how to compare experiment results](/langsmith/compare-experiment-results).

## Download experiment results as a CSV

LangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.

To download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the [Compact toggle](/langsmith/compare-experiment-results#adjust-the-table-display).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f237eb4b252a1018097be113434c22fa" alt="Download CSV" data-og-width="1705" width="1705" data-og-height="1345" height="1345" data-path="langsmith/images/download-experiment-results-as-csv.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=679144e623fdd6a5ff643d66378f5f21 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ca7f980d6a11cf6ac6f54a2e8799ac08 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ae945f1172fad11a20e16c15ae859409 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e64dbaffad47fb30e1d44f032533bdcb 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=72f1815228051375821c98a26edd0452 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=51944ba9dddf5dddaa77b12a8a1c8d8a 2500w" />

## Rename an experiment

<Note>
  Experiment names must be unique per workspace.
</Note>

You can rename an experiment in the LangSmith UI in:

* The [Playground](#renaming-an-experiment-in-the-playground). When running experiments in the Playground, a default name with the format `pg::prompt-name::model::uuid` (eg. `pg::gpt-4o-mini::897ee630`) is automatically assigned.

You can rename an experiment immediately after running it by editing its name in the Playground table header.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5b647ff1894376bbb727dabc4d73f039" alt="Edit name in playground" data-og-width="1372" width="1372" data-og-height="200" height="200" data-path="langsmith/images/rename-in-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d505597b1d2e180ebfa05d4361d3225 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d09703c543257203a19434a8a30458e1 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b3a8f6f2bd9fbfb9ba0c3f6c7477e04c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=591176ffbeb1f0a16f084f31f6717c6f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381a59f8752307204b3e9f82a2fdbd16 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cba501d5c0d826f6f61b57d01e1d94c9 2500w" />

* The [Experiments view](#renaming-an-experiment-in-the-experiments-view). When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=16afa853361ec265a0c7917d815f3132" alt="Edit name in experiments view" data-og-width="1628" width="1628" data-og-height="224" height="224" data-path="langsmith/images/rename-in-experiments-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d87e21875d807e1553289f096137af3f 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=761aab57b06c1e3644ac633f10565e26 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0dd7d731b4aae11a4b08644e06ac0eb9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=333a93f0e1d2ff38f9bdcfcfed33c825 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48d1b7cacab8e71bb4d0fe79573d6f11 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8185ba3772c13f40ea57501dd4982bc3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/analyze-an-experiment.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Log multimodal traces

**URL:** llms-txt#log-multimodal-traces

Source: https://docs.langchain.com/langsmith/log-multimodal-traces

LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs.

In order to log images, use `wrap_openai`/ `wrapOpenAI` in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input.

The image will be rendered as part of the trace in the LangSmith UI.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ff41711a0992c77f86cbc9f523e2ae93" alt="" data-og-width="1600" width="1600" data-og-height="1216" height="1216" data-path="langsmith/images/multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d1119193b53a405869cbda1d17c88544 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147037549a28d87622e4c7d4d15ccc2b 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=145003b20883ea39001adaf0eaf45529 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01cc9ca22bca3312a5d4c1b356e5b8fc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=047b36000ced498139fa793c1815440a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f7868994ae8fd23e9239b4b8d77ee4a0 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-multimodal-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Set up OpenTelemetry trace provider

**URL:** llms-txt#set-up-opentelemetry-trace-provider

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.smith.langchain.com/otel/v1/traces",
    headers={"x-api-key": os.getenv("LANGSMITH_API_KEY"), "Langsmith-Project": "my_project"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

---

## Log user feedback using the SDK

**URL:** llms-txt#log-user-feedback-using-the-sdk

**Contents:**
- Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Source: https://docs.langchain.com/langsmith/attach-user-feedback

<Tip>
  **Key concepts**

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

LangSmith makes it easy to attach feedback to traces.
This feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.

## Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Here we'll walk through how to log feedback using the SDK.

<Info>
  **Child runs**
  You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.
  This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.
</Info>

<Tip>
  **Non-blocking creation (Python only)**
  The Python client will automatically background feedback creation if you pass `trace_id=` to [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).
  This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.
</Tip>

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](/langsmith/access-current-span) for how to get the run ID of an in-progress run.

To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](/langsmith/filter-traces-in-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/attach-user-feedback.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## ❌ This will error

**URL:** llms-txt#❌-this-will-error

agent = create_deep_agent(use_longterm_memory=True)  # Missing store!

---

## Helper function to load files as bytes

**URL:** llms-txt#helper-function-to-load-files-as-bytes

def load_file(file_path: str) -> bytes:
    with open(file_path, "rb") as f:
        return f.read()

---

## Keep our previous handlers...

**URL:** llms-txt#keep-our-previous-handlers...

from langgraph_sdk import Auth

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.create.value,
):
    """Add owner when creating threads.

This handler runs when creating new threads and does two things:
    1. Sets metadata on the thread being created to track ownership
    2. Returns a filter that ensures only the creator can access it
    """
    # Example value:
    #  {'thread_id': UUID('99b045bc-b90b-41a8-b882-dabc541cf740'), 'metadata': {}, 'if_exists': 'raise'}

# Add owner metadata to the thread being created
    # This metadata is stored with the thread and persists
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity

# Return filter to restrict access to just the creator
    return {"owner": ctx.user.identity}

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.read.value,
):
    """Only let users read their own threads.

This handler runs on read operations. We don't need to set
    metadata since the thread already exists - we just need to
    return a filter to ensure users can only see their own threads.
    """
    return {"owner": ctx.user.identity}

@auth.on.assistants
async def on_assistants(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.assistants.value,
):
    # For illustration purposes, we will deny all requests
    # that touch the assistants resource
    # Example value:
    # {
    #     'assistant_id': UUID('63ba56c3-b074-4212-96e2-cc333bbc4eb4'),
    #     'graph_id': 'agent',
    #     'config': {},
    #     'metadata': {},
    #     'name': 'Untitled'
    # }
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="User lacks the required permissions.",
    )

---

## ChatAnthropic

**URL:** llms-txt#chatanthropic

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/anthropic

[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.

This will help you getting started with Anthropic [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).

### Integration details

| Class                                                                                        | Package                                                                      | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/anthropic/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- | :---: | :----------: | :--------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) |   ❌   |       ✅      |                                       ✅                                      | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

You'll need to sign up and obtain an [Anthropic API key](https://www.anthropic.com/), and install the `@langchain/anthropic` integration package.

Head to [Anthropic's website](https://www.anthropic.com/) to sign up to Anthropic and generate an API key. Once you've done this set the `ANTHROPIC_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## OpenAIEmbeddings

**URL:** llms-txt#openaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/openai

This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

### Integration details

| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## We'll use structured outputs to enforce that the model returns only

**URL:** llms-txt#we'll-use-structured-outputs-to-enforce-that-the-model-returns-only

---

## LangSmith API reference

**URL:** llms-txt#langsmith-api-reference

Source: https://docs.langchain.com/langsmith/smith-api-ref

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-api-ref.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Microsoft

**URL:** llms-txt#microsoft

**Contents:**
- Chat models
  - Azure OpenAI
  - Azure AI
  - Azure ML Chat Online Endpoint
- LLMs
  - Azure ML
  - Azure OpenAI
- Embedding Models
  - Azure OpenAI
  - Azure AI

Source: https://docs.langchain.com/oss/python/integrations/providers/microsoft

All LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products.

Microsoft offers three main options for accessing chat models through Azure:

1. [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) - Provides access to OpenAI's powerful models like o3, 4.1, and other models through Microsoft Azure's secure enterprise platform.
2. [Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models) - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.
3. [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/) - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.

> [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.

> [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from OpenAI including the `GPT-3`, `Codex` and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.

Set the environment variables to get access to the `Azure OpenAI` service.

See a [usage example](/oss/python/integrations/chat/azure_chat_openai)

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

Configure your API key and Endpoint.

See a [usage example](/oss/python/integrations/chat/azure_ai)

### Azure ML Chat Online Endpoint

See the documentation [here](/oss/python/integrations/chat/azureml_chat_endpoint) for accessing chat
models hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/).

See a [usage example](/oss/python/integrations/llms/azure_ml).

See a [usage example](/oss/python/integrations/llms/azure_openai).

Microsoft offers two main options for accessing embedding models through Azure:

See a [usage example](/oss/python/integrations/text_embedding/azure_openai)

Configure your API key and Endpoint.

> [Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets
> to cloud storage and register existing data assets from the following sources:
>
> * `Microsoft OneLake`
> * `Azure Blob Storage`
> * `Azure Data Lake gen 2`

First, you need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/azure_ai_data).

### Azure AI Document Intelligence

> [Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known
> as `Azure Form Recognizer`) is machine-learning
> based service that extracts texts (including handwriting), tables, document structures,
> and key-value-pairs
> from digital or scanned PDFs, images, Office and HTML files.
>
> Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/azure_document_intelligence).

### Azure Blob Storage

> [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.

`Azure Blob Storage` is designed for:

* Serving images or documents directly to a browser.
* Storing files for distributed access.
* Streaming video and audio.
* Writing to log files.
* Storing data for backup and restore, disaster recovery, and archiving.
* Storing data for analysis by an on-premises or Azure-hosted service.

See [usage examples for the Azure Blob Storage Loader](/oss/python/integrations/document_loaders/azure_blob_storage).

### Microsoft OneDrive

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onedrive).

### Microsoft OneDrive File

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

> [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_word).

> [Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) is a spreadsheet editor developed by
> Microsoft for Windows, macOS, Android, iOS and iPadOS.
> It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming
> language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.

The `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files.
The page content will be the raw text of the Excel file. If you use the loader in `"elements"` mode, an HTML
representation of the Excel file will be available in the document metadata under the `text_as_html` key.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_excel).

### Microsoft SharePoint

> [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system
> that uses workflow applications, “list” databases, and other web parts and security features to
> empower business teams to work together developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_sharepoint).

### Microsoft PowerPoint

> [Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_powerpoint).

### Microsoft OneNote

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onenote).

### Playwright URL Loader

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/url/#playwright-url-loader).

AI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec\&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode.

Below are two available Azure Cosmos DB APIs that can provide vector store functionalities.

#### Azure Cosmos DB for MongoDB (vCore)

> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support.
> You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.
> Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.

##### Installation and Setup

See [detailed configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

We need to install `pymongo` python package.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.

With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.

[Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

#### Azure Cosmos DB NoSQL

> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview.
> This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors
> directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,
> but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,
> as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the
> efficiency of vector-based operations.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

We need to install `azure-cosmos` python package.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available
in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.

[Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

Since Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

### Azure SQL Database

> [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.

By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/sqlserver).

We need to install the `langchain-sqlserver` python package.

##### Deploy Azure SQL DB on Microsoft Azure

[Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/sqlserver).

[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service
that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid
queries at scale. See [here](/oss/python/integrations/vectorstores/azuresearch) for usage examples.

> [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.

> Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:
>
> * A search engine for full text search over a search index containing user-owned content
> * Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation
> * Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more
> * Programmability through REST APIs and client libraries in Azure SDKs
> * Azure integration at the data layer, machine learning layer, and AI (AI Services)

See [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).

See a [usage example](/oss/python/integrations/retrievers/azure_ai_search).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

You need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

See a [usage example](/oss/python/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

### Azure Container Apps dynamic sessions

We need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.
See the instructions [here](/oss/python/integrations/tools/azure_dynamic_sessions/#setup).

We need to install a python package.

See a [usage example](/oss/python/integrations/tools/azure_dynamic_sessions).

Follow the documentation [here](/oss/python/integrations/tools/bing_search) to get a detail explanations and instructions of this tool.

The environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.

### Azure AI Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_ai_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the following tools:

* Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.image_analysis.AzureAiServicesImageAnalysisTool.html)
* Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.document_intelligence.AzureAiServicesDocumentIntelligenceTool.html)
* Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.speech_to_text.AzureAiServicesSpeechToTextTool.html)
* Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_to_speech.AzureAiServicesTextToSpeechTool.html)
* Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_analytics_for_health.AzureAiServicesTextAnalyticsForHealthTool.html)

### Azure Cognitive Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_cognitive_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:

* `AzureCogsFormRecognizerTool`: Form Recognizer API
* `AzureCogsImageAnalysisTool`: Image Analysis API
* `AzureCogsSpeech2TextTool`: Speech2Text API
* `AzureCogsText2SpeechTool`: Text2Speech API
* `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API

### Microsoft Office 365 email and calendar

We need to install `O365` python package.

See a [usage example](/oss/python/integrations/tools/office365).

#### Office 365 individual tools

You can use individual tools from the Office 365 Toolkit:

* `O365CreateDraftMessage`: creating a draft email in Office 365
* `O365SearchEmails`: searching email messages in Office 365
* `O365SearchEvents`: searching calendar events in Office 365
* `O365SendEvent`: sending calendar events in Office 365
* `O365SendMessage`: sending an email in Office 365

### Microsoft Azure PowerBI

We need to install `azure-identity` python package.

See a [usage example](/oss/python/integrations/tools/powerbi).

#### PowerBI individual tools

You can use individual tools from the Azure PowerBI Toolkit:

* `InfoPowerBITool`: getting metadata about a PowerBI Dataset
* `ListPowerBITool`: getting tables names
* `QueryPowerBITool`: querying a PowerBI Dataset

### PlayWright Browser Toolkit

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/playwright).

#### PlayWright Browser individual tools

You can use individual tools from the PlayWright Browser Toolkit.

### Azure Cosmos DB for Apache Gremlin

We need to install a python package.

See a [usage example](/oss/python/integrations/graphs/azure_cosmosdb_gremlin).

> [Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`,
> is a web search engine owned and operated by `Microsoft`.

See a [usage example](/oss/python/integrations/tools/bing_search).

### Microsoft Presidio

> [Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium ‘protection, garrison’)
> helps to ensure sensitive data is properly managed and governed. It provides fast identification and
> anonymization modules for private entities in text and images such as credit card numbers, names,
> locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.

First, you need to install several python packages and download a `SpaCy` model.

See [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio_data_anonymization).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/microsoft.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Set the environment variables to get access to the `Azure OpenAI` service.
```

Example 3 (unknown):
```unknown
See a [usage example](/oss/python/integrations/chat/azure_chat_openai)
```

Example 4 (unknown):
```unknown
### Azure AI

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

<CodeGroup>
```

---

## Set a sampling rate for traces

**URL:** llms-txt#set-a-sampling-rate-for-traces

**Contents:**
- Set a global sampling rate
- Set different sampling rates per client

Source: https://docs.langchain.com/langsmith/sample-traces

When working with high-volume applications, you may not want to log every trace to LangSmith. Sampling rates allow you to control what percentage of traces are logged, helping you balance observability needs with cost considerations.

## Set a global sampling rate

<Note>
  This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.
</Note>

By default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the `LANGSMITH_TRACING_SAMPLING_RATE` environment variable to any float between `0` (no traces) and `1` (all traces). For instance, setting the following environment variable will log 75% of the traces.

This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:

```python  theme={null}
from langsmith import Client, tracing_context

**Examples:**

Example 1 (unknown):
```unknown
This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:
```

---

## Thread 2: Read from long-term memory (different conversation!)

**URL:** llms-txt#thread-2:-read-from-long-term-memory-(different-conversation!)

config2 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "What are my preferences?"}]
}, config=config2)

---

## Annotate traces and runs inline

**URL:** llms-txt#annotate-traces-and-runs-inline

Source: https://docs.langchain.com/langsmith/annotate-traces-inline

LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue.
You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.
Feedback tags are associated with your [workspace](/langsmith/administration-overview#workspaces).

<Note>
  **You can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.**

This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.
</Note>

To annotate a trace inline, click on the `Annotate` in the upper right corner of trace view for any particular run that is part of the trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=193363baef8b46e592fa63b299b407af" alt="" data-og-width="1722" width="1722" data-og-height="1035" height="1035" data-path="langsmith/images/annotate-trace-inline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3751be3488a8a5a488eb4277b4bc574e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e4c94e427fd4a4c14e5fca6b1a4fce14 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3cc298e64a52af87bb4d46760352c958 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6f13e3f345430b443c6f45642d6031eb 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4259e830834b95f09e4b457b1e9a2807 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f65fa998f3962f7b971c8433c8be36aa 2500w" />

This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow [this guide](./set-up-feedback-criteria) to set up feedback tags for your workspace.
You can also set up new feedback criteria from within the pane itself.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a16e79d91b435f6c5de94d0d58daa59" alt="" data-og-width="1376" width="1376" data-og-height="758" height="758" data-path="langsmith/images/annotation-sidebar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b0ee76da4d19ca7d7b5640865f738a7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2e1acb5f129b2f865eef0399b0b06217 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ac7a1f2abf6faeb918eee11b702bb450 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1454e680d2c557541ea68c794018ceae 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5f45921db9ec69f138be197f71ab6c22 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c0c75ec7d51de574a3b7b1283ed06907 2500w" />

You can use the labeled keyboard shortcuts to streamline the annotation process.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-traces-inline.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Try creating an assistant. This should fail

**URL:** llms-txt#try-creating-an-assistant.-this-should-fail

try:
    await alice.assistants.create("agent")
    print("❌ Alice shouldn't be able to create assistants!")
except Exception as e:
    print("✅ Alice correctly denied access:", e)

---

## Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.

**URL:** llms-txt#optionally-add-the-'traceable'-decorator-to-trace-the-inputs/outputs-of-this-function.

**Contents:**
- UI
  - Pre-built evaluators
- Customize your LLM-as-a-judge evaluator
  - Select/create the evaluator
  - Configure the evaluator
  - Save the evaluator

@traceable
def dummy_app(inputs: dict) -> dict:
    return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

ls_client = Client()
dataset = ls_client.create_dataset("big questions")
examples = [
    {"inputs": {"question": "how will the universe end"}},
    {"inputs": {"question": "are we alone"}},
]
ls_client.create_examples(dataset_id=dataset.id, examples=examples)

results = evaluate(
    dummy_app,
    data=dataset,
    evaluators=[valid_reasoning]
)
```

See [here](/langsmith/code-evaluator) for more on how to write a custom evaluator.

### Pre-built evaluators

Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:

* **Hallucination**: Detect factually incorrect outputs. Requires a reference output.
* **Correctness**: Check semantic similarity to a reference.
* **Conciseness**: Evaluate whether an answer is a concise response to a question.
* **Code checker**: Verify correctness of code answers.

You can configure these evaluators::

* When running an evaluation using the [playground](/langsmith/observability-concepts#prompt-playground)
* As part of a dataset to [automatically run evaluations on experiments](/langsmith/bind-evaluator-to-dataset)
* When running an [online evaluation](/langsmith/online-evaluations#configure-llm-as-judge-evaluators)

## Customize your LLM-as-a-judge evaluator

Add specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.

### Select/create the evaluator

* In the playground or from a dataset: Select the **+Evaluator** button
* From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**

Select the **Create your own evaluator option**. Alternatively, you may start by selecting a pre-built evaluator and editing it.

### Configure the evaluator

Create a new prompt, or choose an existing prompt from the [prompt hub](/langsmith/prompt-engineering-quickstart).

* **Create your own prompt**: Create a custom prompt inline.

* **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.

Select the desired model from the provided options.

#### Mapping variables

Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.

To add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.

You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable.

Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.

#### Improve your evaluator with few-shot examples

To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](/langsmith/create-few-shot-evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.

Learn [how to set up few-shot examples and make corrections](/langsmith/create-few-shot-evaluators).

#### Feedback configuration

Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](/langsmith/observability-concepts#feedback) to a run or example. Defining feedback for your evaluator:

1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.

2. **Add a description**: Describe what the feedback represents.

3. **Choose a feedback type**:

* **Boolean**: True/false feedback.
* **Categorical**: Select from predefined categories.
* **Continuous**: Numerical scoring within a specified range.

Behind the scenes, feedback configuration is added as [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) to the LLM-as-a-judge prompt. If you're using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.

### Save the evaluator

Once your are finished configuring, save your changes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/llm-as-judge.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Example Collector Configuration: Metrics and Traces Gateway

**URL:** llms-txt#example-collector-configuration:-metrics-and-traces-gateway

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-collector.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## >          ]

**URL:** llms-txt#>----------]

---

## baseline_results.to_pandas()

**URL:** llms-txt#baseline_results.to_pandas()

**Contents:**
  - Define and evaluate new system

python  theme={null}
candidate_results = await client.aevaluate(
    agent.with_config(model="gpt-4o"),
    data=dataset_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
    experiment_prefix="candidate-gpt-4o",
)

**Examples:**

Example 1 (unknown):
```unknown
### Define and evaluate new system

Now, let's define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we've made our model configurable we can just update the default config passed to our agent:
```

---

## Scalability & resilience

**URL:** llms-txt#scalability-&-resilience

**Contents:**
- Server scalability
- Queue scalability
- Resilience
- Postgres resilience
- Redis resilience

Source: https://docs.langchain.com/langsmith/scalability-and-resilience

LangSmith is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.

## Server scalability

As you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.

As you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.

While a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.

When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which

* stops accepting new HTTP requests
* gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)
* stops the instance from picking up more runs from the queue

If a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.

## Postgres resilience

For deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the [Cloud deployment option](/langsmith/cloud) for [`Production` deployment types](/langsmith/control-plane#deployment-types) only.

All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the LangGraph Server unavailable.

All data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.

All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LangGraph Server unavailable.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/scalability-and-resilience.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## node_3 does not see the private data.

**URL:** llms-txt#node_3-does-not-see-the-private-data.

builder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])
builder.add_edge(START, "node_1")
graph = builder.compile()

---

## Publish an integration

**URL:** llms-txt#publish-an-integration

Source: https://docs.langchain.com/oss/python/contributing/publish-langgraph

**Make your integration available to the community.**

Now that your package is implemented and tested, you can publish it and add documentation to make it discoverable by the community.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/publish-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## You can tag a specific dataset version with a semantic name, like "prod"

**URL:** llms-txt#you-can-tag-a-specific-dataset-version-with-a-semantic-name,-like-"prod"

**Contents:**
- Evaluate on a specific dataset version
  - Use `list_examples`
- Evaluate on a split / filtered view of a dataset
  - Evaluate on a filtered view of a dataset
  - Evaluate on a dataset split
- Share a dataset
  - Share a dataset publicly
  - Unshare a dataset
- Export a dataset
- Export filtered traces from experiment to dataset

client.update_dataset_tag(
    dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod"
)
python Python theme={null}
  from langsmith import Client

# Assumes actual outputs have a 'class' key.
  # Assumes example outputs have a 'label' key.
  def correct(outputs: dict, reference_outputs: dict) -> bool:
    return outputs["class"] == reference_outputs["label"]

results = ls_client.evaluate(
      lambda inputs: {"class": "Not toxic"},
      # Pass in filtered data here:
      data=ls_client.list_examples(
        dataset_name="Toxic Queries",
        as_of="latest",  # specify version here
      ),
      evaluators=[correct],
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      asOf: "latest",
    }),
    evaluators: [correctLabel],
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key": "desired_value"}),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      metadata: {"desired_key": "desired_value"},
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, splits=["test", "training"]),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      splits: ["test", "training"],
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  ```
</CodeGroup>

For more details on fetching views of a dataset, refer to the guide on [fetching datasets](/langsmith/manage-datasets-programmatically#fetch-datasets).

### Share a dataset publicly

<Warning>
  Sharing a dataset publicly will make the **dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link**, even if they don't have a LangSmith account. Make sure you're not sharing sensitive information.

This feature is only available in the cloud-hosted version of LangSmith.
</Warning>

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Share Dataset**. This will open a dialog where you can copy the link to the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-dataset.gif?s=3788767dadf1c265968fe61d96bacc2d" alt="Share Dataset" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/share-dataset.gif" data-optimize="true" data-opv="3" />

### Unshare a dataset

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared dataset, then **Unshare** in the dialog. <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=98e1807ba9f9a510f56a435b7f81287c" alt="Unshare Dataset" data-og-width="1312" width="1312" data-og-height="803" height="803" data-path="langsmith/images/unshare-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e5dc54489da489188e26e742fc797f71 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=356b04f4abe97829de039db79ddc5567 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4bac2ca43d13295f5e16f29b9da90553 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=768b989e2e27f932d4611cfd5b1d3bc0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b588cc2aad65b4ad721e05563567e2e3 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=605d68739465a17ac63c9f40f00776f1 2500w" />

2. Navigate to your organization's list of publicly shared datasets, by clicking on **Settings** -> **Shared URLs** or [this link](https://smith.langchain.com/settings/shared), then click on **Unshare** next to the dataset you want to unshare.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8a7762947b85af17b36f2c71857badf7" alt="Unshare Trace List" data-og-width="1125" width="1125" data-og-height="519" height="519" data-path="langsmith/images/unshare-trace-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1b4d7b5ec44a3ec6bcdad04df29bf417 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c35f8f9690fd578920a74754ed65eec1 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3383e115479d7e7cab4721a571aa52e3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a5a303b2de245b83369fdfd40390e00b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77b9562fb148ec7dac8f5ae04afc86ea 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ddac9fb12453cb0647f6b0a6279b85c 2500w" />

You can export your LangSmith dataset to a CSV, JSONL, or [OpenAI's fine tuning format](https://platform.openai.com/docs/guides/fine-tuning#example-format) from the LangSmith UI.

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Download Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-dataset-button.gif?s=e71b7c55d70528df0a8985b8884f7597" alt="Export Dataset Button" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/export-dataset-button.gif" data-optimize="true" data-opv="3" />

## Export filtered traces from experiment to dataset

After running an [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) in LangSmith, you may want to export [traces](/langsmith/observability-concepts#traces) that met some evaluation criteria to a dataset.

### View experiment traces

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d05263ae403f7f04e8a00ab956313c01" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1224" height="1224" data-path="langsmith/images/export-filtered-trace-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=165c814edfb48cb4f0ae996b654a8b1d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d097b2b992b036e3f9b1c27a9a5a354e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d2e9824cdc9d6dccdb2b0e0d8cd09dab 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7b9df2d826e18396002aa52fabe9465 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e160dbc0be517b4d9ba3e818a62fb628 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=542ba924541f3c6cc970f37e84c07efb 2500w" />

To do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6ef0c958b5af1f2fe113b8717e698584" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1638" height="1638" data-path="langsmith/images/experiment-tracing-project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=384a1febf8595c2cbf87488a38f7b60a 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=66fbf297dad35d8379572614c17b0c0a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=849acb7d46cf795441c5d983dc7034fb 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bdc68e882b0b94a86f3d28032f2eab 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6567c99736d666c3a0e616b0cd474663 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3331bc1c1eefe3189c6257be0a7f48b8 2500w" />

From there, you can filter the traces based on your evaluation criteria. In this example, we're filtering for all traces that received an accuracy score greater than 0.5.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0a0edc120c230511d10285f80248dab0" alt="Export filtered traces" data-og-width="3438" width="3438" data-og-height="1844" height="1844" data-path="langsmith/images/filtered-traces-from-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2bfcc0f940f0becbe41050683dd97787 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=702fcaaab265f22c7a6e86c4738aecc2 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=352cfc459b7b30c3aaa5b0968b8ce961 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=173ce1d23621756731a4df05956117c9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7f00715f4e840504d19b6f4daba41ef 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0f29eb2c1b85e37af88178ffe69eda39 2500w" />

After applying the filter on the project, we can multi-select runs to add to the dataset, and click **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=d2488fe04acef624c3528ad01c5bedaa" alt="Export filtered traces" data-og-width="3364" width="3364" data-og-height="1834" height="1834" data-path="langsmith/images/add-filtered-traces-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4e41c751ac2fec4a2fd5550af8e5538d 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=79e7789b3543800aec927e96e60f6530 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5fa95115dec4ef56c924260dc3008aef 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4aed75b0e0ef7f1f365ba84860311019 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=84e44cd484135ba5718da7d7c5c68f81 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=22468eea737aaec2ef2faa8e0e34f2ec 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To run an evaluation on a particular tagged version of a dataset, refer to the [Evaluate on a specific dataset version section](#evaluate-on-specific-dataset-version).

## Evaluate on a specific dataset version

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Version a dataset](#version-a-dataset).
  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
</Check>

### Use `list_examples`

You can use `evaluate` / `aevaluate` to pass in an iterable of examples to evaluate on a particular version of a dataset. Use `list_examples` / `listExamples` to fetch examples from a particular version tag using `as_of` / `asOf` and pass that into the `data` argument.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Learn more about how to fetch views of a dataset on the [Create and manage datasets programmatically](/langsmith/manage-datasets-programmatically#fetch-datasets) page.

## Evaluate on a split / filtered view of a dataset

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
  * [Creating and managing dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).
</Check>

### Evaluate on a filtered view of a dataset

You can use the `list_examples` / `listExamples` method to [fetch](/langsmith/manage-datasets-programmatically#fetch-examples) a subset of examples from a dataset to evaluate on.

One common workflow is to fetch examples that have a certain metadata key-value pair.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Memory

**URL:** llms-txt#memory

**Contents:**
- Add short-term memory
  - Use in production
  - Use in subgraphs

Source: https://docs.langchain.com/oss/python/langgraph/add-memory

AI applications need [memory](/oss/python/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:

* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/python/langgraph/graph-api#state) to enable multi-turn conversations.
* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.

## Add short-term memory

**Short-term** memory (thread-level [persistence](/oss/python/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory:

### Use in production

In production, use a checkpointer backed by a database:

<Accordion title="Example: using Postgres checkpointer">

<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Example: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer">

<Note>
    **Setup**
    To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.
  </Note>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Example: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointer">

<Tip>
    You need to call `checkpointer.setup()` the first time you're using Redis checkpointer
  </Tip>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

If your graph contains [subgraphs](/oss/python/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.

```python  theme={null}
from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import InMemorySaver
from typing import TypedDict

class State(TypedDict):
    foo: str

**Examples:**

Example 1 (unknown):
```unknown
### Use in production

In production, use a checkpointer backed by a database:
```

Example 2 (unknown):
```unknown
<Accordion title="Example: using Postgres checkpointer">
```

Example 3 (unknown):
```unknown
<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>

  <Tabs>
    <Tab title="Sync">
```

Example 4 (unknown):
```unknown
</Tab>

    <Tab title="Async">
```

---

## Connect an authentication provider

**URL:** llms-txt#connect-an-authentication-provider

**Contents:**
- Background
- Prerequisites
- 1. Install dependencies
- 2. Set up the authentication provider
- 3. Implement token validation

Source: https://docs.langchain.com/langsmith/add-auth-server

In [the last tutorial](/langsmith/resource-auth), you added resource authorization to give users private conversations. However, you are still using hard-coded tokens for authentication, which is not secure. Now you'll replace those tokens with real user accounts using [OAuth2](/langsmith/deployment-quickstart).

You'll keep the same [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object and [resource-level access control](/langsmith/auth#single-owner-resources), but upgrade authentication to use Supabase as your identity provider. While Supabase is used in this tutorial, the concepts apply to any OAuth2 provider. You'll learn how to:

1. Replace test tokens with real JWT tokens
2. Integrate with OAuth2 providers for secure user authentication
3. Handle user sessions and metadata while maintaining our existing authorization logic

OAuth2 involves three main roles:

1. **Authorization server**: The identity provider (e.g., Supabase, Auth0, Google) that handles user authentication and issues tokens
2. **Application backend**: Your LangGraph application. This validates tokens and serves protected resources (conversation data)
3. **Client application**: The web or mobile app where users interact with your service

A standard OAuth2 flow works something like this:

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file

3. Copy your service role secret key and add it to your `.env` file:

4. Copy your "anon public" key and note it down. This will be used later when you set up our client code.

## 3. Implement token validation

In the previous tutorials, you used the [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object to [validate hard-coded tokens](/langsmith/set-up-custom-auth) and [add resource ownership](/langsmith/resource-auth).

Now you'll upgrade your authentication to validate real JWT tokens from Supabase. The main changes will all be in the [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) decorated function:

* Instead of checking against a hard-coded list of tokens, you'll make an HTTP request to Supabase to validate the token.
* You'll extract real user information (ID, email) from the validated token.
* The existing resource authorization logic remains unchanged.

Update `src/security/auth.py` to implement this:

```python {highlight={8-9,20-30}} title="src/security/auth.py" theme={null}
import os
import httpx
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown
## Prerequisites

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file
```

Example 4 (unknown):
```unknown
3. Copy your service role secret key and add it to your `.env` file:
```

---

## Generate ClickHouse stats

**URL:** llms-txt#generate-clickhouse-stats

**Contents:**
  - Prerequisites
  - Running the clickhouse stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-clickhouse-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_clickhouse_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate ClickHouse stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_clickhouse_stats.sh)

### Running the clickhouse stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, clickhouse\_stats.csv, has been created with Clickhouse statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-clickhouse-stats.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## 3. Pass in configuration at runtime:

**URL:** llms-txt#3.-pass-in-configuration-at-runtime:

**Contents:**
- Add retry policies
- Add node caching
- Create a sequence of steps

print(graph.invoke({}, context={"my_runtime_value": "a"}))  # [!code highlight]
print(graph.invoke({}, context={"my_runtime_value": "b"}))  # [!code highlight]

{'my_state_value': 1}
{'my_state_value': 2}
python  theme={null}
  from dataclasses import dataclass

from langchain.chat_models import init_chat_model
  from langgraph.graph import MessagesState, END, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"

MODELS = {
      "anthropic": init_chat_model("anthropic:claude-3-5-haiku-latest"),
      "openai": init_chat_model("openai:gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      response = model.invoke(state["messages"])
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  # With no configuration, uses default (Anthropic)
  response_1 = graph.invoke({"messages": [input_message]}, context=ContextSchema())["messages"][-1]
  # Or, can set OpenAI
  response_2 = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai"})["messages"][-1]

print(response_1.response_metadata["model_name"])
  print(response_2.response_metadata["model_name"])
  
  claude-3-5-haiku-20241022
  gpt-4.1-mini-2025-04-14
  python  theme={null}
  from dataclasses import dataclass
  from langchain.chat_models import init_chat_model
  from langchain.messages import SystemMessage
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"
      system_message: str | None = None

MODELS = {
      "anthropic": init_chat_model("anthropic:claude-3-5-haiku-latest"),
      "openai": init_chat_model("openai:gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      messages = state["messages"]
      if (system_message := runtime.context.system_message):
          messages = [SystemMessage(system_message)] + messages
      response = model.invoke(messages)
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  response = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai", "system_message": "Respond in Italian."})
  for message in response["messages"]:
      message.pretty_print()
  
  ================================ Human Message ================================

hi
  ================================== Ai Message ==================================

Ciao! Come posso aiutarti oggi?
  python  theme={null}
from langgraph.types import RetryPolicy

builder.add_node(
    "node_name",
    node_function,
    retry_policy=RetryPolicy(),
)
python  theme={null}
  import sqlite3
  from typing_extensions import TypedDict
  from langchain.chat_models import init_chat_model
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.types import RetryPolicy
  from langchain_community.utilities import SQLDatabase
  from langchain.messages import AIMessage

db = SQLDatabase.from_uri("sqlite:///:memory:")
  model = init_chat_model("anthropic:claude-3-5-haiku-latest")

def query_database(state: MessagesState):
      query_result = db.run("SELECT * FROM Artist LIMIT 10;")
      return {"messages": [AIMessage(content=query_result)]}

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": [response]}

# Define a new graph
  builder = StateGraph(MessagesState)
  builder.add_node(
      "query_database",
      query_database,
      retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),
  )
  builder.add_node("model", call_model, retry_policy=RetryPolicy(max_attempts=5))
  builder.add_edge(START, "model")
  builder.add_edge("model", "query_database")
  builder.add_edge("query_database", END)
  graph = builder.compile()
  python  theme={null}
from langgraph.types import CachePolicy

builder.add_node(
    "node_name",
    node_function,
    cache_policy=CachePolicy(ttl=120),
)
python  theme={null}
from langgraph.cache.memory import InMemoryCache

graph = builder.compile(cache=InMemoryCache())
python  theme={null}
from langgraph.graph import START, StateGraph

builder = StateGraph(State)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<Accordion title="Extended example: specifying LLM at runtime">
  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: specifying model and system message at runtime">
  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.
```

---

## 'inputs' will come from your dataset.

**URL:** llms-txt#'inputs'-will-come-from-your-dataset.

---

## Build the graph with input and output schemas specified

**URL:** llms-txt#build-the-graph-with-input-and-output-schemas-specified

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)  # Add the answer node
builder.add_edge(START, "answer_node")  # Define the starting edge
builder.add_edge("answer_node", END)  # Define the ending edge
graph = builder.compile()  # Compile the graph

---

## Explore the results as a Pandas DataFrame.

**URL:** llms-txt#explore-the-results-as-a-pandas-dataframe.

---

## How to use the REST API

**URL:** llms-txt#how-to-use-the-rest-api

**Contents:**
- Create a dataset

Source: https://docs.langchain.com/langsmith/run-evals-api-only

It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly.

This guide will show you how to run evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

Before diving into this content, it might be helpful to read the following:

* [Evaluate LLM applications](/langsmith/evaluate-llm-application)
* [LangSmith API Reference](https://api.smith.langchain.com/redoc)

Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see [this guide](/langsmith/manage-datasets-in-application) for more information.

```python  theme={null}
import os
import requests

from datetime import datetime
from langsmith import Client
from openai import OpenAI
from uuid import uuid4

client = Client()
oa_client = OpenAI()

---

## Share or unshare a trace publicly

**URL:** llms-txt#share-or-unshare-a-trace-publicly

Source: https://docs.langchain.com/langsmith/share-trace

<Warning>
  **Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.**

If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.
</Warning>

To share a trace publicly, simply click on the **Share** button in the upper right hand side of any trace view.
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f4d51afcb8b75809a08cf254b1797172" alt="" data-og-width="2011" width="2011" data-og-height="1005" height="1005" data-path="langsmith/images/share-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2580f397804e880fa5772dd5541347b3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d73de3d28cddf8585257cc5671218af4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9495226170662b9eb0c270e2c9443210 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5f6f3b2a45a50a6610dd16f591651a82 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ef2e10b3ae15f87d85bf8b5bde7f9e02 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2fb5e488e6b18113b6dd82457fc1720 2500w" />

This will open a dialog where you can copy the link to the trace.

Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it.

To "unshare" a trace, either:

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared trace, then **Unshare** in the dialog.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=30504d6c7fe0ee5d3c6bf9b52a9c3d77" alt="" data-og-width="750" width="750" data-og-height="223" height="223" data-path="langsmith/images/unshare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fd6850366fbdadfe8b60af3d675b7e7a 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f992e5a88562a93b88a0ce114452d426 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=322987d1e066b41d6c5eef49ad95f4a7 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0a9ff17d3ebb1e7ab9c3777bc9eea051 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=be39ee08bd403a1c890687feaa3eb870 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=99898693ec9d411c396104fc8dc9196d 2500w" />

2. Navigate to your organization's list of publicly shared traces, by clicking on **Settings** -> **Shared URLs**, then click on **Unshare** next to the trace you want to unshare.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e139222bbde3e2b9530e92164e0e1efe" alt="" data-og-width="2294" width="2294" data-og-height="1113" height="1113" data-path="langsmith/images/unshare-trace-list-share.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcd07bdf6a4968cefb9d13ab1c447e17 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5010be488c821b690b0c63b3eeb47e1c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aba6af3a7889f20a7b7c00ca01633bde 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=efb8fd990042f594a07dde88d61a434c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=309427d94d2db88d43930b7de9b8ac65 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b708410fd513598eb49e354aae33571 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/share-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Note that here we inspect the runtime config for an "env" variable.

**URL:** llms-txt#note-that-here-we-inspect-the-runtime-config-for-an-"env"-variable.

---

## How to define a code evaluator

**URL:** llms-txt#how-to-define-a-code-evaluator

**Contents:**
- Basic example
- Evaluator args
- Evaluator output
- Additional examples
- Related

Source: https://docs.langchain.com/langsmith/code-evaluator

<Info>
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
</Info>

Code evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

Code evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

* [Evaluate aggregate experiment results](/langsmith/summary): Define summary evaluators, which compute metrics for an entire experiment.
* [Run an evaluation comparing two experiments](/langsmith/evaluate-pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/code-evaluator.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Evaluator args

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

## Evaluator output

Code evaluators are expected to return one of the following types:

Python and JS/TS

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to sync prompts with GitHub

**URL:** llms-txt#how-to-sync-prompts-with-github

**Contents:**
- Prerequisites
- Understanding LangSmith "Prompt Commits" and Webhooks
- Implementing a FastAPI Server for Webhook Reception
- Configuring the Webhook in LangSmith
- The Workflow in Action
- Beyond a Simple Commit

Source: https://docs.langchain.com/langsmith/prompt-commit

LangSmith provides a collaborative interface to create, test, and iterate on prompts.

While you can [dynamically fetch prompts](/langsmith/manage-prompts-programmatically#pull-a-prompt) from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.

**Why sync prompts with GitHub?**

* **Version Control:** Keep your prompts versioned alongside your application code in a familiar system.
* **CI/CD Integration:** Trigger automated staging or production deployments when critical prompts change.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7fd1ae2a70f91c14298803a48785f89" alt="Prompt Webhook Diagram" data-og-width="1336" width="1336" data-og-height="343" height="343" data-path="langsmith/images/prompt-excalidraw.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=02f868ec42337b43a533f23effa76417 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=68806f6d9d1e5b8dbaf49d98294190da 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=133aeb6d3a880b989c2246632b8c0d5d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc7091cce5fec3407e005f798db85544 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d789978f57574ec4f0c75beac19fc10c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d6362e5787b7fd1a5db69b3d704912bb 2500w" />

Before we begin, ensure you have the following set up:

1. **GitHub Account:** A standard GitHub account.

2. **GitHub Repository:** Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.

3. **GitHub Personal Access Token (PAT):**

* LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that *you* create.
   * This server requires a GitHub PAT to authenticate and make commits to your repository.
   * Must include the `repo` scope (`public_repo` is sufficient for public repositories).
   * Go to **GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic)**.
   * Click **Generate new token (classic)**.
   * Name it (e.g., "LangSmith Prompt Sync"), set an expiration, and select the required scopes.
   * Click **Generate token** and **copy it immediately** — it won't be shown again.
   * Store the token securely and provide it as an environment variable to your server.

## Understanding LangSmith "Prompt Commits" and Webhooks

In LangSmith, when you save changes to a prompt, you're essentially creating a new version or a "Prompt Commit." These commits are what can trigger webhooks.

The webhook will send a JSON payload containing the new **prompt manifest**.

<Accordion title="Sample Webhook Payload">
  
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI Server for Webhook Reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.

**Key aspects of this server:**

* **Configuration (`.env`):** It expects a `.env` file with your `GITHUB_TOKEN`, `GITHUB_REPO_OWNER`, and `GITHUB_REPO_NAME`. You can also customize `GITHUB_FILE_PATH` (default: `LangSmith_prompt_manifest.json`) and `GITHUB_BRANCH` (default: `main`).
  * **GitHub Interaction:** The `commit_manifest_to_github` function handles the logic of fetching the current file's SHA (to update it) and then committing the new manifest content.
  * **Webhook Endpoint (`/webhook/commit`):** This is the URL path your LangSmith webhook will target.
  * **Error Handling:** Basic error handling for GitHub API interactions is included.

**Deploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., `https://prompt-commit-webhook.onrender.com`).**
</Accordion>

## Configuring the Webhook in LangSmith

Once your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:

1. Navigate to your LangSmith workspace.

2. Go to the **Prompts** section. Here you'll see a list of your prompts.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7e61c83cdd67749970d8f0e401066d60" alt="LangSmith Prompts section" data-og-width="2996" width="2996" data-og-height="852" height="852" data-path="langsmith/images/prompt-commit-main.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f492027577eacd4131954de447fa77f2 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b5f7ea835f9cddd0724a209869c2512e 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9aceebb242173d56a79e4974eb0fc7cd 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2ec5c08e0a4682b84a202ebb52bf981d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7dce8f5f164d40ab0f47dbf7e308382e 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03f8d9d0d9368ec370a71f7605f019a2 2500w" />

3. On the top right of the Prompts page, click the **+ Webhook** button.

4. You'll be presented with a form to configure your webhook:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=775cc6392de007e894c42400117d113e" alt="LangSmith Webhook configuration modal" data-og-width="3008" width="3008" data-og-height="1454" height="1454" data-path="langsmith/images/prompt-commit-webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a1d037dbc657bc0758b444d94d458c91 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=708b8a9d27bd584fdd3a4a8df17be36d 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=080f345e1d97aeac9b0d15a0d762fd42 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=387781e1444e51ff64cc2b0f7f02cd26 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c855c18d16698e49737ea6f581162970 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38e417a519b578f880d59a6c23efb102 2500w" />

* **Webhook URL:** Enter the full public URL of your deployed FastAPI server's endpoint. For our example server, this would be `https://prompt-commit-webhook.onrender.com/webhook/commit`.
   * **Headers (Optional):**
     * You can add custom headers that LangSmith will send with each webhook request.

5. **Test the Webhook:** LangSmith provides a "Send Test Notification" button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).

6. **Save** the webhook configuration.

## The Workflow in Action

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=823988be6300f39a6e9de784b34a2a77" alt="Workflow Diagram showing: User saves prompt in LangSmith, LangSmith sends webhook to FastAPI Server, which interacts with GitHub to update files" data-og-width="2922" width="2922" data-og-height="1014" height="1014" data-path="langsmith/images/prompt-sequence-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13a26887fccdca822c175912ed7fbd3b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=07e46cc25aa6d0ebe76b14ce9057936b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=887758bfcf886c3062177706b7f22fb1 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a5aa30f606eac545f30a715eccfd80a1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=32cae9f09ec30759550214f3c3e490ff 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bced33a052aa88056de397389f9f7d64 2500w" />

Now, with everything set up, here's what happens:

1. **Prompt Modification:** A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new "prompt commit."

2. **Webhook Trigger:** LangSmith detects this new prompt commit and triggers the configured webhook.

3. **HTTP Request:** LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., `https://prompt-commit-webhook.onrender.com/webhook/commit`). The body of this request contains the JSON prompt manifest for the entire workspace.

4. **Server Receives Payload:** Your FastAPI server's endpoint receives the request.

5. **GitHub Commit:** The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to:

* Check if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).
   * Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it's an update from LangSmith.

6. **Confirmation:** You should see the new commit appear in your GitHub repository.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=213d6364ce20e4acf4e3eb7fe8c1b13d" alt="Manifest commited to Github" data-og-width="2982" width="2982" data-og-height="1270" height="1270" data-path="langsmith/images/prompt-commit-github.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f03e1a469e28196f66bc1f92993e6045 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=645f3cf20850a03a89a2e56a53d95719 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=418f7716a8521e3d57a4381d4ebd4d08 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=34af751c0c20b176c715f0c6f86654f1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13309faa18bde23df5e6df56521823fb 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f17d618b6d8e237aa8be023cccea4fd3 2500w" />

You've now successfully synced your LangSmith prompts with GitHub!

## Beyond a Simple Commit

Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server's functionality to perform more sophisticated actions:

* **Granular Commits:** Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.
* **Trigger CI/CD:** Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.
* **Update Databases/Caches:** If your application loads prompts from a database or cache, update these stores directly.
* **Notifications:** Send notifications to Slack, email, or other communication channels about prompt changes.
* **Selective Processing:** Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-commit.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI Server for Webhook Reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

  This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.
```

---

## Implement a LangGraph integration

**URL:** llms-txt#implement-a-langgraph-integration

Source: https://docs.langchain.com/oss/python/contributing/implement-langgraph

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Configure threads

**URL:** llms-txt#configure-threads

**Contents:**
- Group traces into threads
  - Example
- View threads
  - View a thread
  - View feedback
  - Save thread level filter

Source: https://docs.langchain.com/langsmith/threads

Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.

## Group traces into threads

A `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.

To associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread. The key name should be one of:

* `session_id`
* `thread_id`
* `conversation_id`.

The value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`. Check out [this guide](./add-metadata-tags) for instructions on adding metadata to your traces.

This example demonstrates how to log and retrieve conversation history using a structured message format to maintain long-running chats.

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

You can view threads by clicking on the **Threads** tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=45e1c11dce5eaaaf0cf8ae01057647b7" alt="LangSmith UI showing the threads table." data-og-width="1277" width="1277" data-og-height="762" height="762" data-path="langsmith/images/threads-tab-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cb5d147a58a3a9ecbb1c550a3308e871 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=a7cc6f9c8def1e15cc9c93382b82473d 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=23013f31edf91e66da89e102cb6ea302 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1bfbd6daad34b699553a30d8f9663540 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d3fc20ddbe02630ac98a0c336a39caa7 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfa17b6f006faef0a6f8537eedab6532 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=b0ec4964ee49a3ead3a1e8042e406abc" alt="LangSmith UI showing the threads table." data-og-width="1275" width="1275" data-og-height="761" height="761" data-path="langsmith/images/threads-tab-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=98750a8129e2e8283871c096a642f8b8 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=4be143312bdd90ab8318da304c0c1e91 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=c7ace4e000b6a1c925d32b78983fadca 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9ece1b058745be1c3b2dcfffd9a9af58 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=0ab63a1b906fe2a559cfb5772c95dc47 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=6093b81ee1f92978880e74a9630f451d 2500w" />
</div>

You can then click into a particular thread. This will open the history for a particular thread.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />
</div>

Threads can be viewed in two different ways:

* [Thread overview](/langsmith/threads#thread-overview)
* [Trace view](/langsmith/threads#trace-view)

You can use the buttons at the top of the page to switch between the two views or use the keyboard shortcut `T` to toggle between the two views.

The thread overview page shows you a chatbot-like UI where you can see the inputs and outputs for each turn of the conversation. You can configure which fields in the inputs and outputs are displayed in the overview, or show multiple fields by clicking the **Configure** button.

The JSON path for the inputs and outputs supports negative indexing, so you can use `-1` to access the last element of an array. For example, `inputs.messages[-1].content` will access the last message in the `messages` array.

The trace view here is similar to the trace view when looking at a single run, except that you have easy access to all the runs for each turn in the thread.

When viewing a thread, across the top of the page you will see a section called `Feedback`. This is where you can see the feedback for each of the runs that make up the thread. This feedback is aggregated, so if you evaluate each run of a thread for the same criteria, you will see the average score across all the runs displayed. You can also see [thread level feedback](/langsmith/online-evaluations#configure-multi-turn-online-evaluators) left here.

### Save thread level filter

Similar to saving filters at the project level, you can also save commonly used filters at the thread level. To save filters on the threads table, set a filter using the filters button and then click the **Save filter** button.

You can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/threads.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

<CodeGroup>
```

---

## >       value={

**URL:** llms-txt#>-------value={

---

## Test the graph with a valid input

**URL:** llms-txt#test-the-graph-with-a-valid-input

**Contents:**
- Add runtime configuration

graph.invoke({"a": "hello"})
python  theme={null}
try:
    graph.invoke({"a": 123})  # Should be a string
except Exception as e:
    print("An exception was raised because `a` is an integer rather than a string.")
    print(e)

An exception was raised because `a` is an integer rather than a string.
1 validation error for OverallState
a
  Input should be a valid string [type=string_type, input_value=123, input_type=int]
    For further information visit https://errors.pydantic.dev/2.9/v/string_type
python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class NestedModel(BaseModel):
      value: str

class ComplexState(BaseModel):
      text: str
      count: int
      nested: NestedModel

def process_node(state: ComplexState):
      # Node receives a validated Pydantic object
      print(f"Input state type: {type(state)}")
      print(f"Nested type: {type(state.nested)}")
      # Return a dictionary update
      return {"text": state.text + " processed", "count": state.count + 1}

# Build the graph
  builder = StateGraph(ComplexState)
  builder.add_node("process", process_node)
  builder.add_edge(START, "process")
  builder.add_edge("process", END)
  graph = builder.compile()

# Create a Pydantic instance for input
  input_state = ComplexState(text="hello", count=0, nested=NestedModel(value="test"))
  print(f"Input object type: {type(input_state)}")

# Invoke graph with a Pydantic instance
  result = graph.invoke(input_state)
  print(f"Output type: {type(result)}")
  print(f"Output content: {result}")

# Convert back to Pydantic model if needed
  output_model = ComplexState(**result)
  print(f"Converted back to Pydantic: {type(output_model)}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class CoercionExample(BaseModel):
      # Pydantic will coerce string numbers to integers
      number: int
      # Pydantic will parse string booleans to bool
      flag: bool

def inspect_node(state: CoercionExample):
      print(f"number: {state.number} (type: {type(state.number)})")
      print(f"flag: {state.flag} (type: {type(state.flag)})")
      return {}

builder = StateGraph(CoercionExample)
  builder.add_node("inspect", inspect_node)
  builder.add_edge(START, "inspect")
  builder.add_edge("inspect", END)
  graph = builder.compile()

# Demonstrate coercion with string inputs that will be converted
  result = graph.invoke({"number": "42", "flag": "true"})

# This would fail with a validation error
  try:
      graph.invoke({"number": "not-a-number", "flag": "true"})
  except Exception as e:
      print(f"\nExpected validation error: {e}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel
  from langchain.messages import HumanMessage, AIMessage, AnyMessage
  from typing import List

class ChatState(BaseModel):
      messages: List[AnyMessage]
      context: str

def add_message(state: ChatState):
      return {"messages": state.messages + [AIMessage(content="Hello there!")]}

builder = StateGraph(ChatState)
  builder.add_node("add_message", add_message)
  builder.add_edge(START, "add_message")
  builder.add_edge("add_message", END)
  graph = builder.compile()

# Create input with a message
  initial_state = ChatState(
      messages=[HumanMessage(content="Hi")], context="Customer support chat"
  )

result = graph.invoke(initial_state)
  print(f"Output: {result}")

# Convert back to Pydantic model to see message types
  output_model = ChatState(**result)
  for i, msg in enumerate(output_model.messages):
      print(f"Message {i}: {type(msg).__name__} - {msg.content}")
  python  theme={null}
from langgraph.graph import END, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown
Invoke the graph with an **invalid** input
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
See below for additional features of Pydantic model state:

<Accordion title="Serialization Behavior">
  When using Pydantic models as state schemas, it's important to understand how serialization works, especially when:

  * Passing Pydantic objects as inputs
  * Receiving outputs from the graph
  * Working with nested Pydantic models

  Let's see these behaviors in action.
```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Runtime Type Coercion">
  Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.
```

---

## LangChain SDK

**URL:** llms-txt#langchain-sdk

Source: https://docs.langchain.com/oss/python/reference/langchain-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langchain-python.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Run a specific test file

**URL:** llms-txt#run-a-specific-test-file

uv run --group test pytest tests/integration_tests/test_chat_models.py

---

## The overall state of the graph (this is the public state shared across nodes)

**URL:** llms-txt#the-overall-state-of-the-graph-(this-is-the-public-state-shared-across-nodes)

class OverallState(BaseModel):
    a: str

def node(state: OverallState):
    return {"a": "goodbye"}

---

## ...

**URL:** llms-txt#...

print(full.content_blocks)

---

## >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \'30 days\';'},

**URL:** llms-txt#>----------------'arguments':-{'query':-'delete-from-records-where-created_at-<-now()---interval-\'30-days\';'},

---

## Application structure

**URL:** llms-txt#application-structure

**Contents:**
- Overview
- Key Concepts
- File Structure
- Configuration File
  - Examples
- Dependencies
- Graphs
- Environment Variables

Source: https://docs.langchain.com/oss/python/langgraph/application-structure

A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.

This guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified.

To deploy using the LangSmith, the following information should be provided:

1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The [graphs](#graphs) that implement the logic of the application.
3. A file that specifies [dependencies](#dependencies) required to run the application.
4. [Environment variables](#environment-variables) that are required for the application to run.

Below are examples of directory structures for applications:

<Tabs>
  <Tab title="Python (requirements.txt)">
    
  </Tab>

<Tab title="Python (pyproject.toml)">
    
  </Tab>
</Tabs>

<Note>
  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration File

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.

See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

* The dependencies involve a custom local package and the `langchain_openai` package.
* A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
* The environment variables are loaded from the `.env` file.

A LangGraph application may depend on other Python packages.

You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`).

2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application.

3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).

Use the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application.

You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.

## Environment Variables

If you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts).

For a production deployment, you will typically want to configure the environment variables in the deployment environment.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/application-structure.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Python (pyproject.toml)">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

<Note>
  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration File

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.

See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

### Examples

* The dependencies involve a custom local package and the `langchain_openai` package.
* A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
* The environment variables are loaded from the `.env` file.
```

---

## Finally, we compile it!

**URL:** llms-txt#finally,-we-compile-it!

---

## Call the function with traced attachments

**URL:** llms-txt#call-the-function-with-traced-attachments

**Contents:**
  - TypeScript

result = trace_with_attachments(
    val=val,
    text=text,
    image=image_attachment,
    audio=audio_attachment,
    video=video_attachment,
    pdf=pdf_attachment,
    csv=csv_attachment,
)
typescript TypeScript theme={null}
type AttachmentData = Uint8Array | ArrayBuffer;
type Attachments = Record<string, [string, AttachmentData]>;

extractAttachments?: (
    ...args: Parameters<Func>
) => [Attachments | undefined, KVMap];
typescript TypeScript theme={null}
import { traceable } from "langsmith/traceable";

const traceableWithAttachments = traceable(
    (
        val: number,
        text: string,
        attachment: Uint8Array,
        attachment2: ArrayBuffer,
        attachment3: Uint8Array,
        attachment4: ArrayBuffer,
        attachment5: Uint8Array,
    ) =>
        `Processed: ${val}, ${text}, ${attachment.length}, ${attachment2.byteLength}, ${attachment3.length}, ${attachment4.byteLength}, ${attachment5.byteLength}`,
    {
        name: "traceWithAttachments",
        extractAttachments: (
            val: number,
            text: string,
            attachment: Uint8Array,
            attachment2: ArrayBuffer,
            attachment3: Uint8Array,
            attachment4: ArrayBuffer,
            attachment5: Uint8Array,
        ) => [
            {
                "image inputs": ["image/png", attachment],
                "mp3 inputs": ["audio/mpeg", new Uint8Array(attachment2)],
                "video inputs": ["video/mp4", attachment3],
                "pdf inputs": ["application/pdf", new Uint8Array(attachment4)],
                "csv inputs": ["text/csv", new Uint8Array(attachment5)],
            },
            { val, text },
        ],
    }
);

const fs = Deno // or Node.js fs module
const image = await fs.readFile("my_image.png"); // Uint8Array
const mp3Buffer = await fs.readFile("my_mp3.mp3");
const mp3ArrayBuffer = mp3Buffer.buffer; // Convert to ArrayBuffer
const video = await fs.readFile("my_video.mp4"); // Uint8Array
const pdfBuffer = await fs.readFile("my_document.pdf");
const pdfArrayBuffer = pdfBuffer.buffer; // Convert to ArrayBuffer
const csv = await fs.readFile("test-vals.csv"); // Uint8Array

// Define example parameters
const val = 42;
const text = "Hello, world!";

// Call traceableWithAttachments with the files
const result = await traceableWithAttachments(
    val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv
);
```

Here is how the above would look in the LangSmith UI. You can expand each attachment to view its contents.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb21a1c6d8904d1d7b2215652a6127a5" alt="" data-og-width="3012" width="3012" data-og-height="1696" height="1696" data-path="langsmith/images/trace-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4caa7aaa44cd296b2f30ff8d6f6d7199 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01a4392a7dbf3d10184778d2ab3737a1 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e8047eabe350453619c255e88d6fe1d5 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9b0bbb43e10ea30cf466fb65502af907 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0060d9877954b82d34593d8789f7d0a5 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ec15ad89f02769defc2d5f5637d88e11 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-files-with-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### TypeScript

In the TypeScript SDK, you can add attachments to traces by using `Uint8Array` or `ArrayBuffer` as data types. Each attachment's MIME type is specified within `extractAttachments`:

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Wrap your function with `traceable` and include your attachments within the `extractAttachments` option.

In the TypeScript SDK, the `extractAttachments` function is an optional parameter in the `traceable` configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown

```

---

## Evaluator function

**URL:** llms-txt#evaluator-function

async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    """Evaluate if the final response is equivalent to reference response."""

# Note that we assume the outputs has a 'response' dictionary. We'll need to make sure
    # that the target function we define includes this key.
    user = f"""QUESTION: {inputs['question']}
    GROUND TRUTH RESPONSE: {reference_outputs['response']}
    STUDENT RESPONSE: {outputs['response']}"""

grade = await grader_llm.ainvoke([{"role": "system", "content": grader_instructions}, {"role": "user", "content": user}])
    return grade["is_correct"]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'response' key, so lets define a target function that does so.

Also remember that in our refund graph we made the refund node configurable, so that if we specified `config={"env": "test"}`, we would mock out the refunds without actually updating the DB. We'll use this configurable variable in our target `run_graph` method when invoking our graph:
```

---

## Add to conversation history

**URL:** llms-txt#add-to-conversation-history

**Contents:**
  - Tool Message

messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Great! What's 2+2?")
]

response = model.invoke(messages)
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-5-nano")

def get_weather(location: str) -> str:
    """Get the weather at a location."""
    ...

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in Paris?")

for tool_call in response.tool_calls:
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
    print(f"ID: {tool_call['id']}")
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-5-nano")

response = model.invoke("Hello!")
response.usage_metadata

{'input_tokens': 8,
 'output_tokens': 304,
 'total_tokens': 312,
 'input_token_details': {'audio': 0, 'cache_read': 0},
 'output_token_details': {'audio': 0, 'reasoning': 256}}
python  theme={null}
chunks = []
full_message = None
for chunk in model.stream("Hi"):
    chunks.append(chunk)
    print(chunk.text)
    full_message = chunk if full_message is None else full_message + chunk
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="text" type="string">
    The text content of the message.
  </ParamField>

  <ParamField path="content" type="string | dict[]">
    The raw content of the message.
  </ParamField>

  <ParamField path="content_blocks" type="ContentBlock[]">
    The standardized [content blocks](#message-content) of the message.
  </ParamField>

  <ParamField path="tool_calls" type="dict[] | None">
    The tool calls made by the model. Empty if no tools are called.
  </ParamField>

  <ParamField path="id" type="string">
    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)
  </ParamField>

  <ParamField path="usage_metadata" type="dict | None">
    The usage metadata of the message, which can contain token counts when available.
  </ParamField>

  <ParamField path="response_metadata" type="ResponseMetadata | None">
    The response metadata of the message.
  </ParamField>
</Accordion>

#### Tool calls

When models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):
```

Example 2 (unknown):
```unknown
Other structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).

#### Token usage

An [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage.usage_metadata) field:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
See [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage.usage_metadata) for details.

#### Streaming and chunks

During streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:
```

---

## Run an evaluation from the prompt playground

**URL:** llms-txt#run-an-evaluation-from-the-prompt-playground

**Contents:**
- Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")
- Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Source: https://docs.langchain.com/langsmith/run-evaluation-from-prompt-playground

LangSmith allows you to run evaluations directly in the UI. The [**Prompt Playground**](/langsmith/prompt-engineering#prompt-playground) allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.

Before you run an evaluation, you need to have an [existing dataset](/langsmith/evaluation-concepts#datasets). Learn how to [create a dataset from the UI](/langsmith/manage-datasets-in-application#set-up-your-dataset).

If you prefer to run experiments in code, visit [run an evaluation using the SDK](/langsmith/evaluate-llm-application).

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-experiment.gif?s=05ec3d2b1aa6590c443a033924fc6141" alt="" data-og-width="1358" width="1358" data-og-height="720" height="720" data-path="langsmith/images/playground-experiment.gif" data-optimize="true" data-opv="3" />

## Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")

1. **Navigate to the playground** by clicking **Playground** in the sidebar.
2. **Add a prompt** by selecting an existing saved a prompt or creating a new one.
3. **Select a dataset** from the **Test over dataset** dropdown

* Note that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key "blog", which correctly match the input variable of the prompt.
* There is a maximum of 15 input variables allowed in the prompt playground.

4. **Start the experiment** by clicking on the **Start** or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.
5. **View the full results** by clicking **View full experiment**. This will take you to the experiment details page where you can see the results of the experiment.

## Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Evaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the **+Evaluator** button.

To learn more about adding evaluators in via UI, visit [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evaluation-from-prompt-playground.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## artist vectorstore for "prince" we'll get back the value "Prince", which we can then

**URL:** llms-txt#artist-vectorstore-for-"prince"-we'll-get-back-the-value-"prince",-which-we-can-then

---

## For large datasets, lazily load documents

**URL:** llms-txt#for-large-datasets,-lazily-load-documents

**Contents:**
- By category
  - Webpages
  - PDFs
  - Cloud Providers
  - Social Platforms
  - Messaging Services
  - Productivity tools
  - Common File Types
- All document loaders

for document in loader.lazy_load():
    print(document)
```

The below document loaders allow you to load webpages.

| Document Loader                                                             | Description                                                                                                          | Package/API |
| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- |
| [Web](/oss/python/integrations/document_loaders/web_base)                   | Uses urllib and BeautifulSoup to load and parse HTML web pages                                                       | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file) | Uses Unstructured to load and parse web pages                                                                        | Package     |
| [RecursiveURL](/oss/python/integrations/document_loaders/recursive_url)     | Recursively scrapes all child links from a root URL                                                                  | Package     |
| [Sitemap](/oss/python/integrations/document_loaders/sitemap)                | Scrapes all pages on a given sitemap                                                                                 | Package     |
| [Spider](/oss/python/integrations/document_loaders/spider)                  | Crawler and scraper that returns LLM-ready data                                                                      | API         |
| [Firecrawl](/oss/python/integrations/document_loaders/firecrawl)            | API service that can be deployed locally                                                                             | API         |
| [Docling](/oss/python/integrations/document_loaders/docling)                | Uses Docling to load and parse web pages                                                                             | Package     |
| [Hyperbrowser](/oss/python/integrations/document_loaders/hyperbrowser)      | Platform for running and scaling headless browsers, can be used to scrape/crawl any site                             | API         |
| [AgentQL](/oss/python/integrations/document_loaders/agentql)                | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API         |

The below document loaders allow you to load PDF documents.

| Document Loader                                                                    | Description                                          | Package/API |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------- | ----------- |
| [PyPDF](/oss/python/integrations/document_loaders/pypdfloader)                     | Uses `pypdf` to load and parse PDFs                  | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)        | Uses Unstructured's open source library to load PDFs | Package     |
| [Amazon Textract](/oss/python/integrations/document_loaders/amazon_textract)       | Uses AWS API to load PDFs                            | API         |
| [MathPix](/oss/python/integrations/document_loaders/mathpix)                       | Uses MathPix to load PDFs                            | Package     |
| [PDFPlumber](/oss/python/integrations/document_loaders/pdfplumber)                 | Load PDF files using PDFPlumber                      | Package     |
| [PyPDFDirectry](/oss/python/integrations/document_loaders/pypdfdirectory)          | Load a directory with PDF files                      | Package     |
| [PyPDFium2](/oss/python/integrations/document_loaders/pypdfium2)                   | Load PDF files using PyPDFium2                       | Package     |
| [PyMuPDF](/oss/python/integrations/document_loaders/pymupdf)                       | Load PDF files using PyMuPDF                         | Package     |
| [PyMuPDF4LLM](/oss/python/integrations/document_loaders/pymupdf4llm)               | Load PDF content to Markdown using PyMuPDF4LLM       | Package     |
| [PDFMiner](/oss/python/integrations/document_loaders/pdfminer)                     | Load PDF files using PDFMiner                        | Package     |
| [Upstage Document Parse Loader](/oss/python/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader      | Package     |
| [Docling](/oss/python/integrations/document_loaders/docling)                       | Load PDF files using Docling                         | Package     |
| [UnDatasIO](/oss/python/integrations/document_loaders/undatasio)                   | Load PDF files using UnDatasIO                       | Package     |
| [OpenDataLoader PDF](/oss/python/integrations/document_loaders/opendataloader_pdf) | Load PDF files using OpenDataLoader PDF              | Package     |

The below document loaders allow you to load documents from your favorite cloud providers.

| Document Loader                                                                                            | Description                                                 | Partner Package | API reference                                                                                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AWS S3 Directory](/oss/python/integrations/document_loaders/aws_s3_directory)                             | Load documents from an AWS S3 directory                     | ❌               | [`S3DirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_directory.S3DirectoryLoader.html)                          |
| [AWS S3 File](/oss/python/integrations/document_loaders/aws_s3_file)                                       | Load documents from an AWS S3 file                          | ❌               | [`S3FileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html)                                         |
| [Azure AI Data](/oss/python/integrations/document_loaders/azure_ai_data)                                   | Load documents from Azure AI services                       | ❌               | [`AzureAIDataLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.azure_ai_data.AzureAIDataLoader.html)                         |
| [Azure Blob Storage](/oss/python/integrations/document_loaders/azure_blob_storage)                         | Load documents from Azure Blob Storage                      | ✅               | [`AzureBlobStorageLoader`](https://reference.langchain.com/python/integrations/langchain_azure/storage/)                                                                                       |
| [Dropbox](/oss/python/integrations/document_loaders/dropbox)                                               | Load documents from Dropbox                                 | ❌               | [`DropboxLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.dropbox.DropboxLoader.html)                                       |
| [Google Cloud Storage Directory](/oss/python/integrations/document_loaders/google_cloud_storage_directory) | Load documents from GCS bucket                              | ✅               | [`GCSDirectoryLoader`](https://python.langchain.com/api_reference/google_community/gcs_directory/langchain_google_community.gcs_directory.GCSDirectoryLoader.html)                             |
| [Google Cloud Storage File](/oss/python/integrations/document_loaders/google_cloud_storage_file)           | Load documents from GCS file object                         | ✅               | [`GCSFileLoader`](https://python.langchain.com/api_reference/google_community/gcs_file/langchain_google_community.gcs_file.GCSFileLoader.html)                                                 |
| [Google Drive](/oss/python/integrations/document_loaders/google_drive)                                     | Load documents from Google Drive (Google Docs only)         | ✅               | [`GoogleDriveLoader`](https://python.langchain.com/api_reference/google_community/drive/langchain_google_community.drive.GoogleDriveLoader.html)                                               |
| [Huawei OBS Directory](/oss/python/integrations/document_loaders/huawei_obs_directory)                     | Load documents from Huawei Object Storage Service Directory | ❌               | [`OBSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_directory.OBSDirectoryLoader.html)                       |
| [Huawei OBS File](/oss/python/integrations/document_loaders/huawei_obs_file)                               | Load documents from Huawei Object Storage Service File      | ❌               | [`OBSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_file.OBSFileLoader.html)                                      |
| [Microsoft OneDrive](/oss/python/integrations/document_loaders/microsoft_onedrive)                         | Load documents from Microsoft OneDrive                      | ❌               | [`OneDriveLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.onedrive.OneDriveLoader.html)                                    |
| [Microsoft SharePoint](/oss/python/integrations/document_loaders/microsoft_sharepoint)                     | Load documents from Microsoft SharePoint                    | ❌               | [`SharePointLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sharepoint.SharePointLoader.html)                              |
| [Tencent COS Directory](/oss/python/integrations/document_loaders/tencent_cos_directory)                   | Load documents from Tencent Cloud Object Storage Directory  | ❌               | [`TencentCOSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_directory.TencentCOSDirectoryLoader.html) |
| [Tencent COS File](/oss/python/integrations/document_loaders/tencent_cos_file)                             | Load documents from Tencent Cloud Object Storage File       | ❌               | [`TencentCOSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_file.TencentCOSFileLoader.html)                |

The below document loaders allow you to load documents from different social media platforms.

| Document Loader                                              | API reference                                                                                                                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Twitter](/oss/python/integrations/document_loaders/twitter) | [`TwitterTweetLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.twitter.TwitterTweetLoader.html) |
| [Reddit](/oss/python/integrations/document_loaders/reddit)   | [`RedditPostsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.reddit.RedditPostsLoader.html)    |

### Messaging Services

The below document loaders allow you to load data from different messaging platforms.

| Document Loader                                                          | API reference                                                                                                                                                               |
| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Telegram](/oss/python/integrations/document_loaders/telegram)           | [`TelegramChatFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.telegram.TelegramChatFileLoader.html) |
| [WhatsApp](/oss/python/integrations/document_loaders/whatsapp_chat)      | [`WhatsAppChatLoader`](https://python.langchain.com/api_reference/community/chat_loaders/langchain_community.chat_loaders.whatsapp.WhatsAppChatLoader.html)                 |
| [Discord](/oss/python/integrations/document_loaders/discord)             | [`DiscordChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.discord.DiscordChatLoader.html)            |
| [Facebook Chat](/oss/python/integrations/document_loaders/facebook_chat) | [`FacebookChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.facebook_chat.FacebookChatLoader.html)    |
| [Mastodon](/oss/python/integrations/document_loaders/mastodon)           | [`MastodonTootsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.mastodon.MastodonTootsLoader.html)       |

### Productivity tools

The below document loaders allow you to load data from commonly used productivity tools.

| Document Loader                                            | API reference                                                                                                                                                                  |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Figma](/oss/python/integrations/document_loaders/figma)   | [`FigmaFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.figma.FigmaFileLoader.html)                     |
| [Notion](/oss/python/integrations/document_loaders/notion) | [`NotionDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.notion.NotionDirectoryLoader.html)        |
| [Slack](/oss/python/integrations/document_loaders/slack)   | [`SlackDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.slack_directory.SlackDirectoryLoader.html) |
| [Quip](/oss/python/integrations/document_loaders/quip)     | [`QuipLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.quip.QuipLoader.html)                                |
| [Trello](/oss/python/integrations/document_loaders/trello) | [`TrelloLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.trello.TrelloLoader.html)                          |
| [Roam](/oss/python/integrations/document_loaders/roam)     | [`RoamLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.roam.RoamLoader.html)                                |
| [GitHub](/oss/python/integrations/document_loaders/github) | [`GithubFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.github.GithubFileLoader.html)                  |

### Common File Types

The below document loaders allow you to load data from common data formats.

| Document Loader                                                                                | Data Type                                                                                                                                                                    |
| ---------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [CSVLoader](/oss/python/integrations/document_loaders/csv)                                     | CSV files                                                                                                                                                                    |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)                    | Many file types (see [https://docs.unstructured.io/platform/supported-file-types](https://docs.unstructured.io/platform/supported-file-types))                               |
| [JSONLoader](/oss/python/integrations/document_loaders/json)                                   | JSON files                                                                                                                                                                   |
| [BSHTMLLoader](/oss/python/integrations/document_loaders/bshtml)                               | HTML files                                                                                                                                                                   |
| [DoclingLoader](/oss/python/integrations/document_loaders/docling)                             | Various file types (see [https://ds4sd.github.io/docling/](https://ds4sd.github.io/docling/))                                                                                |
| [PolarisAIDataInsightLoader](/oss/python/integrations/document_loaders/polaris_ai_datainsight) | Various file types (see [https://datainsight.polarisoffice.com/documentation?docType=doc\_extract](https://datainsight.polarisoffice.com/documentation?docType=doc_extract)) |

## All document loaders

<Columns cols={3}>
  <Card title="acreom" icon="link" href="/oss/python/integrations/document_loaders/acreom" arrow="true" cta="View guide" />

<Card title="AgentQLLoader" icon="link" href="/oss/python/integrations/document_loaders/agentql" arrow="true" cta="View guide" />

<Card title="AirbyteLoader" icon="link" href="/oss/python/integrations/document_loaders/airbyte" arrow="true" cta="View guide" />

<Card title="Airtable" icon="link" href="/oss/python/integrations/document_loaders/airtable" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud MaxCompute" icon="link" href="/oss/python/integrations/document_loaders/alibaba_cloud_maxcompute" arrow="true" cta="View guide" />

<Card title="Amazon Textract" icon="link" href="/oss/python/integrations/document_loaders/amazon_textract" arrow="true" cta="View guide" />

<Card title="Apify Dataset" icon="link" href="/oss/python/integrations/document_loaders/apify_dataset" arrow="true" cta="View guide" />

<Card title="ArxivLoader" icon="link" href="/oss/python/integrations/document_loaders/arxiv" arrow="true" cta="View guide" />

<Card title="AssemblyAI Audio Transcripts" icon="link" href="/oss/python/integrations/document_loaders/assemblyai" arrow="true" cta="View guide" />

<Card title="AstraDB" icon="link" href="/oss/python/integrations/document_loaders/astradb" arrow="true" cta="View guide" />

<Card title="Async Chromium" icon="link" href="/oss/python/integrations/document_loaders/async_chromium" arrow="true" cta="View guide" />

<Card title="AsyncHtml" icon="link" href="/oss/python/integrations/document_loaders/async_html" arrow="true" cta="View guide" />

<Card title="Athena" icon="link" href="/oss/python/integrations/document_loaders/athena" arrow="true" cta="View guide" />

<Card title="AWS S3 Directory" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_directory" arrow="true" cta="View guide" />

<Card title="AWS S3 File" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_file" arrow="true" cta="View guide" />

<Card title="AZLyrics" icon="link" href="/oss/python/integrations/document_loaders/azlyrics" arrow="true" cta="View guide" />

<Card title="Azure AI Data" icon="link" href="/oss/python/integrations/document_loaders/azure_ai_data" arrow="true" cta="View guide" />

<Card title="Azure Blob Storage" icon="link" href="/oss/python/integrations/document_loaders/azure_blob_storage" arrow="true" cta="View guide" />

<Card title="Azure AI Document Intelligence" icon="link" href="/oss/python/integrations/document_loaders/azure_document_intelligence" arrow="true" cta="View guide" />

<Card title="BibTeX" icon="link" href="/oss/python/integrations/document_loaders/bibtex" arrow="true" cta="View guide" />

<Card title="BiliBili" icon="link" href="/oss/python/integrations/document_loaders/bilibili" arrow="true" cta="View guide" />

<Card title="Blackboard" icon="link" href="/oss/python/integrations/document_loaders/blackboard" arrow="true" cta="View guide" />

<Card title="Blockchain" icon="link" href="/oss/python/integrations/document_loaders/blockchain" arrow="true" cta="View guide" />

<Card title="Box" icon="link" href="/oss/python/integrations/document_loaders/box" arrow="true" cta="View guide" />

<Card title="Brave Search" icon="link" href="/oss/python/integrations/document_loaders/brave_search" arrow="true" cta="View guide" />

<Card title="Browserbase" icon="link" href="/oss/python/integrations/document_loaders/browserbase" arrow="true" cta="View guide" />

<Card title="Browserless" icon="link" href="/oss/python/integrations/document_loaders/browserless" arrow="true" cta="View guide" />

<Card title="BSHTMLLoader" icon="link" href="/oss/python/integrations/document_loaders/bshtml" arrow="true" cta="View guide" />

<Card title="Cassandra" icon="link" href="/oss/python/integrations/document_loaders/cassandra" arrow="true" cta="View guide" />

<Card title="ChatGPT Data" icon="link" href="/oss/python/integrations/document_loaders/chatgpt_loader" arrow="true" cta="View guide" />

<Card title="College Confidential" icon="link" href="/oss/python/integrations/document_loaders/college_confidential" arrow="true" cta="View guide" />

<Card title="Concurrent Loader" icon="link" href="/oss/python/integrations/document_loaders/concurrent" arrow="true" cta="View guide" />

<Card title="Confluence" icon="link" href="/oss/python/integrations/document_loaders/confluence" arrow="true" cta="View guide" />

<Card title="CoNLL-U" icon="link" href="/oss/python/integrations/document_loaders/conll-u" arrow="true" cta="View guide" />

<Card title="Copy Paste" icon="link" href="/oss/python/integrations/document_loaders/copypaste" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/python/integrations/document_loaders/couchbase" arrow="true" cta="View guide" />

<Card title="CSV" icon="link" href="/oss/python/integrations/document_loaders/csv" arrow="true" cta="View guide" />

<Card title="Cube Semantic Layer" icon="link" href="/oss/python/integrations/document_loaders/cube_semantic" arrow="true" cta="View guide" />

<Card title="Datadog Logs" icon="link" href="/oss/python/integrations/document_loaders/datadog_logs" arrow="true" cta="View guide" />

<Card title="Dedoc" icon="link" href="/oss/python/integrations/document_loaders/dedoc" arrow="true" cta="View guide" />

<Card title="Diffbot" icon="link" href="/oss/python/integrations/document_loaders/diffbot" arrow="true" cta="View guide" />

<Card title="Discord" icon="link" href="/oss/python/integrations/document_loaders/discord" arrow="true" cta="View guide" />

<Card title="Docling" icon="link" href="/oss/python/integrations/document_loaders/docling" arrow="true" cta="View guide" />

<Card title="Docugami" icon="link" href="/oss/python/integrations/document_loaders/docugami" arrow="true" cta="View guide" />

<Card title="Docusaurus" icon="link" href="/oss/python/integrations/document_loaders/docusaurus" arrow="true" cta="View guide" />

<Card title="Dropbox" icon="link" href="/oss/python/integrations/document_loaders/dropbox" arrow="true" cta="View guide" />

<Card title="Email" icon="link" href="/oss/python/integrations/document_loaders/email" arrow="true" cta="View guide" />

<Card title="EPub" icon="link" href="/oss/python/integrations/document_loaders/epub" arrow="true" cta="View guide" />

<Card title="Etherscan" icon="link" href="/oss/python/integrations/document_loaders/etherscan" arrow="true" cta="View guide" />

<Card title="EverNote" icon="link" href="/oss/python/integrations/document_loaders/evernote" arrow="true" cta="View guide" />

<Card title="Facebook Chat" icon="link" href="/oss/python/integrations/document_loaders/facebook_chat" arrow="true" cta="View guide" />

<Card title="Fauna" icon="link" href="/oss/python/integrations/document_loaders/fauna" arrow="true" cta="View guide" />

<Card title="Figma" icon="link" href="/oss/python/integrations/document_loaders/figma" arrow="true" cta="View guide" />

<Card title="FireCrawl" icon="link" href="/oss/python/integrations/document_loaders/firecrawl" arrow="true" cta="View guide" />

<Card title="Geopandas" icon="link" href="/oss/python/integrations/document_loaders/geopandas" arrow="true" cta="View guide" />

<Card title="Git" icon="link" href="/oss/python/integrations/document_loaders/git" arrow="true" cta="View guide" />

<Card title="GitBook" icon="link" href="/oss/python/integrations/document_loaders/gitbook" arrow="true" cta="View guide" />

<Card title="GitHub" icon="link" href="/oss/python/integrations/document_loaders/github" arrow="true" cta="View guide" />

<Card title="Glue Catalog" icon="link" href="/oss/python/integrations/document_loaders/glue_catalog" arrow="true" cta="View guide" />

<Card title="Google AlloyDB for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_alloydb" arrow="true" cta="View guide" />

<Card title="Google BigQuery" icon="link" href="/oss/python/integrations/document_loaders/google_bigquery" arrow="true" cta="View guide" />

<Card title="Google Bigtable" icon="link" href="/oss/python/integrations/document_loaders/google_bigtable" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for SQL Server" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mssql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for MySQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mysql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_pg" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage Directory" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_directory" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage File" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_file" arrow="true" cta="View guide" />

<Card title="Google Firestore in Datastore Mode" icon="link" href="/oss/python/integrations/document_loaders/google_datastore" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/document_loaders/google_drive" arrow="true" cta="View guide" />

<Card title="Google El Carro for Oracle Workloads" icon="link" href="/oss/python/integrations/document_loaders/google_el_carro" arrow="true" cta="View guide" />

<Card title="Google Firestore (Native Mode)" icon="link" href="/oss/python/integrations/document_loaders/google_firestore" arrow="true" cta="View guide" />

<Card title="Google Memorystore for Redis" icon="link" href="/oss/python/integrations/document_loaders/google_memorystore_redis" arrow="true" cta="View guide" />

<Card title="Google Spanner" icon="link" href="/oss/python/integrations/document_loaders/google_spanner" arrow="true" cta="View guide" />

<Card title="Google Speech-to-Text" icon="link" href="/oss/python/integrations/document_loaders/google_speech_to_text" arrow="true" cta="View guide" />

<Card title="Grobid" icon="link" href="/oss/python/integrations/document_loaders/grobid" arrow="true" cta="View guide" />

<Card title="Gutenberg" icon="link" href="/oss/python/integrations/document_loaders/gutenberg" arrow="true" cta="View guide" />

<Card title="Hacker News" icon="link" href="/oss/python/integrations/document_loaders/hacker_news" arrow="true" cta="View guide" />

<Card title="Huawei OBS Directory" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_directory" arrow="true" cta="View guide" />

<Card title="Huawei OBS File" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_file" arrow="true" cta="View guide" />

<Card title="HuggingFace Dataset" icon="link" href="/oss/python/integrations/document_loaders/hugging_face_dataset" arrow="true" cta="View guide" />

<Card title="HyperbrowserLoader" icon="link" href="/oss/python/integrations/document_loaders/hyperbrowser" arrow="true" cta="View guide" />

<Card title="iFixit" icon="link" href="/oss/python/integrations/document_loaders/ifixit" arrow="true" cta="View guide" />

<Card title="Images" icon="link" href="/oss/python/integrations/document_loaders/image" arrow="true" cta="View guide" />

<Card title="Image Captions" icon="link" href="/oss/python/integrations/document_loaders/image_captions" arrow="true" cta="View guide" />

<Card title="IMSDb" icon="link" href="/oss/python/integrations/document_loaders/imsdb" arrow="true" cta="View guide" />

<Card title="Iugu" icon="link" href="/oss/python/integrations/document_loaders/iugu" arrow="true" cta="View guide" />

<Card title="Joplin" icon="link" href="/oss/python/integrations/document_loaders/joplin" arrow="true" cta="View guide" />

<Card title="JSONLoader" icon="link" href="/oss/python/integrations/document_loaders/json" arrow="true" cta="View guide" />

<Card title="Jupyter Notebook" icon="link" href="/oss/python/integrations/document_loaders/jupyter_notebook" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/document_loaders/kinetica" arrow="true" cta="View guide" />

<Card title="lakeFS" icon="link" href="/oss/python/integrations/document_loaders/lakefs" arrow="true" cta="View guide" />

<Card title="LangSmith" icon="link" href="/oss/python/integrations/document_loaders/langsmith" arrow="true" cta="View guide" />

<Card title="LarkSuite (FeiShu)" icon="link" href="/oss/python/integrations/document_loaders/larksuite" arrow="true" cta="View guide" />

<Card title="LLM Sherpa" icon="link" href="/oss/python/integrations/document_loaders/llmsherpa" arrow="true" cta="View guide" />

<Card title="Mastodon" icon="link" href="/oss/python/integrations/document_loaders/mastodon" arrow="true" cta="View guide" />

<Card title="MathPixPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/mathpix" arrow="true" cta="View guide" />

<Card title="MediaWiki Dump" icon="link" href="/oss/python/integrations/document_loaders/mediawikidump" arrow="true" cta="View guide" />

<Card title="Merge Documents Loader" icon="link" href="/oss/python/integrations/document_loaders/merge_doc" arrow="true" cta="View guide" />

<Card title="MHTML" icon="link" href="/oss/python/integrations/document_loaders/mhtml" arrow="true" cta="View guide" />

<Card title="Microsoft Excel" icon="link" href="/oss/python/integrations/document_loaders/microsoft_excel" arrow="true" cta="View guide" />

<Card title="Microsoft OneDrive" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onedrive" arrow="true" cta="View guide" />

<Card title="Microsoft OneNote" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onenote" arrow="true" cta="View guide" />

<Card title="Microsoft PowerPoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_powerpoint" arrow="true" cta="View guide" />

<Card title="Microsoft SharePoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_sharepoint" arrow="true" cta="View guide" />

<Card title="Microsoft Word" icon="link" href="/oss/python/integrations/document_loaders/microsoft_word" arrow="true" cta="View guide" />

<Card title="Near Blockchain" icon="link" href="/oss/python/integrations/document_loaders/mintbase" arrow="true" cta="View guide" />

<Card title="Modern Treasury" icon="link" href="/oss/python/integrations/document_loaders/modern_treasury" arrow="true" cta="View guide" />

<Card title="MongoDB" icon="link" href="/oss/python/integrations/document_loaders/mongodb" arrow="true" cta="View guide" />

<Card title="Needle Document Loader" icon="link" href="/oss/python/integrations/document_loaders/needle" arrow="true" cta="View guide" />

<Card title="News URL" icon="link" href="/oss/python/integrations/document_loaders/news" arrow="true" cta="View guide" />

<Card title="Notion DB" icon="link" href="/oss/python/integrations/document_loaders/notion" arrow="true" cta="View guide" />

<Card title="Nuclia" icon="link" href="/oss/python/integrations/document_loaders/nuclia" arrow="true" cta="View guide" />

<Card title="Obsidian" icon="link" href="/oss/python/integrations/document_loaders/obsidian" arrow="true" cta="View guide" />

<Card title="OpenDataLoader PDF" icon="link" href="/oss/python/integrations/document_loaders/opendataloader_pdf" arrow="true" cta="View guide" />

<Card title="Open Document Format (ODT)" icon="link" href="/oss/python/integrations/document_loaders/odt" arrow="true" cta="View guide" />

<Card title="Open City Data" icon="link" href="/oss/python/integrations/document_loaders/open_city_data" arrow="true" cta="View guide" />

<Card title="Oracle Autonomous Database" icon="link" href="/oss/python/integrations/document_loaders/oracleadb_loader" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/document_loaders/oracleai" arrow="true" cta="View guide" />

<Card title="Org-mode" icon="link" href="/oss/python/integrations/document_loaders/org_mode" arrow="true" cta="View guide" />

<Card title="Outline Document Loader" icon="link" href="/oss/python/integrations/document_loaders/outline" arrow="true" cta="View guide" />

<Card title="Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/pandas_dataframe" arrow="true" cta="View guide" />

<Card title="PDFMinerLoader" icon="link" href="/oss/python/integrations/document_loaders/pdfminer" arrow="true" cta="View guide" />

<Card title="PDFPlumber" icon="link" href="/oss/python/integrations/document_loaders/pdfplumber" arrow="true" cta="View guide" />

<Card title="Pebblo Safe DocumentLoader" icon="link" href="/oss/python/integrations/document_loaders/pebblo" arrow="true" cta="View guide" />

<Card title="Polaris AI DataInsight" icon="link" href="/oss/python/integrations/document_loaders/polaris_ai_datainsight" arrow="true" cta="View guide" />

<Card title="Polars DataFrame" icon="link" href="/oss/python/integrations/document_loaders/polars_dataframe" arrow="true" cta="View guide" />

<Card title="Dell PowerScale" icon="link" href="/oss/python/integrations/document_loaders/powerscale" arrow="true" cta="View guide" />

<Card title="Psychic" icon="link" href="/oss/python/integrations/document_loaders/psychic" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/document_loaders/pubmed" arrow="true" cta="View guide" />

<Card title="PullMdLoader" icon="link" href="/oss/python/integrations/document_loaders/pull_md" arrow="true" cta="View guide" />

<Card title="PyMuPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pymupdf" arrow="true" cta="View guide" />

<Card title="PyMuPDF4LLM" icon="link" href="/oss/python/integrations/document_loaders/pymupdf4llm" arrow="true" cta="View guide" />

<Card title="PyPDFDirectoryLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfdirectory" arrow="true" cta="View guide" />

<Card title="PyPDFium2Loader" icon="link" href="/oss/python/integrations/document_loaders/pypdfium2" arrow="true" cta="View guide" />

<Card title="PyPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfloader" arrow="true" cta="View guide" />

<Card title="PySpark" icon="link" href="/oss/python/integrations/document_loaders/pyspark_dataframe" arrow="true" cta="View guide" />

<Card title="Quip" icon="link" href="/oss/python/integrations/document_loaders/quip" arrow="true" cta="View guide" />

<Card title="ReadTheDocs Documentation" icon="link" href="/oss/python/integrations/document_loaders/readthedocs_documentation" arrow="true" cta="View guide" />

<Card title="Recursive URL" icon="link" href="/oss/python/integrations/document_loaders/recursive_url" arrow="true" cta="View guide" />

<Card title="Reddit" icon="link" href="/oss/python/integrations/document_loaders/reddit" arrow="true" cta="View guide" />

<Card title="Roam" icon="link" href="/oss/python/integrations/document_loaders/roam" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/python/integrations/document_loaders/rockset" arrow="true" cta="View guide" />

<Card title="rspace" icon="link" href="/oss/python/integrations/document_loaders/rspace" arrow="true" cta="View guide" />

<Card title="RSS Feeds" icon="link" href="/oss/python/integrations/document_loaders/rss" arrow="true" cta="View guide" />

<Card title="RST" icon="link" href="/oss/python/integrations/document_loaders/rst" arrow="true" cta="View guide" />

<Card title="scrapfly" icon="link" href="/oss/python/integrations/document_loaders/scrapfly" arrow="true" cta="View guide" />

<Card title="ScrapingAnt" icon="link" href="/oss/python/integrations/document_loaders/scrapingant" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/python/integrations/document_loaders/singlestore" arrow="true" cta="View guide" />

<Card title="Sitemap" icon="link" href="/oss/python/integrations/document_loaders/sitemap" arrow="true" cta="View guide" />

<Card title="Slack" icon="link" href="/oss/python/integrations/document_loaders/slack" arrow="true" cta="View guide" />

<Card title="Snowflake" icon="link" href="/oss/python/integrations/document_loaders/snowflake" arrow="true" cta="View guide" />

<Card title="Source Code" icon="link" href="/oss/python/integrations/document_loaders/source_code" arrow="true" cta="View guide" />

<Card title="Spider" icon="link" href="/oss/python/integrations/document_loaders/spider" arrow="true" cta="View guide" />

<Card title="Spreedly" icon="link" href="/oss/python/integrations/document_loaders/spreedly" arrow="true" cta="View guide" />

<Card title="Stripe" icon="link" href="/oss/python/integrations/document_loaders/stripe" arrow="true" cta="View guide" />

<Card title="Subtitle" icon="link" href="/oss/python/integrations/document_loaders/subtitle" arrow="true" cta="View guide" />

<Card title="SurrealDB" icon="link" href="/oss/python/integrations/document_loaders/surrealdb" arrow="true" cta="View guide" />

<Card title="Telegram" icon="link" href="/oss/python/integrations/document_loaders/telegram" arrow="true" cta="View guide" />

<Card title="Tencent COS Directory" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_directory" arrow="true" cta="View guide" />

<Card title="Tencent COS File" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_file" arrow="true" cta="View guide" />

<Card title="TensorFlow Datasets" icon="link" href="/oss/python/integrations/document_loaders/tensorflow_datasets" arrow="true" cta="View guide" />

<Card title="TiDB" icon="link" href="/oss/python/integrations/document_loaders/tidb" arrow="true" cta="View guide" />

<Card title="2Markdown" icon="link" href="/oss/python/integrations/document_loaders/tomarkdown" arrow="true" cta="View guide" />

<Card title="TOML" icon="link" href="/oss/python/integrations/document_loaders/toml" arrow="true" cta="View guide" />

<Card title="Trello" icon="link" href="/oss/python/integrations/document_loaders/trello" arrow="true" cta="View guide" />

<Card title="TSV" icon="link" href="/oss/python/integrations/document_loaders/tsv" arrow="true" cta="View guide" />

<Card title="Twitter" icon="link" href="/oss/python/integrations/document_loaders/twitter" arrow="true" cta="View guide" />

<Card title="UnDatasIO" icon="link" href="/oss/python/integrations/document_loaders/undatasio" arrow="true" cta="View guide" />

<Card title="Unstructured" icon="link" href="/oss/python/integrations/document_loaders/unstructured_file" arrow="true" cta="View guide" />

<Card title="UnstructuredMarkdownLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_markdown" arrow="true" cta="View guide" />

<Card title="UnstructuredPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_pdfloader" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/document_loaders/upstage" arrow="true" cta="View guide" />

<Card title="URL" icon="link" href="/oss/python/integrations/document_loaders/url" arrow="true" cta="View guide" />

<Card title="Vsdx" icon="link" href="/oss/python/integrations/document_loaders/vsdx" arrow="true" cta="View guide" />

<Card title="Weather" icon="link" href="/oss/python/integrations/document_loaders/weather" arrow="true" cta="View guide" />

<Card title="WebBaseLoader" icon="link" href="/oss/python/integrations/document_loaders/web_base" arrow="true" cta="View guide" />

<Card title="WhatsApp Chat" icon="link" href="/oss/python/integrations/document_loaders/whatsapp_chat" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/document_loaders/wikipedia" arrow="true" cta="View guide" />

<Card title="UnstructuredXMLLoader" icon="link" href="/oss/python/integrations/document_loaders/xml" arrow="true" cta="View guide" />

<Card title="Xorbits Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/xorbits" arrow="true" cta="View guide" />

<Card title="YouTube Audio" icon="link" href="/oss/python/integrations/document_loaders/youtube_audio" arrow="true" cta="View guide" />

<Card title="YouTube Transcripts" icon="link" href="/oss/python/integrations/document_loaders/youtube_transcript" arrow="true" cta="View guide" />

<Card title="YoutubeLoaderDL" icon="link" href="/oss/python/integrations/document_loaders/yt_dlp" arrow="true" cta="View guide" />

<Card title="Yuque" icon="link" href="/oss/python/integrations/document_loaders/yuque" arrow="true" cta="View guide" />

<Card title="ZeroxPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/zeroxpdfloader" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/document_loaders/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## minReplicas: 4

**URL:** llms-txt#minreplicas:-4

backend:
  deployment:
    replicas: 40 # OR enable autoscaling to this level (example below)

---

## Key-value stores

**URL:** llms-txt#key-value-stores

**Contents:**
- Overview
- Interface
- Built-in stores for local development
- Custom stores
- All key-value stores

Source: https://docs.langchain.com/oss/python/integrations/stores/index

LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/python/integrations/text_embedding).

All [`BaseStores`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface:

* `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist
* `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys
* `mdelete(key: Sequence[str]) -> None`: delete multiple keys
* `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix

<Note>
  Base stores are designed to work **multiple** key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.
</Note>

## Built-in stores for local development

<Columns cols={2}>
  <Card title="InMemoryByteStore" icon="link" href="/oss/python/integrations/stores/in_memory" arrow="true" cta="View guide" />

<Card title="LocalFileStore" icon="link" href="/oss/python/integrations/stores/file_system" arrow="true" cta="View guide" />
</Columns>

You can also implement your own custom store by extending the [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) class. See the [store interface documentation](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) for more details.

## All key-value stores

<Columns cols={3}>
  <Card title="AstraDBByteStore" icon="link" href="/oss/python/integrations/stores/astradb" arrow="true" cta="View guide" />

<Card title="CassandraByteStore" icon="link" href="/oss/python/integrations/stores/cassandra" arrow="true" cta="View guide" />

<Card title="ElasticsearchEmbeddingsCache" icon="link" href="/oss/python/integrations/stores/elasticsearch" arrow="true" cta="View guide" />

<Card title="RedisStore" icon="link" href="/oss/python/integrations/stores/redis" arrow="true" cta="View guide" />

<Card title="UpstashRedisByteStore" icon="link" href="/oss/python/integrations/stores/upstash_redis" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/stores/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create and run a LangChain application

**URL:** llms-txt#create-and-run-a-langchain-application

**Contents:**
- Supported OpenTelemetry attribute and event mapping
  - Core LangSmith attributes
  - GenAI standard attributes
  - GenAI request parameters
  - GenAI usage metrics
  - TraceLoop attributes
  - OpenInference attributes
  - LLM attributes
  - Prompt template attributes
  - Retriever attributes

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatOpenAI()
chain = prompt | model
result = chain.invoke({"topic": "programming"})
print(result.content)
python  theme={null}
import asyncio
from langsmith.integrations.otel import configure
from google.adk import Runner
from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.genai import types

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  Hybrid tracing is available in version **≥ 0.4.1**. To send traces **only** to your OTEL endpoint, set:

  `LANGSMITH_OTEL_ONLY="true"`
  (Recommendation: use **langsmith ≥ 0.4.25**.)
</Info>

## Supported OpenTelemetry attribute and event mapping

When sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:

### Core LangSmith attributes

| OpenTelemetry attribute        | LangSmith field  | Notes                                                                        |
| ------------------------------ | ---------------- | ---------------------------------------------------------------------------- |
| `langsmith.trace.name`         | Run name         | Overrides the span name for the run                                          |
| `langsmith.span.kind`          | Run type         | Values: `llm`, `chain`, `tool`, `retriever`, `embedding`, `prompt`, `parser` |
| `langsmith.trace.session_id`   | Session ID       | Session identifier for related traces                                        |
| `langsmith.trace.session_name` | Session name     | Name of the session                                                          |
| `langsmith.span.tags`          | Tags             | Custom tags attached to the span (comma-separated)                           |
| `langsmith.metadata.{key}`     | `metadata.{key}` | Custom metadata with langsmith prefix                                        |

### GenAI standard attributes

| OpenTelemetry attribute                 | LangSmith field               | Notes                                                         |
| --------------------------------------- | ----------------------------- | ------------------------------------------------------------- |
| `gen_ai.system`                         | `metadata.ls_provider`        | The GenAI system (e.g., "openai", "anthropic")                |
| `gen_ai.operation.name`                 | Run type                      | Maps "chat"/"completion" to "llm", "embedding" to "embedding" |
| `gen_ai.prompt`                         | `inputs`                      | The input prompt sent to the model                            |
| `gen_ai.completion`                     | `outputs`                     | The output generated by the model                             |
| `gen_ai.prompt.{n}.role`                | `inputs.messages[n].role`     | Role for the nth input message                                |
| `gen_ai.prompt.{n}.content`             | `inputs.messages[n].content`  | Content for the nth input message                             |
| `gen_ai.prompt.{n}.message.role`        | `inputs.messages[n].role`     | Alternative format for role                                   |
| `gen_ai.prompt.{n}.message.content`     | `inputs.messages[n].content`  | Alternative format for content                                |
| `gen_ai.completion.{n}.role`            | `outputs.messages[n].role`    | Role for the nth output message                               |
| `gen_ai.completion.{n}.content`         | `outputs.messages[n].content` | Content for the nth output message                            |
| `gen_ai.completion.{n}.message.role`    | `outputs.messages[n].role`    | Alternative format for role                                   |
| `gen_ai.completion.{n}.message.content` | `outputs.messages[n].content` | Alternative format for content                                |
| `gen_ai.input.messages`                 | `inputs.messages`             | Array of input messages                                       |
| `gen_ai.output.messages`                | `outputs.messages`            | Array of output messages                                      |
| `gen_ai.tool.name`                      | `invocation_params.tool_name` | Tool name, also sets run type to "tool"                       |

### GenAI request parameters

| OpenTelemetry attribute            | LangSmith field                       | Notes                                   |
| ---------------------------------- | ------------------------------------- | --------------------------------------- |
| `gen_ai.request.model`             | `invocation_params.model`             | The model name used for the request     |
| `gen_ai.response.model`            | `invocation_params.model`             | The model name returned in the response |
| `gen_ai.request.temperature`       | `invocation_params.temperature`       | Temperature setting                     |
| `gen_ai.request.top_p`             | `invocation_params.top_p`             | Top-p sampling setting                  |
| `gen_ai.request.max_tokens`        | `invocation_params.max_tokens`        | Maximum tokens setting                  |
| `gen_ai.request.frequency_penalty` | `invocation_params.frequency_penalty` | Frequency penalty setting               |
| `gen_ai.request.presence_penalty`  | `invocation_params.presence_penalty`  | Presence penalty setting                |
| `gen_ai.request.seed`              | `invocation_params.seed`              | Random seed used for generation         |
| `gen_ai.request.stop_sequences`    | `invocation_params.stop`              | Sequences that stop generation          |
| `gen_ai.request.top_k`             | `invocation_params.top_k`             | Top-k sampling parameter                |
| `gen_ai.request.encoding_formats`  | `invocation_params.encoding_formats`  | Output encoding formats                 |

### GenAI usage metrics

| OpenTelemetry attribute                 | LangSmith field                   | Notes                                     |
| --------------------------------------- | --------------------------------- | ----------------------------------------- |
| `gen_ai.usage.input_tokens`             | `usage_metadata.input_tokens`     | Number of input tokens used               |
| `gen_ai.usage.output_tokens`            | `usage_metadata.output_tokens`    | Number of output tokens used              |
| `gen_ai.usage.total_tokens`             | `usage_metadata.total_tokens`     | Total number of tokens used               |
| `gen_ai.usage.prompt_tokens`            | `usage_metadata.input_tokens`     | Number of input tokens used (deprecated)  |
| `gen_ai.usage.completion_tokens`        | `usage_metadata.output_tokens`    | Number of output tokens used (deprecated) |
| `gen_ai.usage.details.reasoning_tokens` | `usage_metadata.reasoning_tokens` | Number of reasoning tokens used           |

### TraceLoop attributes

| OpenTelemetry attribute                  | LangSmith field  | Notes                                            |
| ---------------------------------------- | ---------------- | ------------------------------------------------ |
| `traceloop.entity.input`                 | `inputs`         | Full input value from TraceLoop                  |
| `traceloop.entity.output`                | `outputs`        | Full output value from TraceLoop                 |
| `traceloop.entity.name`                  | Run name         | Entity name from TraceLoop                       |
| `traceloop.span.kind`                    | Run type         | Maps to LangSmith run types                      |
| `traceloop.llm.request.type`             | Run type         | "embedding" maps to "embedding", others to "llm" |
| `traceloop.association.properties.{key}` | `metadata.{key}` | Custom metadata with traceloop prefix            |

### OpenInference attributes

| OpenTelemetry attribute   | LangSmith field          | Notes                                     |
| ------------------------- | ------------------------ | ----------------------------------------- |
| `input.value`             | `inputs`                 | Full input value, can be string or JSON   |
| `output.value`            | `outputs`                | Full output value, can be string or JSON  |
| `openinference.span.kind` | Run type                 | Maps various kinds to LangSmith run types |
| `llm.system`              | `metadata.ls_provider`   | LLM system provider                       |
| `llm.model_name`          | `metadata.ls_model_name` | Model name from OpenInference             |
| `tool.name`               | Run name                 | Tool name when span kind is "TOOL"        |
| `metadata`                | `metadata.*`             | JSON string of metadata to be merged      |

### LLM attributes

| OpenTelemetry attribute      | LangSmith field                       | Notes                                |
| ---------------------------- | ------------------------------------- | ------------------------------------ |
| `llm.input_messages`         | `inputs.messages`                     | Input messages                       |
| `llm.output_messages`        | `outputs.messages`                    | Output messages                      |
| `llm.token_count.prompt`     | `usage_metadata.input_tokens`         | Prompt token count                   |
| `llm.token_count.completion` | `usage_metadata.output_tokens`        | Completion token count               |
| `llm.token_count.total`      | `usage_metadata.total_tokens`         | Total token count                    |
| `llm.usage.total_tokens`     | `usage_metadata.total_tokens`         | Alternative total token count        |
| `llm.invocation_parameters`  | `invocation_params.*`                 | JSON string of invocation parameters |
| `llm.presence_penalty`       | `invocation_params.presence_penalty`  | Presence penalty                     |
| `llm.frequency_penalty`      | `invocation_params.frequency_penalty` | Frequency penalty                    |
| `llm.request.functions`      | `invocation_params.functions`         | Function definitions                 |

### Prompt template attributes

| OpenTelemetry attribute         | LangSmith field | Notes                                            |
| ------------------------------- | --------------- | ------------------------------------------------ |
| `llm.prompt_template.variables` | Run type        | Sets run type to "prompt", used with input.value |

### Retriever attributes

| OpenTelemetry attribute                     | LangSmith field                     | Notes                                         |
| ------------------------------------------- | ----------------------------------- | --------------------------------------------- |
| `retrieval.documents.{n}.document.content`  | `outputs.documents[n].page_content` | Content of the nth retrieved document         |
| `retrieval.documents.{n}.document.metadata` | `outputs.documents[n].metadata`     | Metadata of the nth retrieved document (JSON) |

### Tool attributes

| OpenTelemetry attribute | LangSmith field                    | Notes                                     |
| ----------------------- | ---------------------------------- | ----------------------------------------- |
| `tools`                 | `invocation_params.tools`          | Array of tool definitions                 |
| `tool_arguments`        | `invocation_params.tool_arguments` | Tool arguments as JSON or key-value pairs |

### Logfire attributes

| OpenTelemetry attribute | LangSmith field    | Notes                                            |
| ----------------------- | ------------------ | ------------------------------------------------ |
| `prompt`                | `inputs`           | Logfire prompt input                             |
| `all_messages_events`   | `outputs`          | Logfire message events output                    |
| `events`                | `inputs`/`outputs` | Logfire events array, splits input/choice events |

### OpenTelemetry event mapping

| Event name                  | LangSmith field      | Notes                                                            |
| --------------------------- | -------------------- | ---------------------------------------------------------------- |
| `gen_ai.content.prompt`     | `inputs`             | Extracts prompt content from event attributes                    |
| `gen_ai.content.completion` | `outputs`            | Extracts completion content from event attributes                |
| `gen_ai.system.message`     | `inputs.messages[]`  | System message in conversation                                   |
| `gen_ai.user.message`       | `inputs.messages[]`  | User message in conversation                                     |
| `gen_ai.assistant.message`  | `outputs.messages[]` | Assistant message in conversation                                |
| `gen_ai.tool.message`       | `outputs.messages[]` | Tool response message                                            |
| `gen_ai.choice`             | `outputs`            | Model choice/response with finish reason                         |
| `exception`                 | `status`, `error`    | Sets status to "error" and extracts exception message/stacktrace |

#### Event attribute extraction

For message events, the following attributes are extracted:

* `content` → message content
* `role` → message role
* `id` → tool\_call\_id (for tool messages)
* `gen_ai.event.content` → full message JSON

For choice events:

* `finish_reason` → choice finish reason
* `message.content` → choice message content
* `message.role` → choice message role
* `tool_calls.{n}.id` → tool call ID
* `tool_calls.{n}.function.name` → tool function name
* `tool_calls.{n}.function.arguments` → tool function arguments
* `tool_calls.{n}.type` → tool call type

For exception events:

* `exception.message` → error message
* `exception.stacktrace` → error stacktrace (appended to message)

## Implementation examples

### Trace using the LangSmith SDK

Use the LangSmith SDK's OpenTelemetry helper to configure export:
```

---

## 3. Define the interface to your app

**URL:** llms-txt#3.-define-the-interface-to-your-app

def chatbot(inputs: dict) -> dict:
    return {"answer": inputs["question"] + " is a good question. I don't know the answer."}

---

## Enqueue concurrent

**URL:** llms-txt#enqueue-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/enqueue-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `enqueue` option for double texting, which adds the interruptions to a queue and executes them in the order they are received by the client. Below is a quick example of using the `enqueue` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's start two runs, with the second interrupting the first one with a multitask strategy of "enqueue":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the thread has data from both runs:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/enqueue-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## maxReplicas: 50

**URL:** llms-txt#maxreplicas:-50

---

## Service B: Extract the context and continue the trace

**URL:** llms-txt#service-b:-extract-the-context-and-continue-the-trace

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/process", methods=["POST"])
def service_b_endpoint():
    # Extract the trace context from the request headers
    context = extract(request.headers)
    with tracer.start_as_current_span("service_b_operation", context=context) as span:
        data = request.json
        summary = data.get("summary", "")

# Process the summary with another LLM chain
        prompt = ChatPromptTemplate.from_template("Analyze the sentiment of: {text}")
        model = ChatOpenAI()
        chain = prompt | model
        result = chain.invoke({"text": summary})

return jsonify({"analysis": result.content})

if __name__ == "__main__":
    app.run(port=5000)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-opentelemetry.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith docs

**URL:** llms-txt#langsmith-docs

**Contents:**
- Get started
- Workflow

Source: https://docs.langchain.com/langsmith/home

**LangSmith provides tools for developing, debugging, and deploying LLM applications.**
It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.
LangSmith is framework agnostic, so you can use it with or without LangChain's open-source libraries
[`langchain`](/oss/python/langchain/overview) and [`langgraph`](/oss/python/langgraph/overview).
Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
  LangGraph Platform is now [LangSmith Deployment](/langsmith/deployments). For more information, check out the [Changelog](https://changelog.langchain.com/announcements/product-naming-changes-langsmith-deployment-and-langsmith-studio).
</Callout>

<Steps>
  <Step title="Create an account" icon="user-plus">
    Sign up at [smith.langchain.com](https://smith.langchain.com) (no credit card required).
    You can log in with **Google**, **GitHub**, or **email**.
  </Step>

<Step title="Create an API key" icon="key">
    Go to your [Settings page](https://smith.langchain.com/settings) → **API Keys** → **Create API Key**.
    Copy the key and save it securely.
  </Step>
</Steps>

Once your account and API key are ready, choose a quickstart to begin building with LangSmith:

<Columns cols={3}>
  <Card title="Observability" icon="magnifying-glass" href="/langsmith/observability-quickstart" arrow="true" cta="Start tracing">
    Gain visibility into every step your application takes to debug faster and improve reliability.
  </Card>

<Card title="Evaluation" icon="chart-line" href="/langsmith/evaluation-quickstart" arrow="true" cta="Evaluate your app">
    Measure and track quality over time to ensure your AI applications are consistent and trustworthy.
  </Card>

<Card title="Deployment" icon="cloud-arrow-up" iconType="solid" href="/langsmith/deployments" arrow="true" cta="Deploy your agents">
    Deploy your agents as LangGraph Servers, ready to scale in production.
  </Card>

<Card title="Prompt Testing" icon="flask" href="/langsmith/prompt-engineering-quickstart" arrow="true" cta="Test your prompts">
    Iterate on prompts with built-in versioning and collaboration to ship improvements faster.
  </Card>

<Card title="Studio" icon="window" href="/langsmith/quick-start-studio" arrow="true" cta="Develop with Studio">
    Use a visual interface to design, test, and refine applications end-to-end.
  </Card>

<Card title="Hosting" icon="server" iconType="solid" href="/langsmith/hosting" arrow="true" cta="Choose your hosting mode">
    Host LangSmith in the cloud, in your environment, or hybrid to match your infrastructure and compliance needs.
  </Card>
</Columns>

LangSmith combines observability, evaluation, deployment, and hosting in one integrated workflow—from local development to production.

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=84312d9c72c44e44ac513eaa78abefc6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and hosting in a single workflow from development to production." data-og-width="1138" width="1138" data-og-height="549" height="549" data-path="langsmith/images/overview-light.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=280&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=32fa495af106d1eb30c9d512b59c3926 280w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=560&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f30ac54ecdc55dd22b5762800f33a5bb 560w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=840&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=aee1643d7b4ab4cf74ffb54b555e3788 840w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1100&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=65946469f91ff9cfbd26d154d31c6caf 1100w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1650&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=a310e56592be23e233075ebffa1b124d 1650w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=2500&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f9e2edcac083797d0498806cb512d5de 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f930d2f475ab68e1cdfe7105b5f1abe6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and hosting in a single workflow from development to production." data-og-width="1157" width="1157" data-og-height="549" height="549" data-path="langsmith/images/overview-dark.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=280&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=cab90851ba7c49d855f89c86f6bfdd0b 280w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=560&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=d04a2b423106d1f64fa0a3a05948a19a 560w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=840&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=84c821c5464a20cf4a3d397e5baf7357 840w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1100&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=202f8ed1e7c38c5a6184b9e1b2ea377f 1100w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1650&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f8d03af520cd2f0363f9b41f86428396 1650w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=2500&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=576885f21fd2e6bc4ecb4e4176515a3d 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/home.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Show the workflow

**URL:** llms-txt#show-the-workflow

display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))

---

## candidate_results.to_pandas()

**URL:** llms-txt#candidate_results.to_pandas()

**Contents:**
- Comparing the results

## Comparing the results

After running both experiments, you can view them in your dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1c5d4f1cf212e2c38917319c7bbf7f99" alt="" data-og-width="3022" width="3022" data-og-height="1536" height="1536" data-path="langsmith/images/dataset-page.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=01ebc5373bb6428b614eeade16aeb606 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b2308ef5ed76bb80f111a89000457424 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2c76f9b1194a1a56efaa97d88b885f0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=beb69429d2a2e208b75924593a9a10c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0fa47df443b9695d1e6863a57ca3a016 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d257f3444d8357fb0b0340d09c16e127 2500w" />

The results reveal an interesting tradeoff between the two models:

1. GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis
2. However, GPT-4o is less reliable at staying grounded in the provided search results

To illustrate the grounding issue: in [this example run](https://smith.langchain.com/public/be060e19-0bc0-4798-94f5-c3d35719a5f6/r/07d43e7a-8632-479d-ae28-c7eac6e54da4), GPT-4o included facts about Abū Bakr Muhammad ibn Zakariyyā al-Rāzī's medical contributions that weren't present in the search results. This demonstrates how it's pulling from its internal knowledge rather than strictly using the provided information.

This backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldn't improve our tweet-writer. To effectively use GPT-4o, we would need to:

* Refine our prompts to more strongly emphasize using only provided information
* Or modify our system architecture to better constrain the model's outputs

This insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8ab311399f3d0e69554a62f939fd475" alt="" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/tutorial-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=92dca1af013a79d9ce2ee944a17e23a9 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=16ac3bc225307b5408a49c225646a99e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=517037b9b2a372dd5d4b2dc4e41eac6a 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d1b1d90b0838a53d1c693617fad61eb4 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ecbe835c06c9451cac57d7cf0a16d0b9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77d1da832c66587699653956fc15ccb6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-backtests-new-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LLMs

**URL:** llms-txt#llms

**Contents:**
- All LLMs

Source: https://docs.langchain.com/oss/javascript/integrations/llms/index

<Warning>
  **You are currently on a page documenting the use of text completion models. Many of the latest and most popular models are [chat completion models](/oss/javascript/langchain/models).**

Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/oss/javascript/integrations/chat/).
</Warning>

[LLMs](/oss/javascript/langchain/models) are language models that takes a string as input and return a string as output.

<Columns cols={3}>
  <Card title="AI21" icon="link" href="/oss/javascript/integrations/llms/ai21" arrow="true" cta="View guide" />

<Card title="AlephAlpha" icon="link" href="/oss/javascript/integrations/llms/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Arcjet Redact" icon="link" href="/oss/javascript/integrations/llms/arcjet" arrow="true" cta="View guide" />

<Card title="AWS SageMakerEndpoint" icon="link" href="/oss/javascript/integrations/llms/aws_sagemaker" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/javascript/integrations/llms/azure" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/javascript/integrations/llms/bedrock" arrow="true" cta="View guide" />

<Card title="ChromeAI" icon="link" href="/oss/javascript/integrations/llms/chrome_ai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/javascript/integrations/llms/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/javascript/integrations/llms/cohere" arrow="true" cta="View guide" />

<Card title="Deep Infra" icon="link" href="/oss/javascript/integrations/llms/deep_infra" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/javascript/integrations/llms/fireworks" arrow="true" cta="View guide" />

<Card title="Friendli" icon="link" href="/oss/javascript/integrations/llms/friendli" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/javascript/integrations/llms/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="Gradient AI" icon="link" href="/oss/javascript/integrations/llms/gradient_ai" arrow="true" cta="View guide" />

<Card title="HuggingFaceInference" icon="link" href="/oss/javascript/integrations/llms/huggingface_inference" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/javascript/integrations/llms/ibm" arrow="true" cta="View guide" />

<Card title="JigsawStack Prompt Engine" icon="link" href="/oss/javascript/integrations/llms/jigsawstack" arrow="true" cta="View guide" />

<Card title="Layerup Security" icon="link" href="/oss/javascript/integrations/llms/layerup_security" arrow="true" cta="View guide" />

<Card title="Llama CPP" icon="link" href="/oss/javascript/integrations/llms/llama_cpp" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/javascript/integrations/llms/mistral" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/javascript/integrations/llms/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/javascript/integrations/llms/openai" arrow="true" cta="View guide" />

<Card title="RaycastAI" icon="link" href="/oss/javascript/integrations/llms/raycast" arrow="true" cta="View guide" />

<Card title="Replicate" icon="link" href="/oss/javascript/integrations/llms/replicate" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/javascript/integrations/llms/together" arrow="true" cta="View guide" />

<Card title="WRITER" icon="link" href="/oss/javascript/integrations/llms/writer" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/javascript/integrations/llms/yandex" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llms/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Can equivalently use the 'aevaluate' function directly:

**URL:** llms-txt#can-equivalently-use-the-'aevaluate'-function-directly:

---

## Parent graph

**URL:** llms-txt#parent-graph

**Contents:**
- View subgraph state
- Stream subgraph outputs

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)
python  theme={null}
subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)
python  theme={null}
  from langgraph.graph import START, StateGraph
  from langgraph.checkpoint.memory import MemorySaver
  from langgraph.types import interrupt, Command
  from typing_extensions import TypedDict

class State(TypedDict):
      foo: str

def subgraph_node_1(state: State):
      value = interrupt("Provide value:")
      return {"foo": state["foo"] + value}

subgraph_builder = StateGraph(State)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_edge(START, "subgraph_node_1")

subgraph = subgraph_builder.compile()

builder = StateGraph(State)
  builder.add_node("node_1", subgraph)
  builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}

graph.invoke({"foo": ""}, config)
  parent_state = graph.get_state(config)

# This will be available only when the subgraph is interrupted.
  # Once you resume the graph, you won't be able to access the subgraph state.
  subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state

# resume the subgraph
  graph.invoke(Command(resume="bar"), config)
  python  theme={null}
for chunk in graph.stream(
    {"foo": "foo"},
    subgraphs=True, # [!code highlight]
    stream_mode="updates",
):
    print(chunk)
python  theme={null}
  from typing_extensions import TypedDict
  from langgraph.graph.state import StateGraph, START

# Define subgraph
  class SubgraphState(TypedDict):
      foo: str
      bar: str

def subgraph_node_1(state: SubgraphState):
      return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
      # note that this node is using a state key ('bar') that is only available in the subgraph
      # and is sending update on the shared state key ('foo')
      return {"foo": state["foo"] + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_node(subgraph_node_2)
  subgraph_builder.add_edge(START, "subgraph_node_1")
  subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
  subgraph = subgraph_builder.compile()

# Define parent graph
  class ParentState(TypedDict):
      foo: str

def node_1(state: ParentState):
      return {"foo": "hi! " + state["foo"]}

builder = StateGraph(ParentState)
  builder.add_node("node_1", node_1)
  builder.add_node("node_2", subgraph)
  builder.add_edge(START, "node_1")
  builder.add_edge("node_1", "node_2")
  graph = builder.compile()

for chunk in graph.stream(
      {"foo": "foo"},
      stream_mode="updates",
      subgraphs=True, # [!code highlight]
  ):
      print(chunk)
  
  ((), {'node_1': {'foo': 'hi! foo'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
  ((), {'node_2': {'foo': 'hi! foobar'}})
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories:
```

Example 2 (unknown):
```unknown
## View subgraph state

When you enable [persistence](/oss/python/langgraph/persistence), you can [inspect the graph state](/oss/python/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.

You can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.

<Warning>
  **Available **only** when interrupted**
  Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.
</Warning>

<Accordion title="View interrupted subgraph state">
```

Example 3 (unknown):
```unknown
1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.
</Accordion>

## Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.
```

Example 4 (unknown):
```unknown
<Accordion title="Stream from subgraphs">
```

---

## >          'action_requests': [

**URL:** llms-txt#>----------'action_requests':-[

---

## 2. Define a graph that accesses the config in a node

**URL:** llms-txt#2.-define-a-graph-that-accesses-the-config-in-a-node

class State(TypedDict):
    my_state_value: str

def node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]
    if runtime.context["my_runtime_value"] == "a":  # [!code highlight]
        return {"my_state_value": 1}
    elif runtime.context["my_runtime_value"] == "b":  # [!code highlight]
        return {"my_state_value": 2}
    else:
        raise ValueError("Unknown values.")

builder = StateGraph(State, context_schema=ContextSchema)  # [!code highlight]
builder.add_node(node)
builder.add_edge(START, "node")
builder.add_edge("node", END)

graph = builder.compile()

---

## Node-style: validation after model calls

**URL:** llms-txt#node-style:-validation-after-model-calls

@after_model(can_jump_to=["end"])
def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    last_message = state["messages"][-1]
    if "BLOCKED" in last_message.content:
        return {
            "messages": [AIMessage("I cannot respond to that request.")],
            "jump_to": "end"
        }
    return None

---

## We need this because we want to enable threads (conversations)

**URL:** llms-txt#we-need-this-because-we-want-to-enable-threads-(conversations)

checkpointer = InMemorySaver()

---

## LangGraph v1 migration guide

**URL:** llms-txt#langgraph-v1-migration-guide

**Contents:**
- Summary of changes
- Deprecations
- `create_react_agent` → `create_agent`
- Breaking changes
  - Dropped Python 3.9 support

Source: https://docs.langchain.com/oss/python/migrate/langgraph-v1

This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the [what's new](/oss/python/releases/langgraph-v1) page.

## Summary of changes

LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) in favor of LangChain's new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) function.

The following table lists all items deprecated in LangGraph v1:

| Deprecated item                            | Alternative                                                                                                                                                                                                                                             |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `create_react_agent`                       | [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                                               |
| `AgentState`                               | [`langchain.agents.AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                                                                                                   |
| `AgentStatePydantic`                       | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |
| `AgentStateWithStructuredResponse`         | `langchain.agents.AgentState`                                                                                                                                                                                                                           |
| `AgentStateWithStructuredResponsePydantic` | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |
| `HumanInterruptConfig`                     | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |
| `ActionRequest`                            | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |
| `HumanInterrupt`                           | `langchain.agents.middleware.human_in_the_loop.HITLRequest`                                                                                                                                                                                             |
| `ValidationNode`                           | Tools automatically validate input with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                        |
| `MessageGraph`                             | [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with a `messages` key, like [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides |

## `create_react_agent` → `create_agent`

LangGraph v1 deprecates the [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt. Use LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), which runs on LangGraph and adds a flexible middleware system.

See the LangChain v1 docs for details:

* [Release notes](/oss/python/releases/langchain-v1#createagent)
* [Migration guide](/oss/python/migrate/langchain-v1#migrate-to-create_agent)

### Dropped Python 3.9 support

All LangChain packages now require **Python 3.10 or higher**. Python 3.9 reached [end of life](https://devguide.python.org/versions/) in October 2025.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langgraph-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Summary of changes

LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) in favor of LangChain's new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) function.

## Deprecations

The following table lists all items deprecated in LangGraph v1:

| Deprecated item                            | Alternative                                                                                                                                                                                                                                             |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `create_react_agent`                       | [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                                               |
| `AgentState`                               | [`langchain.agents.AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                                                                                                   |
| `AgentStatePydantic`                       | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |
| `AgentStateWithStructuredResponse`         | `langchain.agents.AgentState`                                                                                                                                                                                                                           |
| `AgentStateWithStructuredResponsePydantic` | `langchain.agents.AgentState` (no more pydantic state)                                                                                                                                                                                                  |
| `HumanInterruptConfig`                     | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |
| `ActionRequest`                            | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig`                                                                                                                                                                                       |
| `HumanInterrupt`                           | `langchain.agents.middleware.human_in_the_loop.HITLRequest`                                                                                                                                                                                             |
| `ValidationNode`                           | Tools automatically validate input with [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)                                                                                                        |
| `MessageGraph`                             | [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with a `messages` key, like [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) provides |

## `create_react_agent` → `create_agent`

LangGraph v1 deprecates the [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt. Use LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), which runs on LangGraph and adds a flexible middleware system.

See the LangChain v1 docs for details:

* [Release notes](/oss/python/releases/langchain-v1#createagent)
* [Migration guide](/oss/python/migrate/langchain-v1#migrate-to-create_agent)

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Configure OpenTelemetry

**URL:** llms-txt#configure-opentelemetry

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## Get customer information from the API

**URL:** llms-txt#get-customer-information-from-the-api

export LANGSMITH_URL="<your_langsmith_url>"
export response=$(curl -s $LANGSMITH_URL/api/v1/info)
export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id') && echo "Customer ID: $CUSTOMER_ID"
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name') && echo "Customer name: $CUSTOMER_NAME"

---

## App development in LangSmith Deployment

**URL:** llms-txt#app-development-in-langsmith-deployment

Source: https://docs.langchain.com/langsmith/app-development

**LangSmith Deployment** builds on the open-source [LangGraph](/oss/python/langgraph/overview) framework for developing stateful, multi-agent applications.
LangGraph provides the core abstractions and execution model, while LangSmith adds managed infrastructure, observability, deployment options, assistants, and concurrency controls—supporting the full lifecycle from development to production.

<Callout icon="cubes" color="#4F46E5" iconType="regular">
  LangSmith Deployment is framework-agnostic: you can deploy agents built with LangGraph or [other frameworks](/langsmith/autogen-integration). To get started with LangGraph itself, refer to the [LangGraph quickstart](/oss/python/langgraph/quickstart).
</Callout>

<CardGroup>
  <Card title="Assistants" cta="Explore assistants" href="/langsmith/assistants" icon="user-gear">
    Manage agent configurations, connect to threads, and build interactive assistants.
  </Card>

<Card title="Runs" cta="Learn about runs" href="/langsmith/background-run" icon="play">
    Execute background jobs, stateless runs, cron jobs, and manage configurable headers.
  </Card>

<Card title="Core capabilities" cta="See core features" href="/langsmith/streaming" icon="gear">
    Streaming, human-in-the-loop, webhooks, and concurrency controls like double-texting.
  </Card>

<Card title="Tutorials" cta="View tutorials" href="/langsmith/autogen-integration" icon="graduation-cap">
    Step-by-step examples: AutoGen integration, streaming UI, and generative UI in React.
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/app-development.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Delete organizations

**URL:** llms-txt#delete-organizations

**Contents:**
  - Prerequisites
  - Running the deletion script for a single organization

Source: https://docs.langchain.com/langsmith/script-delete-an-organization

The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.

This command using the Organization ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete an organization

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_organization_sh)

### Running the deletion script for a single organization

Run the following command to run the organization removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see organization is no longer present.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-an-organization.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Google

**URL:** llms-txt#google

**Contents:**
- Google Generative AI (Gemini API & AI Studio)
  - Chat models

Source: https://docs.langchain.com/oss/python/integrations/providers/google

All LangChain integrations with [Google Cloud](https://cloud.google.com/), [Google Gemini](https://ai.google.dev/gemini-api/docs) and other Google products.

1. **Google Generative AI (Gemini API & AI Studio)**: Access Google Gemini models directly via the Gemini API. Use [Google AI Studio](https://aistudio.google.com/) for rapid prototyping and get started quickly with the `langchain-google-genai` package. This is often the best starting point for individual developers.
2. **Google Cloud (Vertex AI & other services)**: Access Gemini models, Vertex AI Model Garden and a wide range of cloud services (databases, storage, document AI, etc.) via the [Google Cloud Platform](https://cloud.google.com/). Use the `langchain-google-vertexai` package for Vertex AI models and specific packages (e.g., `langchain-google-cloud-sql-pg`, `langchain-google-community`) for other cloud services. This is ideal for developers already using Google Cloud or needing enterprise features like MLOps, specific model tuning or enterprise support.

See Google's guide on [migrating from the Gemini API to Vertex AI](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more details on the differences.

Integration packages for Gemini models and the Vertex AI platform are maintained in the [`langchain-google`](https://github.com/langchain-ai/langchain-google) repository. You can find a host of LangChain integrations with other Google APIs and services in the [googleapis](https://github.com/googleapis?q=langchain-\&type=all\&language=\&sort=)Github organization and the `langchain-google-community` package.

## Google Generative AI (Gemini API & AI Studio)

Access Google Gemini models directly using the Gemini API, best suited for rapid development and experimentation. Gemini models are available in [Google AI Studio](https://aistudio.google.com/).

Start for free and get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey).

Use the `ChatGoogleGenerativeAI` class to interact with Gemini models. See
details in [this guide](/oss/python/integrations/chat/google_generative_ai).

```python  theme={null}
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.messages import HumanMessage

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Start for free and get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey).
```

Example 3 (unknown):
```unknown
### Chat models

Use the `ChatGoogleGenerativeAI` class to interact with Gemini models. See
details in [this guide](/oss/python/integrations/chat/google_generative_ai).
```

---

## Trace with OpenTelemetry

**URL:** llms-txt#trace-with-opentelemetry

**Contents:**
- Trace a LangChain application
- Trace a non-LangChain application
- Send traces to an alternate provider
  - Use environment variables for global configuration
  - Configure alternate OTLP endpoints

Source: https://docs.langchain.com/langsmith/trace-with-opentelemetry

LangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks.

Learn how to trace your LLM applications using OpenTelemetry with LangSmith.

<Note>
  Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use `eu.api.smith.langchain.com`.
</Note>

## Trace a LangChain application

If you're using LangChain or LangGraph, use the built-in integration to trace your application:

1. Install the LangSmith package with OpenTelemetry support:

<CodeGroup>
     
   </CodeGroup>

<Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

<CodeGroup>
     
   </CodeGroup>

3. Create a LangChain application with tracing. For example:

4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

<CodeGroup>
     
   </CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

<CodeGroup>
     
   </CodeGroup>

<Note>
     Depending on how your otel exporter is configured, you may need to append `/v1/traces` to the endpoint if you are only sending traces.
   </Note>

<Note>
     If you're self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append `/api/v1`. For example: `OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel`
   </Note>

Optional: Specify a custom project name other than "default":

<CodeGroup>
     
   </CodeGroup>

This code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes.

4. View the trace in your LangSmith dashboard ([example](https://smith.langchain.com/public/4f2890b1-f105-44aa-a6cf-c777dcc27a37/r)).

## Send traces to an alternate provider

While LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.

<Info>
  Available in LangSmith Python SDK **≥ 0.4.1**. We recommend **≥ 0.4.25** for fixes that improve OTEL export and hybrid fan-out stability.
</Info>

### Use environment variables for global configuration

By default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:

LangSmith uses the HTTP trace exporter by default. If you'd like to use your own tracing provider, you can either:

1. Set the OTEL environment variables as shown above, or
2. Set a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.

### Configure alternate OTLP endpoints

To send traces to a different provider, configure the OTLP exporter with your provider's endpoint:

```python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

   <Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

   <CodeGroup>
```

Example 2 (unknown):
```unknown
</CodeGroup>

3. Create a LangChain application with tracing. For example:
```

Example 3 (unknown):
```unknown
4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

   <CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

   <CodeGroup>
```

---

## Wrap-style: retry logic

**URL:** llms-txt#wrap-style:-retry-logic

@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3 after error: {e}")

---

## Set up resource tags

**URL:** llms-txt#set-up-resource-tags

**Contents:**
- Create a tag
- Assign a tag to a resource
- Delete a tag
- Filter resources by tags

Source: https://docs.langchain.com/langsmith/set-up-resource-tags

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
</Check>

<Info>
  Resource tags are available for Plus and Enterprise plans.
</Info>

While workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.

To create a tag, head to the workspace settings and click on the "Resource Tags" tab. Here, you'll be able to see the existing tag values, grouped by key. Two keys `Application` and `Environment` are created by default.

To create a new tag, click on the "New Tag" button. You'll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7482c51de04fbfa54731159c9f44c4e7" alt="" data-og-width="1460" width="1460" data-og-height="1268" height="1268" data-path="langsmith/images/create-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5a5a406729553197c361b802663af22f 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5618e46f4895c858a234a62b2222b5d7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0dc3a636385e4c0502d23c50013696d8 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a138f20d0755211084e7352a6aa98dbc 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0255e6c9c526c32ea01e46b80dde19c7 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e35a3c90d4bd9584e773e2bd1cb28294 2500w" />

## Assign a tag to a resource

Within the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the "Assign Resources" section and select the resources you want to tag.

<Note>
  You can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.
</Note>

You can also assign tags to resources from the resource's detail page. Click on the Resource tags button to open up the tag panel and assign tags.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95e638f9d51af94017b4b9f077c319b4" alt="" data-og-width="1460" width="1460" data-og-height="607" height="607" data-path="langsmith/images/assign-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6941ebf3198b8b959e23110c9605061e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86c9dcba2b753beac4d229bf15c76aa2 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5997def1f1c4294ffa4152f87edf2579 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=09645f720dc1d2ba3484194d9a0876ee 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3bbee72af92ff7cbe1617ba6686603ce 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d2833a4f8ef5aec89218ad22d619de7c 2500w" />

To un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.

You can delete either a key or a value of a tag from the [workspace settings page](https://smith.langchain.com/settings/workspaces/resource_tags). To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.

Note that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=545b2cd5178bd4da31db56cde48f9f12" alt="" data-og-width="1175" width="1175" data-og-height="1030" height="1030" data-path="langsmith/images/delete-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=059964ddd0ec5fffbe63bdf775dbd114 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b2fd607ef48d43555ec30f3689075d9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe5f0a9b4c6695f9b1041e692bdf1799 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6012f1f83a031d5d84f2921f28f3ae4c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=485c5e41e66ef679ebed451e291ce3ca 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c172ee7ff2edc13aaead3fa392de80ef 2500w" />

## Filter resources by tags

You can use resource tags to organize your experience navigating resources in the workspace.

To filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.

In the homepage, you can see updated counts for resources based on the tags you've selected.

As you navigate through the different product surfaces, you will *only* see resources that match the tags you've selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b71f3118eb60146b576a02d2b7ddb8cd" alt="" data-og-width="1459" width="1459" data-og-height="1265" height="1265" data-path="langsmith/images/filter-by-tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=30edd76e20e69234d15345fc94520d79 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=154f682c20bf3a50cf3773c6f0e4550f 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=21bb41e7cb0c4984ea98f07adcefcc09 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e544fd1d74706c9bc40f72a3a20edd2c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bb90fb78c5e6b646f3a77e3175aef80f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=32da0bb71893aedb79ab0b00cc028f3e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-resource-tags.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Connect to an external Redis database

**URL:** llms-txt#connect-to-an-external-redis-database

**Contents:**
- Requirements
- Connection String
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-redis

LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance. However, you can configure LangSmith to use an external Redis instance. By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance.

* A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:

* [Amazon ElastiCache](https://aws.amazon.com/elasticache/redis/)
  * [Google Cloud Memorystore](https://cloud.google.com/memorystore)
  * [Azure Cache for Redis](https://azure.microsoft.com/en-us/services/cache/)

* Note: We only officially support Redis versions >= 5.

* We do not support Redis Cluster.

* By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.

<Info>
  Certain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster. For example on Azure Cache for Redis, the `Premium` tier and above use Redis Cluster, so you will need to use a lower tier.
</Info>

We use `redis-py` to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format [here](https://redis-py.readthedocs.io/en/stable/#redis.StrictRedis.from_url).

You will need to assemble the connection string for your Redis instance. This connection string should include the following information:

* Host
* Database
* Port
* URL params

This will take the form of:

An example connection string might look like:

To use SSL, you can use the `rediss://` prefix. An example connection string with SSL might look like:

With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-redis.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
"redis://host:port/db?<url_params>"
```

Example 2 (unknown):
```unknown
"redis://langsmith-redis:6379/0"
```

Example 3 (unknown):
```unknown
"rediss://langsmith-redis:6380/0?password=foo"
```

Example 4 (unknown):
```unknown

```

---

## How to run a pairwise evaluation

**URL:** llms-txt#how-to-run-a-pairwise-evaluation

**Contents:**
- Prerequisites
- `evaluate()` comparative args
- Define a pairwise evaluator
  - Evaluator args
  - Evaluator output
- Run a pairwise evaluation
- View pairwise experiments

Source: https://docs.langchain.com/langsmith/evaluate-pairwise

<Info>
  Concept: [Pairwise evaluations](/langsmith/evaluation-concepts#pairwise)
</Info>

LangSmith supports evaluating **existing** experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, you'll use [`evaluate()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) with two existing experiments to [define an evaluator](#define-a-pairwise-evaluator) and [run a pairwise evaluation](#run-a-pairwise-evaluation). Finally, you'll use the LangSmith UI to [view the pairwise experiments](#view-pairwise-experiments).

* If you haven't already created experiments to compare, check out the [quick start](/langsmith/evaluation-quickstart) or the [how-to guide](/langsmith/evaluate-llm-application) to get started with evaluations.
* This guide requires `langsmith` Python version `>=0.2.0` or JS version `>=0.2.9`.

<Info>
  You can also use [`evaluate_comparative()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate_comparative) with more than two existing experiments.
</Info>

## `evaluate()` comparative args

At its simplest, `evaluate` / `aevaluate` function takes the following arguments:

| Argument     | Description                                                                                                                        |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| `target`     | A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names.  |
| `evaluators` | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |

Along with these, you can also pass in the following optional args:

| Argument                                 | Description                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `randomize_order` / `randomizeOrder`     | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |
| `experiment_prefix` / `experimentPrefix` | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.                                                                                                                                                                                                                                                                                    |
| `description`                            | A description of the pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                                    |
| `max_concurrency` / `maxConcurrency`     | The maximum number of concurrent evaluations to run. Defaults to 5.                                                                                                                                                                                                                                                                                                            |
| `client`                                 | The LangSmith client to use. Defaults to None.                                                                                                                                                                                                                                                                                                                                 |
| `metadata`                               | Metadata to attach to your pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                              |
| `load_nested` / `loadNested`             | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.                                                                                                                                                                                                                                        |

## Define a pairwise evaluator

Pairwise evaluators are just functions with an expected signature.

Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A two-item list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metadata (if available).

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `runs` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

Custom evaluators are expected to return one of the following types:

* `dict`: dictionary with keys:

* `key`, which represents the feedback key that will be logged
  * `scores`, which is a mapping from run ID to score for that run.
  * `comment`, which is a string. Most commonly used for model reasoning.

Currently Python only

* `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.

Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.

## Run a pairwise evaluation

The following example uses [a prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.

<Info>
  In the Python example below, we are pulling [this structured prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) from the [LangChain Hub](/langsmith/manage-prompts#public-prompt-hub) and using it with a LangChain chat model wrapper.

**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.
</Info>

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Requires `langsmith>=0.2.9`

## View pairwise experiments

Navigate to the "Pairwise Experiments" tab from the dataset page:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=dddf35fd971055d0d94ae4184c91dea3" alt="Pairwise Experiments Tab" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/pairwise-from-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4c1677867b832da9c3b4338a210570f8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=80d4795bc999156850eb8092e8267c9f 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ed2e5fb624828fb649bf33473e7dc797 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c2248284b4efb2f5a9f38cffef0b9b 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=3014131b2f5ae730aa354afaa7312316 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38c4f707158930cf7d1d155db4021362 2500w" />

Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8afa7467faf707c0bb5ede23b007beda" alt="Pairwise Comparison View" data-og-width="3430" width="3430" data-og-height="1886" height="1886" data-path="langsmith/images/pairwise-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9a837cee527a1bf5dda5a77b8ce16ba6 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebb39f8f2fb7a542d2273cfc64c5b4f4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2f2de8c570a3e6401ba0220da343b3e0 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cf612b4c6938e856b78c7476f8cc6304 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ff05d71cd12f19d0403e6a1e3e64609a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5674d6403a3070935830983b9e36ac2f 2500w" />

You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=677c48099cee9848d2119c154c7b0d88" alt="Pairwise Filtering" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/filter-pairwise.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ceff9156ccfdb48f246f41c7e0d16ab 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=745f3dc2bed9e3e2d8333df0ff57a43e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e81f10f544953ee39366866c1f4a5d71 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f4af39e4da50d0ad081d03aaf7b238e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=63688564868c7a0989c429c6e740e014 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=527537f492665790aa8380e0d75e7fb3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-pairwise.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## You can also wrap the async client as well

**URL:** llms-txt#you-can-also-wrap-the-async-client-as-well

---

## If "env" is set to "test", then we don't actually delete any rows from our database.

**URL:** llms-txt#if-"env"-is-set-to-"test",-then-we-don't-actually-delete-any-rows-from-our-database.

---

## Long-term memory

**URL:** llms-txt#long-term-memory

**Contents:**
- Overview
- Memory storage

Source: https://docs.langchain.com/oss/python/langchain/long-term-memory

LangChain agents use [LangGraph persistence](/oss/python/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.

LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store).

Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.

This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

```python  theme={null}
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]

---

## Trace with Instructor

**URL:** llms-txt#trace-with-instructor

Source: https://docs.langchain.com/langsmith/trace-with-instructor

LangSmith provides a convenient integration with [Instructor](https://python.useinstructor.com/), a popular open-source library for generating structured outputs with LLMs.

In order to use, you first need to set your LangSmith API key.

```shell  theme={null}
export LANGSMITH_API_KEY=<your-api-key>

---

## async_client = wrap_anthropic(anthropic.AsyncAnthropic())

**URL:** llms-txt#async_client-=-wrap_anthropic(anthropic.asyncanthropic())

@traceable(run_type="tool", name="Retrieve Context")
def my_tool(question: str) -> str:
    return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
    context = my_tool(question)
    messages = [
        { "role": "user", "content": f"Question: {question}\nContext: {context}"}
    ]
    messages = client.messages.create(
      model="claude-sonnet-4-20250514",
      messages=messages,
      max_tokens=1024,
      system="You are a helpful assistant. Please respond to the user's request only based on the given context."
    )
    return messages

chat_pipeline("Can you summarize this morning's meetings?")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-anthropic.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Cloud

**URL:** llms-txt#cloud

**Contents:**
- Get started
- Cloud architecture and scalability
  - Architecture
  - Allowlisting IP addresses

Source: https://docs.langchain.com/langsmith/cloud

<Callout icon="rocket" color="#4F46E5" iconType="regular">
  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud hosting model for reference.
</Callout>

The **Cloud** option is a fully managed hosting model where LangChain hosts and operates all LangSmith infrastructure and services:

* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.
* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.
* **Automated CI/CD**: Build process is handled automatically by the platform.
* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).

|                                               | **Who manages it** | **Where it runs** |
| --------------------------------------------- | ------------------ | ----------------- |
| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |
| **Your LangGraph Servers**                    | LangChain          | LangChain's cloud |
| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |

<img src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139" alt="Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your LangGraph Servers." data-og-width="1425" width="1425" data-og-height="1063" height="1063" data-path="langsmith/images/langgraph-cloud-architecture.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w" />

To deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).

## Cloud architecture and scalability

<Note>
  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).

For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).
</Note>

LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment

The US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.

<Note>
  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).
</Note>

#### Regional storage

The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.

|                                                | US                                                                 | EU                                                                       |
| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |
| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |
| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |
| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |

See the [Regions FAQ](/langsmith/regions-faq) for more information.

#### Region-independent storage

Data listed here is stored exclusively in the US:

* Payment and billing information with Stripe and Metronome

LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):

* LangSmith Frontend: serves the LangSmith UI.
* LangSmith Backend: serves the LangSmith API.
* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)
* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.
* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)

LangSmith uses the following GCP storage services:

* Google Cloud Storage (GCS) for runs inputs and outputs.
* Google Cloud SQL PostgreSQL for transactional workloads.
* Google Cloud Memorystore for Redis for queuing and caching.
* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.

Some additional GCP services we use include:

* Google Cloud Load Balancer for routing traffic to the LangSmith services.
* Google Cloud CDN for caching static assets.
* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="Light mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="Dark mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />
</div>

### Allowlisting IP addresses

#### Egress from LangChain SaaS

All traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 34.59.65.97    | 34.13.192.67   |
| 34.67.51.221   | 34.147.105.64  |
| 34.46.212.37   | 34.90.22.166   |
| 34.132.150.88  | 34.147.36.213  |
| 35.188.222.201 | 34.32.137.113  |
| 34.58.194.127  | 34.91.238.184  |
| 34.59.97.173   | 35.204.101.241 |
| 104.198.162.55 | 35.204.48.32   |

It may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.

#### Ingress into LangChain SaaS

The langchain endpoints map to the following static IP addresses:

| US             | EU           |
| -------------- | ------------ |
| 34.8.121.39    | 34.95.92.214 |
| 34.107.251.234 | 34.13.73.122 |

You may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Connect to an OpenAI compliant model provider/proxy

**URL:** llms-txt#connect-to-an-openai-compliant-model-provider/proxy

**Contents:**
- Deploy an OpenAI compliant model
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-openai-compliant-model

The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.

## Deploy an OpenAI compliant model

Many providers offer OpenAI compliant models or proxy services. Some examples of this include:

* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)
* [Ollama](https://ollama.com/)

You can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.

Take a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).

To access the **Prompt Settings** menu:

1. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name.
2. In the **Model Configuration** tab, select the model to edit in the dropdown.
3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.
4. Add your OpenAI Compatible Endpoint to the **Base URL** input.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="897" width="897" data-og-height="572" height="572" data-path="langsmith/images/openai-compatible-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="896" width="896" data-og-height="552" height="552" data-path="langsmith/images/openai-compatible-endpoint-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w" />
   </div>

If everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.

For information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create an account and API key

**URL:** llms-txt#create-an-account-and-api-key

**Contents:**
- API keys
- Create an API key
- Configure the SDK
- Using API keys outside of the SDK

Source: https://docs.langchain.com/langsmith/create-account-api-key

To get started with LangSmith, you need to create an account. You can sign up for a free account [here](https://smith.langchain.com). We support logging in with Google, GitHub, and email.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2a18e674c9b6e96dd0d5af16ddeeaf1a" alt="Create account" data-og-width="1768" width="1768" data-og-height="1252" height="1252" data-path="langsmith/images/create-account.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=45abdaeb33706739a680080f52a5457c 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=991caa9cc07e229e8a6db0a2ac38fdb9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0ff78a93a7e28cfe9fccbfd5a7d54ec5 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=04f93900e425e47461c882b25e5298f7 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=866363d551391983b36edfc481d67404 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=186e16f729e0a637c0eaab35002aa4ad 2500w" />

LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.

Read more about the differences between Service Keys and Personal Access Tokens under [admin concepts](/langsmith/administration-overview)

To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of workspaces, or the entire organization.

To create either type of API key head to the [Settings page](https://smith.langchain.com/settings), then scroll to the **API Keys** section. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified. Enterprise users are also able to assign specific roles to the key, which adjusts its permissions. Set the key's expiration; the key will become unusable after the number of days chosen, or never, if that is selected. Then click **Create API Key.**

<Note>
  The API key will be shown only once, so make sure to copy it and store it in a safe place.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=e27b419a9c317a78f8a98ff5024e1235" alt="Create API key" data-og-width="1224" width="1224" data-og-height="1137" height="1137" data-path="langsmith/images/create-api-key.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=280&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=b0148d07161d0f214a9e6442297e83a3 280w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=560&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=305cf6904089edcee1db749552e41b5f 560w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=840&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=88ead9f8f16475135f43f32ecdfae35a 840w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1100&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=0803eebefa356974e485217545b9cf13 1100w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1650&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=fe671c360990ca8e4e86f5749a7f59db 1650w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=2500&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=02431a31fddd5ee58d30296fed75a238 2500w" />

You may set the following environment variables in addition to `LANGSMITH_API_KEY`.

This is only required if using the EU instance.

`LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com`

This is only required for keys scoped to more than one workspace.

`LANGSMITH_WORKSPACE_ID=<Workspace ID>`

## Using API keys outside of the SDK

See [instructions for managing your organization via API](/langsmith/manage-organization-by-api).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-account-api-key.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Define the schema for the output

**URL:** llms-txt#define-the-schema-for-the-output

class OutputState(TypedDict):
    answer: str

---

## Alice creates a thread and chats

**URL:** llms-txt#alice-creates-a-thread-and-chats

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")

await alice.runs.create(
    thread_id=alice_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Alice's private chat"}]}
)

---

## from langsmith import aevaluate

**URL:** llms-txt#from-langsmith-import-aevaluate

---

## Manage prompts

**URL:** llms-txt#manage-prompts

**Contents:**
- Commit tags
  - Create a tag
  - Move a tag
  - Delete a tag
  - Use tags in code

Source: https://docs.langchain.com/langsmith/manage-prompts

LangSmith provides several tools to help you manage your [*prompts*](/langsmith/prompt-engineering-concepts) effectively. This page describes the following features:

* [Commit tags](#commit-tags) for version control and environment management.
* [Webhook triggers](#trigger-a-webhook-on-prompt-commit) for automating workflows when prompts are updated.
* [Public prompt hub](#public-prompt-hub) for discovering and using community-created prompts.

[*Commit tags*](/langsmith/prompt-engineering-concepts#tags) are labels that reference a specific [*commit*](/langsmith/prompt-engineering-concepts#commits) in your prompt's version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.

Each tag references exactly one commit, though you can reassign a tag to point to a different commit.

Use cases for commit tags can include:

* **Environment-specific tags**: Mark commits for `production` or `staging` environments, which allows you to switch between different versions without changing your code.
* **Version control**: Mark stable versions of your prompts, for example, `v1`, `v2`, which lets you reference specific versions in your code and track changes over time.
* **Collaboration**: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.

To create a tag, navigate to the **Commits** tab for a prompt. Click on the tag icon next to the commit you want to tag. Click **New Tag** and enter a name for the tag.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0cb7961f70d5c0bab9af960041bc54f" alt="" data-og-width="2868" width="2868" data-og-height="992" height="992" data-path="langsmith/images/commits-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5786acfc0582c73c73efb5535a954ab4 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=410aa69f0ef5cc13ff7651df331495b7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b9845d256b71636a9bb6ee60c774a29e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e16686b15bf2a95709a5d963ffc339d0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cb9560e6aa774505cb9ff1857f95ae61 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bef88e11fa0de6173fc4dfd9c9339f39 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cc27df9c5392a9b71319969c14924c61" alt="" data-og-width="1410" width="1410" data-og-height="872" height="872" data-path="langsmith/images/create-new-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ed473769995e6148b60aec9cdd652ab6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f817be111dfc54af9717f9cdb664752 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb0b979bf3362bd8faf796e4d617ac1c 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=22c63285bd22dd0c82d53b8d46c6638b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b4d05163bd948c4bce8ea08b5ceb70f 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c34b5ce29b2f1914399b6a084885e09a 2500w" />

To point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3cb3c6218961cbdd8f6f1fb6d06b50e3" alt="" data-og-width="874" width="874" data-og-height="694" height="694" data-path="langsmith/images/move-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2094e7486cfd3a7f4d7979e7883d1bc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=163481cc4580e4fe7934a5e02497fb9f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9d53ef06dc6d69cc4024821fe8cfc32d 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=00ce6c08638961bfbbe5713c0066929b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fa898ee3d7089b443e741d4f10b7845a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cd97d2004b3b0a2ac0d3c7e4d3e7fff7 2500w" />

To delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.

Tags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.

Here is an example of pulling a prompt by tag in Python:

```python  theme={null}
prompt = client.pull_prompt("joke-generator:prod")

---

## with information about the graph node where the LLM was called and other information

**URL:** llms-txt#with-information-about-the-graph-node-where-the-llm-was-called-and-other-information

**Contents:**
- Stream custom data
- Use with any LLM

for msg, metadata in graph.stream(
    inputs,
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the langgraph_node field in the metadata
    # to only include the tokens from the specified node
    if msg.content and metadata["langgraph_node"] == "some_node_name":
        ...
python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

class State(TypedDict):
        topic: str
        joke: str
        poem: str

def write_joke(state: State):
        topic = state["topic"]
        joke_response = model.invoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}]
        )
        return {"joke": joke_response.content}

def write_poem(state: State):
        topic = state["topic"]
        poem_response = model.invoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}]
        )
        return {"poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(write_joke)
        .add_node(write_poem)
        # write both the joke and the poem concurrently
        .add_edge(START, "write_joke")
        .add_edge(START, "write_poem")
        .compile()
  )

# The "messages" stream mode returns a tuple of (message_chunk, metadata)
  # where message_chunk is the token streamed by the LLM and metadata is a dictionary
  # with information about the graph node where the LLM was called and other information
  for msg, metadata in graph.stream(
      {"topic": "cats"},
      stream_mode="messages",  # [!code highlight]
  ):
      # Filter the streamed tokens by the langgraph_node field in the metadata
      # to only include the tokens from the write_poem node
      if msg.content and metadata["langgraph_node"] == "write_poem":
          print(msg.content, end="|", flush=True)
  python  theme={null}
    from typing import TypedDict
    from langgraph.config import get_stream_writer
    from langgraph.graph import StateGraph, START

class State(TypedDict):
        query: str
        answer: str

def node(state: State):
        # Get the stream writer to send custom data
        writer = get_stream_writer()
        # Emit a custom key-value pair (e.g., progress update)
        writer({"custom_key": "Generating custom data inside node"})
        return {"answer": "some data"}

graph = (
        StateGraph(State)
        .add_node(node)
        .add_edge(START, "node")
        .compile()
    )

inputs = {"query": "example"}

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
    from langchain.tools import tool
    from langgraph.config import get_stream_writer

@tool
    def query_database(query: str) -> str:
        """Query the database."""
        # Access the stream writer to send custom data
        writer = get_stream_writer()  # [!code highlight]
        # Emit a custom key-value pair (e.g., progress update)
        writer({"data": "Retrieved 0/100 records", "type": "progress"})  # [!code highlight]
        # perform query
        # Emit another custom key-value pair
        writer({"data": "Retrieved 100/100 records", "type": "progress"})
        return "some-answer"

graph = ... # define a graph that uses this tool

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
from langgraph.config import get_stream_writer

def call_arbitrary_model(state):
    """Example node that calls an arbitrary model and streams the output"""
    # Get the stream writer to send custom data
    writer = get_stream_writer()  # [!code highlight]
    # Assume you have a streaming client that yields chunks
    # Generate LLM tokens using your custom streaming client
    for chunk in your_custom_streaming_client(state["topic"]):
        # Use the writer to send custom data to the stream
        writer({"custom_llm_chunk": chunk})  # [!code highlight]
    return {"result": "completed"}

graph = (
    StateGraph(State)
    .add_node(call_arbitrary_model)
    # Add other nodes and edges as needed
    .compile()
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming LLM tokens from specific nodes">
```

Example 2 (unknown):
```unknown
</Accordion>

## Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) to access the stream writer and emit custom data.
2. Set `stream_mode="custom"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

<Warning>
  **No [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async for Python \< 3.11**
  In async code running on Python \< 3.11, [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work.
  Instead, add a `writer` parameter to your node or tool and pass it manually.
  See [Async with Python \< 3.11](#async) for usage examples.
</Warning>

<Tabs>
  <Tab title="node">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="tool">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Use with any LLM

You can use `stream_mode="custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.
```

---

## Try without a token (should fail)

**URL:** llms-txt#try-without-a-token-(should-fail)

client = get_client(url="http://localhost:2024")
try:
    thread = await client.threads.create()
    print("❌ Should have failed without token!")
except Exception as e:
    print("✅ Correctly blocked access:", e)

---

## ✅ Good: Organized and descriptive

**URL:** llms-txt#✅-good:-organized-and-descriptive

/memories/user_preferences/language.txt
/memories/projects/project_alpha/status.txt
/memories/research/quantum_computing/sources.txt

---

## First session: save user info

**URL:** llms-txt#first-session:-save-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

---

## Run the export script with customer information as variables

**URL:** llms-txt#run-the-export-script-with-customer-information-as-variables

**Contents:**
  - Status update

sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_traces_backfill_export.sql \
  --output ls_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_nodes_backfill_export.sql \
  --output lgp_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_traces_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_nodes_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-pg-support-queries.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To export LangSmith usage:
```

Example 2 (unknown):
```unknown
### Status update

These scripts update the status of usage events in your installation to reflect that the events have been successfully processed by LangChain.

The scripts require passing in the corresponding `backfill_id`, which will be confirmed by your LangChain rep.

To update LangSmith trace usage:
```

Example 3 (unknown):
```unknown
To update LangSmith usage:
```

---

## Load files and create attachments

**URL:** llms-txt#load-files-and-create-attachments

image_data = load_file("my_image.png")
audio_data = load_file("my_mp3.mp3")
video_data = load_file("my_video.mp4")
pdf_data = load_file("my_document.pdf")

image_attachment = Attachment(mime_type="image/png", data=image_data)
audio_attachment = Attachment(mime_type="audio/mpeg", data=audio_data)
video_attachment = Attachment(mime_type="video/mp4", data=video_data)
pdf_attachment = ("application/pdf", pdf_data) # Can just define as tuple of (mime_type, data)
csv_attachment = Attachment(mime_type="text/csv", data=Path(os.getcwd()) / "my_csv.csv")

---

## Vector stores

**URL:** llms-txt#vector-stores

**Contents:**
- Overview
  - Interface
  - Initialization
  - Adding documents
  - Deleting documents
  - Similarity search
  - Similarity metrics & indexing
  - Metadata filtering
- Top integrations
- All vector stores

Source: https://docs.langchain.com/oss/python/integrations/vectorstores/index

A vector store stores [embedded](/oss/python/integrations/text_embedding) data and performs similarity search.

LangChain provides a unified interface for vector stores, allowing you to:

* `add_documents` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similarity_search` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

To initialize a vector store, provide it with an embedding model:

Add [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects (holding `page_content` and optional metadata) like so:

### Deleting documents

Delete by specifying IDs:

### Similarity search

Issue a semantic query using `similarity_search`, which returns the closest embedded documents:

Many vector stores support parameters like:

* `k` — number of results to return
* `filter` — conditional filtering based on metadata

### Similarity metrics & indexing

Embedding similarity may be computed using:

* **Cosine similarity**
* **Euclidean distance**
* **Dot product**

Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.

### Metadata filtering

Filtering by metadata (e.g., source, date) can refine search results:

<important>
  Support for metadata-based filtering varies between implementations.
  Check the documentation of your chosen vector store for details.
</important>

**Select embedding model:**

<AccordionGroup>
  <Accordion title="OpenAI">
    <CodeGroup>

<Accordion title="Azure">

<Accordion title="Google Gemini">

<Accordion title="Google Vertex">

<Accordion title="AWS">

<Accordion title="HuggingFace">

<Accordion title="Ollama">

<Accordion title="Cohere">

<Accordion title="Mistral AI">

<Accordion title="Nomic">

<Accordion title="NVIDIA">

<Accordion title="Voyage AI">

<Accordion title="IBM watsonx">

<Accordion title="Fake">

<Accordion title="xAI">

<Accordion title="Perplexity">

<Accordion title="DeepSeek">

</Accordion>
</AccordionGroup>

**Select vector store:**

<AccordionGroup>
  <Accordion title="In-memory">
    <CodeGroup>

<Accordion title="AstraDB">
    <CodeGroup>

<Accordion title="Azure Cosmos DB NoSQL">
    <CodeGroup>

<Accordion title="Azure Cosmos DB Mongo vCore">
    <CodeGroup>

<Accordion title="Chroma">
    <CodeGroup>

<Accordion title="FAISS">

<Accordion title="Milvus">
    <CodeGroup>

<Accordion title="MongoDB">

<Accordion title="PGVector">
    <CodeGroup>

<Accordion title="PGVectorStore">
    <CodeGroup>

<Accordion title="Pinecone">
    <CodeGroup>

<Accordion title="Qdrant">
    <CodeGroup>

</Accordion>
</AccordionGroup>

| Vectorstore                                                                                                                                          | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |
| ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | --------- | ---------------- | ----------------- | ----- | --------------------- | ------------- | -------------------- |
| [`AstraDBVectorStore`](/oss/python/integrations/vectorstores/astradb)                                                                                | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`AzureCosmosDBNoSqlVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql)                                                      | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |
| [`AzureCosmosDBMongoVCoreVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore)                                            | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |
| [`Chroma`](/oss/python/integrations/vectorstores/chroma)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`Clickhouse`](/oss/python/integrations/vectorstores/clickhouse)                                                                                     | ✅            | ✅         | ❌                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |
| [`CouchbaseSearchVectorStore`](/oss/python/integrations/vectorstores/couchbase)                                                                      | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`DatabricksVectorSearch`](/oss/python/integrations/vectorstores/databricks_vector_search)                                                           | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`ElasticsearchStore`](/oss/python/integrations/vectorstores/elasticsearch)                                                                          | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`FAISS`](/oss/python/integrations/vectorstores/faiss)                                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`InMemoryVectorStore`](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | ✅            | ✅         | ❌                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`Milvus`](/oss/python/integrations/vectorstores/milvus)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`Moorcheh`](/oss/python/integrations/vectorstores/moorcheh)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`MongoDBAtlasVectorSearch`](/oss/python/integrations/vectorstores/mongodb_atlas)                                                                    | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`openGauss`](/oss/python/integrations/vectorstores/opengauss)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ❌             | ✅                    |
| [`PGVector`](/oss/python/integrations/vectorstores/pgvector)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`PGVectorStore`](/oss/python/integrations/vectorstores/pgvectorstore)                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |
| [`PineconeVectorStore`](/oss/python/integrations/vectorstores/pinecone)                                                                              | ✅            | ✅         | ✅                | ❌                 | ✅     | ❌                     | ❌             | ✅                    |
| [`QdrantVectorStore`](/oss/python/integrations/vectorstores/qdrant)                                                                                  | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`Weaviate`](/oss/python/integrations/vectorstores/weaviate)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`SQLServer`](/oss/python/integrations/vectorstores/sqlserver)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |
| [`ZeusDB`](/oss/python/integrations/vectorstores/zeusdb)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |

<Columns cols={3}>
  <Card title="Activeloop Deep Lake" icon="link" href="/oss/python/integrations/vectorstores/activeloop_deeplake" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud OpenSearch" icon="link" href="/oss/python/integrations/vectorstores/alibabacloud_opensearch" arrow="true" cta="View guide" />

<Card title="AnalyticDB" icon="link" href="/oss/python/integrations/vectorstores/analyticdb" arrow="true" cta="View guide" />

<Card title="Annoy" icon="link" href="/oss/python/integrations/vectorstores/annoy" arrow="true" cta="View guide" />

<Card title="Apache Doris" icon="link" href="/oss/python/integrations/vectorstores/apache_doris" arrow="true" cta="View guide" />

<Card title="ApertureDB" icon="link" href="/oss/python/integrations/vectorstores/aperturedb" arrow="true" cta="View guide" />

<Card title="Astra DB Vector Store" icon="link" href="/oss/python/integrations/vectorstores/astradb" arrow="true" cta="View guide" />

<Card title="Atlas" icon="link" href="/oss/python/integrations/vectorstores/atlas" arrow="true" cta="View guide" />

<Card title="AwaDB" icon="link" href="/oss/python/integrations/vectorstores/awadb" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB Mongo vCore" icon="link" href="/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB No SQL" icon="link" href="/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql" arrow="true" cta="View guide" />

<Card title="Azure AI Search" icon="link" href="/oss/python/integrations/vectorstores/azuresearch" arrow="true" cta="View guide" />

<Card title="Bagel" icon="link" href="/oss/python/integrations/vectorstores/bagel" arrow="true" cta="View guide" />

<Card title="BagelDB" icon="link" href="/oss/python/integrations/vectorstores/bageldb" arrow="true" cta="View guide" />

<Card title="Baidu Cloud ElasticSearch VectorSearch" icon="link" href="/oss/python/integrations/vectorstores/baiducloud_vector_search" arrow="true" cta="View guide" />

<Card title="Baidu VectorDB" icon="link" href="/oss/python/integrations/vectorstores/baiduvectordb" arrow="true" cta="View guide" />

<Card title="Apache Cassandra" icon="link" href="/oss/python/integrations/vectorstores/cassandra" arrow="true" cta="View guide" />

<Card title="Chroma" icon="link" href="/oss/python/integrations/vectorstores/chroma" arrow="true" cta="View guide" />

<Card title="Clarifai" icon="link" href="/oss/python/integrations/vectorstores/clarifai" arrow="true" cta="View guide" />

<Card title="ClickHouse" icon="link" href="/oss/python/integrations/vectorstores/clickhouse" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/python/integrations/vectorstores/couchbase" arrow="true" cta="View guide" />

<Card title="DashVector" icon="link" href="/oss/python/integrations/vectorstores/dashvector" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/vectorstores/databricks_vector_search" arrow="true" cta="View guide" />

<Card title="IBM Db2" icon="link" href="/oss/python/integrations/vectorstores/db2" arrow="true" cta="View guide" />

<Card title="DingoDB" icon="link" href="/oss/python/integrations/vectorstores/dingo" arrow="true" cta="View guide" />

<Card title="DocArray HnswSearch" icon="link" href="/oss/python/integrations/vectorstores/docarray_hnsw" arrow="true" cta="View guide" />

<Card title="DocArray InMemorySearch" icon="link" href="/oss/python/integrations/vectorstores/docarray_in_memory" arrow="true" cta="View guide" />

<Card title="Amazon Document DB" icon="link" href="/oss/python/integrations/vectorstores/documentdb" arrow="true" cta="View guide" />

<Card title="DuckDB" icon="link" href="/oss/python/integrations/vectorstores/duckdb" arrow="true" cta="View guide" />

<Card title="China Mobile ECloud ElasticSearch" icon="link" href="/oss/python/integrations/vectorstores/ecloud_vector_search" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/vectorstores/elasticsearch" arrow="true" cta="View guide" />

<Card title="Epsilla" icon="link" href="/oss/python/integrations/vectorstores/epsilla" arrow="true" cta="View guide" />

<Card title="Faiss" icon="link" href="/oss/python/integrations/vectorstores/faiss" arrow="true" cta="View guide" />

<Card title="Faiss (Async)" icon="link" href="/oss/python/integrations/vectorstores/faiss_async" arrow="true" cta="View guide" />

<Card title="FalkorDB" icon="link" href="/oss/python/integrations/vectorstores/falkordbvector" arrow="true" cta="View guide" />

<Card title="Gel" icon="link" href="/oss/python/integrations/vectorstores/gel" arrow="true" cta="View guide" />

<Card title="Google AlloyDB" icon="link" href="/oss/python/integrations/vectorstores/google_alloydb" arrow="true" cta="View guide" />

<Card title="Google BigQuery Vector Search" icon="link" href="/oss/python/integrations/vectorstores/google_bigquery_vector_search" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for MySQL" icon="link" href="/oss/python/integrations/vectorstores/google_cloud_sql_mysql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/python/integrations/vectorstores/google_cloud_sql_pg" arrow="true" cta="View guide" />

<Card title="Firestore" icon="link" href="/oss/python/integrations/vectorstores/google_firestore" arrow="true" cta="View guide" />

<Card title="Google Memorystore for Redis" icon="link" href="/oss/python/integrations/vectorstores/google_memorystore_redis" arrow="true" cta="View guide" />

<Card title="Google Spanner" icon="link" href="/oss/python/integrations/vectorstores/google_spanner" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Feature Store" icon="link" href="/oss/python/integrations/vectorstores/google_vertex_ai_feature_store" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Vector Search" icon="link" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search" arrow="true" cta="View guide" />

<Card title="Hippo" icon="link" href="/oss/python/integrations/vectorstores/hippo" arrow="true" cta="View guide" />

<Card title="Hologres" icon="link" href="/oss/python/integrations/vectorstores/hologres" arrow="true" cta="View guide" />

<Card title="Jaguar Vector Database" icon="link" href="/oss/python/integrations/vectorstores/jaguar" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/vectorstores/kinetica" arrow="true" cta="View guide" />

<Card title="LanceDB" icon="link" href="/oss/python/integrations/vectorstores/lancedb" arrow="true" cta="View guide" />

<Card title="Lantern" icon="link" href="/oss/python/integrations/vectorstores/lantern" arrow="true" cta="View guide" />

<Card title="Lindorm" icon="link" href="/oss/python/integrations/vectorstores/lindorm" arrow="true" cta="View guide" />

<Card title="LLMRails" icon="link" href="/oss/python/integrations/vectorstores/llm_rails" arrow="true" cta="View guide" />

<Card title="ManticoreSearch" icon="link" href="/oss/python/integrations/vectorstores/manticore_search" arrow="true" cta="View guide" />

<Card title="MariaDB" icon="link" href="/oss/python/integrations/vectorstores/mariadb" arrow="true" cta="View guide" />

<Card title="Marqo" icon="link" href="/oss/python/integrations/vectorstores/marqo" arrow="true" cta="View guide" />

<Card title="Meilisearch" icon="link" href="/oss/python/integrations/vectorstores/meilisearch" arrow="true" cta="View guide" />

<Card title="Amazon MemoryDB" icon="link" href="/oss/python/integrations/vectorstores/memorydb" arrow="true" cta="View guide" />

<Card title="Milvus" icon="link" href="/oss/python/integrations/vectorstores/milvus" arrow="true" cta="View guide" />

<Card title="Momento Vector Index" icon="link" href="/oss/python/integrations/vectorstores/momento_vector_index" arrow="true" cta="View guide" />

<Card title="Moorcheh" icon="link" href="/oss/python/integrations/vectorstores/moorcheh" arrow="true" cta="View guide" />

<Card title="MongoDB Atlas" icon="link" href="/oss/python/integrations/vectorstores/mongodb_atlas" arrow="true" cta="View guide" />

<Card title="MyScale" icon="link" href="/oss/python/integrations/vectorstores/myscale" arrow="true" cta="View guide" />

<Card title="Neo4j Vector Index" icon="link" href="/oss/python/integrations/vectorstores/neo4jvector" arrow="true" cta="View guide" />

<Card title="NucliaDB" icon="link" href="/oss/python/integrations/vectorstores/nucliadb" arrow="true" cta="View guide" />

<Card title="Oceanbase" icon="link" href="/oss/python/integrations/vectorstores/oceanbase" arrow="true" cta="View guide" />

<Card title="openGauss" icon="link" href="/oss/python/integrations/vectorstores/opengauss" arrow="true" cta="View guide" />

<Card title="OpenSearch" icon="link" href="/oss/python/integrations/vectorstores/opensearch" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/vectorstores/oracle" arrow="true" cta="View guide" />

<Card title="Pathway" icon="link" href="/oss/python/integrations/vectorstores/pathway" arrow="true" cta="View guide" />

<Card title="Postgres Embedding" icon="link" href="/oss/python/integrations/vectorstores/pgembedding" arrow="true" cta="View guide" />

<Card title="PGVecto.rs" icon="link" href="/oss/python/integrations/vectorstores/pgvecto_rs" arrow="true" cta="View guide" />

<Card title="PGVector" icon="link" href="/oss/python/integrations/vectorstores/pgvector" arrow="true" cta="View guide" />

<Card title="PGVectorStore" icon="link" href="/oss/python/integrations/vectorstores/pgvectorstore" arrow="true" cta="View guide" />

<Card title="Pinecone" icon="link" href="/oss/python/integrations/vectorstores/pinecone" arrow="true" cta="View guide" />

<Card title="Pinecone (sparse)" icon="link" href="/oss/python/integrations/vectorstores/pinecone_sparse" arrow="true" cta="View guide" />

<Card title="Qdrant" icon="link" href="/oss/python/integrations/vectorstores/qdrant" arrow="true" cta="View guide" />

<Card title="Relyt" icon="link" href="/oss/python/integrations/vectorstores/relyt" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/python/integrations/vectorstores/rockset" arrow="true" cta="View guide" />

<Card title="SAP HANA Cloud Vector Engine" icon="link" href="/oss/python/integrations/vectorstores/sap_hanavector" arrow="true" cta="View guide" />

<Card title="ScaNN" icon="link" href="/oss/python/integrations/vectorstores/scann" arrow="true" cta="View guide" />

<Card title="SemaDB" icon="link" href="/oss/python/integrations/vectorstores/semadb" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/python/integrations/vectorstores/singlestore" arrow="true" cta="View guide" />

<Card title="scikit-learn" icon="link" href="/oss/python/integrations/vectorstores/sklearn" arrow="true" cta="View guide" />

<Card title="SQLiteVec" icon="link" href="/oss/python/integrations/vectorstores/sqlitevec" arrow="true" cta="View guide" />

<Card title="SQLite-VSS" icon="link" href="/oss/python/integrations/vectorstores/sqlitevss" arrow="true" cta="View guide" />

<Card title="SQLServer" icon="link" href="/oss/python/integrations/vectorstores/sqlserver" arrow="true" cta="View guide" />

<Card title="StarRocks" icon="link" href="/oss/python/integrations/vectorstores/starrocks" arrow="true" cta="View guide" />

<Card title="Supabase" icon="link" href="/oss/python/integrations/vectorstores/supabase" arrow="true" cta="View guide" />

<Card title="SurrealDB" icon="link" href="/oss/python/integrations/vectorstores/surrealdb" arrow="true" cta="View guide" />

<Card title="Tablestore" icon="link" href="/oss/python/integrations/vectorstores/tablestore" arrow="true" cta="View guide" />

<Card title="Tair" icon="link" href="/oss/python/integrations/vectorstores/tair" arrow="true" cta="View guide" />

<Card title="Tencent Cloud VectorDB" icon="link" href="/oss/python/integrations/vectorstores/tencentvectordb" arrow="true" cta="View guide" />

<Card title="ThirdAI NeuralDB" icon="link" href="/oss/python/integrations/vectorstores/thirdai_neuraldb" arrow="true" cta="View guide" />

<Card title="TiDB Vector" icon="link" href="/oss/python/integrations/vectorstores/tidb_vector" arrow="true" cta="View guide" />

<Card title="Tigris" icon="link" href="/oss/python/integrations/vectorstores/tigris" arrow="true" cta="View guide" />

<Card title="TileDB" icon="link" href="/oss/python/integrations/vectorstores/tiledb" arrow="true" cta="View guide" />

<Card title="Timescale Vector" icon="link" href="/oss/python/integrations/vectorstores/timescalevector" arrow="true" cta="View guide" />

<Card title="Typesense" icon="link" href="/oss/python/integrations/vectorstores/typesense" arrow="true" cta="View guide" />

<Card title="Upstash Vector" icon="link" href="/oss/python/integrations/vectorstores/upstash" arrow="true" cta="View guide" />

<Card title="USearch" icon="link" href="/oss/python/integrations/vectorstores/usearch" arrow="true" cta="View guide" />

<Card title="Vald" icon="link" href="/oss/python/integrations/vectorstores/vald" arrow="true" cta="View guide" />

<Card title="VDMS" icon="link" href="/oss/python/integrations/vectorstores/vdms" arrow="true" cta="View guide" />

<Card title="Vearch" icon="link" href="/oss/python/integrations/vectorstores/vearch" arrow="true" cta="View guide" />

<Card title="Vectara" icon="link" href="/oss/python/integrations/vectorstores/vectara" arrow="true" cta="View guide" />

<Card title="Vespa" icon="link" href="/oss/python/integrations/vectorstores/vespa" arrow="true" cta="View guide" />

<Card title="viking DB" icon="link" href="/oss/python/integrations/vectorstores/vikingdb" arrow="true" cta="View guide" />

<Card title="vlite" icon="link" href="/oss/python/integrations/vectorstores/vlite" arrow="true" cta="View guide" />

<Card title="Weaviate" icon="link" href="/oss/python/integrations/vectorstores/weaviate" arrow="true" cta="View guide" />

<Card title="Xata" icon="link" href="/oss/python/integrations/vectorstores/xata" arrow="true" cta="View guide" />

<Card title="YDB" icon="link" href="/oss/python/integrations/vectorstores/ydb" arrow="true" cta="View guide" />

<Card title="Yellowbrick" icon="link" href="/oss/python/integrations/vectorstores/yellowbrick" arrow="true" cta="View guide" />

<Card title="Zep" icon="link" href="/oss/python/integrations/vectorstores/zep" arrow="true" cta="View guide" />

<Card title="Zep Cloud" icon="link" href="/oss/python/integrations/vectorstores/zep_cloud" arrow="true" cta="View guide" />

<Card title="ZeusDB" icon="link" href="/oss/python/integrations/vectorstores/zeusdb" arrow="true" cta="View guide" />

<Card title="Zilliz" icon="link" href="/oss/python/integrations/vectorstores/zilliz" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/vectorstores/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Interface

LangChain provides a unified interface for vector stores, allowing you to:

* `add_documents` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similarity_search` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

### Initialization

To initialize a vector store, provide it with an embedding model:
```

Example 2 (unknown):
```unknown
### Adding documents

Add [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects (holding `page_content` and optional metadata) like so:
```

Example 3 (unknown):
```unknown
### Deleting documents

Delete by specifying IDs:
```

Example 4 (unknown):
```unknown
### Similarity search

Issue a semantic query using `similarity_search`, which returns the closest embedded documents:
```

---

## Define input schema

**URL:** llms-txt#define-input-schema

class InputState(TypedDict):
    question: str

---

## >                'name': 'execute_sql',

**URL:** llms-txt#>----------------'name':-'execute_sql',

---

## For certain unit tests, you may need to set certain flags and environment variables:

**URL:** llms-txt#for-certain-unit-tests,-you-may-need-to-set-certain-flags-and-environment-variables:

TIKTOKEN_CACHE_DIR=tiktoken_cache uv run --group test pytest --disable-socket --allow-unix-socket tests/unit_tests/

---

## Structured output

**URL:** llms-txt#structured-output

**Contents:**
- Response Format
- Provider strategy
- Tool calling strategy
  - Custom tool message content
  - Error handling

Source: https://docs.langchain.com/oss/python/langchain/structured-output

Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.

LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `'structured_response'` key of the agent's state.

Controls how the agent returns structured data:

* **`ToolStrategy[StructuredResponseT]`**: Uses tool calling for structured output
* **`ProviderStrategy[StructuredResponseT]`**: Uses provider-native structured output
* **`type[StructuredResponseT]`**: Schema type - automatically selects best strategy based on model capabilities
* **`None`**: No structured output

When a schema type is provided directly, LangChain automatically chooses:

* `ProviderStrategy` for models supporting native structured output (e.g. [OpenAI](/oss/python/integrations/providers/openai), [Grok](/oss/python/integrations/providers/xai))
* `ToolStrategy` for all other models

The structured response is returned in the `structured_response` key of the agent's final state.

Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.

To use this strategy, configure a `ProviderStrategy`:

<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

* **Pydantic models**: `BaseModel` subclasses with field validation
  * **Dataclasses**: Python dataclasses with type annotations
  * **TypedDict**: Typed dictionary classes
  * **JSON Schema**: Dictionary with JSON schema specification
</ParamField>

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to [`create_agent.response_format`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(response_format\)) and the model supports native structured output:

Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.

<Note>
  If the provider natively supports structured output for your model choice, it is functionally equivalent to write `response_format=ProductReview` instead of `response_format=ToolStrategy(ProductReview)`. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.
</Note>

## Tool calling strategy

For models that don't support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.

To use this strategy, configure a `ToolStrategy`:

<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

* **Pydantic models**: `BaseModel` subclasses with field validation
  * **Dataclasses**: Python dataclasses with type annotations
  * **TypedDict**: Typed dictionary classes
  * **JSON Schema**: Dictionary with JSON schema specification
  * **Union types**: Multiple schema options. The model will choose the most appropriate schema based on the context.
</ParamField>

<ParamField path="tool_message_content">
  Custom content for the tool message returned when structured output is generated.
  If not provided, defaults to a message showing the structured response data.
</ParamField>

<ParamField path="handle_errors">
  Error handling strategy for structured output validation failures. Defaults to `True`.

* **`True`**: Catch all errors with default error template
  * **`str`**: Catch all errors with this custom message
  * **`type[Exception]`**: Only catch this exception type with default message
  * **`tuple[type[Exception], ...]`**: Only catch these exception types with default message
  * **`Callable[[Exception], str]`**: Custom function that returns error message
  * **`False`**: No retry, let exceptions propagate
</ParamField>

### Custom tool message content

The `tool_message_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated:

Without `tool_message_content`, our final [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) would be:

Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.

#### Multiple structured outputs error

When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) and prompts the model to retry:

#### Schema validation error

When structured output doesn't match the expected schema, the agent provides specific error feedback:

#### Error handling strategies

You can customize how errors are handled using the `handle_errors` parameter:

**Custom error message:**

If `handle_errors` is a string, the agent will *always* prompt the model to re-try with a fixed tool message:

**Handle specific exceptions only:**

If `handle_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.

**Handle multiple exception types:**

If `handle_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.

**Custom error handler function:**

On `StructuredOutputValidationError`:

On `MultipleStructuredOutputsError`:

**No error handling:**

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/structured-output.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Response Format

Controls how the agent returns structured data:

* **`ToolStrategy[StructuredResponseT]`**: Uses tool calling for structured output
* **`ProviderStrategy[StructuredResponseT]`**: Uses provider-native structured output
* **`type[StructuredResponseT]`**: Schema type - automatically selects best strategy based on model capabilities
* **`None`**: No structured output

When a schema type is provided directly, LangChain automatically chooses:

* `ProviderStrategy` for models supporting native structured output (e.g. [OpenAI](/oss/python/integrations/providers/openai), [Grok](/oss/python/integrations/providers/xai))
* `ToolStrategy` for all other models

The structured response is returned in the `structured_response` key of the agent's final state.

## Provider strategy

Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.

To use this strategy, configure a `ProviderStrategy`:
```

Example 2 (unknown):
```unknown
<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

  * **Pydantic models**: `BaseModel` subclasses with field validation
  * **Dataclasses**: Python dataclasses with type annotations
  * **TypedDict**: Typed dictionary classes
  * **JSON Schema**: Dictionary with JSON schema specification
</ParamField>

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to [`create_agent.response_format`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(response_format\)) and the model supports native structured output:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Egress for subscription metrics and operational metadata

**URL:** llms-txt#egress-for-subscription-metrics-and-operational-metadata

**Contents:**
- LangSmith Telemetry
  - What we use it for
  - What we collect
  - How to disable
- Example payloads
  - License Verification
  - Usage Reporting
  - Telemetry: Metrics
  - Telemetry: Traces
- Our Commitment

Source: https://docs.langchain.com/langsmith/self-host-egress

<Info>
  This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.
</Info>

Self-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.

In the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.

<Warning>
  **This will require egress to `https://beacon.langchain.com` from your network. Refer to the [allowlisting IP section](/langsmith/cloud#allowlisting-ip-addresses) for static IP addresses, if needed.**
</Warning>

Generally, data that we send to Beacon can be categorized as follows:

* Subscription Metrics

* Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:

* Number of traces
    * Seats allocated per contract
    * Seats in currently use

* Operational Metadata
  * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.

## LangSmith Telemetry

As of version ***0.11***, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We ***do not collect PII*** (personally identifiable information) in any form.

### What we use it for

* To provide more proactive support and faster troubleshooting of self-hosted instances.
* Assisting with performance tuning.
* Understanding real-world usage to prioritize improvements.

* **Request metadata**: anonymized request counts, sizes, and durations.
* **Database metrics**: query durations, error rates, and performance counters.
* **Distributed traces**: end-to-end traces with timing and error information for high-latency or failed requests.

<Info>
  We do not collect actual payload contents, database records, or any data that can identify your end users or customers.
</Info>

Set the following values in your `langsmith_config.yaml` file:

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

`POST beacon.langchain.com/v1/beacon/verify`

`POST beacon.langchain.com/v1/beacon/ingest-traces`

### Telemetry: Metrics

`POST beacon.langchain.com/v1/beacon/v1/metrics`

### Telemetry: Traces

`POST beacon.langchain.com/v1/beacon/v1/traces`

LangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-egress.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example payloads

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/verify`

**Request:**
```

Example 2 (unknown):
```unknown
**Response:**
```

Example 3 (unknown):
```unknown
### Usage Reporting

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/ingest-traces`

**Request:**
```

Example 4 (unknown):
```unknown
**Response:**
```

---

## Use cron jobs

**URL:** llms-txt#use-cron-jobs

**Contents:**
- Setup
- Cron job on a thread
- Cron job stateless

Source: https://docs.langchain.com/langsmith/cron-jobs

There are many situations in which it is useful to run an assistant on a schedule.

For example, say that you're building an assistant that runs daily and sends an email summary
of the day's news. You could use a cron job to run the assistant every day at 8:00 PM.

LangSmith Deployment supports cron jobs, which run on a user-defined schedule. The user specifies a schedule, an assistant, and some input. After that, on the specified schedule, the server will:

* Create a new thread with the specified assistant
* Send the specified input to that thread

Note that this sends the same input to the thread every time.

The LangGraph Server API provides several endpoints for creating and managing cron jobs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/) for more details.

Sometimes you don't want to run your graph based on user interaction, but rather you would like to schedule your graph to run on a schedule - for example if you wish for your graph to compose and send out a weekly email of to-dos for your team. LangSmith Deployment allows you to do this without having to write your own script by using the `Crons` client. To schedule a graph job, you need to pass a [cron expression](https://crontab.cronhub.io/) to inform the client when you want to run the graph. `Cron` jobs are run in the background and do not interfere with normal invocations of the graph.

First, let's set up our SDK client, assistant, and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Note that it is **very** important to delete `Cron` jobs that are no longer useful. Otherwise you could rack up unwanted API charges to the LLM! You can delete a `Cron` job using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job stateless

You can also create stateless cron jobs by using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Again, remember to delete your job once you are done with it!

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cron-jobs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
```

---

## Configure custom TLS certificates

**URL:** llms-txt#configure-custom-tls-certificates

**Contents:**
- Mount internal CAs for TLS
- Use custom TLS certificates for model providers

Source: https://docs.langchain.com/langsmith/self-host-custom-tls-certificates

Use this guide to configure TLS in LangSmith. Start by mounting internal certificate authorities (CAs) so your deployment trusts the right roots system‑wide, for database or external service calls. You can then configure [Playground](/langsmith/prompt-engineering-concepts#prompt-playground)-specific mTLS for communicating securely with supported model providers.

* [Mounting internal certificate authorities](#mount-internal-cas-for-tls) (CAs) system-wide to enable TLS for database connections and Playground model calls
* Using Playground-specific TLS settings to provide client certs/keys for mTLS with supported model providers

## Mount internal CAs for TLS

<Note>
  You must use Helm chart version 0.11.9 or later to mount internal CAs using the configuration below.
</Note>

Use this approach to make internal/public CAs trusted system‑wide by LangSmith (Playground model calls and [database/external service connections](/langsmith/self-hosted#storage-services)).

1. Create a file containing all CAs required for TLS with databases and external services. If your deployment is communicating directly to `beacon.langchain.com` without a proxy, make sure to include a public trusted CA. All certs should be concatenated in this file with an empty line in between.
   
2. Create a Kubernetes secret with a key containing the contents of this file.
   
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
   
4. Make sure to use TLS supported connection strings:
   * <b>Postgres</b>: Add `?sslmode=verify-full&sslrootcert=system` to the end.
   * <b>Redis</b>: Use `rediss://` instead of `redis://` as the prefix.

## Use custom TLS certificates for model providers

<Note>
  This feature is currently only available for the following model providers:

* Azure OpenAI
  * OpenAI
  * Custom (our custom model server). Refer to the [custom model server documentation](/langsmith/custom-endpoint) for more information.

These TLS settings apply to all invocations of the selected model providers (including Online Evaluation). Use them when the provider requires mutual TLS (client cert/key) or when you must override trust with a specific CA for provider calls. They complement the internal CA bundle configured above.
</Note>

You can use custom TLS certificates to connect to model providers in the LangSmith Playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority, or mutual TLS authentication.

To use custom TLS certificates, set the following environment variables. See the [self hosted deployment section](/langsmith/architectural-overview) for more information on how to configure application settings.

* `LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS`: A comma-separated list of model providers that require custom TLS certificates. Note that `azure_openai`, `openai`, and `custom` are currently the only supported model providers, but more providers will be supported in the future.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_KEY`: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CERT`: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CA`: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume). Use this to mount CAs only if you're using a helm version below `0.11.9`; otherwise, use the [Mount internal CAs for TLS](./self-host-custom-tls-certificates#mount-internal-cas-for-tls) section above.

Once you have set these environment variables, enter the LangSmith Playground **Settings** page and select the **Provider** that requires custom TLS certificates. Set your model provider configuration as usual, and the custom TLS certificates will be used when connecting to the model provider.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-custom-tls-certificates.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
-----BEGIN CERTIFICATE-----
   <PUBLIC_CA>
   -----END CERTIFICATE-----

   -----BEGIN CERTIFICATE-----
   <INTERNAL_CA>
   -----END CERTIFICATE-----

   ...
```

Example 2 (unknown):
```unknown
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
```

---

## >                'allowed_decisions': ['approve', 'reject']

**URL:** llms-txt#>----------------'allowed_decisions':-['approve',-'reject']

---

## A2A endpoint in LangGraph Server

**URL:** llms-txt#a2a-endpoint-in-langgraph-server

**Contents:**
- Agent Card Discovery
- Requirements
- Usage overview
- Creating an A2A-compatible agent

Source: https://docs.langchain.com/langsmith/server-a2a

[Agent2Agent (A2A)](https://a2a-protocol.org/latest/) is Google's protocol for enabling communication between conversational AI agents. [LangSmith implements A2A support](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/a2a/post/a2a/\{assistant_id}), allowing your agents to communicate with other A2A-compatible agents through a standardized protocol.

The A2A endpoint is available in [LangGraph Server](/langsmith/langgraph-server) at `/a2a/{assistant_id}`.

## Agent Card Discovery

Each assistant automatically exposes an A2A Agent Card that describes its capabilities and provides the information needed for other agents to connect. You can retrieve the agent card for any assistant using:

The agent card includes the assistant's name, description, available skills, supported input/output modes, and the A2A endpoint URL for communication.

To use A2A, ensure you have the following dependencies installed:

* `langgraph-api >= 0.4.9`

* Upgrade to use langgraph-api>=0.4.9.
* Deploy your agent with message-based state structure.
* Connect with other A2A-compatible agents using the endpoint.

## Creating an A2A-compatible agent

This example creates an A2A-compatible agent that processes incoming messages using OpenAI's API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol's message format.

To be compatible with the [A2A "text" parts](https://a2a-protocol.org/dev/specification/#651-textpart-object), the agent must have a `messages` key in state. Here's an example:

```python  theme={null}
"""LangGraph A2A conversational agent.

Supports the A2A protocol with messages input for conversational interactions.
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Any, Dict, List, TypedDict

from langgraph.graph import StateGraph
from langgraph.runtime import Runtime
from openai import AsyncOpenAI

class Context(TypedDict):
    """Context parameters for the agent."""
    my_configurable_param: str

@dataclass
class State:
    """Input state for the agent.

Defines the initial structure for A2A conversational messages.
    """
    messages: List[Dict[str, Any]]

async def call_model(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    """Process conversational messages and returns output using OpenAI."""
    # Initialize OpenAI client
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Process the incoming messages
    latest_message = state.messages[-1] if state.messages else {}
    user_content = latest_message.get("content", "No message content")

# Create messages for OpenAI API
    openai_messages = [
        {
            "role": "system",
            "content": "You are a helpful conversational agent. Keep responses brief and engaging."
        },
        {
            "role": "user",
            "content": user_content
        }
    ]

try:
        # Make OpenAI API call
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=openai_messages,
            max_tokens=100,
            temperature=0.7
        )

ai_response = response.choices[0].message.content

except Exception as e:
        ai_response = f"I received your message but had trouble processing it. Error: {str(e)[:50]}..."

# Create a response message
    response_message = {
        "role": "assistant",
        "content": ai_response
    }

return {
        "messages": state.messages + [response_message]
    }

**Examples:**

Example 1 (unknown):
```unknown
GET /.well-known/agent-card.json?assistant_id={assistant_id}
```

Example 2 (unknown):
```unknown
## Usage overview

To enable A2A:

* Upgrade to use langgraph-api>=0.4.9.
* Deploy your agent with message-based state structure.
* Connect with other A2A-compatible agents using the endpoint.

## Creating an A2A-compatible agent

This example creates an A2A-compatible agent that processes incoming messages using OpenAI's API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol's message format.

To be compatible with the [A2A "text" parts](https://a2a-protocol.org/dev/specification/#651-textpart-object), the agent must have a `messages` key in state. Here's an example:
```

---

## export LANGSMITH_API_KEY="your-api-key"

**URL:** llms-txt#export-langsmith_api_key="your-api-key"

**Contents:**
  - Installation
- Instantiation
- Indexing and Retrieval
- Direct Usage
  - Embed single texts
  - Embed multiple texts
- Specifying dimensions
- Custom URLs
- API reference

bash npm theme={null}
  npm install @langchain/openai @langchain/core
  bash yarn theme={null}
  yarn add @langchain/openai @langchain/core
  bash pnpm theme={null}
  pnpm add @langchain/openai @langchain/core
  typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  batchSize: 512, // Default value if omitted is 512. Max is 2048
  model: "text-embedding-3-large",
});
typescript  theme={null}
// Create a vector store with a sample text
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
output  theme={null}
LangChain is the framework for building context-aware reasoning applications
typescript  theme={null}
const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
output  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
typescript  theme={null}
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
output  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
[
   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,
  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,
    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,
    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,
    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,
   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,
    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,
   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,
  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,
   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,
   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,
   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,
   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,
   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,
   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,
     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,
    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,
    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,
    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,
   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552
]
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddingsDefaultDimensions = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
});

const vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments(["some text"]);
console.log(vectorsDefaultDimensions[0].length);
output  theme={null}
3072
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings1024 = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
  dimensions: 1024,
});

const vectors1024 = await embeddings1024.embedDocuments(["some text"]);
console.log(vectors1024[0].length);
output  theme={null}
1024
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const model = new OpenAIEmbeddings({
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});
```

You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).

For detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Installation

The LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Instantiation

Now we can instantiate our model object and generate chat completions:
```

---

## Rebuild graph at runtime

**URL:** llms-txt#rebuild-graph-at-runtime

**Contents:**
- Prerequisites
- Define graphs
  - No rebuild
  - Rebuild

Source: https://docs.langchain.com/langsmith/graph-rebuild

You might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.

<Note>
  **Note**
  In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it
</Note>

Make sure to check out [this how-to guide](/langsmith/setup-app-requirements-txt) on setting up your app for deployment first.

Let's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:

where the graph is defined in `openai_agent.py`.

In the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:

To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:

```python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph
from langgraph.graph.state import StateGraph
from langgraph.graph.message import add_messages
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langchain.messages import BaseMessage
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

model = ChatOpenAI(temperature=0)

def make_default_graph():
    """Make a simple LLM agent"""
    graph_workflow = StateGraph(State)
    def call_model(state):
        return {"messages": [model.invoke(state["messages"])]}

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_edge("agent", END)
    graph_workflow.add_edge(START, "agent")

agent = graph_workflow.compile()
    return agent

def make_alternative_graph():
    """Make a tool-calling agent"""

@tool
    def add(a: float, b: float):
        """Adds two numbers."""
        return a + b

tool_node = ToolNode([add])
    model_with_tools = model.bind_tools([add])
    def call_model(state):
        return {"messages": [model_with_tools.invoke(state["messages"])]}

def should_continue(state: State):
        if state["messages"][-1].tool_calls:
            return "tools"
        else:
            return END

graph_workflow = StateGraph(State)

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_node("tools", tool_node)
    graph_workflow.add_edge("tools", "agent")
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_conditional_edges("agent", should_continue)

agent = graph_workflow.compile()
    return agent

**Examples:**

Example 1 (unknown):
```unknown
my-app/
|-- requirements.txt
|-- .env
|-- openai_agent.py     # code for your graph
```

Example 2 (unknown):
```unknown
To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:
```

Example 3 (unknown):
```unknown
### Rebuild

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:
```

---

## Log in as user 1

**URL:** llms-txt#log-in-as-user-1

user1_token = await login(email1, password)
user1_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user1_token}"}
)

---

## Streaming

**URL:** llms-txt#streaming

**Contents:**
- Supported stream modes
- Basic usage example
- Stream multiple modes
- Stream graph state
- Stream subgraph outputs
  - Debugging
- LLM tokens

Source: https://docs.langchain.com/oss/python/langgraph/streaming

LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

What's possible with LangGraph streaming:

* <Icon icon="share-nodes" size={16} /> [**Stream graph state**](#stream-graph-state) — get state updates / values with `updates` and `values` modes.
* <Icon icon="square-poll-horizontal" size={16} /> [**Stream subgraph outputs**](#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.
* <Icon icon="square-binary" size={16} /> [**Stream LLM tokens**](#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.
* <Icon icon="table" size={16} /> [**Stream custom data**](#stream-custom-data) — send custom updates or progress signals directly from tool functions.
* <Icon icon="layer-plus" size={16} /> [**Use multiple streaming modes**](#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).

## Supported stream modes

Pass one or more of the following stream modes as a list to the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods:

| Mode       | Description                                                                                                                                                                         |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |
| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |
| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |

## Basic usage example

LangGraph graphs expose the [`stream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) (sync) and [`astream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.astream) (async) methods to yield streamed outputs as iterators.

<Accordion title="Extended example: streaming updates">

## Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Tabs>
  <Tab title="updates">
    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tab title="values">
    Use this to stream the **full state** of the graph after each step.

## Stream subgraph outputs

To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

The outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.

<Accordion title="Extended example: streaming from subgraphs">

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

<Warning>
  **Manual config required for async in Python \< 3.11**
  When using Python \< 3.11 with async code, you must explicitly pass [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to `ainvoke()` to enable proper streaming. See [Async with Python \< 3.11](#async) for details or upgrade to Python 3.11+.
</Warning>

```python  theme={null}
from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
    topic: str
    joke: str = ""

model = init_chat_model(model="openai:gpt-4o-mini")

def call_model(state: MyState):
    """Call the LLM to generate a joke about a topic"""
    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream
    model_response = model.invoke(  # [!code highlight]
        [
            {"role": "user", "content": f"Generate a joke about {state.topic}"}
        ]
    )
    return {"joke": model_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming updates">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Accordion>

## Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.
```

Example 4 (unknown):
```unknown
## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.
```

---

## > ]

**URL:** llms-txt#>-]

---

## Set stream_mode="custom" to receive the custom data in the stream

**URL:** llms-txt#set-stream_mode="custom"-to-receive-the-custom-data-in-the-stream

**Contents:**
- Disable streaming for specific chat models
  - Async with Python \< 3.11

for chunk in graph.stream(
    {"topic": "cats"},
    stream_mode="custom",  # [!code highlight]

):
    # The chunk will contain the custom data streamed from the llm
    print(chunk)
python  theme={null}
  import operator
  import json

from typing import TypedDict
  from typing_extensions import Annotated
  from langgraph.graph import StateGraph, START

from openai import AsyncOpenAI

openai_client = AsyncOpenAI()
  model_name = "gpt-4o-mini"

async def stream_tokens(model_name: str, messages: list[dict]):
      response = await openai_client.chat.completions.create(
          messages=messages, model=model_name, stream=True
      )
      role = None
      async for chunk in response:
          delta = chunk.choices[0].delta

if delta.role is not None:
              role = delta.role

if delta.content:
              yield {"role": role, "content": delta.content}

# this is our tool
  async def get_items(place: str) -> str:
      """Use this tool to list items one might find in a place you're asked about."""
      writer = get_stream_writer()
      response = ""
      async for msg_chunk in stream_tokens(
          model_name,
          [
              {
                  "role": "user",
                  "content": (
                      "Can you tell me what kind of items "
                      f"i might find in the following place: '{place}'. "
                      "List at least 3 such items separating them by a comma. "
                      "And include a brief description of each item."
                  ),
              }
          ],
      ):
          response += msg_chunk["content"]
          writer(msg_chunk)

class State(TypedDict):
      messages: Annotated[list[dict], operator.add]

# this is the tool-calling graph node
  async def call_tool(state: State):
      ai_message = state["messages"][-1]
      tool_call = ai_message["tool_calls"][-1]

function_name = tool_call["function"]["name"]
      if function_name != "get_items":
          raise ValueError(f"Tool {function_name} not supported")

function_arguments = tool_call["function"]["arguments"]
      arguments = json.loads(function_arguments)

function_response = await get_items(**arguments)
      tool_message = {
          "tool_call_id": tool_call["id"],
          "role": "tool",
          "name": function_name,
          "content": function_response,
      }
      return {"messages": [tool_message]}

graph = (
      StateGraph(State)
      .add_node(call_tool)
      .add_edge(START, "call_tool")
      .compile()
  )
  python  theme={null}
  inputs = {
      "messages": [
          {
              "content": None,
              "role": "assistant",
              "tool_calls": [
                  {
                      "id": "1",
                      "function": {
                          "arguments": '{"place":"bedroom"}',
                          "name": "get_items",
                      },
                      "type": "function",
                  }
              ],
          }
      ]
  }

async for chunk in graph.astream(
      inputs,
      stream_mode="custom",
  ):
      print(chunk["content"], end="|", flush=True)
  python  theme={null}
    from langchain.chat_models import init_chat_model

model = init_chat_model(
        "anthropic:claude-sonnet-4-5",
        # Set disable_streaming=True to disable streaming for the chat model
        disable_streaming=True  # [!code highlight]

)
    python  theme={null}
    from langchain_openai import ChatOpenAI

# Set disable_streaming=True to disable streaming for the chat model
    model = ChatOpenAI(model="o1-preview", disable_streaming=True)
    python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain.chat_models import init_chat_model

model = init_chat_model(model="openai:gpt-4o-mini")

class State(TypedDict):
      topic: str
      joke: str

# Accept config as an argument in the async node function
  async def call_model(state, config):
      topic = state["topic"]
      print("Generating joke...")
      # Pass config to model.ainvoke() to ensure proper context propagation
      joke_response = await model.ainvoke(  # [!code highlight]
          [{"role": "user", "content": f"Write a joke about {topic}"}],
          config,
      )
      return {"joke": joke_response.content}

graph = (
      StateGraph(State)
      .add_node(call_model)
      .add_edge(START, "call_model")
      .compile()
  )

# Set stream_mode="messages" to stream LLM tokens
  async for chunk, metadata in graph.astream(
      {"topic": "ice cream"},
      stream_mode="messages",  # [!code highlight]
  ):
      if chunk.content:
          print(chunk.content, end="|", flush=True)
  python  theme={null}
  from typing import TypedDict
  from langgraph.types import StreamWriter

class State(TypedDict):
        topic: str
        joke: str

# Add writer as an argument in the function signature of the async node or tool
  # LangGraph will automatically pass the stream writer to the function
  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]
        writer({"custom_key": "Streaming custom data while generating a joke"})
        return {"joke": f"This is a joke about {state['topic']}"}

graph = (
        StateGraph(State)
        .add_node(generate_joke)
        .add_edge(START, "generate_joke")
        .compile()
  )

# Set stream_mode="custom" to receive the custom data in the stream  # [!code highlight]
  async for chunk in graph.astream(
        {"topic": "ice cream"},
        stream_mode="custom",
  ):
        print(chunk)
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming arbitrary chat model">
```

Example 2 (unknown):
```unknown
Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:
```

Example 3 (unknown):
```unknown
</Accordion>

## Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.

Set `disable_streaming=True` when initializing the model.

<Tabs>
  <Tab title="init_chat_model">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="chat model interface">
```

---

## Basic authentication with email and password

**URL:** llms-txt#basic-authentication-with-email-and-password

**Contents:**
- Requirements and features
  - Migrating from None auth
  - Configuration

Source: https://docs.langchain.com/langsmith/self-host-basic-auth

LangSmith supports login via username/password with a few limitations:

* You cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. **A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing `None` type installation (see below).**
* Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.
* You cannot use both basic auth and OAuth with client secret at the same time.

## Requirements and features

* There is a single `Default` organization that is provisioned during initial installation, and creating additional organizations is not supported
* Your initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol
* There are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: `openssl rand -base64 32`

### Migrating from None auth

**Only supported in versions 0.7 and above.**

Migrating an installation from [None](/langsmith/authentication-methods#none) auth mode replaces the single "default" user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains `00000000-0000-0000-0000-000000000000`, but everything else about the migrated installation is standard for a basic auth installation.

To migrate, simply update your configuration as shown below and run `helm upgrade` (or `docker-compose up`) as usual.

<Note>
  Changing the JWT secret will log out your users
</Note>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:

Once configured, you will see a login screen like the one below. You should be able to login with the `initialOrgAdminEmail` and `initialOrgAdminPassword` values, and your user will be auto-provisioned with role `Organization Admin`. See the [admin guide](/langsmith/administration-overview#organization-roles) for more details on organization roles.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2f88a4bffacc308d7fbd22cba1f1c9f1" alt="LangSmith UI with basic auth" data-og-width="1424" width="1424" data-og-height="1156" height="1156" data-path="langsmith/images/langsmith-ui-basic-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d0297842dd132b61fa295b52e93859e 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=975b75e1f4262b5278e2d56896bec326 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=54a1827e8611f72359a121dd209b5860 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42618816bdfbd171e32d064fd00652da 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a73742556b59333c002f9d0f5d314e6e 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=93b93edf04b03f1ab652df38270364c2 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-basic-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:
```

---

## Instructions for routing.

**URL:** llms-txt#instructions-for-routing.

route_instructions = """You are managing an online music store that sells song tracks. \
You can help customers in two types of ways: (1) answering general questions about \
tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.

Based on the following conversation, determine if the user is currently seeking general \
information about song tracks or if they are trying to refund a specific purchase.

Return 'refund' if they are trying to get a refund and 'question_answering' if they are \
asking a general music question. Do NOT return anything else. Do NOT try to respond to \
the user.
"""

---

## The interrupt contains the full HITL request with action_requests and review_configs

**URL:** llms-txt#the-interrupt-contains-the-full-hitl-request-with-action_requests-and-review_configs

print(result['__interrupt__'])

---

## Studio troubleshooting

**URL:** llms-txt#studio-troubleshooting

**Contents:**
- Safari Connection Issues
  - Solution 1: Use Cloudflare Tunnel
  - Solution 2: Use Chromium Browser
- Brave Connection Issues
  - Solution 1: Disable Brave Shields
  - Solution 2: Use Cloudflare Tunnel
- Graph Edge Issues
  - Solution 1: Path Map
  - Solution 2: Router Type Definition
- Experiment troubleshooting in Studio

Source: https://docs.langchain.com/langsmith/troubleshooting-studio

## Safari Connection Issues

Safari blocks plain-HTTP traffic on localhost. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium Browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Brave to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

Undefined conditional edges may show unexpected connections in your graph. This is
because without proper definition, Studio assumes the conditional edge could access all other nodes. To address this, explicitly define the routing paths using one of these methods:

### Solution 1: Path Map

Define a mapping between router outputs and target nodes:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

### Solution 2: Router Type Definition

Specify possible routing destinations using Python's `Literal` type:

## Experiment troubleshooting in Studio

### **Run experiment** button is disabled

* **Deployed application**: If your application is deployed on LangSmith, you may need to create a new revision to enable this feature.
* **Local development server**: If you are running your application locally, make sure you have upgraded to the latest version of the `langgraph-cli` (`pip install -U langgraph-cli`). Additionally, ensure you have tracing enabled by setting the `LANGSMITH_API_KEY` in your project's `.env` file.

### Evaluator results are missing

When you run an experiment, any attached evaluators are scheduled for execution in a queue. If you don't see results immediately, it likely means they are still pending.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JS">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

The command outputs a URL in this format:
```

Example 3 (unknown):
```unknown
Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium Browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JS">
```

---

## Add edges to connect nodes

**URL:** llms-txt#add-edges-to-connect-nodes

orchestrator_worker_builder.add_edge(START, "orchestrator")
orchestrator_worker_builder.add_conditional_edges(
    "orchestrator", assign_workers, ["llm_call"]
)
orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
orchestrator_worker_builder.add_edge("synthesizer", END)

---

## Functional API

**URL:** llms-txt#functional-api

**Contents:**
- Related

@entrypoint(checkpointer=checkpointer)
def workflow(messages: list[str]) -> str:
    ...
```

* Use the LangGraph API so you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you.

* Read more about [persistence](/oss/python/langgraph/persistence).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/errors/MISSING_CHECKPOINTER.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Improve LLM-as-judge evaluators using human feedback

**URL:** llms-txt#improve-llm-as-judge-evaluators-using-human-feedback

**Contents:**
- How it works
- Prerequisites
  - Offline evaluations
  - Online evaluations
- Getting started
- 1. Select experiments or runs
- 2. Label examples
- 3. Test your evaluator prompt against the labeled examples
- 4. Repeat to improve evaluator alignment
  - Tips for improving evaluator alignment

Source: https://docs.langchain.com/langsmith/improve-judge-evaluator-feedback

<Check>
  Before working through this page, it might be helpful to read the following:

* [Evaluation concepts](/langsmith/evaluation-concepts#evaluators)
  * [Creating LLM-as-a-judge evaluators](/langsmith/llm-as-judge)
</Check>

Reliable [*LLM-as-a-judge evaluators*](/langsmith/evaluation-concepts#llm-as-judge) are critical for making informed decisions about your AI applications (e.g., prompt, model, architecture changes). Defining the evaluator prompt correctly can be difficult, but it directly affects the trustworthiness of your evaluations.

This guide describes how to align your LLM-as-a-judge evaluator using human feedback to improve your evaluator's quality and help you build reliable AI applications.

LangSmith's **Align Evaluator** feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback. You can use this feature to align evaluators that run on a dataset for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) or for [online evaluations](/langsmith/evaluation-concepts#online-evaluation). In either case, the steps are similar:

1. **Select experiments or runs** that contain outputs from your application.
2. Add the selected experiments or runs to an **annotation queue** where a human expert can label the data.
3. **Test your LLM-as-a-judge evaluator prompt** against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement.
4. **Refine and repeat** to improve evaluator alignment. Update your LLM-as-a-judge evaluator prompt and test again.

You'll need the following before starting this guide for [offline evaluations](#offline-evaluations) or [online evaluations](#online-evaluations):

### Offline evaluations

* A [dataset](/langsmith/evaluation-concepts#datasets) with at least one [experiment](/langsmith/evaluation-concepts#experiment).
* You'll need to upload or create datasets via the [SDK](/langsmith/manage-datasets-programmatically#create-a-dataset) or the [UI](/langsmith/manage-datasets-in-application#set-up-your-dataset) and run an experiment via the [SDK](/langsmith/evaluate-llm-application#run-the-evaluation) or the [Playground](/langsmith/run-evaluation-from-prompt-playground#5-run-your-evaluation).

### Online evaluations

* An application that’s already sending traces to LangSmith.
* Configure this with one of the [tracing integrations](/langsmith/observability-concepts#integrations) to start.

You can enter the alignment flow for both new and existing evaluators in datasets and tracing projects.

|                                              | Dataset Evaluators                                                                                                                                                                                     | Tracing Project Evaluators                                                                                                                                                                         |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Create an aligned evaluator from scratch** | 1. **Datasets & Experiments** and select your dataset<br />2. Click **+ Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback key name (e.g. `correctness`, `hallucination`) | 1. **Projects** and select your project<br />2. Click **+ New** > **Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback‑key name (e.g. `correctness`, `hallucination`) |
| **Align an existing evaluator**              | 1. **Datasets & Experiments** > select your dataset > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                     | 1. **Projects** > select your project > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                               |

## 1. Select experiments or runs

Select one or more experiments (or runs) to send for human labeling. This will add runs to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-evaluator-queue.gif?s=3c457faaaf2c7f31425ba510a518260e" alt="Add to evaluator queue" data-og-width="1976" width="1976" data-og-height="1080" height="1080" data-path="langsmith/images/add-to-evaluator-queue.gif" data-optimize="true" data-opv="3" />

To add any new experiments/runs to an existing annotation queue, head to the **Evaluators** tab, select the evaluator you are aligning and click **Add to Queue.**

<Check>
  Datasets should be representative of inputs and outputs you expect to see in production.

While you don’t need to cover every possible scenario, it’s important to include examples across the full range of expected use cases. For example, if you're building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.
</Check>

Label examples in the annotation queue by adding a feedback score. Once you've labeled an example, click **Add to Reference Dataset**.

<Check>
  If you have a large number of examples in your experiments, you don't need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recommend that the examples that you label are diverse (balanced in both 0 and 1 labels) to ensure that you're building a well rounded evaluator prompt.
</Check>

## 3. Test your evaluator prompt against the labeled examples

Once you have labeled examples, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the **Evaluator Playground**.

To go to the evaluator playground: Click the **View evaluator** button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the **Evaluator Playground** button to access the playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-pg.gif?s=c8108ce3abc95ce756fcbd1b3726916d" alt="Evaluator Playground" data-og-width="1916" width="1916" data-og-height="1080" height="1080" data-path="langsmith/images/evaluator-pg.gif" data-optimize="true" data-opv="3" />

In the evaluator playground you can create or edit your evaluator prompt and click **Start Alignment** to run it over the set of labeled examples that you created in Step 2. After running your evaluator, you'll see how its generated scores compare to your human labels. The alignment score is the percentage of examples where the evaluator's judgment matches that of the human expert.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/langchain-5e9cc07a/langsmith/images/alignment-evaluator-pg.gif" alt="Evaluator Playground" />

## 4. Repeat to improve evaluator alignment

Iterate by updating your prompt and testing again to improve evaluator alignment.

<Check>
  Updates to your evaluator prompt are **not saved by default**. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.

The evaluator playground will show the alignment score for the most recently saved version of your evaluator prompt for comparison when you're iterating on your prompt.
</Check>

Improving the alignment score of your evaluator isn't an exact science but there are a few strategies that are helpful in increasing the alignment score.

### Tips for improving evaluator alignment

**1. Investigate misaligned examples**

Digging into misaligned examples and trying to group them into common failure modes is a great first step for improving your evaluator alignment.

Once you have identified the common failure modes, add instructions to your evaluator prompt so the LLM knows about them. For example, you could explain that "MFA stands for "multi-factor authentication" if you notice it not understanding that specific acronym. Or you could tell it that "a good response will always contain at least 3 potential hotels to book" if it is confused on what good/bad means in your evaluator's context.

**2. Inspect the reasoning behind the LLM score**

To understand why the LLM scored an example the way it did, you can enable reasoning for your LLM-as-a-judge evaluator. Reasoning is helpful to understand the LLM's thought process and can help you identify common failure modes to incorporate into your evaluator prompt as well..

In order to see the reasoning in the evaluator playground, hover over the LLM score.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/enable-reasoning.gif?s=d6ed09e4149a63723ef9305544b8b652" alt="Enable reasoning" data-og-width="1520" width="1520" data-og-height="1080" height="1080" data-path="langsmith/images/enable-reasoning.gif" data-optimize="true" data-opv="3" />

This will show the reasoning behind the LLM's score in the evaluator playground.

**3. Add more labeled examples and validate performance**

To avoid overfitting to the labeled examples, it's important to add more labeled examples and test performance, especially if you started off with a small number of examples.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/-9o94oj4x0A?si=wfv9cN3L4DalMD2e" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/improve-judge-evaluator-feedback.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## minReplicas: 40

**URL:** llms-txt#minreplicas:-40

backend:
  deployment:
    replicas: 50 # OR enable autoscaling to this level (example below)

---

## How to interact with a deployment using RemoteGraph

**URL:** llms-txt#how-to-interact-with-a-deployment-using-remotegraph

**Contents:**
- Prerequisites
- Initialize the graph
  - Use a URL
  - Use a client
- Invoke the graph
  - Asynchronously
  - Synchronously
- Persist state at the thread level
- Use as a subgraph

Source: https://docs.langchain.com/langsmith/use-remote-graph

[`RemoteGraph`](https://langchain-ai.github.io/langgraph/reference/remote_graph) is a client-side interface that allows you to interact with your [deployment](/langsmith/deployments) as if it were a local graph. It provides API parity with [`CompiledGraph`](/oss/python/langgraph/graph-api#compiling-your-graph), which means that you can use the same methods (`invoke()`, `stream()`, `get_state()`, etc.) in your development and production environments. This page describes how to initialize a `RemoteGraph` and interact with it.

`RemoteGraph` is useful for the following:

* Separation of development and deployment: Build and test a graph locally with `CompiledGraph`, deploy it to LangSmith, and then [use `RemoteGraph`](#initialize-the-graph) to call it in production while working with the same API interface.
* Thread-level persistence: [Persist and fetch the state](#persist-state-at-the-thread-level) of a conversation across calls with a thread ID.
* Subgraph embedding: Compose modular graphs for a multi-agent workflow by embedding a `RemoteGraph` as a [subgraph](#use-as-a-subgraph) within another graph.
* Reusable workflows: Use deployed graphs as nodes or [tools](https://langchain-ai.github.io/langgraph/reference/remote_graph/#langgraph.pregel.remote.RemoteGraph.as_tool), so that you can reuse and expose complex logic.

<Warning>
  **Important: Avoid calling the same deployment**

`RemoteGraph` is designed to call graphs on other deployments. Do not use `RemoteGraph` to call itself or another graph on the same deployment, as this can lead to deadlocks and resource exhaustion. Instead, use local graph composition or [subgraphs](/oss/python/langgraph/use-subgraphs) for graphs within the same deployment.
</Warning>

Before getting started with `RemoteGraph`, make sure you have:

* Access to [LangSmith](/langsmith/home), where your graphs are developed and managed.
* A running [LangGraph Server](/langsmith/langgraph-server), which hosts your deployed graphs for remote interaction.

## Initialize the graph

When initializing a `RemoteGraph`, you must always specify:

* `name`: The name of the graph you want to interact with **or** an assistant ID. If you specify a graph name, the default assistant will be used. If you specify an assistant ID, that specific assistant will be used. The graph name is the same name you use in the `langgraph.json` configuration file for your deployment.
* `api_key`: A valid [LangSmith API key](/langsmith/create-account-api-key). You can set as an environment variable (`LANGSMITH_API_KEY`) or pass directly in the `api_key` argument. You can also provide the API key in the `client` / `sync_client` arguments, if `LangGraphClient` / `SyncLangGraphClient` was initialized with the `api_key` argument.

Additionally, you have to provide one of the following:

* [`url`](#use-a-url): The URL of the deployment you want to interact with. If you pass the `url` argument, both sync and async clients will be created using the provided URL, headers (if provided), and default configuration values (e.g., timeout).
* [`client`](#use-a-client): A `LangGraphClient` instance for interacting with the deployment asynchronously (e.g., using `.astream()`, `.ainvoke()`, `.aget_state()`, `.aupdate_state()`).
* `sync_client`: A `SyncLangGraphClient` instance for interacting with the deployment synchronously (e.g., using `.stream()`, `.invoke()`, `.get_state()`, `.update_state()`).

<Note>
  If you pass both `client` or `sync_client` as well as the `url` argument, they will take precedence over the `url` argument. If none of the `client` / `sync_client` / `url` arguments are provided, `RemoteGraph` will raise a `ValueError` at runtime.
</Note>

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<Note>
  To use the graph synchronously, you must provide either the `url` or `sync_client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
  
</CodeGroup>

## Persist state at the thread level

By default, graph runs (for example, calls made with `.invoke()` or `.stream()`) are stateless, which means that intermediate checkpoints and the final state are not persisted after a run.

If you want to preserve the outputs of a run—for example, to support human-in-the-loop workflows—you can create a thread and pass its ID through the `config` argument. This works the same way as with a regular compiled graph:

<Note>
  If you need to use a `checkpointer` with a graph that has a `RemoteGraph` subgraph node, make sure to use UUIDs as thread IDs.
</Note>

A graph can also call out to multiple `RemoteGraph` instances as [*subgraph*](/oss/python/langgraph/use-subgraphs) nodes. This allows for modular, scalable workflows where different responsibilities are split across separate graphs.

`RemoteGraph` exposes the same interface as a regular `CompiledGraph`, so you can use it directly as a subgraph inside another graph. For example:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-remote-graph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Use a client

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Invoke the graph

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

### Asynchronously

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
```

---

## LangSmith Observability

**URL:** llms-txt#langsmith-observability

Source: https://docs.langchain.com/langsmith/observability

The following sections help you set up and use tracing, monitoring, and observability features:

<Columns cols={3}>
  <Card title="Set up tracing" icon="gear" href="/langsmith/observability-quickstart" arrow="true">
    Configure tracing with basic options, framework integrations, or advanced settings for full control.
  </Card>

<Card title="View traces" icon="route" href="/langsmith/filter-traces-in-application" arrow="true">
    Access and manage traces via UI or API with filtering, exporting, sharing, and comparison tools.
  </Card>

<Card title="Monitor performance" icon="chart-area" href="/langsmith/dashboards" arrow="true">
    Create dashboards and set alerts to track performance and get notified when issues arise.
  </Card>

<Card title="Configure automations" icon="robot" href="/langsmith/rules" arrow="true">
    Use rules, webhooks, and online evaluations to streamline observability workflows.
  </Card>

<Card title="Collect feedback" icon="users" href="/langsmith/attach-user-feedback" arrow="true">
    Gather and manage annotations on outputs using queues and inline annotation.
  </Card>

<Card title="Trace a RAG app" icon="book-open" href="/langsmith/observability-llm-tutorial" arrow="true">
    Follow a step-by-step tutorial to trace a Retrieval-Augmented Generation application from start to finish.
  </Card>
</Columns>

For terminology definitions and core concepts, refer to [Observability concepts](/langsmith/observability-concepts).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## If you have a higher tiered Tavily API plan you can increase this

**URL:** llms-txt#if-you-have-a-higher-tiered-tavily-api-plan-you-can-increase-this

rate_limiter = InMemoryRateLimiter(requests_per_second=0.08)

---

## Messages and content

**URL:** llms-txt#messages-and-content

from langchain.messages import AIMessage, HumanMessage

---

## export LANGSMITH_TRACING="true"

**URL:** llms-txt#export-langsmith_tracing="true"

---

## Set this to true in order to log the input user prompts

**URL:** llms-txt#set-this-to-true-in-order-to-log-the-input-user-prompts

export OTEL_LOG_USER_PROMPTS=1

---

## Guardrails

**URL:** llms-txt#guardrails

**Contents:**
- Built-in guardrails
  - PII detection

Source: https://docs.langchain.com/oss/python/langchain/guardrails

Implement safety checks and content filtering for your agents

Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.

Common use cases include:

* Preventing PII leakage
* Detecting and blocking prompt injection attacks
* Blocking inappropriate or harmful content
* Enforcing business rules and compliance requirements
* Validating output quality and accuracy

You can implement guardrails using [middleware](/oss/python/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1" alt="Middleware flow diagram" className="rounded-lg" data-og-width="500" width="500" data-og-height="560" height="560" data-path="oss/images/middleware_final.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w" />
</div>

Guardrails can be implemented using two complementary approaches:

<CardGroup cols={2}>
  <Card title="Deterministic guardrails" icon="list-check">
    Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.
  </Card>

<Card title="Model-based guardrails" icon="brain">
    Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.
  </Card>
</CardGroup>

LangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach.

## Built-in guardrails

LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.

PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.

The PII middleware supports multiple strategies for handling detected PII:

| Strategy | Description                             | Example               |
| -------- | --------------------------------------- | --------------------- |
| `redact` | Replace with `[REDACTED_TYPE]`          | `[REDACTED_EMAIL]`    |
| `mask`   | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |
| `hash`   | Replace with deterministic hash         | `a8f5f167...`         |
| `block`  | Raise exception when detected           | Error thrown          |

```python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="openai:gpt-4o",
    tools=[customer_service_tool, email_tool],
    middleware=[
        # Redact emails in user input before sending to model
        PIIMiddleware(
            "email",
            strategy="redact",
            apply_to_input=True,
        ),
        # Mask credit cards in user input
        PIIMiddleware(
            "credit_card",
            strategy="mask",
            apply_to_input=True,
        ),
        # Block API keys - raise error if detected
        PIIMiddleware(
            "api_key",
            detector=r"sk-[a-zA-Z0-9]{32}",
            strategy="block",
            apply_to_input=True,
        ),
    ],
)

---

## Trace with OpenAI

**URL:** llms-txt#trace-with-openai

Source: https://docs.langchain.com/langsmith/trace-openai

The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Graph node for looking up the users purchases

**URL:** llms-txt#graph-node-for-looking-up-the-users-purchases

def lookup(state: State) -> dict:
    args = (
        state[k]
        for k in (
            "customer_first_name",
            "customer_last_name",
            "customer_phone",
            "track_name",
            "album_title",
            "artist_name",
            "purchase_date_iso_8601",
        )
    )
    results = _lookup(*args)
    if not results:
        response = "We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?"
        followup = response
    else:
        response = f"Which of the following purchases would you like to be refunded for?\n\n"
        followup = f"Which of the following purchases would you like to be refunded for?\n\n{tabulate(results, headers='keys')}"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": followup,
        "invoice_line_ids": [res["invoice_line_id"] for res in results],
    }

---

## How to fetch performance metrics for an experiment

**URL:** llms-txt#how-to-fetch-performance-metrics-for-an-experiment

Source: https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment

<Check>
  Tracing projects and experiments use the same underlying data structure in our backend, which is called a "session."

You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.

We are working on unifying the terminology across our documentation and APIs.
</Check>

When you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.

The payload for experiment details includes the following values:

From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.
```

---

## A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments

**URL:** llms-txt#a-comparative-experiment-allows-you-to-provide-a-preferential-ranking-on-the-outputs-of-two-or-more-experiments

---

## ... can add custom routes if needed.

**URL:** llms-txt#...-can-add-custom-routes-if-needed.

**Contents:**
- Configure `langgraph.json`
- Start server
- Deploying
- Next steps

json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
bash  theme={null}
langgraph dev --no-browser
```

You should see your startup message printed when the server starts, and your cleanup message when you stop it with `Ctrl+C`.

You can deploy your app as-is to cloud or to your self-hosted platform.

Now that you've added lifespan events to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or [custom middleware](/langsmith/custom-middleware) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-lifespan.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## we can add them as nodes directly.

**URL:** llms-txt#we-can-add-them-as-nodes-directly.

**Contents:**
- Evaluations
  - Final response evaluator

graph_builder.add_node("refund_agent", refund_graph)
graph_builder.add_node("question_answering_agent", qa_graph)
graph_builder.add_node(compile_followup)

graph_builder.set_entry_point("intent_classifier")
graph_builder.add_edge("refund_agent", "compile_followup")
graph_builder.add_edge("question_answering_agent", "compile_followup")
graph_builder.add_edge("compile_followup", END)

graph = graph_builder.compile()
python  theme={null}
display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
state = await graph.ainvoke(
    {"messages": [{"role": "user", "content": "what james brown songs do you have"}]}
)
print(state["followup"])

I found 20 James Brown songs in the database, all from the album "Sex Machine". Here they are: ...
python  theme={null}
state = await graph.ainvoke({"messages": [
    {
        "role": "user",
        "content": "my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i'd like refunded",
    }
]})
print(state["followup"])

Which of the following purchases would you like to be refunded for? ...
python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our compiled parent graph including all of its subgraphs:
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=619f9b540ea69b1662b2a599ce78241b" alt="graph" data-og-width="646" width="646" data-og-height="680" height="680" data-path="langsmith/images/agent-tutorial-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=227790d90780a4c56233650b957130df 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=30ae6a9b1bc367152a57d4a0c3e41af7 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=37f29b6e783cf2a80714c29ab0be3c5f 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=423ad48e0266ac257b6d76962697b45d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=581821d6b377b98108507712d6b08c51 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=126ef194ff5042f691c8f52cf3a1cb75 2500w" />

#### Try it out

Let's give our custom support agent a whirl!
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## initialize the langsmith client with the anonymization functions

**URL:** llms-txt#initialize-the-langsmith-client-with-the-anonymization-functions

langsmith_client = Client(
  hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize
)

---

## Upload files with traces

**URL:** llms-txt#upload-files-with-traces

**Contents:**
  - Python

Source: https://docs.langchain.com/langsmith/upload-files-with-traces

<Check>
  Before diving into this content, it would be helpful to read the following guides:

* [Trace with LangSmith using the traceable decorator or wrapper](/langsmith/annotate-code#use-traceable--traceable)
</Check>

<Note>
  The following features are available in the following SDK versions:

* Python SDK: >=0.1.141
  * JS/TS SDK: >=0.2.5
</Note>

LangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.

In both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the `Attachment` type in Python and `Uint8Array` / `ArrayBuffer` in TypeScript.

In the Python SDK, you can use the `Attachment` type to add files to your traces. Each `Attachment` requires:

* `mime_type` (str): The MIME type of the file (e.g., `"image/png"`).
* `data` (bytes | Path): The binary content of the file, or the file path.

You can also define an attachment with a tuple tuple of the form `(mime_type, data)` for convenience.

Simply decorate a function with `@traceable` and include your `Attachment` instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the `dangerously_allow_filesystem` flag to `True` in your traceable decorator.

```python Python theme={null}
from langsmith import traceable
from langsmith.schemas import Attachment
from pathlib import Path
import os

---

## How to return categorical vs numerical metrics

**URL:** llms-txt#how-to-return-categorical-vs-numerical-metrics

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/metric-type

LangSmith supports both categorical and numerical metrics, and you can return either when writing a custom evaluator.

For an evaluator result to be logged as a numerical metric, it must returned as:

* (Python only) an `int`, `float`, or `bool`
* a dict of the form `{"key": "metric_name", "score": int | float | bool}`

For an evaluator result to be logged as a categorical metric, it must be returned as:

* (Python only) a `str`
* a dict of the form `{"key": "metric_name", "value": str | int | float | bool}`

Here are some examples:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/metric-type.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__delete

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__delete

dataset_id = dataset.id
params = { "dataset": dataset_id }

resp = requests.get(
    "https://api.smith.langchain.com/api/v1/examples",
    params=params,
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

examples = resp.json()
python  theme={null}
os.environ["OPENAI_API_KEY"] = "sk-..."

def run_completion_on_example(example, model_name, experiment_id):
    """Run completions on a list of examples."""
    # We are using the OpenAI API here, but you can use any model you like

def _post_run(run_id, name, run_type, inputs, parent_id=None):
        """Function to post a new run to the API."""
        data = {
            "id": run_id.hex,
            "name": name,
            "run_type": run_type,
            "inputs": inputs,
            "start_time": datetime.utcnow().isoformat(),
            "reference_example_id": example["id"],
            "session_id": experiment_id,
        }
        if parent_id:
            data["parent_run_id"] = parent_id.hex
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/runs", # Update appropriately for self-hosted installations or the EU region
            json=data,
            headers=headers
        )
        resp.raise_for_status()

def _patch_run(run_id, outputs):
        """Function to patch a run with outputs."""
        resp = requests.patch(
            f"https://api.smith.langchain.com/api/v1/runs/{run_id}",
            json={
                "outputs": outputs,
                "end_time": datetime.utcnow().isoformat(),
            },
            headers=headers,
        )
        resp.raise_for_status()

# Send your API Key in the request headers
    headers = {"x-api-key": os.environ["LANGSMITH_API_KEY"]}

text = example["inputs"]["text"]

messages = [
        {
            "role": "system",
            "content": "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
        },
        {"role": "user", "content": text},
    ]

# Create parent run
    parent_run_id = uuid4()
    _post_run(parent_run_id, "LLM Pipeline", "chain", {"text": text})

# Create child run
    child_run_id = uuid4()
    _post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

# Generate completion
    chat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)
    output_text = chat_completion.choices[0].message.content

# End run
    _patch_run(child_run_id, {
    "messages": messages,
        "output": output_text,
        "model": model_name
    })

_patch_run(parent_run_id, {"label": output_text})
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Next, we'll define a method that will create a run for a single example.
```

Example 2 (unknown):
```unknown
We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini.
```

---

## View server logs for a trace

**URL:** llms-txt#view-server-logs-for-a-trace

**Contents:**
- Access server logs from trace view
- Server logs view
- Filtering logs by trace ID

Source: https://docs.langchain.com/langsmith/platform-logs

When viewing a trace that was generated by a run in LangSmith, you can access the associated server logs directly from the trace view.

<Note>
  Viewing server logs for a trace only works with the [Cloud SaaS](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#cloud-saas) and [fully self-hosted](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#self-hosted-control-plane) deployment options.
</Note>

## Access server logs from trace view

In the trace view, use the **See Logs** button in the top right corner, next to the **Run in Studio** button.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=87e85d7f38169b5259ba9b335aa89d2f" alt="" data-og-width="1595" width="1595" data-og-height="821" height="821" data-path="langsmith/images/view-server-logs-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d70119d610bea0b05196653a2ab13cdd 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b395732952d78a269e9fa776b102465 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b69563779a026c19efed909fc1fc8cd3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=13b31e6677cc41c18f5f7d6826489423 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ea7803aa4ee13332c14809b953bb03a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f260eb50eae4ee0e5c094391fa180f5c 2500w" />

Clicking this button will take you to the server logs view for the associated deployment in LangSmith.

The server logs view displays logs from both:

* **LangGraph Server's own operational logs** - Internal server operations, API calls, and system events
* **User application logs** - Logs written in your graph with:
  * Python: Use the `logging` or `structlog` libraries
  * JavaScript: Use the re-exported Winston logger from `@langchain/langgraph-sdk/logging`:

## Filtering logs by trace ID

When you navigate from the trace view, the **Filters** box will automatically pre-fill with the Trace ID from the trace you just viewed.

This allows you to quickly filter the logs to see only those related to your specific trace execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb6f3c15ca3c8d462ee10c5fd190c73e" alt="" data-og-width="1348" width="1348" data-og-height="681" height="681" data-path="langsmith/images/lgp-server-logs-filters.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8eecae6d552d9e58b4b59949afe5cdf4 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=543817c9df78b3fa98d063956189ea6f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16989edf602b9f025464c0cd7b83bfa5 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f109fbba34ebd01b856baf6be0d4d18 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d456efb3eb004361f2d2a8c7c9d99e34 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb86476afe8c3f573bdb3a80d51075aa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/platform-logs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Hugging Face

**URL:** llms-txt#hugging-face

**Contents:**
- Chat models
  - ChatHuggingFace
- LLMs
  - HuggingFaceEndpoint
  - HuggingFacePipeline
- Embedding Models
  - HuggingFaceEmbeddings
  - HuggingFaceEndpointEmbeddings
  - HuggingFaceInferenceAPIEmbeddings
  - HuggingFaceInstructEmbeddings

Source: https://docs.langchain.com/oss/python/integrations/providers/huggingface

All LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index).

We can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.

See a [usage example](/oss/python/integrations/chat/huggingface).

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).

### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInferenceAPIEmbeddings

We can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via [Inference Providers](https://huggingface.co/docs/inference-providers).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInstructEmbeddings

We can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/instruct_embeddings).

### HuggingFaceBgeEmbeddings

> [BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).
> BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.

See a [usage example](/oss/python/integrations/text_embedding/bge_huggingface).

### Hugging Face dataset

> [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000
> [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages
> that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.
> They used for a diverse range of tasks such as translation, automatic speech
> recognition, and image classification.

We need to install `datasets` python package.

See a [usage example](/oss/python/integrations/document_loaders/hugging_face_dataset).

### Hugging Face model loader

> Load model information from `Hugging Face Hub`, including README content.
>
> This loader interfaces with the `Hugging Face Models API` to fetch
> and load model metadata and README files.
> The API allows you to search and filter models based on
> specific criteria such as model tags, authors, and more.

It uses the Hugging Face models to generate image captions.

We need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/image_captions).

### Hugging Face Hub Tools

> [Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools)
> support text I/O and are loaded using the `load_huggingface_tool` function.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/huggingface_tools).

### Hugging Face Text-to-Speech Model Inference.

> It is a wrapper around `OpenAI Text-to-Speech API`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/huggingface.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## LLMs

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).
```

Example 2 (unknown):
```unknown
### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).
```

Example 3 (unknown):
```unknown
## Embedding Models

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

Example 4 (unknown):
```unknown
### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

---

## >                'action_name': 'execute_sql',

**URL:** llms-txt#>----------------'action_name':-'execute_sql',

---

## Philosophy

**URL:** llms-txt#philosophy

**Contents:**
- History

Source: https://docs.langchain.com/oss/python/langchain/philosophy

LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.

LangChain is driven by a few core beliefs:

* Large Language Models (LLMs) are great, powerful new technology.
* LLMs are even better when you combine them with external sources of data.
* LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.
* It is still very early on in that transformation.
* While it's easy to build a prototype of those agentic applications, it's still really hard to build agents that are reliable enough to put into production.

With LangChain, we have two core focuses:

<Steps>
  <Step title="We want to enable developers to build with the best models.">
    Different providers expose different APIs, with different model parameters and different message formats.
    Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.
  </Step>

<Step title="We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.">
    Models should be used for more than just *text generation* - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define [tools](/oss/python/langchain/tools) that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.
  </Step>
</Steps>

Given the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:

<Update label="2022-10-24" description="v0.0.1">
  A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:

* LLM abstractions
  * "Chains", or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.

The name LangChain comes from "Language" (like Language models) and "Chains".
</Update>

<Update label="2022-12">
  The first general purpose agents were added to LangChain.

These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.
</Update>

<Update label="2023-01">
  OpenAI releases a 'Chat Completion' API.

Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.
</Update>

<Update label="2023-01">
  LangChain releases a JavaScript version.

LLMs and agents will change how applications are built and JavaScript is the language of application developers.
</Update>

<Update label="2023-02">
  **LangChain Inc. was formed as a company** around the open source LangChain project.

The main goal was to "make intelligent agents ubiquitous". The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.
</Update>

<Update label="2023-03">
  OpenAI releases 'function calling' in their API.

This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).
</Update>

<Update label="2023-06">
  **LangSmith is released** as closed source platform by LangChain Inc., providing observability and evals

The main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.
</Update>

<Update label="2024-01" description="v0.1.0">
  **LangChain releases 0.1.0**, its first non-0.0.x.

The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.
</Update>

<Update label="2024-02">
  **LangGraph is released** as an open-source library.

The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.

When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.
</Update>

<Update label="2024-06">
  **LangChain has over 700 integrations.**

Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.
</Update>

<Update label="2024-10">
  LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.

As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.
</Update>

<Update label="2025-04">
  Model APIs become more multimodal.

Models started to accept files, images, videos, and more. We updated the `langchain-core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.
</Update>

<Update label="2025-10-20" description="v1.0.0">
  **LangChain releases 1.0** with two major changes:

1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.

For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `langchain-classic` package.

2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.
</Update>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/philosophy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## To approve

**URL:** llms-txt#to-approve

graph.invoke(Command(resume=True), config=config)

---

## Get the API response and extract customer information

**URL:** llms-txt#get-the-api-response-and-extract-customer-information

export LANGSMITH_URL="<your_langsmith_url>"
response=$(curl -s $LANGSMITH_URL/api/v1/info)

---

## Matches the "thread" resource and all actions - create, read, update, delete, search

**URL:** llms-txt#matches-the-"thread"-resource-and-all-actions---create,-read,-update,-delete,-search

---

## Authentication methods

**URL:** llms-txt#authentication-methods

**Contents:**
- Cloud
  - Email/Password
  - Social Providers
  - SAML SSO
- Self-Hosted
  - SSO with OAuth 2.0 and OIDC
  - Email/Password a.k.a. basic auth
  - None

Source: https://docs.langchain.com/langsmith/authentication-methods

LangSmith supports multiple authentication methods for easy sign-up and login.

Users can use an email address and password to sign up and login to LangSmith.

Users can alternatively use their credentials from GitHub or Google.

Enterprise customers can configure [SAML SSO](/langsmith/user-management) and [SCIM](/langsmith/user-management)

Self-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see [the self-hosting docs](/langsmith/self-hosted) and [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith).

### SSO with OAuth 2.0 and OIDC

Production installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the [SSO configuration guide](/langsmith/self-host-sso)

### Email/Password a.k.a. basic auth

This auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the [basic auth configuration guide](/langsmith/self-host-basic-auth)

<Warning>
  This authentication mode will be removed after the launch of Basic Auth.
</Warning>

If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up. This configuration should only be used for verifying installation at the infrastructure level, as the feature set supported in this mode is restricted with only a single organization and workspace.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/authentication-methods.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Only keep post title, headers, and content from the full HTML.

**URL:** llms-txt#only-keep-post-title,-headers,-and-content-from-the-full-html.

**Contents:**
  - Splitting documents
  - Storing documents
- 2. Retrieval and Generation
  - RAG agents

bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")
output  theme={null}
Total characters: 43131
python  theme={null}
print(docs[0].page_content[:500])
output  theme={null}
      LLM Powered Autonomous Agents

Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng

Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview#
In
python  theme={null}
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")
output  theme={null}
Split blog post into 66 sub-documents.
python  theme={null}
document_ids = vector_store.add_documents(documents=all_splits)

print(document_ids[:3])
output  theme={null}
['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']
python  theme={null}
from langchain.tools import tool

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
python  theme={null}
  from typing import Literal

def retrieve_context(query: str, section: Literal["beginning", "middle", "end"]):
  python  theme={null}
from langchain.agents import create_agent

tools = [retrieve_context]

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
**Go deeper**

`DocumentLoader`: Object that loads data from a source as list of `Documents`.

* [Integrations](/oss/python/integrations/document_loaders/): 160+ integrations to choose from.
* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.

### Splitting documents

Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.

To handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.

As in the [semantic search tutorial](/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.
```

---

## How to add custom lifespan events

**URL:** llms-txt#how-to-add-custom-lifespan-events

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-lifespan

When deploying agents to LangSmith, you often need to initialize resources like database connections when your server starts up, and ensure they're properly closed when it shuts down. Lifespan events let you hook into your server's startup and shutdown sequence to handle these critical setup and teardown tasks.

This works the same way as [adding custom routes](/langsmith/custom-routes). You just need to provide your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps).

Below is an example using FastAPI.

<Note>
  "Python only"
  We currently only support custom lifespan events in Python deployments with `langgraph-api>=0.0.26`.
</Note>

Starting from an **existing** LangSmith application, add the following lifespan code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={19}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## All integration providers

**URL:** llms-txt#all-integration-providers

**Contents:**
- Providers

Source: https://docs.langchain.com/oss/python/integrations/providers/all_providers

Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Abso" href="/oss/python/integrations/providers/abso" icon="link">
    Custom AI integration platform for enterprise workflows.
  </Card>

<Card title="Acreom" href="/oss/python/integrations/providers/acreom" icon="link">
    Knowledge management platform with AI-powered organization.
  </Card>

<Card title="ActiveLoop DeepLake" href="/oss/python/integrations/providers/activeloop_deeplake" icon="link">
    Vector database for AI applications with deep learning focus.
  </Card>

<Card title="Ads4GPTs" href="/oss/python/integrations/providers/ads4gpts" icon="link">
    Advertising platform for GPT applications and AI services.
  </Card>

<Card title="AgentQL" href="/oss/python/integrations/providers/agentql" icon="link">
    Web scraping with natural language queries.
  </Card>

<Card title="AI21" href="/oss/python/integrations/providers/ai21" icon="link">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="AIM Tracking" href="/oss/python/integrations/providers/aim_tracking" icon="link">
    Experiment tracking and management platform.
  </Card>

<Card title="AI/ML API" href="/oss/python/integrations/providers/aimlapi" icon="link">
    Unified API for multiple AI and ML services.
  </Card>

<Card title="AI Network" href="/oss/python/integrations/providers/ainetwork" icon="link">
    Decentralized AI computing network platform.
  </Card>

<Card title="Airbyte" href="/oss/python/integrations/providers/airbyte" icon="link">
    Data integration platform for ETL and ELT pipelines.
  </Card>

<Card title="Airtable" href="/oss/python/integrations/providers/airtable" icon="link">
    Cloud-based spreadsheet and database platform.
  </Card>

<Card title="Alchemy" href="/oss/python/integrations/providers/alchemy" icon="link">
    Blockchain development platform and APIs.
  </Card>

<Card title="Aleph Alpha" href="/oss/python/integrations/providers/aleph_alpha" icon="link">
    European AI company's multilingual language models.
  </Card>

<Card title="Alibaba Cloud" href="/oss/python/integrations/providers/alibaba_cloud" icon="link">
    Alibaba's cloud computing and AI services.
  </Card>

<Card title="AnalyticDB" href="/oss/python/integrations/providers/analyticdb" icon="link">
    Alibaba Cloud's real-time analytics database.
  </Card>

<Card title="Anchor Browser" href="/oss/python/integrations/providers/anchor_browser" icon="link">
    Browser automation and web scraping tools.
  </Card>

<Card title="Annoy" href="/oss/python/integrations/providers/annoy" icon="link">
    Approximate nearest neighbors search library.
  </Card>

<Card title="Anthropic" href="/oss/python/integrations/providers/anthropic" icon="anthropic">
    Claude models for advanced reasoning and conversation.
  </Card>

<Card title="Anyscale" href="/oss/python/integrations/providers/anyscale" icon="link">
    Distributed computing platform for ML workloads.
  </Card>

<Card title="Apache Doris" href="/oss/python/integrations/providers/apache_doris" icon="link">
    Real-time analytical database management system.
  </Card>

<Card title="Apache" href="/oss/python/integrations/providers/apache" icon="link">
    Apache Software Foundation tools and libraries.
  </Card>

<Card title="Apify" href="/oss/python/integrations/providers/apify" icon="link">
    Web scraping and automation platform.
  </Card>

<Card title="Apple" href="/oss/python/integrations/providers/apple" icon="link">
    Apple's machine learning and AI frameworks.
  </Card>

<Card title="ArangoDB" href="/oss/python/integrations/providers/arangodb" icon="link">
    Multi-model database with graph capabilities.
  </Card>

<Card title="Arcee" href="/oss/python/integrations/providers/arcee" icon="link">
    Domain-specific language model training platform.
  </Card>

<Card title="ArcGIS" href="/oss/python/integrations/providers/arcgis" icon="link">
    Geographic information system platform.
  </Card>

<Card title="Argilla" href="/oss/python/integrations/providers/argilla" icon="link">
    Data labeling and annotation platform for NLP.
  </Card>

<Card title="Arize" href="/oss/python/integrations/providers/arize" icon="link">
    ML observability and performance monitoring.
  </Card>

<Card title="Arthur Tracking" href="/oss/python/integrations/providers/arthur_tracking" icon="link">
    AI model monitoring and governance platform.
  </Card>

<Card title="arXiv" href="/oss/python/integrations/providers/arxiv" icon="link">
    Academic paper repository and search platform.
  </Card>

<Card title="Ascend" href="/oss/python/integrations/providers/ascend" icon="link">
    Data engineering and pipeline automation platform.
  </Card>

<Card title="Ask News" href="/oss/python/integrations/providers/asknews" icon="link">
    Real-time news search and analysis API.
  </Card>

<Card title="AssemblyAI" href="/oss/python/integrations/providers/assemblyai" icon="link">
    Speech-to-text and audio intelligence API.
  </Card>

<Card title="AstraDB" href="/oss/python/integrations/providers/astradb" icon="link">
    DataStax Astra DB vector database platform.
  </Card>

<Card title="Atlas" href="/oss/python/integrations/providers/atlas" icon="link">
    Data visualization and exploration platform.
  </Card>

<Card title="AwaDB" href="/oss/python/integrations/providers/awadb" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="AWS" href="/oss/python/integrations/providers/aws" icon="aws">
    Amazon Web Services cloud platform and AI services.
  </Card>

<Card title="AZLyrics" href="/oss/python/integrations/providers/azlyrics" icon="link">
    Song lyrics database and search platform.
  </Card>

<Card title="Azure AI" href="/oss/python/integrations/providers/azure_ai" icon="microsoft">
    Microsoft Azure AI and cognitive services.
  </Card>

<Card title="BAAI" href="/oss/python/integrations/providers/baai" icon="link">
    Beijing Academy of AI research and models.
  </Card>

<Card title="Bagel" href="/oss/python/integrations/providers/bagel" icon="link">
    Vector database and semantic search platform.
  </Card>

<Card title="BagelDB" href="/oss/python/integrations/providers/bageldb" icon="link">
    Multi-modal AI database and storage system.
  </Card>

<Card title="Baichuan" href="/oss/python/integrations/providers/baichuan" icon="link">
    Chinese language model from Baichuan AI.
  </Card>

<Card title="Baidu" href="/oss/python/integrations/providers/baidu" icon="link">
    Baidu's AI services and language models.
  </Card>

<Card title="BananaDev" href="/oss/python/integrations/providers/bananadev" icon="link">
    Serverless GPU infrastructure for ML models.
  </Card>

<Card title="Baseten" href="/oss/python/integrations/providers/baseten" icon="link">
    ML model deployment and serving platform.
  </Card>

<Card title="Beam" href="/oss/python/integrations/providers/beam" icon="link">
    Serverless GPU computing platform.
  </Card>

<Card title="Beautiful Soup" href="/oss/python/integrations/providers/beautiful_soup" icon="link">
    HTML and XML parsing library for web scraping.
  </Card>

<Card title="BibTeX" href="/oss/python/integrations/providers/bibtex" icon="link">
    Bibliography management and citation format.
  </Card>

<Card title="Bilibili" href="/oss/python/integrations/providers/bilibili" icon="link">
    Chinese video sharing platform integration.
  </Card>

<Card title="Bittensor" href="/oss/python/integrations/providers/bittensor" icon="link">
    Decentralized AI network and incentive protocol.
  </Card>

<Card title="Blackboard" href="/oss/python/integrations/providers/blackboard" icon="link">
    Educational technology and learning management.
  </Card>

<Card title="Bodo DataFrames" href="/oss/python/integrations/providers/bodo" icon="link">
    High-performance analytics and data processing.
  </Card>

<Card title="BookendAI" href="/oss/python/integrations/providers/bookendai" icon="link">
    AI-powered reading and research assistant.
  </Card>

<Card title="Box" href="/oss/python/integrations/providers/box" icon="link">
    Cloud content management and collaboration.
  </Card>

<Card title="Brave Search" href="/oss/python/integrations/providers/brave_search" icon="link">
    Privacy-focused search engine API.
  </Card>

<Card title="Breebs" href="/oss/python/integrations/providers/breebs" icon="link">
    AI knowledge management and retrieval platform.
  </Card>

<Card title="Brightdata" href="/oss/python/integrations/providers/brightdata" icon="link">
    Web data platform and proxy services.
  </Card>

<Card title="Browserbase" href="/oss/python/integrations/providers/browserbase" icon="link">
    Headless browser automation platform.
  </Card>

<Card title="Browserless" href="/oss/python/integrations/providers/browserless" icon="link">
    Serverless browser automation service.
  </Card>

<Card title="ByteDance" href="/oss/python/integrations/providers/byte_dance" icon="link">
    ByteDance's AI models and services.
  </Card>

<Card title="Cassandra" href="/oss/python/integrations/providers/cassandra" icon="link">
    Distributed NoSQL database management system.
  </Card>

<Card title="Cerebras" href="/oss/python/integrations/providers/cerebras" icon="link">
    AI compute platform with specialized processors.
  </Card>

<Card title="CerebriumAI" href="/oss/python/integrations/providers/cerebriumai" icon="link">
    Serverless GPU platform for AI applications.
  </Card>

<Card title="Chaindesk" href="/oss/python/integrations/providers/chaindesk" icon="link">
    No-code AI chatbot and automation platform.
  </Card>

<Card title="Chroma" href="/oss/python/integrations/providers/chroma" icon="link">
    Open-source embedding database for AI apps.
  </Card>

<Card title="Clarifai" href="/oss/python/integrations/providers/clarifai" icon="link">
    Computer vision and AI model platform.
  </Card>

<Card title="ClearML Tracking" href="/oss/python/integrations/providers/clearml_tracking" icon="link">
    ML experiment tracking and automation.
  </Card>

<Card title="ClickHouse" href="/oss/python/integrations/providers/clickhouse" icon="link">
    Fast columnar database for analytics.
  </Card>

<Card title="ClickUp" href="/oss/python/integrations/providers/clickup" icon="link">
    Project management and productivity platform.
  </Card>

<Card title="Cloudflare" href="/oss/python/integrations/providers/cloudflare" icon="link">
    Web infrastructure and security services.
  </Card>

<Card title="Clova" href="/oss/python/integrations/providers/clova" icon="link">
    Naver's AI assistant and NLP platform.
  </Card>

<Card title="CnosDB" href="/oss/python/integrations/providers/cnosdb" icon="link">
    Time series database for IoT and analytics.
  </Card>

<Card title="Cognee" href="/oss/python/integrations/providers/cognee" icon="link">
    Memory layer for AI applications and agents.
  </Card>

<Card title="CogniSwitch" href="/oss/python/integrations/providers/cogniswitch" icon="link">
    AI knowledge management and retrieval system.
  </Card>

<Card title="Cohere" href="/oss/python/integrations/providers/cohere" icon="cohere">
    Language AI platform for enterprise applications.
  </Card>

<Card title="College Confidential" href="/oss/python/integrations/providers/college_confidential" icon="link">
    College admissions and education platform.
  </Card>

<Card title="Comet Tracking" href="/oss/python/integrations/providers/comet_tracking" icon="link">
    ML experiment tracking and model management.
  </Card>

<Card title="Confident" href="/oss/python/integrations/providers/confident" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Confluence" href="/oss/python/integrations/providers/confluence" icon="link">
    Team collaboration and documentation platform.
  </Card>

<Card title="Connery" href="/oss/python/integrations/providers/connery" icon="link">
    Plugin system for AI agents and applications.
  </Card>

<Card title="Context" href="/oss/python/integrations/providers/context" icon="link">
    Context management for AI applications.
  </Card>

<Card title="Contextual" href="/oss/python/integrations/providers/contextual" icon="link">
    Contextual AI and language understanding.
  </Card>

<Card title="Couchbase" href="/oss/python/integrations/providers/couchbase" icon="link">
    NoSQL cloud database platform.
  </Card>

<Card title="Coze" href="/oss/python/integrations/providers/coze" icon="link">
    Conversational AI platform and chatbot builder.
  </Card>

<Card title="CrateDB" href="/oss/python/integrations/providers/cratedb" icon="link">
    Distributed SQL database for machine data.
  </Card>

<Card title="CTransformers" href="/oss/python/integrations/providers/ctransformers" icon="link">
    Python bindings for transformer models in C/C++.
  </Card>

<Card title="CTranslate2" href="/oss/python/integrations/providers/ctranslate2" icon="link">
    Fast inference engine for Transformer models.
  </Card>

<Card title="Cube" href="/oss/python/integrations/providers/cube" icon="link">
    Semantic layer for building data applications.
  </Card>

<Card title="Dappier" href="/oss/python/integrations/providers/dappier" icon="link">
    Real-time AI data platform and API.
  </Card>

<Card title="DashVector" href="/oss/python/integrations/providers/dashvector" icon="link">
    Alibaba Cloud's vector database service.
  </Card>

<Card title="Databricks" href="/oss/python/integrations/providers/databricks" icon="link">
    Unified analytics platform for big data and ML.
  </Card>

<Card title="Datadog" href="/oss/python/integrations/providers/datadog" icon="link">
    Monitoring and analytics platform for applications.
  </Card>

<Card title="Datadog Logs" href="/oss/python/integrations/providers/datadog_logs" icon="link">
    Log management and analysis platform.
  </Card>

<Card title="DataForSEO" href="/oss/python/integrations/providers/dataforseo" icon="link">
    SEO and SERP data API platform.
  </Card>

<Card title="DataHerald" href="/oss/python/integrations/providers/dataherald" icon="link">
    Natural language to SQL query platform.
  </Card>

<Card title="Dedoc" href="/oss/python/integrations/providers/dedoc" icon="link">
    Document analysis and structure detection.
  </Card>

<Card title="DeepInfra" href="/oss/python/integrations/providers/deepinfra" icon="link">
    Serverless inference for deep learning models.
  </Card>

<Card title="DeepLake" href="/oss/python/integrations/providers/deeplake" icon="link">
    Vector database for deep learning applications.
  </Card>

<Card title="DeepSeek" href="/oss/python/integrations/providers/deepseek" icon="link">
    Advanced reasoning and coding AI models.
  </Card>

<Card title="DeepSparse" href="/oss/python/integrations/providers/deepsparse" icon="link">
    Inference runtime for sparse neural networks.
  </Card>

<Card title="Dell" href="/oss/python/integrations/providers/dell" icon="link">
    Dell Technologies AI and computing solutions.
  </Card>

<Card title="Diffbot" href="/oss/python/integrations/providers/diffbot" icon="link">
    Web data extraction and knowledge graph.
  </Card>

<Card title="Dingo" href="/oss/python/integrations/providers/dingo" icon="link">
    Distributed vector database system.
  </Card>

<Card title="Discord" href="/oss/python/integrations/providers/discord" icon="link">
    Communication platform integration and bots.
  </Card>

<Card title="Discord Shikenso" href="/oss/python/integrations/providers/discord-shikenso" icon="link">
    Discord analytics and moderation tools.
  </Card>

<Card title="DocArray" href="/oss/python/integrations/providers/docarray" icon="link">
    Data structure for multimodal AI applications.
  </Card>

<Card title="Docling" href="/oss/python/integrations/providers/docling" icon="link">
    Document processing and AI integration.
  </Card>

<Card title="Doctran" href="/oss/python/integrations/providers/doctran" icon="link">
    Document transformation and processing.
  </Card>

<Card title="Docugami" href="/oss/python/integrations/providers/docugami" icon="link">
    Document AI and semantic processing.
  </Card>

<Card title="Docusaurus" href="/oss/python/integrations/providers/docusaurus" icon="link">
    Documentation website generator and platform.
  </Card>

<Card title="Dria" href="/oss/python/integrations/providers/dria" icon="link">
    Decentralized knowledge retrieval network.
  </Card>

<Card title="Dropbox" href="/oss/python/integrations/providers/dropbox" icon="link">
    Cloud storage and file sharing platform.
  </Card>

<Card title="DuckDB" href="/oss/python/integrations/providers/duckdb" icon="link">
    In-process SQL OLAP database management system.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/python/integrations/providers/duckduckgo_search" icon="link">
    Privacy-focused search engine integration.
  </Card>

<Card title="E2B" href="/oss/python/integrations/providers/e2b" icon="link">
    Cloud development environment platform.
  </Card>

<Card title="EdenAI" href="/oss/python/integrations/providers/edenai" icon="link">
    Unified API for multiple AI services.
  </Card>

<Card title="Elasticsearch" href="/oss/python/integrations/providers/elasticsearch" icon="link">
    Distributed search and analytics engine.
  </Card>

<Card title="ElevenLabs" href="/oss/python/integrations/providers/elevenlabs" icon="link">
    AI voice synthesis and speech platform.
  </Card>

<Card title="EmbedChain" href="/oss/python/integrations/providers/embedchain" icon="link">
    Framework for creating RAG applications.
  </Card>

<Card title="Epsilla" href="/oss/python/integrations/providers/epsilla" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="Etherscan" href="/oss/python/integrations/providers/etherscan" icon="link">
    Ethereum blockchain explorer and analytics.
  </Card>

<Card title="EverlyAI" href="/oss/python/integrations/providers/everlyai" icon="link">
    Serverless AI inference platform.
  </Card>

<Card title="Evernote" href="/oss/python/integrations/providers/evernote" icon="link">
    Note-taking and organization platform.
  </Card>

<Card title="Exa Search" href="/oss/python/integrations/providers/exa_search" icon="link">
    AI-powered search engine for developers.
  </Card>

<Card title="Facebook" href="/oss/python/integrations/providers/facebook" icon="link">
    Meta's social platform integration and APIs.
  </Card>

<Card title="FalkorDB" href="/oss/python/integrations/providers/falkordb" icon="link">
    Graph database with ultra-low latency.
  </Card>

<Card title="Fauna" href="/oss/python/integrations/providers/fauna" icon="link">
    Serverless, globally distributed database.
  </Card>

<Card title="Featherless AI" href="/oss/python/integrations/providers/featherless-ai" icon="link">
    Fast and efficient AI model serving.
  </Card>

<Card title="Fiddler" href="/oss/python/integrations/providers/fiddler" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Figma" href="/oss/python/integrations/providers/figma" icon="link">
    Design collaboration and prototyping platform.
  </Card>

<Card title="FireCrawl" href="/oss/python/integrations/providers/firecrawl" icon="link">
    Web scraping and crawling API service.
  </Card>

<Card title="Fireworks" href="/oss/python/integrations/providers/fireworks" icon="link">
    Fast inference platform for open-source models.
  </Card>

<Card title="Flyte" href="/oss/python/integrations/providers/flyte" icon="link">
    Workflow orchestration for ML and data processing.
  </Card>

<Card title="FMP Data" href="/oss/python/integrations/providers/fmp-data" icon="link">
    Financial market data and analytics API.
  </Card>

<Card title="ForefrontAI" href="/oss/python/integrations/providers/forefrontai" icon="link">
    Fine-tuning platform for language models.
  </Card>

<Card title="Friendli" href="/oss/python/integrations/providers/friendli" icon="link">
    Optimized serving engine for AI models.
  </Card>

<Card title="Galaxia" href="/oss/python/integrations/providers/galaxia" icon="link">
    Prompt-driven engineering assistant.
  </Card>

<Card title="Gel" href="/oss/python/integrations/providers/gel" icon="link">
    Knowledge extraction and NLP platform.
  </Card>

<Card title="GeoPandas" href="/oss/python/integrations/providers/geopandas" icon="link">
    Geographic data analysis with Python.
  </Card>

<Card title="Git" href="/oss/python/integrations/providers/git" icon="link">
    Version control system integration.
  </Card>

<Card title="GitBook" href="/oss/python/integrations/providers/gitbook" icon="link">
    Documentation platform and knowledge base.
  </Card>

<Card title="GitHub" href="/oss/python/integrations/providers/github" icon="link">
    Code hosting and collaboration platform.
  </Card>

<Card title="GitLab" href="/oss/python/integrations/providers/gitlab" icon="link">
    DevOps platform and code repository.
  </Card>

<Card title="GOAT" href="/oss/python/integrations/providers/goat" icon="link">
    Tool use framework for AI agents.
  </Card>

<Card title="Golden" href="/oss/python/integrations/providers/golden" icon="link">
    Knowledge graph and data platform.
  </Card>

<Card title="Goodfire" href="/oss/python/integrations/providers/goodfire" icon="link">
    Interpretable AI and model analysis.
  </Card>

<Card title="Google" href="/oss/python/integrations/providers/google" icon="google">
    Google's AI services and cloud platform.
  </Card>

<Card title="Google Serper" href="/oss/python/integrations/providers/google_serper" icon="google">
    Google Search API service.
  </Card>

<Card title="GooseAI" href="/oss/python/integrations/providers/gooseai" icon="link">
    Fully managed NLP-as-a-Service platform.
  </Card>

<Card title="GPT4All" href="/oss/python/integrations/providers/gpt4all" icon="link">
    Open-source LLM ecosystem for local deployment.
  </Card>

<Card title="Gradient" href="/oss/python/integrations/providers/gradient" icon="link">
    AI model training and deployment platform.
  </Card>

<Card title="GradientAI" href="/oss/python/integrations/providers/gradientai" icon="link">
    Private AI model training platform.
  </Card>

<Card title="Graph RAG" href="/oss/python/integrations/providers/graph_rag" icon="link">
    Graph-based retrieval augmented generation.
  </Card>

<Card title="GraphSignal" href="/oss/python/integrations/providers/graphsignal" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="GreenNode" href="/oss/python/integrations/providers/greennode" icon="link">
    Sustainable AI computing platform.
  </Card>

<Card title="GROBID" href="/oss/python/integrations/providers/grobid" icon="link">
    Machine learning library for bibliographic data.
  </Card>

<Card title="Groq" href="/oss/python/integrations/providers/groq" icon="link">
    Ultra-fast inference with specialized hardware.
  </Card>

<Card title="Gutenberg" href="/oss/python/integrations/providers/gutenberg" icon="link">
    Project Gutenberg digital library access.
  </Card>

<Card title="Hacker News" href="/oss/python/integrations/providers/hacker_news" icon="link">
    Tech news and discussion platform.
  </Card>

<Card title="Hazy Research" href="/oss/python/integrations/providers/hazy_research" icon="link">
    Machine learning research and tools.
  </Card>

<Card title="Helicone" href="/oss/python/integrations/providers/helicone" icon="link">
    LLM observability and monitoring platform.
  </Card>

<Card title="Hologres" href="/oss/python/integrations/providers/hologres" icon="link">
    Real-time interactive analytics service.
  </Card>

<Card title="HTML2Text" href="/oss/python/integrations/providers/html2text" icon="link">
    HTML to plain text conversion utility.
  </Card>

<Card title="Huawei" href="/oss/python/integrations/providers/huawei" icon="link">
    Huawei Cloud AI services and models.
  </Card>

<Card title="Hugging Face" href="/oss/python/integrations/providers/huggingface" icon="link">
    Open platform for ML models and datasets.
  </Card>

<Card title="HyperBrowser" href="/oss/python/integrations/providers/hyperbrowser" icon="link">
    Web automation and scraping platform.
  </Card>

<Card title="IBM" href="/oss/python/integrations/providers/ibm" icon="link">
    IBM Watson AI and enterprise solutions.
  </Card>

<Card title="IEIT Systems" href="/oss/python/integrations/providers/ieit_systems" icon="link">
    Enterprise AI and system integration.
  </Card>

<Card title="iFixit" href="/oss/python/integrations/providers/ifixit" icon="link">
    Repair guides and technical documentation.
  </Card>

<Card title="iFlytek" href="/oss/python/integrations/providers/iflytek" icon="link">
    Chinese speech and language AI platform.
  </Card>

<Card title="IMSDb" href="/oss/python/integrations/providers/imsdb" icon="link">
    Internet Movie Script Database access.
  </Card>

<Card title="InfinispanVS" href="/oss/python/integrations/providers/infinispanvs" icon="link">
    Distributed cache and data grid platform.
  </Card>

<Card title="Infinity" href="/oss/python/integrations/providers/infinity" icon="link">
    High-performance embedding inference server.
  </Card>

<Card title="Infino" href="/oss/python/integrations/providers/infino" icon="link">
    Observability and monitoring platform.
  </Card>

<Card title="Intel" href="/oss/python/integrations/providers/intel" icon="link">
    Intel's AI optimization tools and libraries.
  </Card>

<Card title="IUGU" href="/oss/python/integrations/providers/iugu" icon="link">
    Brazilian payment processing platform.
  </Card>

<Card title="Jaguar" href="/oss/python/integrations/providers/jaguar" icon="link">
    Vector database and search platform.
  </Card>

<Card title="Javelin AI Gateway" href="/oss/python/integrations/providers/javelin_ai_gateway" icon="link">
    AI model gateway and management platform.
  </Card>

<Card title="Jenkins" href="/oss/python/integrations/providers/jenkins" icon="link">
    Automation server and CI/CD platform.
  </Card>

<Card title="Jina" href="/oss/python/integrations/providers/jina" icon="link">
    Neural search framework and cloud platform.
  </Card>

<Card title="John Snow Labs" href="/oss/python/integrations/providers/johnsnowlabs" icon="link">
    Enterprise NLP and healthcare AI platform.
  </Card>

<Card title="Joplin" href="/oss/python/integrations/providers/joplin" icon="link">
    Open-source note taking and organization.
  </Card>

<Card title="KDB.AI" href="/oss/python/integrations/providers/kdbai" icon="link">
    Time-series vector database platform.
  </Card>

<Card title="Kinetica" href="/oss/python/integrations/providers/kinetica" icon="link">
    Real-time analytics and database platform.
  </Card>

<Card title="KoboldAI" href="/oss/python/integrations/providers/koboldai" icon="link">
    Browser-based AI writing assistant.
  </Card>

<Card title="Konko" href="/oss/python/integrations/providers/konko" icon="link">
    Generative AI platform and model hosting.
  </Card>

<Card title="KoNLPy" href="/oss/python/integrations/providers/konlpy" icon="link">
    Korean natural language processing toolkit.
  </Card>

<Card title="Kuzu" href="/oss/python/integrations/providers/kuzu" icon="link">
    Embedded graph database management system.
  </Card>

<Card title="Label Studio" href="/oss/python/integrations/providers/labelstudio" icon="link">
    Data labeling and annotation platform.
  </Card>

<Card title="LakeFS" href="/oss/python/integrations/providers/lakefs" icon="link">
    Git-like version control for data lakes.
  </Card>

<Card title="LanceDB" href="/oss/python/integrations/providers/lancedb" icon="link">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LangChain Decorators" href="/oss/python/integrations/providers/langchain_decorators" icon="link">
    Syntactic sugar and utilities for LangChain.
  </Card>

<Card title="LangFair" href="/oss/python/integrations/providers/langfair" icon="link">
    Bias testing framework for language models.
  </Card>

<Card title="LangFuse" href="/oss/python/integrations/providers/langfuse" icon="link">
    LLM engineering platform and observability.
  </Card>

<Card title="Lantern" href="/oss/python/integrations/providers/lantern" icon="link">
    PostgreSQL vector database extension.
  </Card>

<Card title="Lindorm" href="/oss/python/integrations/providers/lindorm" icon="link">
    Alibaba Cloud's multi-model database service.
  </Card>

<Card title="LinkUp" href="/oss/python/integrations/providers/linkup" icon="link">
    Real-time job market data and search.
  </Card>

<Card title="LiteLLM" href="/oss/python/integrations/providers/litellm" icon="link">
    Unified interface for 100+ LLM APIs.
  </Card>

<Card title="LlamaIndex" href="/oss/python/integrations/providers/llama_index" icon="link">
    Data framework for LLM applications.
  </Card>

<Card title="LlamaCPP" href="/oss/python/integrations/providers/llamacpp" icon="link">
    Port of Meta's LLaMA model in C/C++.
  </Card>

<Card title="LlamaEdge" href="/oss/python/integrations/providers/llamaedge" icon="link">
    Edge computing platform for LLaMA models.
  </Card>

<Card title="LlamaFile" href="/oss/python/integrations/providers/llamafile" icon="link">
    Single-file executable for running LLMs.
  </Card>

<Card title="LLMonitor" href="/oss/python/integrations/providers/llmonitor" icon="link">
    Observability platform for LLM applications.
  </Card>

<Card title="LocalAI" href="/oss/python/integrations/providers/localai" icon="link">
    Self-hosted OpenAI-compatible API server.
  </Card>

<Card title="Log10" href="/oss/python/integrations/providers/log10" icon="link">
    LLM data management and observability.
  </Card>

<Card title="MariaDB" href="/oss/python/integrations/providers/mariadb" icon="link">
    Open-source relational database management.
  </Card>

<Card title="MaritALK" href="/oss/python/integrations/providers/maritalk" icon="link">
    Brazilian Portuguese language model.
  </Card>

<Card title="Marqo" href="/oss/python/integrations/providers/marqo" icon="link">
    End-to-end vector search engine.
  </Card>

<Card title="MediaWiki Dump" href="/oss/python/integrations/providers/mediawikidump" icon="link">
    Wikipedia and MediaWiki data processing.
  </Card>

<Card title="Meilisearch" href="/oss/python/integrations/providers/meilisearch" icon="link">
    Lightning-fast search engine platform.
  </Card>

<Card title="Memcached" href="/oss/python/integrations/providers/memcached" icon="link">
    Distributed memory caching system.
  </Card>

<Card title="Memgraph" href="/oss/python/integrations/providers/memgraph" icon="link">
    Real-time graph database platform.
  </Card>

<Card title="Metal" href="/oss/python/integrations/providers/metal" icon="link">
    Managed vector search and retrieval.
  </Card>

<Card title="Microsoft" href="/oss/python/integrations/providers/microsoft" icon="microsoft">
    Microsoft Azure AI and enterprise services.
  </Card>

<Card title="Milvus" href="/oss/python/integrations/providers/milvus" icon="link">
    Open-source vector database for AI applications.
  </Card>

<Card title="MindsDB" href="/oss/python/integrations/providers/mindsdb" icon="link">
    AI layer for databases and data platforms.
  </Card>

<Card title="Minimax" href="/oss/python/integrations/providers/minimax" icon="link">
    Chinese AI company's language models.
  </Card>

<Card title="MistralAI" href="/oss/python/integrations/providers/mistralai" icon="link">
    Efficient open-source language models.
  </Card>

<Card title="MLflow" href="/oss/python/integrations/providers/mlflow" icon="link">
    ML lifecycle management platform.
  </Card>

<Card title="MLflow Tracking" href="/oss/python/integrations/providers/mlflow_tracking" icon="link">
    Experiment tracking and model registry.
  </Card>

<Card title="MLX" href="/oss/python/integrations/providers/mlx" icon="link">
    Apple's machine learning framework.
  </Card>

<Card title="Modal" href="/oss/python/integrations/providers/modal" icon="link">
    Serverless cloud computing for data science.
  </Card>

<Card title="ModelScope" href="/oss/python/integrations/providers/modelscope" icon="link">
    Alibaba's open-source model hub.
  </Card>

<Card title="Modern Treasury" href="/oss/python/integrations/providers/modern_treasury" icon="link">
    Payment operations and treasury management.
  </Card>

<Card title="Momento" href="/oss/python/integrations/providers/momento" icon="link">
    Serverless cache and vector index.
  </Card>

<Card title="MongoDB" href="/oss/python/integrations/providers/mongodb" icon="link">
    Document-based NoSQL database platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/python/integrations/providers/mongodb_atlas" icon="link">
    Cloud-hosted MongoDB with vector search.
  </Card>

<Card title="MotherDuck" href="/oss/python/integrations/providers/motherduck" icon="link">
    Serverless analytics with DuckDB in the cloud.
  </Card>

<Card title="Motorhead" href="/oss/python/integrations/providers/motorhead" icon="link">
    Long-term memory for AI conversations.
  </Card>

<Card title="MyScale" href="/oss/python/integrations/providers/myscale" icon="link">
    SQL-compatible vector database platform.
  </Card>

<Card title="Naver" href="/oss/python/integrations/providers/naver" icon="link">
    Naver's AI services and language models.
  </Card>

<Card title="Nebius" href="/oss/python/integrations/providers/nebius" icon="link">
    AI cloud platform and infrastructure.
  </Card>

<Card title="Neo4j" href="/oss/python/integrations/providers/neo4j" icon="link">
    Native graph database and analytics platform.
  </Card>

<Card title="NetMind" href="/oss/python/integrations/providers/netmind" icon="link">
    Decentralized AI computing network.
  </Card>

<Card title="Nimble" href="/oss/python/integrations/providers/nimble" icon="link">
    Web intelligence and data extraction.
  </Card>

<Card title="NLP Cloud" href="/oss/python/integrations/providers/nlpcloud" icon="link">
    Production-ready NLP API platform.
  </Card>

<Card title="Nomic" href="/oss/python/integrations/providers/nomic" icon="link">
    Open-source embedding models and tools.
  </Card>

<Card title="Notion" href="/oss/python/integrations/providers/notion" icon="link">
    All-in-one workspace and collaboration platform.
  </Card>

<Card title="Nuclia" href="/oss/python/integrations/providers/nuclia" icon="link">
    AI-powered search and understanding platform.
  </Card>

<Card title="NVIDIA" href="/oss/python/integrations/providers/nvidia" icon="link">
    NVIDIA's AI computing platform and models.
  </Card>

<Card title="Obsidian" href="/oss/python/integrations/providers/obsidian" icon="link">
    Connected note-taking and knowledge management.
  </Card>

<Card title="OceanBase" href="/oss/python/integrations/providers/oceanbase" icon="link">
    Distributed relational database system.
  </Card>

<Card title="OCI" href="/oss/python/integrations/providers/oci" icon="link">
    Oracle Cloud Infrastructure AI services.
  </Card>

<Card title="OctoAI" href="/oss/python/integrations/providers/octoai" icon="link">
    Efficient AI compute and model serving.
  </Card>

<Card title="Ollama" href="/oss/python/integrations/providers/ollama" icon="link">
    Run large language models locally.
  </Card>

<Card title="Ontotext GraphDB" href="/oss/python/integrations/providers/ontotext_graphdb" icon="link">
    RDF database and semantic graph platform.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/providers/openai" icon="openai">
    GPT models and comprehensive AI platform.
  </Card>

<Card title="OpenDataLoader PDF" href="/oss/python/integrations/providers/opendataloader_pdf" icon="link">
    Safe, Open, High-Performance — PDF for AI
  </Card>

<Card title="OpenGradient" href="/oss/python/integrations/providers/opengradient" icon="link">
    AI model training and fine-tuning platform.
  </Card>

<Card title="OpenLLM" href="/oss/python/integrations/providers/openllm" icon="link">
    Operating LLMs in production environment.
  </Card>

<Card title="OpenSearch" href="/oss/python/integrations/providers/opensearch" icon="link">
    Distributed search and analytics suite.
  </Card>

<Card title="OpenWeatherMap" href="/oss/python/integrations/providers/openweathermap" icon="link">
    Weather data and forecasting API.
  </Card>

<Card title="Oracle AI" href="/oss/python/integrations/providers/oracleai" icon="link">
    Oracle's AI and machine learning services.
  </Card>

<Card title="Outline" href="/oss/python/integrations/providers/outline" icon="link">
    Team knowledge base and wiki platform.
  </Card>

<Card title="Outlines" href="/oss/python/integrations/providers/outlines" icon="link">
    Structured generation for language models.
  </Card>

<Card title="Oxylabs" href="/oss/python/integrations/providers/oxylabs" icon="link">
    Web scraping and proxy services.
  </Card>

<Card title="Pandas" href="/oss/python/integrations/providers/pandas" icon="link">
    Data analysis and manipulation library.
  </Card>

<Card title="Perigon" href="/oss/python/integrations/providers/perigon" icon="link">
    Real-time news and media monitoring.
  </Card>

<Card title="Permit" href="/oss/python/integrations/providers/permit" icon="link">
    Authorization and access control platform.
  </Card>

<Card title="Perplexity" href="/oss/python/integrations/providers/perplexity" icon="link">
    AI-powered search and reasoning engine.
  </Card>

<Card title="Petals" href="/oss/python/integrations/providers/petals" icon="link">
    Distributed inference for large language models.
  </Card>

<Card title="PG Embedding" href="/oss/python/integrations/providers/pg_embedding" icon="link">
    PostgreSQL vector embedding extensions.
  </Card>

<Card title="pgvector" href="/oss/python/integrations/providers/pgvector" icon="link">
    Vector similarity search for PostgreSQL.
  </Card>

<Card title="Pinecone" href="/oss/python/integrations/providers/pinecone" icon="link">
    Managed vector database for ML applications.
  </Card>

<Card title="PipelineAI" href="/oss/python/integrations/providers/pipelineai" icon="link">
    ML pipeline and model deployment platform.
  </Card>

<Card title="Pipeshift" href="/oss/python/integrations/providers/pipeshift" icon="link">
    AI-powered content moderation platform.
  </Card>

<Card title="PolarisAIDataInsight" href="/oss/python/integrations/providers/polaris_ai_datainsight" icon="link">
    Document-loaders for various file formats.
  </Card>

<Card title="Portkey" href="/oss/python/integrations/providers/portkey/logging_tracing_portkey" icon="link">
    AI gateway and observability platform.
  </Card>

<Card title="Predibase" href="/oss/python/integrations/providers/predibase" icon="link">
    Fine-tuning platform for large language models.
  </Card>

<Card title="PredictionGuard" href="/oss/python/integrations/providers/predictionguard" icon="link">
    AI model security and compliance platform.
  </Card>

<Card title="PreMAI" href="/oss/python/integrations/providers/premai" icon="link">
    AI platform for model deployment and management.
  </Card>

<Card title="Privy" href="/oss/python/integrations/providers/privy" icon="link">
    Wallets and payments for AI agents.
  </Card>

<Card title="Prolog" href="/oss/python/integrations/providers/prolog" icon="link">
    Logic programming language integration.
  </Card>

<Card title="PromptLayer" href="/oss/python/integrations/providers/promptlayer" icon="link">
    Prompt engineering and observability platform.
  </Card>

<Card title="Psychic" href="/oss/python/integrations/providers/psychic" icon="link">
    Universal API for SaaS integrations.
  </Card>

<Card title="PubMed" href="/oss/python/integrations/providers/pubmed" icon="link">
    Biomedical literature database access.
  </Card>

<Card title="Pull MD" href="/oss/python/integrations/providers/pull-md" icon="link">
    Markdown content extraction and processing.
  </Card>

<Card title="PygmalionAI" href="/oss/python/integrations/providers/pygmalionai" icon="link">
    Conversational AI model platform.
  </Card>

<Card title="PyMuPDF4LLM" href="/oss/python/integrations/providers/pymupdf4llm" icon="link">
    PDF processing optimized for LLM ingestion.
  </Card>

<Card title="Qdrant" href="/oss/python/integrations/providers/qdrant" icon="link">
    Vector similarity search engine.
  </Card>

<Card title="Ragatouille" href="/oss/python/integrations/providers/ragatouille" icon="link">
    RAG toolkit with ColBERT indexing.
  </Card>

<Card title="Rank BM25" href="/oss/python/integrations/providers/rank_bm25" icon="link">
    BM25 ranking algorithm implementation.
  </Card>

<Card title="Ray Serve" href="/oss/python/integrations/providers/ray_serve" icon="link">
    Scalable model serving framework.
  </Card>

<Card title="Rebuff" href="/oss/python/integrations/providers/rebuff" icon="link">
    Prompt injection detection and prevention.
  </Card>

<Card title="Reddit" href="/oss/python/integrations/providers/reddit" icon="link">
    Social media platform integration and APIs.
  </Card>

<Card title="Redis" href="/oss/python/integrations/providers/redis" icon="link">
    In-memory data structure store and cache.
  </Card>

<Card title="Remembrall" href="/oss/python/integrations/providers/remembrall" icon="link">
    AI memory and context management.
  </Card>

<Card title="Replicate" href="/oss/python/integrations/providers/replicate" icon="link">
    Cloud platform for running ML models.
  </Card>

<Card title="Roam" href="/oss/python/integrations/providers/roam" icon="link">
    Research and note-taking platform.
  </Card>

<Card title="Robocorp" href="/oss/python/integrations/providers/robocorp" icon="link">
    Python automation and RPA platform.
  </Card>

<Card title="Rockset" href="/oss/python/integrations/providers/rockset" icon="link">
    Real-time analytics database platform.
  </Card>

<Card title="RunPod" href="/oss/python/integrations/providers/runpod" icon="link">
    GPU cloud platform for AI workloads.
  </Card>

<Card title="Salesforce" href="/oss/python/integrations/providers/salesforce" icon="link">
    CRM platform and business automation.
  </Card>

<Card title="SambaNova" href="/oss/python/integrations/providers/sambanova" icon="link">
    AI platform with specialized hardware.
  </Card>

<Card title="SAP" href="/oss/python/integrations/providers/sap" icon="link">
    Enterprise software and AI solutions.
  </Card>

<Card title="ScrapeGraph" href="/oss/python/integrations/providers/scrapegraph" icon="link">
    AI-powered web scraping framework.
  </Card>

<Card title="Scrapeless" href="/oss/python/integrations/providers/scrapeless" icon="link">
    Web scraping API and proxy service.
  </Card>

<Card title="SearchAPI" href="/oss/python/integrations/providers/searchapi" icon="link">
    Real-time search engine results API.
  </Card>

<Card title="SearX" href="/oss/python/integrations/providers/searx" icon="link">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SemaDB" href="/oss/python/integrations/providers/semadb" icon="link">
    Vector database for semantic search.
  </Card>

<Card title="SerpAPI" href="/oss/python/integrations/providers/serpapi" icon="link">
    Google Search results scraping API.
  </Card>

<Card title="Shale Protocol" href="/oss/python/integrations/providers/shaleprotocol" icon="link">
    Decentralized AI inference protocol.
  </Card>

<Card title="SingleStore" href="/oss/python/integrations/providers/singlestore" icon="link">
    Distributed database with vector capabilities.
  </Card>

<Card title="scikit-learn" href="/oss/python/integrations/providers/sklearn" icon="link">
    Machine learning library for Python.
  </Card>

<Card title="Slack" href="/oss/python/integrations/providers/slack" icon="link">
    Business communication and collaboration.
  </Card>

<Card title="Snowflake" href="/oss/python/integrations/providers/snowflake" icon="link">
    Cloud data platform and analytics.
  </Card>

<Card title="spaCy" href="/oss/python/integrations/providers/spacy" icon="link">
    Industrial-strength NLP library.
  </Card>

<Card title="Spark" href="/oss/python/integrations/providers/spark" icon="link">
    Unified analytics engine for big data.
  </Card>

<Card title="SparkLLM" href="/oss/python/integrations/providers/sparkllm" icon="link">
    iFlytek's multilingual language model.
  </Card>

<Card title="Spreedly" href="/oss/python/integrations/providers/spreedly" icon="link">
    Payment orchestration platform.
  </Card>

<Card title="SQLite" href="/oss/python/integrations/providers/sqlite" icon="link">
    Embedded relational database engine.
  </Card>

<Card title="StackExchange" href="/oss/python/integrations/providers/stackexchange" icon="link">
    Q\&A platform network integration.
  </Card>

<Card title="StarRocks" href="/oss/python/integrations/providers/starrocks" icon="link">
    High-performance analytical database.
  </Card>

<Card title="StochasticAI" href="/oss/python/integrations/providers/stochasticai" icon="link">
    GPU cloud platform for ML acceleration.
  </Card>

<Card title="Streamlit" href="/oss/python/integrations/providers/streamlit" icon="link">
    Web app framework for data science.
  </Card>

<Card title="Stripe" href="/oss/python/integrations/providers/stripe" icon="link">
    Online payment processing platform.
  </Card>

<Card title="Supabase" href="/oss/python/integrations/providers/supabase" icon="link">
    Open-source Firebase alternative.
  </Card>

<Card title="SurrealDB" href="/oss/python/integrations/providers/surrealdb" icon="link">
    Multi-model database for modern applications.
  </Card>

<Card title="Symbl.ai Nebula" href="/oss/python/integrations/providers/symblai_nebula" icon="link">
    Conversation intelligence platform.
  </Card>

<Card title="Tableau" href="/oss/python/integrations/providers/tableau" icon="link">
    Data visualization and business intelligence.
  </Card>

<Card title="Taiga" href="/oss/python/integrations/providers/taiga" icon="link">
    Project management platform for agile teams.
  </Card>

<Card title="Tair" href="/oss/python/integrations/providers/tair" icon="link">
    Alibaba Cloud's in-memory database.
  </Card>

<Card title="Tavily" href="/oss/python/integrations/providers/tavily" icon="link">
    AI-optimized search API for applications.
  </Card>

<Card title="Telegram" href="/oss/python/integrations/providers/telegram" icon="link">
    Messaging platform and bot integration.
  </Card>

<Card title="Tencent" href="/oss/python/integrations/providers/tencent" icon="link">
    Tencent Cloud AI services and models.
  </Card>

<Card title="TensorFlow Datasets" href="/oss/python/integrations/providers/tensorflow_datasets" icon="link">
    Collection of ready-to-use datasets.
  </Card>

<Card title="TensorLake" href="/oss/python/integrations/providers/tensorlake" icon="link">
    Data infrastructure for ML applications.
  </Card>

<Card title="TiDB" href="/oss/python/integrations/providers/tidb" icon="link">
    Distributed SQL database platform.
  </Card>

<Card title="TigerGraph" href="/oss/python/integrations/providers/tigergraph" icon="link">
    Scalable graph database and analytics.
  </Card>

<Card title="Tigris" href="/oss/python/integrations/providers/tigris" icon="link">
    Globally distributed database platform.
  </Card>

<Card title="Tilores" href="/oss/python/integrations/providers/tilores" icon="link">
    Entity resolution and data matching.
  </Card>

<Card title="Together" href="/oss/python/integrations/providers/together" icon="link">
    Fast inference for open-source models.
  </Card>

<Card title="ToMarkdown" href="/oss/python/integrations/providers/tomarkdown" icon="link">
    HTML to Markdown conversion utility.
  </Card>

<Card title="Toolbox LangChain" href="/oss/python/integrations/providers/toolbox" icon="link">
    Extended toolkit for LangChain applications.
  </Card>

<Card title="Transwarp" href="/oss/python/integrations/providers/transwarp" icon="link">
    Big data platform and analytics suite.
  </Card>

<Card title="Trello" href="/oss/python/integrations/providers/trello" icon="link">
    Visual project management and collaboration.
  </Card>

<Card title="Trubrics" href="/oss/python/integrations/providers/trubrics" icon="link">
    LLM evaluation and analytics platform.
  </Card>

<Card title="TrueFoundry" href="/oss/python/integrations/providers/truefoundry" icon="link">
    ML platform for model deployment.
  </Card>

<Card title="TrueLens" href="/oss/python/integrations/providers/trulens" icon="link">
    Evaluation framework for LLM applications.
  </Card>

<Card title="Twitter" href="/oss/python/integrations/providers/twitter" icon="link">
    Social media platform integration.
  </Card>

<Card title="Typesense" href="/oss/python/integrations/providers/typesense" icon="link">
    Fast and typo-tolerant search engine.
  </Card>

<Card title="UnDatasIO" href="/oss/python/integrations/providers/undatasio" icon="link">
    Data extraction and processing platform.
  </Card>

<Card title="Unstructured" href="/oss/python/integrations/providers/unstructured" icon="link">
    Document processing and data extraction.
  </Card>

<Card title="Upstage" href="/oss/python/integrations/providers/upstage" icon="link">
    Document AI and OCR platform.
  </Card>

<Card title="Upstash" href="/oss/python/integrations/providers/upstash" icon="link">
    Serverless data platform for Redis and Kafka.
  </Card>

<Card title="UpTrain" href="/oss/python/integrations/providers/uptrain" icon="link">
    ML observability and evaluation platform.
  </Card>

<Card title="USearch" href="/oss/python/integrations/providers/usearch" icon="link">
    Single-file vector search engine.
  </Card>

<Card title="Valthera" href="/oss/python/integrations/providers/valthera" icon="link">
    AI platform for healthcare applications.
  </Card>

<Card title="Valyu" href="/oss/python/integrations/providers/valyu" icon="link">
    AI-powered data analysis platform.
  </Card>

<Card title="VDMS" href="/oss/python/integrations/providers/vdms" icon="link">
    Visual data management system.
  </Card>

<Card title="Vearch" href="/oss/python/integrations/providers/vearch" icon="link">
    Distributed vector search engine.
  </Card>

<Card title="Vectara" href="/oss/python/integrations/providers/vectara" icon="link">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vectorize" href="/oss/python/integrations/providers/vectorize" icon="link">
    Vector database and semantic search.
  </Card>

<Card title="Vespa" href="/oss/python/integrations/providers/vespa" icon="link">
    Big data serving engine for vector search.
  </Card>

<Card title="VLite" href="/oss/python/integrations/providers/vlite" icon="link">
    Simple vector database for embeddings.
  </Card>

<Card title="VoyageAI" href="/oss/python/integrations/providers/voyageai" icon="link">
    Embedding models and semantic search.
  </Card>

<Card title="Weights & Biases" href="/oss/python/integrations/providers/wandb" icon="link">
    ML experiment tracking and collaboration.
  </Card>

<Card title="Weights & Biases Tracking" href="/oss/python/integrations/providers/wandb_tracking" icon="link">
    Experiment tracking and model management.
  </Card>

<Card title="Weights & Biases Tracing" href="/oss/python/integrations/providers/wandb_tracing" icon="link">
    LLM tracing and observability.
  </Card>

<Card title="Weather" href="/oss/python/integrations/providers/weather" icon="link">
    Weather data and forecasting services.
  </Card>

<Card title="Weaviate" href="/oss/python/integrations/providers/weaviate" icon="link">
    Open-source vector database with GraphQL.
  </Card>

<Card title="WhatsApp" href="/oss/python/integrations/providers/whatsapp" icon="link">
    Messaging platform integration and automation.
  </Card>

<Card title="WhyLabs Profiling" href="/oss/python/integrations/providers/whylabs_profiling" icon="link">
    AI observability and data monitoring.
  </Card>

<Card title="Wikipedia" href="/oss/python/integrations/providers/wikipedia" icon="link">
    Wikipedia content access and search.
  </Card>

<Card title="Wolfram Alpha" href="/oss/python/integrations/providers/wolfram_alpha" icon="link">
    Computational knowledge engine.
  </Card>

<Card title="Writer" href="/oss/python/integrations/providers/writer" icon="link">
    Enterprise AI writing platform.
  </Card>

<Card title="XAI" href="/oss/python/integrations/providers/xai" icon="link">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Xata" href="/oss/python/integrations/providers/xata" icon="link">
    Serverless database with vector search.
  </Card>

<Card title="Xinference" href="/oss/python/integrations/providers/xinference" icon="link">
    Distributed inference framework for LLMs.
  </Card>

<Card title="Yahoo" href="/oss/python/integrations/providers/yahoo" icon="link">
    Yahoo services and data integration.
  </Card>

<Card title="Yandex" href="/oss/python/integrations/providers/yandex" icon="link">
    Yandex AI services and language models.
  </Card>

<Card title="YDB" href="/oss/python/integrations/providers/ydb" icon="link">
    Yandex Database distributed storage system.
  </Card>

<Card title="YeagerAI" href="/oss/python/integrations/providers/yeagerai" icon="link">
    AI agent framework and development platform.
  </Card>

<Card title="Yellowbrick" href="/oss/python/integrations/providers/yellowbrick" icon="link">
    Data warehouse and analytics platform.
  </Card>

<Card title="Yi" href="/oss/python/integrations/providers/yi" icon="link">
    01.AI's bilingual language models.
  </Card>

<Card title="You" href="/oss/python/integrations/providers/you" icon="link">
    You.com search engine and AI platform.
  </Card>

<Card title="YouTube" href="/oss/python/integrations/providers/youtube" icon="link">
    Video platform integration and content access.
  </Card>

<Card title="Zep" href="/oss/python/integrations/providers/zep" icon="link">
    Long-term memory for AI assistants.
  </Card>

<Card title="ZeusDB" href="/oss/python/integrations/providers/zeusdb" icon="link">
    High-performance vector database.
  </Card>

<Card title="ZhipuAI" href="/oss/python/integrations/providers/zhipuai" icon="link">
    ChatGLM and other Chinese language models.
  </Card>

<Card title="Zilliz" href="/oss/python/integrations/providers/zilliz" icon="link">
    Managed Milvus vector database service.
  </Card>

<Card title="Zotero" href="/oss/python/integrations/providers/zotero" icon="link">
    Reference management and research tool.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/all_providers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## We set up a `secret` query parameter

**URL:** llms-txt#we-set-up-a-`secret`-query-parameter

**Contents:**
  - Hooking it up

def f(data: dict, secret: str = Query(...)):
    # You can import dependencies you don't have locally inside Modal functions
    from langsmith import Client

# First, we validate the secret key we pass
    import os

if secret != os.environ["LS_WEBHOOK"]:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect bearer token",
            headers={"WWW-Authenticate": "Bearer"},
        )

# This is where we put the logic for what should happen inside this webhook
    ls_client = Client()
    runs = data["runs"]
    ids = [r["id"] for r in runs]
    feedback = list(ls_client.list_feedback(run_ids=ids))
    for r, f in zip(runs, feedback):
        try:
            ls_client.create_example(
                inputs=r["inputs"],
                outputs={"output": f.correction},
                dataset_name="classifier-github-issues",
            )
        except Exception:
            raise ValueError(f"{r} and {f}")
    # Function body
    return "success!"

✓ Created objects.
├── 🔨 Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py
├── 🔨 Created mount PythonPackage:langsmith
└── 🔨 Created f => https://hwchase17--auth-example-f.modal.run
✓ App deployed! 🎉

View Deployment: https://modal.com/apps/hwchase17/auth-example

https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}
```

Replace `{SECRET}` with the secret key you created to access the Modal service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/webhooks.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
We can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).

You should now get something like:
```

Example 2 (unknown):
```unknown
The important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.

### Hooking it up

We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like:
```

---

## Define target function that uses attachments

**URL:** llms-txt#define-target-function-that-uses-attachments

**Contents:**
  - Define custom evaluators
- Update examples with attachments
- UI
  - 1. Create examples with attachments
  - 2. Create a multimodal prompt
  - Define custom evaluators
  - Update examples with attachments

def file_qa(inputs, attachments):
    # Read the audio bytes from the reader and encode them in base64
    audio_reader = attachments["my_wav"]["reader"]
    audio_b64 = base64.b64encode(audio_reader.read()).decode('utf-8')

audio_completion = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": inputs["audio_question"]
                    },
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": audio_b64,
                            "format": "wav"
                        }
                    }
                ]
            }
        ]
    )

# Most models support taking in an image URL directly in addition to base64 encoded images
    # You can pipe the image pre-signed URL directly to the model
    image_url = attachments["my_img"]["presigned_url"]
    image_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
          {
            "role": "user",
            "content": [
              {"type": "text", "text": inputs["image_question"]},
              {
                "type": "image_url",
                "image_url": {
                  "url": image_url,
                },
              },
            ],
          }
        ],
    )

return {
        "audio_answer": audio_completion.choices[0].message.content,
        "image_answer": image_completion.choices[0].message.content,
    }
typescript  theme={null}
{
  presigned_url: string,
  mime_type: string,
}
typescript  theme={null}
import OpenAI from "openai";
import { wrapOpenAI } from "langsmith/wrappers";

const client: any = wrapOpenAI(new OpenAI());

async function fileQA(inputs: Record<string, any>, config?: Record<string, any>) {
  const presignedUrl = config?.attachments?.["my_wav"]?.presigned_url;
  if (!presignedUrl) {
    throw new Error("No presigned URL provided for audio.");
  }

const response = await fetch(presignedUrl);
  if (!response.ok) {
    throw new Error(`Failed to fetch audio: ${response.statusText}`);
  }

const arrayBuffer = await response.arrayBuffer();
  const uint8Array = new Uint8Array(arrayBuffer);
  const audioB64 = Buffer.from(uint8Array).toString("base64");

const audioCompletion = await client.chat.completions.create({
    model: "gpt-4o-audio-preview",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["audio_question"] },
          {
            type: "input_audio",
            input_audio: {
              data: audioB64,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

const imageUrl = config?.attachments?.["my_img"]?.presigned_url
  const imageCompletion = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["image_question"] },
          {
            type: "image_url",
            image_url: {
              url: imageUrl,
            },
          },
        ],
      },
    ],
  });

return {
    audio_answer: audioCompletion.choices[0].message.content,
    image_answer: imageCompletion.choices[0].message.content,
  };
}
python Python theme={null}
  # Assumes you've installed pydantic
  from pydantic import BaseModel

def valid_image_description(outputs: dict, attachments: dict) -> bool:
    """Use an LLM to judge if the image description and images are consistent."""
    instructions = """
    Does the description of the following image make sense?
    Please carefully review the image and the description to determine if the description is valid.
    """

class Response(BaseModel):
        description_is_valid: bool

image_url = attachments["my_img"]["presigned_url"]
    response = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": instructions
            },
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": outputs["image_answer"]}
                ]
            }
        ],
        response_format=Response
    )
    return response.choices[0].message.parsed.description_is_valid

ls_client.evaluate(
    file_qa,
    data=dataset_name,
    evaluators=[valid_image_description],
  )
  typescript TypeScript theme={null}
  import { zodResponseFormat } from 'openai/helpers/zod';
  import { z } from 'zod';
  import { evaluate } from "langsmith/evaluation";

const DescriptionResponse = z.object({
    description_is_valid: z.boolean(),
  });

async function validImageDescription({
    outputs,
    attachments,
  }: {
    outputs?: any;
    attachments?: any;
  }): Promise<{ key: string; score: boolean}> {
    const instructions = `Does the description of the following image make sense?
  Please carefully review the image and the description to determine if the description is valid.`;

const imageUrl = attachments?.["my_img"]?.presigned_url
    const completion = await client.beta.chat.completions.parse({
        model: "gpt-4o",
        messages: [
            {
                role: "system",
                content: instructions,
            },
            {
                role: "user",
                content: [
                    { type: "image_url", image_url: { url: imageUrl } },
                    { type: "text", text: outputs?.image_answer },
                ],
            },
        ],
        response_format: zodResponseFormat(DescriptionResponse, 'imageResponse'),
    });

const score: boolean = completion.choices[0]?.message?.parsed?.description_is_valid ?? false;
    return { key: "valid_image_description", score };
  }

const resp = await evaluate(fileQA, {
    data: datasetName,
    // Need to pass flag to include attachments
    includeAttachments: true,
    evaluators: [validImageDescription],
    client: langsmithClient
  });
  python Python theme={null}
  example_update = {
    "id": example_id,
    "attachments": {
        # These are net new attachments
        "my_new_file": ("text/plain", b"foo bar"),
    },
    "inputs": inputs,
    "outputs": outputs,
    # Any attachments not in rename/retain will be deleted.
    # In this case, that would be "my_img" if we uploaded it.
    "attachments_operations": {
        # Retained attachments will stay exactly the same
        "retain": ["my_pdf"],
        # Renaming attachments preserves the original data
        "rename": {
            "my_wav": "my_new_wav",
        }
    },
  }

ls_client.update_examples(dataset_id=dataset.id, updates=[example_update])
  typescript TypeScript theme={null}
  import { ExampleUpdateWithAttachments } from "langsmith/schemas";

const exampleUpdate: ExampleUpdateWithAttachments = {
    id: exampleId,
    attachments: {
      // These are net new attachments
      "my_new_file": {
        mimeType: "text/plain",
        data: Buffer.from("foo bar")
      },
    },
    attachments_operations: {
      // Retained attachments will stay exactly the same
      retain: ["my_img"],
      // Renaming attachments preserves the original data
      rename: {
        "my_wav": "my_new_wav",
      },
      // Any attachments not in rename/retain will be deleted
      // In this case, that would be "my_pdf"
    },
  };

await langsmithClient.updateExamplesMultipart(dataset.id, [exampleUpdate]);
  ```
</CodeGroup>

### 1. Create examples with attachments

You can add examples with attachments to a dataset in a few different ways.

#### From existing runs

When adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see [this guide](/langsmith/manage-datasets-in-application#add-runs-from-the-tracing-project-ui).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b8fa62cb39c4f1fc67d9b24fa78d1653" alt="Add trace with attachments to dataset" data-og-width="1662" width="1662" data-og-height="679" height="679" data-path="langsmith/images/add-trace-with-attachments-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=72ee339359616ed8f03f4ddbfe86bc23 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cb6f558f8a0391588583a7b5d520a27f 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc51ed2e68af972e488051fc1ae01caf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d58d097b70ad0f6b7a327058c659d8d9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cbac645e6534290036d24963c231a878 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b4ff5c59dd6c2b23d97bfd5e34206a50 2500w" />

You can create examples with attachments directly from the LangSmith UI. Click the `+ Example` button in the `Examples` tab of the dataset UI. Then upload attachments using the "Upload Files" button:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=183f929c807f59157e93d40354057933" alt="Create example with attachments" data-og-width="3456" width="3456" data-og-height="1856" height="1856" data-path="langsmith/images/create-example-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c80316a80e8359d14aa42aac6767b677 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1921a813aea4e6fa62bcf7cfd169662e 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=35d6882ec8757e56d81eba2033220cf7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0b93d237b11ab6bf8693b287ee19dc4b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=38384c5a8166b41535657ec8a6337b49 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0f8622286209391451895c2e4c7b03b 2500w" />

Once uploaded, you can view examples with attachments in the LangSmith UI. Each attachment will be rendered with a preview for easy inspection. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8f813bdf7d3bfd5a840e5f8c47693ed3" alt="Attachments with examples" data-og-width="1331" width="1331" data-og-height="593" height="593" data-path="langsmith/images/attachments-with-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ecc6a1a59da5972731c4f36fc0154d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f7985b2778973b594e8522964eb13770 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f062086524f7aff66c094ac0e259c04e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970e61cff6a2cfc749c14481afe2a3f9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4c34be3eb64547daf4879b2f2dab3ec4 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=700119fd1a262ce0f85d924b684141a4 2500w" />

### 2. Create a multimodal prompt

The LangSmith UI allows you to include attachments in your prompts when evaluating multimodal models:

First, click the file icon in the message where you want to add multimodal content. Next, add a template variable for the attachment(s) you want to include for each example.

* For a single attachment type: Use the suggested variable name. Note: all examples must have an attachment with this name.
* For multiple attachments or if your attachments have varying names from one example to another: Use the `All attachments` variable to include all available attachments for each example.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/adding-multimodal-variable.gif?s=07a15c9fc5e6fc743f92b6b41ab8c9e0" alt="Adding multimodal variable" data-og-width="1700" width="1700" data-og-height="1080" height="1080" data-path="langsmith/images/adding-multimodal-variable.gif" data-optimize="true" data-opv="3" />

### Define custom evaluators

<Note>
  The LangSmith playground does not currently support pulling multimodal content into evaluators. If this would be helpful for your use case, please let us know in the [LangChain Forum](https://forum.langchain.com/) (sign up [here](https://www.langchain.com/join-community) if you're not already a member)!
</Note>

You can evaluate a model's text output by adding an evaluator that takes in the example's inputs and outputs. Even without multimodal support in your evaluators, you can still run text-only evaluations. For example:

* OCR → text correction: Use a vision model to extract text from a document, then evaluate the accuracy of the extracted output.
* Speech-to-text → transcription quality: Use a voice model to transcribe audio to text, then evaluate the transcription against your reference.

For more information on defining custom evaluators, see the [LLM as Judge](/langsmith/llm-as-judge) guide.

### Update examples with attachments

<Note>
  Attachments are limited to 20MB in size in the UI.
</Note>

When editing an example in the UI, you can:

* Upload new attachments
* Rename and delete attachments
* Reset attachments to their previous state using the quick reset button

Changes are not saved until you click submit.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachment-editing.gif?s=4f165ed98fe81722961778ebbe1691ed" alt="Attachment editing" data-og-width="1204" width="1204" data-og-height="720" height="720" data-path="langsmith/images/attachment-editing.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-with-attachments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

In the TypeScript SDK, the `config` argument is used to pass in the attachments to the target function if `includeAttachments` is set to `true`.

The `config` will contain `attachments` which is an object mapping the attachment name to an object of the form:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
### Define custom evaluators

The exact same rules apply as above to determine whether the evaluator should receive attachments.

The evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see [this guide](/langsmith/llm-as-judge).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use time-travel

**URL:** llms-txt#use-time-travel

**Contents:**
- In a workflow
  - Setup

Source: https://docs.langchain.com/oss/python/langgraph/use-time-travel

When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:

1. <Icon icon="lightbulb" size={16} /> **Understand reasoning**: Analyze the steps that led to a successful result.
2. <Icon icon="bug" size={16} /> **Debug mistakes**: Identify where and why errors occurred.
3. <Icon icon="magnifying-glass" size={16} /> **Explore alternatives**: Test different paths to uncover better solutions.

LangGraph provides [time travel](/oss/python/langgraph/use-time-travel) functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.

To use [time-travel](/oss/python/langgraph/use-time-travel) in LangGraph:

1. [Run the graph](#1-run-the-graph) with initial inputs using [`invoke`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.invoke) or [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) methods.
2. [Identify a checkpoint in an existing thread](#2-identify-a-checkpoint): Use the [`get_state_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history) method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
   Alternatively, set an [interrupt](/oss/python/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.
3. [Update the graph state (optional)](#3-update-the-state-optional): Use the [`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) method to modify the graph's state at the checkpoint and resume execution from alternative state.
4. [Resume execution from the checkpoint](#4-resume-execution-from-the-checkpoint): Use the `invoke` or `stream` methods with an input of `None` and a configuration containing the appropriate `thread_id` and `checkpoint_id`.

<Tip>
  For a conceptual overview of time-travel, see [Time travel](/oss/python/langgraph/use-time-travel).
</Tip>

This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.

First we need to install the packages required

Next, we need to set API keys for Anthropic (the LLM we will use)

<Tip>
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>

```python  theme={null}
import uuid

from typing_extensions import TypedDict, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import InMemorySaver

class State(TypedDict):
    topic: NotRequired[str]
    joke: NotRequired[str]

model = init_chat_model(
    "anthropic:claude-sonnet-4-5",
    temperature=0,
)

def generate_topic(state: State):
    """LLM call to generate a topic for the joke"""
    msg = model.invoke("Give me a funny topic for a joke")
    return {"topic": msg.content}

def write_joke(state: State):
    """LLM call to write a joke based on the topic"""
    msg = model.invoke(f"Write a short joke about {state['topic']}")
    return {"joke": msg.content}

**Examples:**

Example 1 (unknown):
```unknown
Next, we need to set API keys for Anthropic (the LLM we will use)
```

Example 2 (unknown):
```unknown
<Tip>
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>
```

---

## Use DuckDuckGo if you don't have a Tavily API key:

**URL:** llms-txt#use-duckduckgo-if-you-don't-have-a-tavily-api-key:

---

## Document transformers

**URL:** llms-txt#document-transformers

Source: https://docs.langchain.com/oss/javascript/integrations/document_transformers/index

<Columns cols={3}>
  <Card title="html-to-text" icon="link" href="/oss/javascript/integrations/document_transformers/html-to-text" arrow="true" cta="View guide" />

<Card title="mozilla/readability" icon="link" href="/oss/javascript/integrations/document_transformers/mozilla_readability" arrow="true" cta="View guide" />

<Card title="OpenAI functions metadata tagger" icon="link" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/document_transformers/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith data plane

**URL:** llms-txt#langsmith-data-plane

**Contents:**
- Server infrastructure
- "Listener" application
- PostgreSQL
- Redis
  - Communication
  - Ephemeral metadata
- Data plane features
  - Data region
  - Autoscaling
  - Static IP addresses

Source: https://docs.langchain.com/langsmith/data-plane

The *data plane* consists of your [LangGraph Servers](/langsmith/langgraph-server) (deployments), their supporting infrastructure, and the "listener" application that continuously polls for updates from the [LangSmith control plane](/langsmith/control-plane).

## Server infrastructure

In addition to the [LangGraph Server](/langsmith/langgraph-server) itself, the following infrastructure components for each server are also included in the broad definition of "data plane":

* **PostgreSQL**: persistence layer for user, run, and memory data.
* **Redis**: communication and ephemeral metadata for workers.
* **Secrets store**: secure management of environment secrets.
* **Autoscalers**: scale server containers based on load.

## "Listener" application

The data plane "listener" application periodically calls [control plane APIs](/langsmith/control-plane#control-plane-api) to:

* Determine if new deployments should be created.
* Determine if existing deployments should be updated (i.e. new revisions).
* Determine if existing deployments should be deleted.

In other words, the data plane "listener" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.

PostgreSQL is the persistence layer for all user, run, and long-term memory data in a LangGraph Server. This stores both checkpoints (see more info [here](/oss/python/langgraph/persistence)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](/oss/python/langgraph/persistence#memory-store)).

Redis is used in each LangGraph Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.

All runs in a LangGraph Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.

1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from PostgreSQL by the worker.
2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.
3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.

### Ephemeral metadata

Runs in a LangGraph Server may be retried for specific failures (currently only for transient PostgreSQL errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.

## Data plane features

This section describes various features of the data plane.

<Info>
  **Only for Cloud**
  Data regions are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

Deployments can be created in 2 data regions: US and EU

The data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.

[`Production` type](/langsmith/control-plane#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:

1. CPU utilization
2. Memory utilization
3. Number of pending (in progress) [runs](/langsmith/assistants#execution)

For CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.

For number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs is 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).

Each metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.

Scale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This "cool down" period ensures that deployments do not scale up and down too frequently.

### Static IP addresses

<Info>
  **Only for Cloud**
  Static IP addresses are only available for [Cloud](/langsmith/cloud) deployments.
</Info>

All traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 35.197.29.146  | 34.13.192.67   |
| 34.145.102.123 | 34.147.105.64  |
| 34.169.45.153  | 34.90.22.166   |
| 34.82.222.17   | 34.147.36.213  |
| 35.227.171.135 | 34.32.137.113  |
| 34.169.88.30   | 34.91.238.184  |
| 34.19.93.202   | 35.204.101.241 |
| 34.19.34.50    | 35.204.48.32   |
| 34.59.244.194  |                |
| 34.9.99.224    |                |
| 34.68.27.146   |                |
| 34.41.178.137  |                |
| 34.123.151.210 |                |
| 34.135.61.140  |                |
| 34.121.166.52  |                |
| 34.31.121.70   |                |

### Custom PostgreSQL

<Info>
  Custom PostgreSQL instances are only available for [hybrid](/langsmith/hybrid) and [self-hosted](/langsmith/self-hosted) deployments.
</Info>

A custom PostgreSQL instance can be used instead of the [one automatically created by the control plane](/langsmith/control-plane#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](/langsmith/env-var#postgres_uri_custom) environment variable to use a custom PostgreSQL instance.

Multiple deployments can share the same PostgreSQL instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.

<Info>
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

A custom Redis instance can be used instead of the one automatically created by the control plane. Specify the [REDIS\_URI\_CUSTOM](/langsmith/env-var#redis_uri_custom) environment variable to use a custom Redis instance.

Multiple deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/2`. `1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.

### LangSmith tracing

LangGraph Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.

| Cloud                                  | Hybrid                                                    | Self-Hosted                                                                                |
| -------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Required<br />Trace to LangSmith SaaS. | Optional<br />Disable tracing or trace to LangSmith SaaS. | Optional<br />Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |

LangGraph Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.

| Cloud                             | Hybrid                            | Self-Hosted                                                                                                              |
| --------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key.<br />Telemetry sent to LangSmith SaaS for LangSmith License Key. |

LangGraph Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.

| Cloud                                               | Hybrid                                              | Self-Hosted                                                                      |
| --------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------- |
| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or Platform License Key validated against LangSmith SaaS. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Filter traces

**URL:** llms-txt#filter-traces

**Contents:**
- Creating and Applying Filters
  - Filtering by run attributes
  - Filtering by time range
  - Filter operators
- Specific Filtering Techniques
  - Filter for intermediate runs (spans)
  - Filter based on inputs and outputs
  - Filter based on input / output key-value pairs
  - Example: Filtering for tool calls
  - Negative filtering on key-value pairs

Source: https://docs.langchain.com/langsmith/filter-traces-in-application

<Tip>**Recommended reading**: It might be helpful to read the [Conceptual guide on tracing](/langsmith/observability-concepts) to gain familiarity with the concepts mentioned on this page.</Tip>

Tracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to:

* **Have focused investigations**: Quickly narrow down to specific runs for ad-hoc analysis
* **Debug and analyze**: Identify and examine errors, failed runs, and performance bottlenecks

This page contains a series of guides for how to filter runs in a tracing project. If you are programmatically exporting runs for analysis via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs), please refer to the [exporting traces guide](./export-traces) for more information.

## Creating and Applying Filters

### Filtering by run attributes

There are two ways to filter runs in a tracing project:

1. **Filters**: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d3751c19f28b7dffecec87e27d6c6d53" alt="Filtering" data-og-width="1156" width="1156" data-og-height="551" height="551" data-path="langsmith/images/filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f03bd4a75a5b8b0cc2cb2d96b0908000 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=68f5583f17be5060e8815ae14edd6619 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35af2365bef2b299b70e336209c54797 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc2bc02daf5f56c914cd0cf7f274d200 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e45165a47a471aaf0e94dcb5e2abbb7d 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98ee26df269a2bdabad8a3f5bea1211a 2500w" />

2. **Filter Shortcuts**: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your project's runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c70333d17e9f38f813a6ddef017c7a29" alt="Filter Shortcuts" data-og-width="1330" width="1330" data-og-height="1078" height="1078" data-path="langsmith/images/filter-shortcuts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2e073e3f070674704f3f6506e851bc2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dbdcf6272d1f115a158f4b1dad02f4b7 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=808645fd9ebcd95e424b8b0a47a75cde 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=de2a53f560cfaecb13673b400a46de55 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73ac5c2913b9e13ae99822c2a80b290e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f126680bfdf0fe958a7cc06429387582 2500w" />

<Info>
  **Default filter**

By default, the `IsTrace` is `true` filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.
</Info>

### Filtering by time range

In addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=214ec0a0bcec5e5bf07a54becec35a80" alt="Filtering on time" data-og-width="1325" width="1325" data-og-height="680" height="680" data-path="langsmith/images/filter-time.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cf9a2351d13ebfcada25090fc8a56f6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35ef8e40fcb515e8b9b9b98d49312521 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7c69f28b9a1debb36dd1929292d0dca3 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a538ee5b3b711f1d257cc733e701ef0c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=acb075b10621f69b098b929fc64d54d3 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e11f9d866b5ad3b89a29fad8167f5aac 2500w" />

The available filter operators depend on the data type of the attribute you are filtering on. Here's an overview of common operators:

* **is**: Exact match on the filter value
* **is not**: Negative match on the filter value
* **contains**: Partial match on the filter value
* **does not contain**: Negative partial match on the filter value
* **is one of**: Match on any of the values in the list
* `>` / `<`: Available for numeric fields

## Specific Filtering Techniques

### Filter for intermediate runs (spans)

In order to filter for intermediate runs (spans), you first need to remove the default `IsTrace` is `true` filter. For example, you would do this if you wanted to filter by `run name` for sub runs or filter by `run type`.

Run metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out [this guide](./add-metadata-tags).

### Filter based on inputs and outputs

You can filter runs based on the content in the inputs and outputs of the run.

To filter either inputs or outputs, you can use the `Full-Text Search` filter which will match keywords in either field. For more targeted search, you can use the `Input` or `Output` filters which will only match content based on the respective field.

<Note>
  For performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using [Input/Output key-value search](/langsmith/filter-traces-in-application#filter-based-on-input-%2F-output-key-value-pairs) instead.
</Note>

You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.

Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9d233d67218491a50cc759335a8ce6fa" alt="Filtering" data-og-width="368" width="368" data-og-height="301" height="301" data-path="langsmith/images/filter-full-text.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf7a2397bf1fda0c41a460bd8c2ae9f2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ca993100e989cdfe8628b0d788fbd37 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1c379c825a1759dc3e13f7c182475159 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e1eab353b75b25d22a901a01caff403 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8fd894a5a8725b5f048bcc9d03b27d07 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f3a68f1e2dc9ab3451f1d5e10733322c 2500w" />

Based on the filters above, the system will search for `python` and `tensorflow` in either inputs or outputs, and `embedding` in the inputs along with `fine` and `tune` in the outputs.

### Filter based on input / output key-value pairs

In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.

<Note>
  We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance.
</Note>

To filter based on key-value pairs, select the `Input Key` or `Output Key` filter from the filters dropdown.

For example, to match the following input:

Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:

Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:

With the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.

LangSmith will break it into the following set of searchable key-value pairs:

| Key                                                | Value                                                                        |
| -------------------------------------------------- | ---------------------------------------------------------------------------- |
| `generations.type`                                 | `ChatGeneration`                                                             |
| `generations.message.type`                         | `constructor`                                                                |
| `generations.message.kwargs.type`                  | `ai`                                                                         |
| `generations.message.kwargs.id`                    | `run-ca7f7531-f4de-4790-9c3e-960be7f8b109`                                   |
| `generations.message.kwargs.tool_calls.name`       | `Plan`                                                                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Research LangGraph's node configuration capabilities`                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Investigate how to add a Python code execution node`                        |
| `generations.message.kwargs.tool_calls.args.steps` | `Find an example or create a sample implementation of a code execution node` |
| `generations.message.kwargs.tool_calls.id`         | `toolu_01XexPzAVknT3gRmUB5PK5BP`                                             |
| `generations.message.kwargs.tool_calls.type`       | `tool_call`                                                                  |
| `type`                                             | `LLMResult`                                                                  |

To search for a specific tool call, you can use the following Output Key search while removing the root runs filter:

`generations.message.kwargs.tool_calls.name` = `Plan`

This will match root and non-root runs where the `tool_calls` name is `Plan`.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d763b8572d14db7f1bc59a3cf4b08a5" alt="Filtering" data-og-width="629" width="629" data-og-height="98" height="98" data-path="langsmith/images/search-kv-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cd9ec450a02572f4bdcec3850bbf4c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a0988bd110eb51fc73f7e5e2875117b2 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=21710580f012a7a62eeaf528f735f4c9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5778f4ef1f156e7d2d0a04f45339aa72 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae736245285787c485a6a085e5eec65e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=178a7e4dec57f7260393ee8d0b16030a 2500w" />

### Negative filtering on key-value pairs

Different types of negative filtering can be applied to `Metadata`, `Input Key`, and `Output Key` fields to exclude specific runs from your results.

For example, to find all runs where the metadata key `phone` is not equal to `1234567890`, set the `Metadata` `Key` operator to `is` and `Key` field to `phone`, then set the `Value` operator to `is not` and the `Value` field to `1234567890`. This will match all runs that have a metadata key `phone` with any value except `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acbd683725e073e1ac78b8ba132e6d43" alt="Filtering" data-og-width="549" width="549" data-og-height="100" height="100" data-path="langsmith/images/negative-filtering-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a0fd6f85165cae0ca7a7d96be57805e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=714cdc9f0ad4e6c2b67d49d79740e9b2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf74951b9cf760cbd6c5653b6d15b95a 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4ae07131bdb12d5e6c5676e51f54ee7c 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bfb45574bec84ad88d9866b025ac61ba 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4a74fca06b1bf2940f15528b04f1b21 2500w" />

To find runs that don't have a specific metadata key, set the `Key` operator to `is not`. For example, setting the `Key` operator to `is not` with `phone` as the key will match all runs that don't have a `phone` field in their metadata.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2c298181ed0aef202ca52f5c30b3d62b" alt="Filtering" data-og-width="419" width="419" data-og-height="128" height="128" data-path="langsmith/images/negative-filtering-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb11546a30cf22c9653c05519b263409 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a7f5a2ac54f187e449f24011af6f160 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=161ed2a1433d0fa695ef527b32292ee6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d7f8b99d1af28d2e27cdab3c3b03fdad 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ef65a43c85924cf25da164bc049e7029 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c79162abd3187782b27f06f28a9d590 2500w" />

You can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key `phone` nor any field with the value `1234567890`, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is not` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18ca1b82babfda201b184a8884d120eb" alt="Filtering" data-og-width="571" width="571" data-og-height="125" height="125" data-path="langsmith/images/negative-filtering-3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=50c9bee0afc47df246ddb3313e1b7784 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5bbf4d4da70355e861ce7b7adf5ca4b5 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba52269765451c26687cc7b74a572a68 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b2b21485dc7073c4edc8b0d1b6718b1 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18b134952892cc7e5203a5dfaa25eda4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f546a96e2b6b229cc593e8a31ec7572f 2500w" />

Finally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no `phone` key but there is a value of `1234567890` for some other key, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=60e118205cc6524432058ce968aad478" alt="Filtering" data-og-width="546" width="546" data-og-height="126" height="126" data-path="langsmith/images/negative-filtering-4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=edd2fee83ba3d63bd8a44f82cfc2c66c 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=20bfead7865f62ee4db7890734c811ac 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42d6832b0db4843441c5a8dcd95c88b7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6361c16f6321f6f47ca83867f3f5bfbd 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fcbe01d2be9672356101c4b9a585e78 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4dcb53d4331f8a1802ec49b1c3d1971a 2500w" />

Note that you can use the `does not contain` operator instead of `is not` to perform a substring match.

Saving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.

In the filter box, click the **Save filter** button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=27236a7b0e60a352d16b7affd9b913e5" alt="Filtering" data-og-width="854" width="854" data-og-height="775" height="775" data-path="langsmith/images/save-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8bf93a5ab643e0f0ef5194529a084879 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6cb4d039bf74e97e65688465707560ab 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381acf9bb9f3c2443faeb4dc0c84506f 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fd1660d33217beb5031de6546ff2cdf7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4b4bc3898549d5f505d0e8989307d5f4 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a9927179e81ace45255aef6a99c77aa3 2500w" />

#### Use a saved filter

After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a "more" menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89774c393f6303c981adeba6750f7cad" alt="Filtering" data-og-width="1256" width="1256" data-og-height="404" height="404" data-path="langsmith/images/selecting-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a204feab24664b271ddc70ea8dd60698 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4806fcac0a82770fccdb66b06181736e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9600c7b41c39c9759347a7a5f6ba7d4d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2f4a06c4a4dd87c995b54cae9f177e23 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5f5df78152c343362d6f3ba0c7a9a82a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d681b4c6a42c7c4a9440525bb1a3557 2500w" />

#### Update a saved filter

With the filter selected, make any changes to filter parameters. Then click **Update filter** > **Update** to update the filter.

In the same menu, you can also create a new saved filter by clicking **Update filter** > **Create new**.

#### Delete a saved filter

Click the settings icon in the saved filter bar, and delete a filter using the trash icon.

You can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs).

In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.

This will give you a string representing the filter in the LangSmith query language. For example: `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))`. For more information on the query language syntax, please refer to [this reference](/langsmith/trace-query-syntax#filter-query-language).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f9f60cca965c1466fe58ec4e8cc52ea5" alt="Copy Filter" data-og-width="1123" width="1123" data-og-height="382" height="382" data-path="langsmith/images/copy-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8966107385058659add5005faa0570d5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=54cdbad3c852e011f1bbf9bf59cd553d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198de895cd5c70de24e72b9eb5f8975d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b500fb05f2705eb74157d5a1ba2bebbe 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67cdd3110def30e481ec0266eaffac66 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2366d6a3ab857b74bcbd551122ec350 2500w" />

## Filtering runs within the trace view

You can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here.

By default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from "Filtered Only" to "Show All" or "Most relevant".

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13ea9f30d6efd9bb3f96c8c2376a6858" alt="Filtering within trace view" data-og-width="1341" width="1341" data-og-height="764" height="764" data-path="langsmith/images/filter-runs-in-trace-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=506069ddcc28d8959213f1bfa5d4d2eb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8b4e2a130a76953f7291e983fb13c86d 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f702ad52060f28de4febdc556daafc6e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c6506211f69e03b54793fce9c85760b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ad5849855b303c6357705708e038a9d6 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6fd17200291f3c6eb680861aa1ab7d80 2500w" />

## Manually specify a raw query in LangSmith query language

If you have [copied a previously constructed filter](/langsmith/filter-traces-in-application#copy-the-filter), you may want to manually apply this raw query in a future session.

In order to do this, you can click on **Advanced filters** on the bottom of the filters popover. From there you can paste a raw query into the text box.

Note that this will add that query to the existing queries, not overwrite it.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0657c7420e760fb57782531d039c0b0d" alt="Raw Query" data-og-width="495" width="495" data-og-height="570" height="570" data-path="langsmith/images/raw-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8ebe5718f7e50c0e5186d23ea6467620 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=121dab45e0a3f1599ad9dd26132f2c59 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0d3ca66c6498f2c1d4e33bf4b63068e8 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef61bc19b9fe41eb0e5d80566049e288 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ccaeac2952f3a2fde4d99c5d69c8055d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f15ec18a7e3cb748defc6d10c3033d28 2500w" />

## Use an AI Query to auto-generate a query (Experimental)

Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added an `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.

For example: "All runs longer than 10 seconds"

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ef4266d159753dacafbd5af2bcad0ab8" alt="AI Query" data-og-width="464" width="464" data-og-height="319" height="319" data-path="langsmith/images/ai-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9589aa2ef6d4fd16533c0b9123ab552e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f03177102e2c6fa2ec2043bce0d3aa7d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3f6c3c22f37565790e77fa8a55e459a6 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8ba2568d2044c01364206560d7dd8efc 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8bf30e3c64545cca142bf0a6745e19f6 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c38f6e1b2e00ef271d25f9f674aa27a4 2500w" />

### Filter for intermediate runs (spans) on properties of the root

A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.

In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a520558efb53e4dd3d6751db76739f90" alt="Filtering" data-og-width="551" width="551" data-og-height="542" height="542" data-path="langsmith/images/trace-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5303c1b8db95a047593d52ff52ebc55f 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d21e37d06556b6cd20d88c6b72293f68 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=700e831f6c8d473bb9eb3beca7131b4e 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=43467c081d7cfc83582220fe6b7266fe 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cda650d2069d852c95d8e51893e424d 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0fa57df70f665ef672c917fe6231c15 2500w" />

### Filter for runs (spans) whose child runs have some attribute

This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.

In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specify apply to all child runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b70fc03e9f46333d7af3566619204f0" alt="Filtering" data-og-width="471" width="471" data-og-height="602" height="602" data-path="langsmith/images/child-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e7f54072ed1f198804ffca3e073710c 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e7b6627d5edece1b4f90879c03310 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=265d349fc921cf06be7910f20f275a5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8445bba9df0671da9e718e63c4e2cc48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cfd020e86ca4b5e2cc77ee43a0c35746 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=199dab9b8f3adbaecadf3c2206ab008c 2500w" />

### Example: Filtering on all runs whose tree contains the tool call filter

Extending the [tool call filtering example](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) from above, if you would like to filter for all runs *whose tree contains* the tool filter call, you can use the tree filter in the [advanced filters](/langsmith/filter-traces-in-application#advanced-filters) setting:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b11deac10cd42be7efff94134658b25b" alt="Filtering" data-og-width="669" width="669" data-og-height="462" height="462" data-path="langsmith/images/search-kv-tool-tree.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3f487a02af0891c8c97935b8674d8aba 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4334fbf4536432920e35d1df4f5cff2e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7b2edb49f520ebe3e2f03651c8d9d39 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6c107e48006c1ee4d2246935b1259595 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be785994ef014f06767d1001f5d5dbf1 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c686bc45b1d33a12ddcc89aa9c5c0a35 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-traces-in-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:
```

Example 2 (unknown):
```unknown
Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:
```

---

## Receivers

**URL:** llms-txt#receivers

**Contents:**
- Logs
- Metrics
  - Traces
- Processors
  - Recommended OTEL Processors
- Exporters

This is an example for a ***Sidecar*** collector to read logs from its own pod, excluding logs from non domain-specific containers. A Sidecar configuration is useful here because we require access to every container's filesystem. A DaemonSet can also be used.

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:

<Note>
  **The OTel Collector also supports exporting directly to a [Datadog](https://docs.datadoghq.com/opentelemetry/setup/collector_exporter) endpoint.**
</Note>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

## Metrics

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:
```

Example 2 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

### Traces

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:
```

Example 3 (unknown):
```unknown
## Processors

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

## Exporters

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:
```

---

## Metrics: [OTel Example](/langsmith/langsmith-collector#metrics)

**URL:** llms-txt#metrics:-[otel-example](/langsmith/langsmith-collector#metrics)

**Contents:**
- LangSmith Services
- Frontend Nginx
- Postgres + Redis
- Clickhouse

## LangSmith Services

The following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.

* **Backend**: `http://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics`
* **Platform Backend**: `http://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics`
* **Playground**: `http://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics`
* **(LangSmith Control Plane only) Host Backend**: `http://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics`

You can use a [Prometheus](https://prometheus.io/docs/prometheus/latest/getting_started/#configure-prometheus-to-monitor-the-sample-targets) or [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) collector to scrape the endpoints, and export metrics to the backend of your choice.

The frontend service exposes its Nginx metrics at the following endpoint: `langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status`. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the [LangSmith Observability Helm Chart](/langsmith/observability-stack)

<Warning>
  **The following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.**
</Warning>

If you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the [LangSmith Observability Helm Chart](/langsmith/observability-stack) to deploy an exporter for you.

The in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics at `http://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics`

---

## LangSmith-managed ClickHouse

**URL:** llms-txt#langsmith-managed-clickhouse

**Contents:**
- Architecture Overview
- Requirements
- Data storage
  - Stored feedback data fields
  - Stored run data fields

Source: https://docs.langchain.com/langsmith/langsmith-managed-clickhouse

<Check>
  Please read the [LangSmith architectural overview](/langsmith/self-hosted) and [guide on connecting to external ClickHouse](/langsmith/self-host-external-clickhouse) before proceeding with this guide.
</Check>

LangSmith uses ClickHouse as the primary storage engine for **traces** and **feedback**. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external ClickHouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.

## Architecture Overview

The architecture of using LangSmith-managed ClickHouse with your self-hosted LangSmith instance is similar to using a fully self-hosted ClickHouse instance, with a few key differences:

* You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.
* With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of ClickHouse to ensure that sensitive information doesn't leave your VPC. For more details on where particular data fields are stored, refer to [Data storage](#data-storage).
* The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.

The overall architecture looks like this:

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=26fae5c3f413c15302ea0c00bebf8e93" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b9cb43d51325b0d9858123066c0f8812 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f21865c150f4047d9cfd0eee690099af 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3df50b05e66bc3092558e23e321d11fe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=821f427c87e452c5d93aec971de3e5c3 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ffd26e217759a683feeacdacde65c047 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=75b39fb829a3ed7491f3550ad5323965 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a3062f45f9c01f05e6917bca3f34735e" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c2a591803a03df470900d351a3ba59dc 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=80f1fd346d92de102121180ac7995e0f 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6bb18f947823c721cba47bc275999cc2 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0d7313f3682d35e743a82f44465b9af6 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=2f96d154dff0885379ce9d0b6a8e40e9 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b750629ab6f6ce608695538ff3ad46f5 2500w" />

* **You must use a supported blob storage option.** Read the [blob storage guide](/langsmith/self-host-blob-storage) for more information.
* To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported [region](https://clickhouse.com/docs/en/cloud/reference/supported-regions). Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to allowlist your traffic.
* You must have a VPC that can connect to the LangSmith-managed ClickHouse service. You will need to work with our team to set up the necessary networking.
* You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both [Kubernetes](/langsmith/kubernetes) and [Docker](/langsmith/docker) installations.

ClickHouse stores **runs** and **feedback** data, specifically:

* All feedback data fields.
* Some run data fields.

For a list of fields, refer to [Stored run data fields](#stored-run-data-fields) and [Stored feedback data fields](#stored-feedback-data-fields).

LangChain defines sensitive application data as `inputs`, `outputs`, `errors`, `manifests`, `extras`, and `events` of a run, since these fields may contain LLM prompts and completions. With LangSmith-managed ClickHouse, these sensitive fields are stored in cloud object storage (S3 or GCS) within your cloud, while the rest of the run data is stored in ClickHouse, ensuring sensitive information never leaves your VPC.

### Stored feedback data fields

<Note>
  Because all feedback data is stored in ClickHouse, do not send sensitive information in feedback (scores and annotations/comments) or in any other run fields that are mentioned in [Stored run data fields](#stored-run-data-fields).
</Note>

Using a LangSmith-managed ClickHouse setup, **all feedback data fields are stored in ClickHouse**:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

This [reference doc](/langsmith/feedback-data-format) explains the stored feedback format, which is the LangSmith's way of representing evaluation scores and annotations on runs.

### Stored run data fields

Run data fields are split between the managed ClickHouse database and your cloud object storage (e.g., S3 or GCS).

<Note>
  For run fields stored in object storage, only a reference or pointer is kept in ClickHouse. For example, `inputs` and `outputs` content are offloaded to S3/GCS, with the ClickHouse record storing corresponding S3 URLs in the `inputs_s3_urls` and `outputs_s3_urls` fields.
</Note>

The table details each run field and where it is stored:

| Field                          | Storage Location   |
| ------------------------------ | ------------------ |
| `id`                           | ClickHouse         |
| `name`                         | ClickHouse         |
| `inputs`                       | **Object Storage** |
| `run_type`                     | ClickHouse         |
| `start_time`                   | ClickHouse         |
| `end_time`                     | ClickHouse         |
| `extra`                        | **Object Storage** |
| `error`                        | **Object Storage** |
| `outputs`                      | **Object Storage** |
| `events`                       | **Object Storage** |
| `tags`                         | ClickHouse         |
| `trace_id`                     | ClickHouse         |
| `dotted_order`                 | ClickHouse         |
| `status`                       | ClickHouse         |
| `child_run_ids`                | ClickHouse         |
| `direct_child_run_ids`         | ClickHouse         |
| `parent_run_ids`               | ClickHouse         |
| `feedback_stats`               | ClickHouse         |
| `reference_example_id`         | ClickHouse         |
| `total_tokens`                 | ClickHouse         |
| `prompt_tokens`                | ClickHouse         |
| `completion_tokens`            | ClickHouse         |
| `total_cost`                   | ClickHouse         |
| `prompt_cost`                  | ClickHouse         |
| `completion_cost`              | ClickHouse         |
| `first_token_time`             | ClickHouse         |
| `session_id`                   | ClickHouse         |
| `in_dataset`                   | ClickHouse         |
| `parent_run_id`                | ClickHouse         |
| `execution_order` (deprecated) | ClickHouse         |
| `serialized`                   | ClickHouse         |
| `manifest_id` (deprecated)     | ClickHouse         |
| `manifest_s3_id`               | ClickHouse         |
| `inputs_s3_urls`               | ClickHouse         |
| `outputs_s3_urls`              | ClickHouse         |
| `price_model_id`               | ClickHouse         |
| `app_path`                     | ClickHouse         |
| `last_queued_at`               | ClickHouse         |
| `share_token`                  | ClickHouse         |

This [reference doc](/langsmith/run-data-format) explains the format of stored runs (spans), which are the building blocks of traces.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-managed-clickhouse.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up custom authentication

**URL:** llms-txt#set-up-custom-authentication

**Contents:**
- 1. Create your app
- 2. Add authentication

Source: https://docs.langchain.com/langsmith/set-up-custom-auth

In this tutorial, we will build a chatbot that only lets specific users access it. We'll start with the LangGraph template and add token-based security step by step. By the end, you'll have a working chatbot that checks for valid tokens before allowing access.

This is part 1 of our authentication series:

1. Set up custom authentication (you are here) - Control who can access your bot
2. [Make conversations private](/langsmith/resource-auth) - Let users have private conversations
3. [Connect an authentication provider](/langsmith/add-auth-server) - Add real user accounts and validate using OAuth2 for production

This guide assumes basic familiarity with the following concepts:

* [**Authentication & Access Control**](/langsmith/auth)
* [**LangSmith**](/langsmith/home)

<Note>
  Custom auth is only available for LangSmith SaaS deployments or Enterprise Self-Hosted deployments.
</Note>

## 1. Create your app

Create a new chatbot using the LangGraph starter template:

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

The server will start and open [Studio](/langsmith/studio) in your browser:

If you were to self-host this on the public internet, anyone could access it.

<img src="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=3ca2c9a8d65891ef71abfb7ad0aae7d3" alt="No authentication: the dev server is publicly reachable, anyone can access the bot if exposed to the internet." data-og-width="1974" width="1974" data-og-height="1412" height="1412" data-path="langsmith/images/no-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=280&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=67bfe450ee04d2432e5e1b86cfa0af1c 280w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=560&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=087f8083981ae85eeca794bdca3e0e05 560w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=840&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=7967264c0243918be1a8561d4db89586 840w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1100&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=fd9d62383850b137834bfc7e35bc4533 1100w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1650&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=66100323459f51596c53b3d941e3f85d 1650w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=2500&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=a7b9157f67b3094370cc5399d917a164 2500w" />

## 2. Add authentication

Now that you have a base LangGraph app, add authentication to it.

<Note>
  In this tutorial, you will start with a hard-coded token for example purposes. You will get to a "production-ready" authentication scheme in the third tutorial.
</Note>

The [`Auth`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.Auth) object lets you register an authentication function that the LangSmith deployment will run on every request. This function receives each request and decides whether to accept or reject.

Create a new file `src/security/auth.py`. This is where your code will live to check if users are allowed to access your bot:

```python {highlight={10,15-16}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Run an evaluation with multimodal content

**URL:** llms-txt#run-an-evaluation-with-multimodal-content

**Contents:**
- SDK
  - 1. Create examples with attachments

Source: https://docs.langchain.com/langsmith/evaluate-with-attachments

LangSmith lets you create dataset examples with file attachments—like images, audio files, or documents—so you can reference them when evaluating an application that uses multimodal inputs or outputs.

While you can include multimodal data in your examples by base64 encoding it, this approach is inefficient - the encoded data takes up more space than the original binary files, resulting in slower transfers to and from LangSmith. Using attachments instead provides two key benefits:

1. Faster upload and download speeds due to more efficient binary file transfers
2. Enhanced visualization of different file types in the LangSmith UI

### 1. Create examples with attachments

To upload examples with attachments using the SDK, use the [create\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_examples) / [update\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.update_examples) Python methods or the [uploadExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#uploadexamplesmultipart) / [updateExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#updateexamplesmultipart) TypeScript methods.

Requires `langsmith>=0.3.13`

```python  theme={null}
import requests
import uuid
from pathlib import Path
from langsmith import Client

---

## Thread 1: Write to long-term memory

**URL:** llms-txt#thread-1:-write-to-long-term-memory

config1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "Save my preferences to /memories/preferences.txt"}]
}, config=config1)

---

## Define a new graph

**URL:** llms-txt#define-a-new-graph

workflow = StateGraph(State)

---

## Learn

**URL:** llms-txt#learn

**Contents:**
- Use Cases
  - LangChain
  - LangGraph
- Conceptual Overviews
- Additional Resources

Source: https://docs.langchain.com/oss/python/learn

Tutorials, conceptual guides, and resources to help you get started.

In the **Learn** section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.

Below are tutorials for common use cases, organized by framework.

[LangChain](/oss/python/langchain/overview) [agent](/oss/python/langchain/agents) implementations make it easy to get started for most use cases.

<Card title="Semantic Search" icon="magnifying-glass" href="/oss/python/langchain/knowledge-base" horizontal>
  Build a semantic search engine over a PDF with LangChain components.
</Card>

<Card title="RAG Agent" icon="user-magnifying-glass" href="/oss/python/langchain/rag" horizontal>
  Create a Retrieval Augmented Generation (RAG) agent.
</Card>

<Card title="SQL Agent" icon="database" href="/oss/python/langchain/sql-agent" horizontal>
  Build a SQL agent to interact with databases with human-in-the-loop review.
</Card>

<Card title="Supervisor Agent" icon="sitemap" href="/oss/python/langchain/supervisor" horizontal>
  Build a personal assistant that delegates to sub-agents.
</Card>

LangChain's [agent](/oss/python/langchain/agents) implementations use [LangGraph](/oss/python/langgraph/overview) primitives.
If deeper customization is required, agents can be implemented directly in LangGraph.

<Card title="Custom RAG Agent" icon="user-magnifying-glass" href="/oss/python/langgraph/agentic-rag" horizontal>
  Build a RAG agent using LangGraph primitives for fine-grained control.
</Card>

<Card title="Custom SQL Agent" icon="database" href="/oss/python/langgraph/sql-agent" horizontal>
  Implement a SQL agent directly in LangGraph for maximum flexibility.
</Card>

## Conceptual Overviews

These guides explain the core concepts and APIs underlying LangChain and LangGraph.

<Card title="Memory" icon="brain" href="/oss/python/concepts/memory" horizontal>
  Understand persistence of interactions within and across threads.
</Card>

<Card title="Context engineering" icon="book-open" href="/oss/python/concepts/context" horizontal>
  Learn methods for providing AI applications the right information and tools to accomplish a task.
</Card>

<Card title="Graph API" icon="chart-network" href="/oss/python/langgraph/graph-api" horizontal>
  Explore LangGraph’s declarative graph-building API.
</Card>

<Card title="Functional API" icon="code" href="/oss/python/langgraph/functional-api" horizontal>
  Build agents as a single function.
</Card>

## Additional Resources

<Card title="LangChain Academy" icon="graduation-cap" href="https://academy.langchain.com/" horizontal>
  Courses and exercises to level up your LangChain skills.
</Card>

<Card title="Case Studies" icon="screen-users" href="/oss/python/langgraph/case-studies" horizontal>
  See how teams are using LangChain and LangGraph in production.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/learn.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Evaluator

**URL:** llms-txt#evaluator

def correct(outputs: dict, reference_outputs: dict) -> bool:
    """Check if the agent chose the correct route."""
    return outputs["route"] == reference_outputs["route"]

---

## Assumes you organize information in store like (user_id, resource_type, resource_id)

**URL:** llms-txt#assumes-you-organize-information-in-store-like-(user_id,-resource_type,-resource_id)

@auth.on.store()
async def authorize_store(ctx: Auth.types.AuthContext, value: dict):
    # The "namespace" field for each store item is a tuple you can think of as the directory of an item.
    namespace: tuple = value["namespace"]
    assert namespace[0] == ctx.user.identity, "Not authorized"
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that instead of one global handler, you now have specific handlers for:

1. Creating threads
2. Reading threads
3. Accessing assistants

The first three of these match specific **actions** on each resource (see [resource actions](/langsmith/auth#resource-specific-handlers)), while the last one (`@auth.on.assistants`) matches *any* action on the `assistants` resource. For each request, LangGraph will run the most specific handler that matches the resource and action being accessed. This means that the four handlers above will run rather than the broadly scoped "`@auth.on`" handler.

Try adding the following test code to your test file:
```

---

## Create child run

**URL:** llms-txt#create-child-run

child_run_id = uuid4()
post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

---

## Custom instrumentation

**URL:** llms-txt#custom-instrumentation

**Contents:**
- Use `@traceable` / `traceable`
- Use the `trace` context manager (Python only)
- Use the `RunTree` API
- Example usage

Source: https://docs.langchain.com/langsmith/annotate-code

<Note>
  If you've decided you no longer want to trace your runs, you can remove the `LANGSMITH_TRACING` environment variable. Note that this does not affect the `RunTree` objects or API users, as these are meant to be low-level and not affected by the tracing toggle.
</Note>

There are several ways to log traces to LangSmith.

<Check>
  If you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the [LangChain-specific instructions](/langsmith/trace-with-langchain).
</Check>

## Use `@traceable` / `traceable`

LangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

The `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.

Note that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to
ensure the trace is logged correctly.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.

## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

You can extend the utilities above to conveniently trace any code. Below are some example extensions:

Trace any public method in a class:

```python  theme={null}
from typing import Any, Callable, Type, TypeVar

def traceable_cls(cls: Type[T]) -> Type[T]:
    """Instrument all public methods in a class."""
    def wrap_method(name: str, method: Any) -> Any:
        if callable(method) and not name.startswith("__"):
            return traceable(name=f"{cls.__name__}.{name}")(method)
        return method

# Handle __dict__ case
    for name in dir(cls):
        if not name.startswith("_"):
            try:
                method = getattr(cls, name)
                setattr(cls, name, wrap_method(name, method))
            except AttributeError:
                # Skip attributes that can't be set (e.g., some descriptors)
                pass

# Handle __slots__ case
    if hasattr(cls, "__slots__"):
        for slot in cls.__slots__:  # type: ignore[attr-defined]
            if not slot.startswith("__"):
                try:
                    method = getattr(cls, slot)
                    setattr(cls, slot, wrap_method(slot, method))
                except AttributeError:
                    # Skip slots that don't have a value yet
                    pass

@traceable_cls
class MyClass:
    def __init__(self, some_val: int):
        self.some_val = some_val

def combine(self, other_val: int):
        return self.some_val + other_val

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.
```

Example 3 (unknown):
```unknown
## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Monitor projects with dashboards

**URL:** llms-txt#monitor-projects-with-dashboards

**Contents:**
- Prebuilt dashboards
  - Dashboard sections
  - Group by
- Custom Dashboards
  - Creating a new dashboard
  - Adding charts to your dashboard
  - Chart configuration
  - Save and manage charts
- Linking to a dashboard from a tracing project
- Example: user-journey monitoring

Source: https://docs.langchain.com/langsmith/dashboards

Dashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the **Monitoring** tab in the left sidebar.

LangSmith offers two dashboard types:

* **Prebuilt dashboards**: Automatically generated for every tracing project.
* **Custom dashboards**: Fully configurable collections of charts tailored to your needs.

## Prebuilt dashboards

Prebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the **Dashboard** button on the top right of the tracing project page.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt.gif?s=cbb7410db7c9036ed6c03af251f13a99" alt="prebuilt" data-og-width="1392" width="1392" data-og-height="1080" height="1080" data-path="langsmith/images/prebuilt.gif" data-optimize="true" data-opv="3" />

<Note>**You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.**</Note>

### Dashboard sections

Prebuilt dashboards are broken down into the following sections:

| Section         | What it shows                                                                                                                                                                                                                                                                                                    |
| :-------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Traces          | Trace count, latency and error rates. A [trace](/langsmith/observability-concepts#traces) is a collection of [runs](/langsmith/observability-concepts#runs) related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace. |
| LLM Calls       | LLM call count and latency. Includes all runs where run type is "llm".                                                                                                                                                                                                                                           |
| Cost & Tokens   | Total and per-trace token counts and costs, broken down by token type. Costs are measured using [LangSmith's cost tracking](/langsmith/log-llm-trace#manually-provide-token-counts).                                                                                                                             |
| Tools           | Run counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is "tool". Limits to top 5 most frequently occurring tools.                                                                                                                                      |
| Run Types       | Run counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table.                                              |
| Feedback Scores | Aggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback.                                                                                                                                        |

For example, for the following trace, the following runs have a depth of 1:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=382b7d6064e09c23efc6770fcd983a69" alt="" data-og-width="524" width="524" data-og-height="810" height="810" data-path="langsmith/images/run-depth-explained.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=03503ce7bfd170a22ff5152a7564e130 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a84d5785aaa93bb3e8ab7d2a4143f063 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d6df2d7b6dbe6e358ad2f03df341661 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74abfc6a192c9a433dcf62b01a571594 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9e3089b4311d828f80594649647f85bd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c6e3a31aded7c2ad7ae96f948613c983 2500w" />

Group by [run tag or metadata](/langsmith/add-metadata-tags) can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won't take effect; the global group by will apply to all other charts.

<Note>When adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.</Note>

Create tailored collections of charts for tracking metrics that matter most for your application.

### Creating a new dashboard

1. Navigate to the **Monitor** tab in the left sidebar.
2. Click on the **+ New Dashboard** button.
3. Give your dashboard a name and a description.
4. Click on **Create**.

### Adding charts to your dashboard

1. Within a dashboard, click on the **+ New Chart** button to open up the chart creation pane.
2. Give your chart a name and a description.
3. Configure the chart.

### Chart configuration

#### Select tracing projects and filter runs

* Select one or more tracing projects to track metrics for.
* Use the **Chart filters** section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on [filtering traces in application](./filter-traces-in-application).

* Choose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you'll see a preview of your chart and the matching runs.
* For certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3543d454f25f5d11046fbd5bcab7aeff" alt="Multiple metrics" data-og-width="1475" width="1475" data-og-height="741" height="741" data-path="langsmith/images/compare-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=75f80a706d3e3ddd09117bc9d317ecdc 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2606ae254c47349e2c5bd14ae4fc49b8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4239ab64861d8e3768f90f837dbefe67 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0e8d5f3e7fcf40fe6f33b6066afa346f 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d45519451d443538ad76fdcbb0d21f62 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33024dfb462b8229b16c4fbd9cc650ba 2500w" />

There are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):

1. **Group by**: Group runs by [run tag or metadata](/langsmith/add-metadata-tags), run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency.

2. **Data series**: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e5ed30b317abdb90aea612702d94cf04" alt="Multiple data series" data-og-width="2796" width="2796" data-og-height="1396" height="1396" data-path="langsmith/images/multiple-data-series.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=75134cada0ab88b532d6073c3317dc36 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a0b345dd25435cae0c6e25fee62942 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aede683fd53d369cbfe7c1649d128953 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e96e54ecf149d6a5881bc44d81941220 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fee4a18237d105668c227eafb014d0d5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a93006b73bc0451c68acf10189155a4 2500w" />

#### Pick a chart type

* Choose between a line chart and a bar chart for visualizing

### Save and manage charts

* Click `Save` to save your chart to the dashboard.
* Edit or delete a chart by clicking the triple dot button in the top right of the chart.
* Clone a chart by clicking the triple line button in the top right of the chart and selecting **+ Clone**. This will open a new chart creation pane with the same configurations as the original.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b40f5f132a2c0770cfe04a125dcbc7f2" alt="More actions bar" data-og-width="2102" width="2102" data-og-height="758" height="758" data-path="langsmith/images/more-actions-bar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4033fc996ce15ac3eee9e42430f3089 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0176aa9470908b2ee287c6588f8898ae 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=38af0f07ca3394b40dbdd4f6d83af5ba 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b41c74e251946e2f6f31739c71a42096 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c5a6f79532146e2596857be1f85f342a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=623e4b00a5a04e950e8312cf2a2233b6 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d0c5597d6adfa7d02888ef4b89ac616d" alt="Expanded chart" data-og-width="2238" width="2238" data-og-height="1662" height="1662" data-path="langsmith/images/expanded-chart.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73fa53c7cdb5a89eaac5e685035bfc82 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=df3f46036e1692777f6c0d460b78ca0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9c4abdd5d8cf517a9e537d09f6d6e761 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91a2ff0f83ad36710058479fc5f30698 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7f16a10cf75c87fb38f3cf6db39e1fe4 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d6ff4e7841cdf169c4e069605c7089 2500w" />

## Linking to a dashboard from a tracing project

You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:

1. In your tracing project, click the three dots next to the **Dashboard** button.
2. Choose a dashboard to set as the new default.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06b63ab9c4f8d7185c72a77e84862f3a" alt="Tracing project to dashboard" data-og-width="2080" width="2080" data-og-height="770" height="770" data-path="langsmith/images/tracing-project-to-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b06a5e1a4fe6b82260032624bdc1ce68 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ea5dd2397337b290c996ef26948836ea 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=26832107ff0613d0575432fc4f2424d6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2ace3a16bd11cd8a8906bbdab77c3f10 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c5b29e7497d837a0d84b97f5c3c522a6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6055757e9ac1f5fd9381d14bd92cf088 2500w" />

## Example: user-journey monitoring

Use monitoring charts for mapping the decisions made by an agent at a particular node.

Consider an email assistant agent. At a particular node it makes a decision about an email to:

* send an email back
* notify the user
* no response needed

We can create a chart to track and visualize the breakdown of these decisions.

**Creating the chart**

1. **Metric Selection**: Select the metric `Run count`.

2. **Chart Filters**: Add a tree filter to include all of the traces with name `triage_input`. This means we only include traces that hit the `triage_input` node. Also add a chart filter for `Is Root` is `true`, so our count is not inflated by the number of nodes in the trace.
   <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f769fb23624b3b1b6c042ff5cfd910e6" alt="Decision at node" data-og-width="2620" width="2620" data-og-height="1698" height="1698" data-path="langsmith/images/chart-filters-for-node-decision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e09f22192fa9cb75b32f6812071e1e07 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e3108ae084660ecc151a68a47fab6912 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb75ff0f6c0e9140bc4554cceb93134b 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=704fb96e453fdd92dcce1e34a35a48f2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=902d095d27fb3f20b8e403f4ddc18e24 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9afff6512a968017060e6fde0a5fb992 2500w" />

3. **Data Series**: Create a data series for each decision made at the `triage_input` node. The output of the decision is stored in the `triage.response` field of the output object, and the value of the decision is either `no`, `email`, or `notify`. Each of these decisions generates a separate data series in the chart.
   <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eb98a2c2c7988b5b6c5c3db9740ed172" alt="Decision at node" data-og-width="2578" width="2578" data-og-height="1692" height="1692" data-path="langsmith/images/decision-at-node.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=665c6cf0c3368c8f180db15cc7bf4cba 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e68f21187ec95e74e033850200893c66 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33f729b74f697b7b7554badb6c76cec0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ebed1b633a724ed5c9e2b93d62b1c2e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8d3287007e200f37151955edd6cc2632 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ee0d68172a80dce21c1f1afad533298d 2500w" />

Now we can visualize the decisions made at the `triage_input` node over time.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/VxsIvf9NdxI?si=7ksp9qyw-i0lcwxg" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dashboards.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ... database connection and query code

**URL:** llms-txt#...-database-connection-and-query-code

**Contents:**
  - Define the customer support agent

[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
python  theme={null}
import sqlite3

def _refund(invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False) -> float:
    ...

def _lookup( ...
`python  theme={null}
from typing import Literal
import json

from langchain.chat_models import init_chat_model
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, StateGraph
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.types import Command, interrupt
from tabulate import tabulate
from typing_extensions import Annotated, TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
And here's the database schema (image from [https://github.com/lerocha/chinook-database](https://github.com/lerocha/chinook-database)):

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5da2a8dcca68f02dfcec11f9c472d341" alt="Chinook DB" data-og-width="1672" width="1672" data-og-height="1132" height="1132" data-path="langsmith/images/chinook-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ea7b3a27e9780b556aa90f6914dcef30 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d9cf3ddad46562213014ffb1a77b1e45 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e1e70e9be2cf07111b2211e1ef9b7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970f7f48c80222219b493211331ee22f 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3188acb57c4abc0156f8687fa9e229d8 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86b9655b9fd1bc38fcf9e054b11833df 2500w" />

### Define the customer support agent

We'll create a [LangGraph](https://langchain-ai.github.io/langgraph/) agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:

* Lookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: "What songs do you have by Jimi Hendrix?"
* Refund: The customer can request a refund on their past purchases. For example: "My name is Claude Shannon and I'd like a refund on a purchase I made last week, could you help me?"

For simplicity in this demo, we'll implement refunds by deleting the corresponding database records. We'll skip implementing user authentication and other production security measures.

The agent's logic will be structured as two separate subgraphs (one for lookups and one for refunds), with a parent graph that routes requests to the appropriate subgraph.

#### Refund agent

Let's build the refund processing agent. This agent needs to:

1. Find the customer's purchase records in the database
2. Delete the relevant Invoice and InvoiceLine records to process the refund

We'll create two SQL helper functions:

1. A function to execute the refund by deleting records
2. A function to look up a customer's purchase history

To make testing easier, we'll add a "mock" mode to these functions. When mock mode is enabled, the functions will simulate database operations without actually modifying any data.
```

Example 3 (unknown):
```unknown
Now let's define our graph. We'll use a simple architecture with three main paths:

1. Extract customer and purchase information from the conversation

2. Route the request to one of three paths:

   * Refund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund
   * Lookup path: If we have enough customer information (name and phone) to search their purchase history
   * Response path: If we need more information, respond to the user requesting the specific details needed

The graph's state will track:

* The conversation history (messages between user and agent)
* All customer and purchase information extracted from the conversation
* The next message to send to the user (followup text)
```

---

## Send your API Key in the request headers

**URL:** llms-txt#send-your-api-key-in-the-request-headers

headers = {
    "x-api-key": os.environ["LANGSMITH_API_KEY"],
    "x-tenant-id": os.environ["LANGSMITH_WORKSPACE_ID"]
}

def post_run(run_id, name, run_type, inputs, parent_id=None):
    """Function to post a new run to the API."""
    data = {
        "id": run_id.hex,
        "name": name,
        "run_type": run_type,
        "inputs": inputs,
        "start_time": datetime.utcnow().isoformat(),
        # "session_name": "project-name",  # the name of the project to trace to
        # "session_id": "project-id",  # the ID of the project to trace to. specify one of session_name or session_id
    }
    if parent_id:
        data["parent_run_id"] = parent_id.hex

requests.post(
        "https://api.smith.langchain.com/runs",  # Update appropriately for self-hosted installations or the EU region
        json=data,
        headers=headers
    )

def patch_run(run_id, outputs):
    """Function to patch a run with outputs."""
    requests.patch(
        f"https://api.smith.langchain.com/runs/{run_id}",
        json={
            "outputs": outputs,
            "end_time": datetime.now(timezone.utc).isoformat(),
        },
        headers=headers,
    )

---

## LLM-as-judge instructions

**URL:** llms-txt#llm-as-judge-instructions

grader_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.

Here is the grade criteria to follow:
(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.
(2) Ensure that the student response does not contain any conflicting statements.
(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.

Correctness:
True means that the student's response meets all of the criteria.
False means that the student's response does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct."""

---

## Load environment variables

**URL:** llms-txt#load-environment-variables

dotenv.load_dotenv(".env.local")

---

## Build workflow

**URL:** llms-txt#build-workflow

orchestrator_worker_builder = StateGraph(State)

---

## Short-term memory

**URL:** llms-txt#short-term-memory

**Contents:**
- Overview
- Usage
  - In production
- Customizing agent memory

Source: https://docs.langchain.com/oss/python/langchain/short-term-memory

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.

Short term memory lets your application remember previous interactions within a single thread or conversation.

<Note>
  A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.
</Note>

Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors.

Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get "distracted" by stale or off-topic content, all while suffering from slower response times and higher costs.

Chat models accept context using [messages](/oss/python/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or "forget" stale information.

To add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.

<Info>
  LangChain's agent manages short-term memory as a part of your agent's state.

By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads.

State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.

Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.
</Info>

In production, use a checkpointer backed by a database:

## Customizing agent memory

By default, agents use [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to manage short term memory, specifically the conversation history via a `messages` key.

You can extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to add additional fields. Custom state schemas are passed to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) using the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter.

```python  theme={null}
from langchain.agents import create_agent, AgentState
from langgraph.checkpoint.memory import InMemorySaver

class CustomAgentState(AgentState):  # [!code highlight]
    user_id: str  # [!code highlight]
    preferences: dict  # [!code highlight]

agent = create_agent(
    "openai:gpt-5",
    [get_user_info],
    state_schema=CustomAgentState,  # [!code highlight]
    checkpointer=InMemorySaver(),
)

**Examples:**

Example 1 (unknown):
```unknown
### In production

In production, use a checkpointer backed by a database:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
## Customizing agent memory

By default, agents use [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to manage short term memory, specifically the conversation history via a `messages` key.

You can extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to add additional fields. Custom state schemas are passed to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) using the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter.
```

---

## Note: BigQueryVectorSearch might be in langchain or langchain_community depending on version

**URL:** llms-txt#note:-bigqueryvectorsearch-might-be-in-langchain-or-langchain_community-depending-on-version

---

## Instructions for extracting the user/purchase info from the conversation.

**URL:** llms-txt#instructions-for-extracting-the-user/purchase-info-from-the-conversation.

gather_info_instructions = """You are managing an online music store that sells song tracks. \
Customers can buy multiple tracks at a time and these purchases are recorded in a database as \
an Invoice per purchase and an associated set of Invoice Lines for each purchased track.

Your task is to help customers who would like a refund for one or more of the tracks they've \
purchased. In order for you to be able refund them, the customer must specify the Invoice ID \
to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \
Line IDs if they would like refunds on individual tracks.

Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \
would like a refund. In this case you can help them look up their invoices by asking them to \
specify:
- Required: Their first name, last name, and phone number.
- Optionally: The track name, artist name, album name, or purchase date.

If the customer has not specified the required information (either Invoice/Invoice Line IDs \
or first name, last name, phone) then please ask them to specify it."""

---

## Implement distributed tracing

**URL:** llms-txt#implement-distributed-tracing

**Contents:**
- Distributed tracing in Python

Source: https://docs.langchain.com/langsmith/distributed-tracing

Sometimes, you need to trace a request across multiple services.

LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).

Example client-server setup:

* Trace starts on client
* Continues on server

## Distributed tracing in Python

```python  theme={null}

---

## {'graph_output': 'My name is Lance'}

**URL:** llms-txt#{'graph_output':-'my-name-is-lance'}

**Contents:**
  - Reducers
  - Working with Messages in Graph State

python  theme={null}
from typing_extensions import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
There are two subtle and important points to note here:

1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.

2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

### Reducers

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

#### Default Reducer

These two examples show how to use the default reducer:

**Example A:**
```

Example 2 (unknown):
```unknown
In this example, no reducer functions are specified for any key. Let's assume the input to the graph is:

`{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}`

**Example B:**
```

Example 3 (unknown):
```unknown
In this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together.

### Working with Messages in Graph State

#### Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) (user input) or [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.

#### Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.

However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

#### Serialization

In addition to keeping track of message IDs, the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:
```

---

## Check imports in the usage example.

**URL:** llms-txt#check-imports-in-the-usage-example.

**Contents:**
  - Retrievers

from langchain.vectorstores import BigQueryVectorSearch # Or langchain_community.vectorstores
bash pip theme={null}
  pip install langchain-google-memorystore-redis
  bash uv theme={null}
  uv add langchain-google-memorystore-redis
  python  theme={null}
from langchain_google_memorystore_redis import RedisVectorStore
bash pip theme={null}
  pip install langchain-google-spanner
  bash uv theme={null}
  uv add langchain-google-spanner
  python  theme={null}
from langchain_google_spanner import SpannerVectorStore
bash pip theme={null}
  pip install langchain-google-firestore
  bash uv theme={null}
  uv add langchain-google-firestore
  python  theme={null}
from langchain_google_firestore import FirestoreVectorStore
bash pip theme={null}
  pip install langchain-google-cloud-sql-mysql
  bash uv theme={null}
  uv add langchain-google-cloud-sql-mysql
  python  theme={null}
from langchain_google_cloud_sql_mysql import MySQLVectorStore # MySQLEngine also available
bash pip theme={null}
  pip install langchain-google-cloud-sql-pg
  bash uv theme={null}
  uv add langchain-google-cloud-sql-pg
  python  theme={null}
from langchain_google_cloud_sql_pg import PostgresVectorStore # PostgresEngine also available
bash pip theme={null}
  pip install langchain-google-vertexai
  bash uv theme={null}
  uv add langchain-google-vertexai
  python  theme={null}
from langchain_google_vertexai import VectorSearchVectorStore
python  theme={null}
from langchain_google_vertexai import VectorSearchVectorStoreDatastore
python  theme={null}
from langchain_google_vertexai import VectorSearchVectorStoreGCS
bash pip theme={null}
  pip install google-cloud-discoveryengine langchain-google-community
  bash uv theme={null}
  uv add google-cloud-discoveryengine langchain-google-community
  python  theme={null}
from langchain_google_community import VertexAIMultiTurnSearchRetriever
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
#### Memorystore for Redis

> Vector store using [Memorystore for Redis](https://cloud.google.com/memorystore/docs/redis).

Install the python package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See [usage example](/oss/python/integrations/vectorstores/google_memorystore_redis).
```

Example 4 (unknown):
```unknown
#### Spanner

> Vector store using [Cloud Spanner](https://cloud.google.com/spanner/docs).

Install the python package:

<CodeGroup>
```

---

## Extraction schema, mirrors the graph state.

**URL:** llms-txt#extraction-schema,-mirrors-the-graph-state.

class PurchaseInformation(TypedDict):
    """All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value."""

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None
    followup: Annotated[
        str | None,
        ...,
        "If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.",
    ]

---

## for every request. This will determine whether the request is allowed or not

**URL:** llms-txt#for-every-request.-this-will-determine-whether-the-request-is-allowed-or-not

**Contents:**
- 3. Test your bot
- 4. Chat with your bot

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Check if the user's token is valid."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"
    # Check if token is valid
    if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

# Return user info if valid
    user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }
json {highlight={7-9}} title="langgraph.json" theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "auth": {
    "path": "src/security/auth.py:auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
json  theme={null}
{
    "auth": {
        "path": "src/security/auth.py:auth",
        "disable_studio_auth": "true"
    }
}
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
Notice that your [authentication](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.Auth.authenticate) handler does two important things:

1. Checks if a valid token is provided in the request's [Authorization header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization)
2. Returns the user's [identity](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.types.MinimalUserDict)

Now tell LangGraph to use authentication by adding the following to the [`langgraph.json`](/langsmith/cli#configuration-file) configuration:
```

Example 2 (unknown):
```unknown
## 3. Test your bot

Start the server again to test everything out:
```

Example 3 (unknown):
```unknown
If you didn't add the `--no-browser`, the Studio UI will open in the browser. By default, we also permit access from Studio, even when using custom auth. This makes it easier to develop and test your bot in Studio. You can remove this alternative authentication option by setting `disable_studio_auth: "true"` in your auth configuration:
```

Example 4 (unknown):
```unknown
## 4. Chat with your bot

You should now only be able to access the bot if you provide a valid token in the request header. Users will still, however, be able to access each other's resources until you add [resource authorization handlers](/langsmith/auth#resource-specific-handlers) in the next section of the tutorial.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3ccfa86789baea630b8f418e9eb5b648" alt="Auth gate passes requests with a valid token, but no per-resource filters are applied yet—so users share visibility until authorization handlers are added in the next step." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authentication.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0216c0cf0cb74f67f43e65561a787c96 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c4a9ab37e2413a38dc61ef6f4288c1b1 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=d1146dee549ab13c949efae260706988 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=e430d8ebe4534d20c1d4d3d887f0c938 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3aeaa334e3ff1457c165821966721f94 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=74d6dd4c73baa0182624085d13f08530 2500w" />

Run the following code in a file or notebook:
```

---

## Define output schema

**URL:** llms-txt#define-output-schema

class OutputState(TypedDict):
    answer: str

---

## Create clients for both users

**URL:** llms-txt#create-clients-for-both-users

alice = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user1-token"}
)

bob = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user2-token"}
)

---

## so the conversation can be paused and resumed (as is needed for human review).

**URL:** llms-txt#so-the-conversation-can-be-paused-and-resumed-(as-is-needed-for-human-review).

config = {"configurable": {"thread_id": "some_id"}} # [!code highlight]

---

## Thread creation. This will match only on thread create actions

**URL:** llms-txt#thread-creation.-this-will-match-only-on-thread-create-actions

---

## set the same access key credentials and region as you used for the destination

**URL:** llms-txt#set-the-same-access-key-credentials-and-region-as-you-used-for-the-destination

> AWS Access Key ID: <access_key_id>
> AWS Secret Access Key: <secret_access_key>
> Default region name [us-east-1]: <region>

---

## Fetch the files as bytes

**URL:** llms-txt#fetch-the-files-as-bytes

pdf_bytes = requests.get(pdf_url).content
wav_bytes = requests.get(wav_url).content
img_bytes = requests.get(img_url).content

---

## INVALID_CONCURRENT_GRAPH_UPDATE

**URL:** llms-txt#invalid_concurrent_graph_update

**Contents:**
- Troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE

A LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) received concurrent updates to its state from multiple nodes to a state property that doesn't
support it.

One way this can occur is if you are using a [fanout](/oss/python/langgraph/graph-api#map-reduce-and-the-send-api)
or other parallel execution in your graph and you have defined a graph like this:

If a node in the above graph returns `{ "some_key": "some_string_value" }`, this will overwrite the state value for `"some_key"` with `"some_string_value"`.
However, if multiple nodes in e.g. a fanout within a single step return values for `"some_key"`, the graph will throw this error because
there is uncertainty around how to update the internal state.

To get around this, you can define a reducer that combines multiple values:

This will allow you to define logic that handles the same key returned from multiple nodes executed in parallel.

The following may help resolve this error:

* If your graph executes nodes in parallel, make sure you have defined relevant state keys with a reducer.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If a node in the above graph returns `{ "some_key": "some_string_value" }`, this will overwrite the state value for `"some_key"` with `"some_string_value"`.
However, if multiple nodes in e.g. a fanout within a single step return values for `"some_key"`, the graph will throw this error because
there is uncertainty around how to update the internal state.

To get around this, you can define a reducer that combines multiple values:
```

---

## > [Interrupt(value='Do you approve this action?')]

**URL:** llms-txt#>-[interrupt(value='do-you-approve-this-action?')]

---

## First call

**URL:** llms-txt#first-call

config = {"configurable": {"thread_id": "my-thread"}}
result = agent.invoke(input, config=config)

---

## Fetch the runs we want to convert to a dataset/experiment

**URL:** llms-txt#fetch-the-runs-we-want-to-convert-to-a-dataset/experiment

---

## Human-in-the-loop requires a thread ID for persistence

**URL:** llms-txt#human-in-the-loop-requires-a-thread-id-for-persistence

config = {"configurable": {"thread_id": "some_id"}}

---

## Try searching for assistants. This also should fail

**URL:** llms-txt#try-searching-for-assistants.-this-also-should-fail

try:
    await alice.assistants.search()
    print("❌ Alice shouldn't be able to search assistants!")
except Exception as e:
    print("✅ Alice correctly denied access to searching assistants:", e)

---

## - Name: Foo

**URL:** llms-txt#--name:-foo

---

## Extract customer_id and customer_name using jq

**URL:** llms-txt#extract-customer_id-and-customer_name-using-jq

export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id')
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name')

---

## Enables Claude Code to emit OTEL events

**URL:** llms-txt#enables-claude-code-to-emit-otel-events

export CLAUDE_CODE_ENABLE_TELEMETRY=1

---

## Use the graph API

**URL:** llms-txt#use-the-graph-api

**Contents:**
- Setup
- Define and update state
  - Define state
  - Update state
  - Process state updates with reducers
  - Define input and output schemas

Source: https://docs.langchain.com/oss/python/langgraph/use-graph-api

This guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with "hops" across nodes.

<Tip>
  **Set up LangSmith for better debugging**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](https://docs.smith.langchain.com).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema)
2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed.

[State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.

This state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

Let's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:

This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add_node`](/oss/python/langgraph/graph-api#nodes) populate our graph.

LangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cf3d978b707847e166d5ed15bc7cbbe4" alt="Simple graph with single node" data-og-width="107" width="107" data-og-height="134" height="134" data-path="oss/images/graph_api_image_1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=498bbdb0192eb26ab115d51b53fcb64c 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=94cbad4b92d5b887dff2bfbb6f8e0c6c 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d90d58640d49e3fd4e558ab56acf4817 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cad59990b0c551a2aa96b684b102b953 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=318736f22c69f66c48f4189db3e39235 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=6740141ec001a9a4275cecfac67b9c55 2500w" />

In this case, our graph just executes a single node. Let's proceed with a simple invocation:

* We kicked off invocation by updating a single key of the state.
* We receive the entire state in the invocation result.

For convenience, we frequently inspect the content of [message objects](https://python.langchain.com/docs/concepts/messages/) via pretty-print:

### Process state updates with reducers

Each key in the state can have its own independent [reducer](/oss/python/langgraph/graph-api#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.

For `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.

In the earlier example, our node updated the `"messages"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:

Now our node can be simplified:

In practice, there are additional considerations for updating lists of messages:

* We may wish to update an existing message in the state.
* We may want to accept short-hands for [message formats](/oss/python/langgraph/graph-api#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format).

LangGraph includes a built-in reducer [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) that handles these considerations:

This is a versatile representation of state for applications involving [chat models](https://python.langchain.com/docs/concepts/chat_models/). LangGraph includes a pre-built `MessagesState` for convenience, so that we can have:

### Define input and output schemas

By default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph.

When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.

Below, we'll see how to define distinct input and output schema.

```python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Tip>
  **Set up LangSmith for better debugging**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](https://docs.smith.langchain.com).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema)
2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed.

### Define state

[State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.
```

Example 3 (unknown):
```unknown
This state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

### Update state

Let's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:
```

Example 4 (unknown):
```unknown
This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add_node`](/oss/python/langgraph/graph-api#nodes) populate our graph.
```

---

## RemoteGraph

**URL:** llms-txt#remotegraph

Source: https://docs.langchain.com/langsmith/remote-graph

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/remote-graph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Alerts in LangSmith

**URL:** llms-txt#alerts-in-langsmith

**Contents:**
- Overview
- Configuring an alert
  - Step 1: Navigate To Create Alert
  - Step 2: Select Metric Type
  - Step 2: Define Alert Conditions
  - Step 3: Configure Notification Channel
- Best Practices

Source: https://docs.langchain.com/langsmith/alerts

<Note>
  **Self-hosted Version Requirement**

Access to alerts requires Helm chart version **0.10.3** or later.
</Note>

Effective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith's alerts feature helps identify critical issues such as:

* API rate limit violations from model providers
* Latency increases for your application
* Application changes that affect feedback scores reflecting end-user experience

Alerts in LangSmith are project-scoped, requiring separate configuration for each monitored project.

## Configuring an alert

### Step 1: Navigate To Create Alert

First navigate to the Tracing project that you would like to configure alerts for. Click the Alerts icon on the top right hand corner of the page to view existing alerts for that project and set up a new alert.

### Step 2: Select Metric Type

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=932f55b512d866906160e3ebe9a78ad7" alt="Alert Metrics" data-og-width="597" width="597" data-og-height="134" height="134" data-path="langsmith/images/alert-metric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9a0140bfcf9df907ccaeffc0abc6d324 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=774b40c4cf122330c3b7e7e39bffecde 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=599617a29917cffe79547c1a85d110c3 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e963933afa346141fc2623286f55b48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fcb38466705fd5d8b94443ec9916a6ee 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d738df80eee5db727e6627c4a0e85ce9 2500w" />
</div>

LangSmith offers threshold-based alerting on three core metrics:

| Metric Type        | Description                         | Use Case                                                                                                                                                |
| ------------------ | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Errored Runs**   | Track runs with an error status     | Monitors for failures in an application.                                                                                                                |
| **Feedback Score** | Measures the average feedback score | Track [feedback from end users](/langsmith/attach-user-feedback) or [online evaluation results](/langsmith/online-evaluations) to alert on regressions. |
| **Latency**        | Measures average run execution time | Tracks the latency of your application to alert on spikes and performance bottlenecks.                                                                  |

Additionally, for **Errored Runs** and **Run Latency**, you can define filters to narrow down the runs that trigger alerts. For example, you might create an error alert filter for all `llm` runs tagged with `support_agent` that encounter a `RateLimitExceeded` error.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b2dd48ba21e857c8a99a26a0d896f950" alt="Alert Metrics" data-og-width="407" width="407" data-og-height="273" height="273" data-path="langsmith/images/alerts-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d776aa4bb261605c45f4691b95822ad1 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1cace263d141b044c73a8615c4c9cd15 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a77dfdb2a2e5a119d11675fc01a857ce 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d582ea675732440f5b4bae57ae35b766 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8780c7b52bc0a61c938a7c75357cd068 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d2d7349e8856d8575bed75ccde61871 2500w" />
</div>

### Step 2: Define Alert Conditions

Alert conditions consist of several components:

* **Aggregation Method**: Average, Percentage, or Count
* **Comparison Operator**: `>=`, `<=`, or exceeds threshold
* **Threshold Value**: Numerical value triggering the alert
* **Aggregation Window**: Time period for metric calculation (currently choose between 5 or 15 minutes)
* **Feedback Key** (Feedback Score alerts only): Specific feedback metric to monitor

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d92406d84dec4f1b827b82a989df30b9" alt="Alert Condition Configuration" data-og-width="597" width="597" data-og-height="112" height="112" data-path="langsmith/images/define-conditions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3311a45f1a32527a54c71d4966fdac3b 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6ed12bea3c447c20bfff16e4e58d27e6 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=78955506ecd68ba0bac2ea7053837d6e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4a0bf3da7b34bdd56777a350315b3f6a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=56a4a9e40b9c2a870b999c52dd13dd68 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5207fb3afe3b40873280d9f23e3e0e24 2500w" />
</div>

**Example:** The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.

You can preview alert behavior over a historical time window to understand how many datapoints—and which ones—would have triggered an alert at a chosen threshold (indicated in red). For example, setting an average latency threshold of 60 seconds for a project lets you visualize potential alerts, as shown in the image below.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d7f26bce1113c50bec8f5853c6448415" alt="Alert Metrics" data-og-width="863" width="863" data-og-height="545" height="545" data-path="langsmith/images/alert-preview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a508e02a73579624ae120276664e0e6a 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4f7c5616752dfea80a346be50532f442 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd7d2d27fdb2335640d5ac43b6747baf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=abbaea739f003fcbe97ee00e55e68927 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=55672ba9518816caf74921bc26694ffa 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f67b99c4b5d709b1756d5b674a20dba1 2500w" />
</div>

### Step 3: Configure Notification Channel

LangSmith supports the following notification channels:

1. [PagerDuty Integration](/langsmith/alerts-pagerduty)
2. [Webhook Notifications](/langsmith/alerts-webhook)

Select the appropriate channel to ensure notifications reach the responsible team members.

* Adjust sensitivity based on application criticality
* Start with broader thresholds and refine based on observed patterns
* Ensure alert routing reaches appropriate on-call personnel

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Try to access without a token

**URL:** llms-txt#try-to-access-without-a-token

unauthenticated_client = get_client(url="http://localhost:2024")
try:
    await unauthenticated_client.threads.create()
    print("❌ Unauthenticated access should fail!")
except Exception as e:
    print("✅ Unauthenticated access blocked:", e)

---

## Trace JS functions in serverless environments

**URL:** llms-txt#trace-js-functions-in-serverless-environments

**Contents:**
- Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

Source: https://docs.langchain.com/langsmith/serverless-environments

<Note>
  This section is relevant for those using the LangSmith JS SDK version 0.2.0 and higher. If you are tracing using LangChain.js or LangGraph.js in serverless environments, see [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless).
</Note>

When tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, it's important to ensure that all tracing data is properly flushed before the function completes.

To make sure this occurs, you can either:

* Set an environment variable named `LANGSMITH_TRACING_BACKGROUND` to `"false"`. This will cause your traced functions to wait for tracing to complete before returning.
  * Note that this is named differently from the [environment variable](https://js.langchain.com/docs/how_to/callbacks_serverless) in LangChain.js because LangSmith can be used without LangChain.
* Pass a custom client into your traced runs and `await` the `client.awaitPendingTraceBatches();` method.

Here's an example of using `awaitPendingTraceBatches` alongside the [`traceable`](/langsmith/annotate-code) method:

## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:

And then manually calling `client.flush()` like this before your serverless function closes:

Note that this will prevent runs from appearing in the LangSmith UI until you call `.flush()`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/serverless-environments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:
```

Example 2 (unknown):
```unknown
And then manually calling `client.flush()` like this before your serverless function closes:
```

---

## -- This code should be in a separate file or service --

**URL:** llms-txt#---this-code-should-be-in-a-separate-file-or-service---

**Contents:**
- Interoperability between LangChain (Python) and LangSmith SDK

@chain
def parent_chain(inputs):
    rt = get_current_run_tree()
    headers = rt.to_headers()
    # ... make a request to another service with the headers
    # The headers should be passed to the other service, eventually to the child_wrapper function

parent_chain.invoke({"test": 1})
python  theme={null}
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langsmith import traceable

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),
    ("user", "Question: {question}\nContext: {context}")
])

model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()
chain = prompt | model | output_parser

**Examples:**

Example 1 (unknown):
```unknown
## Interoperability between LangChain (Python) and LangSmith SDK

If you are using LangChain for part of your application and the LangSmith SDK (see [this guide](/langsmith/annotate-code)) for other parts, you can still trace the entire application seamlessly.

LangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.
```

---

## [

**URL:** llms-txt#[

---

## Second session: get user info

**URL:** llms-txt#second-session:-get-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})

---

## Set up SSO with OAuth2.0 and OIDC

**URL:** llms-txt#set-up-sso-with-oauth2.0-and-oidc

**Contents:**
- Overview
- With Client Secret (Recommended)
  - Prerequisites
  - Configuration
  - Session length controls
  - Override Sub Claim
  - Google Workspace IdP setup
  - Okta IdP setup
- Without Client Secret (PKCE) (Deprecated)
  - Requirements

Source: https://docs.langchain.com/langsmith/self-host-sso

LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. This will delegate authentication to your Identity Provider (IdP) to manage access to LangSmith.

Our implementation supports almost anything that is OIDC compliant, with a few exceptions. Once configured, you will see a login screen like this:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bbe6447424ea3e97a486b67d21cd4f6b" alt="LangSmith UI with OAuth SSO" data-og-width="1596" width="1596" data-og-height="994" height="994" data-path="langsmith/images/langsmith-ui-sso.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6046e421d56b069227060520ecf3c2a8 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fc4a6909af9f15dca14af7e3891cda 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53693db675b2bdd6cefd7e9b8b605af6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4b2135a1a76bf2e34eb914b8b00f58dc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=78141a4d748d77bc1cceede89e62087d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8532b1891c98046f171f1bded2523f3a 2500w" />

<Note>
  You may upgrade a [basic auth](/langsmith/self-host-basic-auth) installation to this mode, but not a [none auth](/langsmith/authentication-methods#none) installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth *only*. **In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth.**
</Note>

<Warning>
  LangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth.
</Warning>

## With Client Secret (Recommended)

By default, LangSmith Self-Hosted supports the `Authorization Code` flow with `Client Secret`. In this version of the flow, your client secret is stored security in LangSmith (not on the frontend) and used for authentication and establishing auth sessions.

* You must be self-hosted and on an Enterprise plan.
* Your IdP must support the `Authorization Code` flow with `Client Secret`.
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.

<Note>
  LangSmith SSO is only supported over `https`.
</Note>

* You will need to set the callback URL in your IdP to `https://<host>/api/v1/oauth/custom-oidc/callback`, where `host` is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId`, `oauthClientSecret`, `hostname`, and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.
* If you have **not** already configured Oauth with client secret or if you only have personal orgs, you must provide an email address to assign as the initial org admin for the newly provisioned SSO org. If you are upgrading from basic auth, your existing org will be reused instead.

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:

If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:

### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<div id="via-okta-custom-app-integration">
  <b>Via Custom App Integration</b>
</div>

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.
2. Under **Applications** > **Applications** click **Create App Integration**.
3. Select **OIDC - OpenID Connect** as the Sign-in method and **Web Application** as the Application type, then click **Next**.
4. Enter an `App integration name` (e.g., `LangSmith`).
5. Recommended: Check **Core grants > Refresh Token** (see [session length controls](#session-length-controls)).
6. In **Sign-in redirect URIs** put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback`, e.g., `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`. If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `https://langsmith.yourdomain.com/prefix/api/v1/oauth/custom-oidc/callback`.
7. Remove the default URI under **Sign-out redirect URIs**.
8. Under **Trusted Origins > Base URIs** add your langsmith URL with the protocol, e.g., `https://langsmith.yourdomain.com`.
9. Select your desired option under **Assignments > Controlled access**:
   * Allow everyone in your organization to access.
   * Limit access to selected groups.
   * Skip group assignment for now.
10. Click **Save**.
11. Under **Sign On > OpenID Connect ID Token** set **Issuer** to **Okta URL**.
12. (Optional) Under **General > Login** set **Login initiated by** to `Either Okta or App` to enable IdP-initiated login.
13. (Recommended) Under **General > Login > Email verification experience** fill in the **Callback URI** with the LangSmith URL, e.g., `https://langsmith.yourdomain.com`.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

#### SP-initiated SSO

Users can sign in using the **Login via SSO** button on the LangSmith homepage.

## Without Client Secret (PKCE) (Deprecated)

We recommend running with a `Client Secret` if possible (previously we didn't support this). However, if your IdP does not support this, you can use the `Authorization Code with PKCE` flow.

This flow does *not* require a `Client Secret`. For the alternative workflow, refer to [With client secret](#with-client-secret-recommended).

There are a couple of requirements for using OAuth SSO with LangSmith:

* Your IdP must support the `Authorization Code with PKCE` [flow](https://www.oauth.com/oauth2-servers/pkce) (Google does not support this flow for example, but see [above](#with-client-secret-recommended) for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a "Single Page Application (SPA)"
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.
* You will need to set the callback URL in your IdP to `http://<host>/oauth-callback`, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId` and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-sso.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:
```

Example 3 (unknown):
```unknown
If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:
```

Example 4 (unknown):
```unknown
### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

   1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

    1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

### Okta IdP setup

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<CodeGroup>
```

---

## GRAPH_RECURSION_LIMIT

**URL:** llms-txt#graph_recursion_limit

**Contents:**
- Troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

Your LangGraph [`StateGraph`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) reached the maximum number of steps before hitting a stop condition.
This is often due to an infinite loop caused by code like the example below:

However, complex graphs may hit the default limit naturally.

* If you are not expecting your graph to go through many iterations, you likely have a cycle. Check your logic for infinite loops.

* If you have a complex graph, you can pass in a higher `recursion_limit` value into your `config` object when invoking your graph like this:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/errors/GRAPH_RECURSION_LIMIT.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
However, complex graphs may hit the default limit naturally.

## Troubleshooting

* If you are not expecting your graph to go through many iterations, you likely have a cycle. Check your logic for infinite loops.

* If you have a complex graph, you can pass in a higher `recursion_limit` value into your `config` object when invoking your graph like this:
```

---

## Co-marketing

**URL:** llms-txt#co-marketing

**Contents:**
  - Content we're excited to promote

Source: https://docs.langchain.com/oss/python/contributing/comarketing

With over 60 million monthly downloads, LangChain has a large audience of developers building LLM applications. Beyond just listing integrations, we aim to highlight high-quality, educational examples that inspire developers and advance the ecosystem.

<Note>
  While we occasionally share integrations, we prioritize content that provides
  meaningful insights and best practices. Our main social channels are [Twitter](https://x.com/LangChainAI) and
  [LinkedIn](https://www.linkedin.com/company/langchain/), where we highlight the best examples.
</Note>

### Content we're excited to promote

<AccordionGroup>
  <Accordion title="Educational content" icon="graduation-cap">
    Blogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as "here's how to use integration XYZ", but rather "here's how to do ABC", as we find that is more educational and helpful for developers.
  </Accordion>

<Accordion title="End-to-end applications" icon="cube">
    End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use [LangGraph](https://github.com/langchain-ai/langgraph) as the orchestration framework. We get particularly excited about anything involving:

* Long-term memory systems
    * Human-in-the-loop interaction patterns
    * Multi-agent architectures
  </Accordion>

<Accordion title="Research" icon="flask">
    We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.
  </Accordion>
</AccordionGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/comarketing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## This will become important when we're running our evaluations.

**URL:** llms-txt#this-will-become-important-when-we're-running-our-evaluations.

def refund(state: State, config: RunnableConfig) -> dict:
    # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'.
    mock = config.get("configurable", {}).get("env", "prod") == "test"
    refunded = _refund(
        invoice_id=state["invoice_id"], invoice_line_ids=state["invoice_line_ids"], mock=mock
    )
    response = f"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": response,
    }

---

## The sky

**URL:** llms-txt#the-sky

---

## Use different sampling rates for different operations

**URL:** llms-txt#use-different-sampling-rates-for-different-operations

with tracing_context(client=client_1):
    # Your code here - will be traced with 50% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_2):
    # Your code here - will be traced with 25% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_no_trace):
    # Your code here - will not be traced
    agent_1.invoke(...)
```

This allows you to control sampling rates at the operation level.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/sample-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## This can be retrieved in a retrieval step

**URL:** llms-txt#this-can-be-retrieved-in-a-retrieval-step

context = "During this morning's meeting, we solved all world conflict."

messages = [
    {"role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context."},
    {"role": "user", "content": f"Question: {question}\nContext: {context}"}
]

---

## This can be a user input to your app

**URL:** llms-txt#this-can-be-a-user-input-to-your-app

question = "Can you summarize this morning's meetings?"

---

## Trace with AutoGen

**URL:** llms-txt#trace-with-autogen

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-autogen

LangSmith can capture traces generated by [AutoGen](https://microsoft.github.io/autogen/stable/) using OpenInference's AutoGen instrumentation. This guide shows you how to automatically capture traces from your AutoGen multi-agent conversations and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:
```

---

## Initialize multiple instrumentors

**URL:** llms-txt#initialize-multiple-instrumentors

OpenAIInstrumentor().instrument()
DSPyInstrumentor().instrument()

---

## Versioning

**URL:** llms-txt#versioning

**Contents:**
- Version numbering
- API stability
  - Stable APIs
  - Beta APIs
  - Alpha APIs
  - Deprecated APIs
  - Internal APIs
- Release cycles
- Version support policy
  - Long-term support (LTS) releases

Source: https://docs.langchain.com/oss/python/versioning

Each LangChain and LangGraph version number follows the format: `MAJOR.MINOR.PATCH`

* **Major**: Breaking API updates that require code changes.
* **Minor**: New features and improvements that maintain backward compatibility.
* **Patch**: Bug fixes and minor improvements.

LangChain and LangGraph follow [Semantic Versioning](https://semver.org/) principles:

* `1.0.0`: First stable release with production-ready APIs
* `1.1.0`: New features added in a backward-compatible manner
* `1.0.1`: Backward-compatible bug fixes

We communicate the stability of our APIs as follows:

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.

APIs marked as `beta` are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.

APIs marked as `alpha` are experimental and subject to significant changes. Use these with caution in production environments.

APIs marked as `deprecated` will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:

1. Switch to the recommended alternative API
2. Follow the migration guide (released alongside major releases)
3. Use automated migration tools when available

Certain APIs are explicitly marked as "internal" in a couple of ways:

* Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
* Functions, methods, and other objects prefixed by a leading underscore (**`_`**). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`**, it's an internal API.
  * **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are *meant* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

<AccordionGroup>
  <Accordion title="Major releases">
    Major releases (e.g., `1.0.0` → `2.0.0`) may include:

* Breaking API changes
    * Removal of deprecated features
    * Significant architectural improvements

* Detailed migration guides
    * Automated migration tools when possible
    * Extended support period for the previous major version
  </Accordion>

<Accordion title="Minor releases">
    Minor releases (e.g., `1.0.0` → `1.1.0`) include:

* New features and capabilities
    * Performance improvements
    * New optional parameters
    * Backward-compatible enhancements
  </Accordion>

<Accordion title="Patch releases">
    Patch releases (e.g., `1.0.0` → `1.0.1`) include:

* Bug fixes
    * Security updates
    * Documentation improvements
    * Performance optimizations without API changes
  </Accordion>
</AccordionGroup>

## Version support policy

* **Latest major version**: Full support with active development (ACTIVE status)
* **Previous major version**: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status)
* **Older versions**: Community support only

### Long-term support (LTS) releases

Both LangChain and LangGraph 1.0 are designated as LTS releases:

* Version 1.0 will remain in ACTIVE status until version 2.0 is released
* After version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year
* LTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions
* Legacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026

For detailed information about release status and support timelines, see the [Release policy](/oss/python/release-policy).

## Check your version

To check your installed version:

## Pre-release versions

We occasionally release alpha and beta versions for early testing:

* **Alpha** (e.g., `1.0.0a1`): Early preview, significant changes expected
* **Beta** (e.g., `1.0.0b1`): Feature-complete, minor changes possible
* **Release Candidate** (e.g., `1.0.0rc1`): Final testing before stable release

* [Release policy](/oss/python/release-policy) - Detailed release and deprecation policies
* [Releases](/oss/python/releases) - Version-specific release notes and migration guides

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/versioning.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Upgrade

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Automatically run evaluators on experiments

**URL:** llms-txt#automatically-run-evaluators-on-experiments

**Contents:**
- Configuring an evaluator on a dataset
- LLM-as-a-judge evaluators
- Custom code evaluators
- Next steps

Source: https://docs.langchain.com/langsmith/bind-evaluator-to-dataset

LangSmith supports two ways to grade experiments created via the SDK:

* **Programmatically**, by specifying evaluators in your code (see [this guide](/langsmith/evaluate-llm-application) for details)
* By **binding evaluators to a dataset** in the UI. This will automatically run the evaluators on any new experiments created, in addition to any evaluators you've set up via the SDK. This is useful when you're iterating on your application (target function), and have a standard set of evaluators you want to run for all experiments.

## Configuring an evaluator on a dataset

1. Click on the **Datasets and Experiments** tab in the sidebar.
2. Select the dataset you want to configure the evaluator for.
3. Click on the **+ Evaluator** button to add an evaluator to the dataset. This will open a pane you can use to configure the evaluator.

<Note>
  When you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.
</Note>

## LLM-as-a-judge evaluators

The process for binding evaluators to a dataset is very similar to the process for configuring a LLM-as-a-judge evaluator in the Playground. View instructions for [configuring an LLM-as-a-judge evaluator in the Playground.](/langsmith/llm-as-judge?mode=ui)

## Custom code evaluators

The process for binding a code evaluators to a dataset is very similar to the process for configuring a code evaluator in online evaluation. View instruction for [configuring code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator).

The only difference between configuring a code evaluator in online evaluation and binding a code evaluator to a dataset is that the custom code evaluator can reference outputs that are part of the dataset's `Example`.

For custom code evaluators bound to a dataset, the evaluator function takes in two arguments:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing.
* An `Example` ([reference](/langsmith/example-data-format)). This represents the reference example in your dataset that the chain or model you are testing uses. The `inputs` to the Run and Example should be the same. If your Example has a reference `outputs`, then you can use this to compare to the run's output for scoring.

The code below shows an example of a simple evaluator function that checks that the outputs exactly equal the reference outputs.

* Analyze your experiment results in the [experiments tab](/langsmith/analyze-an-experiment)
* Compare your experiment results in the [comparison view](/langsmith/compare-experiment-results)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/bind-evaluator-to-dataset.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Add the function to the kernel

**URL:** llms-txt#add-the-function-to-the-kernel

code_analyzer = kernel.add_function(
    function_name="analyzeCode",
    plugin_name="codeAnalysisPlugin",
    prompt_template_config=prompt_template_config,
)

---

## Add nodes

**URL:** llms-txt#add-nodes

workflow.add_node("generate_topic", generate_topic)
workflow.add_node("write_joke", write_joke)

---

## [{'expensive_node': {'result': 10}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10}}]

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

---

## Schema for structured output

**URL:** llms-txt#schema-for-structured-output

from pydantic import BaseModel, Field

class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Query that is optimized web search.")
    justification: str = Field(
        None, description="Why this query is relevant to the user's request."
    )

---

## Combine input and output

**URL:** llms-txt#combine-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Invoke the graph

**URL:** llms-txt#invoke-the-graph

config = {"configurable": {"thread_id": "2", "user_id": "1"}}

---

## The stream_mode is set to "messages" to stream LLM tokens

**URL:** llms-txt#the-stream_mode-is-set-to-"messages"-to-stream-llm-tokens

---

## How to upload experiments run outside of LangSmith with the REST API

**URL:** llms-txt#how-to-upload-experiments-run-outside-of-langsmith-with-the-rest-api

**Contents:**
- Request body schema
- Considerations
- Example request
- View the experiment in the UI

Source: https://docs.langchain.com/langsmith/upload-existing-experiments

Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.

This guide will show you how to upload evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

## Request body schema

Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the `results` represents a "row" in the experiment - a single dataset example, along with an associated run. Note that `dataset_id` and `dataset_name` refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).

You may use the following schema to upload experiments to the `/datasets/upload-experiment` endpoint:

The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.

Below is the response received:

Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this information in the request body).

## View the experiment in the UI

Now, login to the UI and click on your newly-created dataset! You should see a single experiment: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=797dd62e7cd3f833cd13bafcedfa5607" alt="Uploaded experiments table" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/uploaded-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b1209e0ffca0c29ca3e0d7e42f0e8ac8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a0dc70688d773066f6844e49b0654c5d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=abbe1374d734e276503394edb09aab40 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=22595d2f890ce7918da47809c2ce18cd 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=303c085b3c5e8a5c9e49f5deb54852f0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b5ee82414e1af652674022087d5dc131 2500w" />

Your examples will have been uploaded: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=093061568ece423d5a2c4cb2b5df2721" alt="Uploaded examples" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-dataset-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2112ea321eb2b791f29b2817e4ecfa70 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8fa00a59f3ebe49c8dc413fcb8cdfff6 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3a34d7d6ab89007cfb849b3b1e9fed6e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4949985fdd2517513234cbec7fa70274 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bdbc201e179f1ba78da86efe792837c9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fafd5f13e4a508ecc26f370f85a143d2 2500w" />

Clicking on your experiment will bring you to the comparison view: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d66c9ea1cfbb1acf4f591b11f54a71da" alt="Uploaded experiment comparison view" data-og-width="3452" width="3452" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9839c5164dffd92bb302b1858e6f36e5 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a013685347c8915b42925027212052e5 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=513f29d729a87e70f1b1c0bc0f42c4b5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=eb4eda99fb12b2ebee2c1f4529fa63a8 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b79aabcad20cca0bfc19cc363c6704d0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b78901795d6b7886a0c251a9abba8d52 2500w" />

As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-existing-experiments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

## Considerations

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

## Example request

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.
```

Example 2 (unknown):
```unknown
Below is the response received:
```

---

## Name of the dataset we want to create

**URL:** llms-txt#name-of-the-dataset-we-want-to-create

dataset_name = f'{project_name}-backtesting {start_time.strftime("%Y-%m-%d")}-{end_time.strftime("%Y-%m-%d")}'

---

## Pass your API key and desired tracing project through headers

**URL:** llms-txt#pass-your-api-key-and-desired-tracing-project-through-headers

export OTEL_EXPORTER_OTLP_HEADERS="x-api-key=<api-key>,Langsmith-Project=<project-name>"

---

## Trace query syntax

**URL:** llms-txt#trace-query-syntax

**Contents:**
- Filter arguments
- Filter query language

Source: https://docs.langchain.com/langsmith/trace-query-syntax

Using the method in the SDK or endpoint in the API, you can filter runs to analyze and export.

| Keys                          | Description                                                                                                                                                                                                                    |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `project_id` / `project_name` | The project(s) to fetch runs from - can be a single project or a list of projects.                                                                                                                                             |
| `trace_id`                    | Fetch runs that are part of a specific trace.                                                                                                                                                                                  |
| `run_type`                    | The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.                                                                                                                                                      |
| `dataset_name` / `dataset_id` | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.                                                                              |
| `reference_example_id`        | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.                                                                                                   |
| `parent_run_id`               | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.                                                                  |
| `error`                       | Fetch runs that errored or did not error.                                                                                                                                                                                      |
| `run_ids`                     | Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**                                                                                                                             |
| `filter`                      | Fetch runs that match a given structured filter statement. See the guide below for more information.                                                                                                                           |
| `trace_filter`                | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.                            |
| `tree_filter`                 | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace. |
| `is_root`                     | Only return root runs.                                                                                                                                                                                                         |
| `select`                      | Select the fields to return in the response. By default, all fields are returned.                                                                                                                                              |
| `query` (*experimental*)      | Natural language query, which translates your query into a filter statement.                                                                                                                                                   |

## Filter query language

LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.

The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:

* `gte` (greater than or equal to)
* `gt` (greater than)
* `lte` (less than or equal to)
* `lt` (less than)
* `eq` (equal to)
* `neq` (not equal to)
* `has` (check if run contains a tag or metadata json blob)
* `search` (search for a substring in a string field)

Additionally, you can combine multiple comparisons through the `and` operator.

These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-query-syntax.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## - Email: foo@langchain.dev

**URL:** llms-txt#--email:-foo@langchain.dev

python wrap theme={null}
from langchain.tools import tool, ToolRuntime

@tool
def get_weather(city: str, runtime: ToolRuntime) -> str:
    """Get weather for a given city."""
    writer = runtime.stream_writer

# Stream custom updates as the tool executes
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")

return f"It's always sunny in {city}!"
```

<Note>
  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Stream Writer

Stream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.
```

---

## Example: caching a query embedding

**URL:** llms-txt#example:-caching-a-query-embedding

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"First call took: {time.time() - tic:.2f} seconds")

---

## You can then create edges to/from this node by referencing it as `"my_node"`

**URL:** llms-txt#you-can-then-create-edges-to/from-this-node-by-referencing-it-as-`"my_node"`

**Contents:**
  - `START` Node
  - `END` Node
  - Node Caching

python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import END

graph.add_edge("node_a", END)
python  theme={null}
import time
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.cache.memory import InMemoryCache
from langgraph.types import CachePolicy

class State(TypedDict):
    x: int
    result: int

builder = StateGraph(State)

def expensive_node(state: State) -> dict[str, int]:
    # expensive computation
    time.sleep(2)
    return {"result": state["x"] * 2}

builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3))
builder.set_entry_point("expensive_node")
builder.set_finish_point("expensive_node")

graph = builder.compile(cache=InMemoryCache())

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

**Examples:**

Example 1 (unknown):
```unknown
### `START` Node

The [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.
```

Example 2 (unknown):
```unknown
### `END` Node

The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.
```

Example 3 (unknown):
```unknown
### Node Caching

LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:

* Specify a cache when compiling a graph (or specifying an entrypoint)
* Specify a cache policy for nodes. Each cache policy supports:
  * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.
  * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.

For example:
```

---

## Sets the output format to use Open Telemetry Protocol

**URL:** llms-txt#sets-the-output-format-to-use-open-telemetry-protocol

export OTEL_LOGS_EXPORTER=otlp

---

## How to set up an application with pyproject.toml

**URL:** llms-txt#how-to-set-up-an-application-with-pyproject.toml

**Contents:**
- Specify Dependencies
- Specify Environment Variables
- Define Graphs

Source: https://docs.langchain.com/langsmith/setup-pyproject

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `pyproject.toml` to define your package's dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example-pyproject), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `pyproject.toml` file:

Example file directory:

## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example-pyproject) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify Dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `pyproject.toml` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify Environment Variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## get the latest state snapshot

**URL:** llms-txt#get-the-latest-state-snapshot

config = {"configurable": {"thread_id": "1"}}
graph.get_state(config)

---

## Troubleshoot trace nesting

**URL:** llms-txt#troubleshoot-trace-nesting

**Contents:**
- Python
  - Context propagation using asyncio
  - Context propagation using threading

Source: https://docs.langchain.com/langsmith/nest-traces

When tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.

If you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known "edge cases".

The following outlines common causes for "split" traces when building with python.

### Context propagation using asyncio

When using async calls (especially with streaming) in Python versions \< 3.11, you may encounter issues with trace nesting. This is because Python's `asyncio` only [added full support for passing context](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) in version 3.11.

LangChain and LangSmith SDK use [contextvars](https://docs.python.org/3/library/contextvars.html) to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), `asyncio` tasks lack proper `contextvar` support, which can lead to disconnected traces.

1. **Upgrade Python Version (Recommended)** If possible, upgrade to Python 3.11 or later for automatic context propagation.

2. **Manual Context Propagation** If upgrading isn't an option, you'll need to manually propagate the tracing context. The method varies depending on your setup:

a) **Using LangGraph or LangChain** Pass the parent `config` to the child call:

b) **Using LangSmith Directly** Pass the run tree directly:

c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:

### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

1. **Using LangSmith's ContextThreadPoolExecutor**

LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:

2. **Manually providing the parent run tree**

Alternatively, you can manually pass the parent run tree to the inner function:

In this approach, we use `get_current_run_tree()` to obtain the current run tree and pass it to the inner function using the `langsmith_extra` parameter.

Both methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/nest-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
b) **Using LangSmith Directly** Pass the run tree directly:
```

Example 2 (unknown):
```unknown
c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:
```

Example 3 (unknown):
```unknown
### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

#### Why

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

#### To resolve

1. **Using LangSmith's ContextThreadPoolExecutor**

   LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:
```

Example 4 (unknown):
```unknown
2. **Manually providing the parent run tree**

   Alternatively, you can manually pass the parent run tree to the inner function:
```

---

## Define dataset: these are your test cases

**URL:** llms-txt#define-dataset:-these-are-your-test-cases

**Contents:**
- Define metrics
- Run Evaluations
- Comparing results
- Set up automated testing to run in CI/CD
- Track results over time
- Conclusion
- Reference code

dataset_name = "QA Example Dataset"
dataset = client.create_dataset(dataset_name)

client.create_examples(
    dataset_id=dataset.id,
    examples=[
        {
            "inputs": {"question": "What is LangChain?"},
            "outputs": {"answer": "A framework for building LLM applications"},
        },
        {
            "inputs": {"question": "What is LangSmith?"},
            "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
        },
        {
            "inputs": {"question": "What is OpenAI?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        },
        {
            "inputs": {"question": "What is Google?"},
            "outputs": {"answer": "A technology company known for search"},
        },
        {
            "inputs": {"question": "What is Mistral?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        }
    ]
)
python  theme={null}
import openai
from langsmith import wrappers

openai_client = wrappers.wrap_openai(openai.OpenAI())

eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    user_content = f"""You are grading the following question:
{inputs['question']}
Here is the real answer:
{reference_outputs['answer']}
You are grading the following predicted answer:
{outputs['response']}
Respond with CORRECT or INCORRECT:
Grade:"""
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": eval_instructions},
            {"role": "user", "content": user_content},
        ],
    ).choices[0].message.content
    return response == "CORRECT"
python  theme={null}
def concision(outputs: dict, reference_outputs: dict) -> bool:
    return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))
python  theme={null}
default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
    return openai_client.chat.completions.create(
        model=model,
        temperature=0,
        messages=[
            {"role": "system", "content": instructions},
            {"role": "user", "content": question},
        ],
    ).choices[0].message.content
python  theme={null}
def ls_target(inputs: str) -> dict:
    return {"response": my_app(inputs["question"])}
python  theme={null}
experiment_results = client.evaluate(
    ls_target, # Your AI system
    data=dataset_name, # The data to predict and grade over
    evaluators=[concision, correctness], # The evaluators to score the results
    experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
)
python  theme={null}
def ls_target_v2(inputs: str) -> dict:
    return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results = client.evaluate(
    ls_target_v2,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="openai-4-turbo",
)
python  theme={null}
instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
    response = my_app(
        inputs["question"],
        model="gpt-4-turbo",
        instructions=instructions_v3
    )
    return {"response": response}

experiment_results = client.evaluate(
    ls_target_v3,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="strict-openai-4-turbo",
)
python  theme={null}
def test_length_score() -> None:
    """Test that the length score is at least 80%."""
    experiment_results = evaluate(
        ls_target, # Your AI system
        data=dataset_name, # The data to predict and grade over
        evaluators=[concision, correctness], # The evaluators to score the results
    )
    # This will be cleaned up in the next release:
    feedback = client.list_feedback(
        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
        feedback_key="concision"
    )
    scores = [f.score for f in feedback]
    assert sum(scores) / len(scores) >= 0.8, "Aggregate score should be at least .8"
python  theme={null}
  import openai
  from langsmith import Client, wrappers

# Application code
  openai_client = wrappers.wrap_openai(openai.OpenAI())

default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
      return openai_client.chat.completions.create(
          model=model,
          temperature=0,
          messages=[
              {"role": "system", "content": instructions},
              {"role": "user", "content": question},
          ],
      ).choices[0].message.content

# Define dataset: these are your test cases
  dataset_name = "QA Example Dataset"
  dataset = client.create_dataset(dataset_name)

client.create_examples(
      dataset_id=dataset.id,
      examples=[
          {
              "inputs": {"question": "What is LangChain?"},
              "outputs": {"answer": "A framework for building LLM applications"},
          },
          {
              "inputs": {"question": "What is LangSmith?"},
              "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
          },
          {
              "inputs": {"question": "What is OpenAI?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          },
          {
              "inputs": {"question": "What is Google?"},
              "outputs": {"answer": "A technology company known for search"},
          },
          {
              "inputs": {"question": "What is Mistral?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          }
      ]
  )

# Define evaluators
  eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
      user_content = f"""You are grading the following question:
  {inputs['question']}
  Here is the real answer:
  {reference_outputs['answer']}
  You are grading the following predicted answer:
  {outputs['response']}
  Respond with CORRECT or INCORRECT:
  Grade:"""
      response = openai_client.chat.completions.create(
          model="gpt-4o-mini",
          temperature=0,
          messages=[
              {"role": "system", "content": eval_instructions},
              {"role": "user", "content": user_content},
          ],
      ).choices[0].message.content
      return response == "CORRECT"

def concision(outputs: dict, reference_outputs: dict) -> bool:
      return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))

# Run evaluations
  def ls_target(inputs: str) -> dict:
      return {"response": my_app(inputs["question"])}

experiment_results_v1 = client.evaluate(
      ls_target, # Your AI system
      data=dataset_name, # The data to predict and grade over
      evaluators=[concision, correctness], # The evaluators to score the results
      experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
  )

def ls_target_v2(inputs: str) -> dict:
      return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results_v2 = client.evaluate(
      ls_target_v2,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="openai-4-turbo",
  )

instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
      response = my_app(
          inputs["question"],
          model="gpt-4-turbo",
          instructions=instructions_v3
      )
      return {"response": response}

experiment_results_v3 = client.evaluate(
      ls_target_v3,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="strict-openai-4-turbo",
  )
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-chatbot-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9ab5110714d009d5865ba0e2d8ee0ffa" alt="" data-og-width="1251" width="1251" data-og-height="560" height="560" data-path="langsmith/images/testing-tutorial-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e4b38ded6968e649ed8ab507f63f1f3e 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f7aee5327f8058dd99684cd43e44c791 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e853ed05b0a2ad40f9e4d0403e7004c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331654a31885b89a93924eaac4fa95da 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=833bf2a60b392323bba47fbe42655537 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3410e4bc7ac5c28f8838fc5fb88026bd 2500w" />

## Define metrics

After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.

In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.

Let's go ahead and define these two metrics.

For the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:
```

Example 2 (unknown):
```unknown
For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.
```

Example 3 (unknown):
```unknown
## Run Evaluations

Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:
```

Example 4 (unknown):
```unknown
Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.
```

---

## Start local development server with LangGraph Studio

**URL:** llms-txt#start-local-development-server-with-langgraph-studio

**Contents:**
  - Method 1: LangSmith Deployment UI
  - Method 2: Control Plane API

langgraph dev
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will:

* Spin up a local server with Studio.
* Allow you to visualize and interact with your graph.
* Validate that your agent works correctly before deployment.

<Note>
  If your agent runs locally without any errors, it means that deployment to LangSmith will likely succeed. This local testing helps catch configuration issues, dependency problems, and agent logic errors before attempting deployment.
</Note>

See the [LangGraph CLI documentation](/langsmith/cli#dev) for more details.

### Method 1: LangSmith Deployment UI

Deploy your agent using the LangSmith deployment interface:

1. Go to your [LangSmith dashboard](https://smith.langchain.com).
2. Navigate to the **Deployments** section.
3. Click the **+ New Deployment** button in the top right.
4. Select your GitHub repository containing your LangGraph agent from the dropdown menu.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration with dropdown menu
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Specify your image URI in the Image Path field (e.g., `docker.io/username/my-agent:latest`)

<Info>
  **Benefits:**

  * Simple UI-based deployment
  * Direct integration with your GitHub repository (cloud)
  * No manual Docker image management required (cloud)
</Info>

### Method 2: Control Plane API

Deploy using the Control Plane API with different approaches for each deployment type:

**For Cloud LangSmith:**

* Use the Control Plane API to create deployments by pointing to your GitHub repository
* No Docker image building required for cloud deployments

**For Self-Hosted/Hybrid LangSmith:**
```

---

## null

**URL:** llms-txt#null

**Contents:**
- Interface

Source: https://docs.langchain.com/oss/python/integrations/document_loaders/index

Document loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) format.
This ensures that data can be handled consistently regardless of the source.

All document loaders implement the [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader) interface.

Each document loader may define its own parameters, but they share a common API:

* `.load()` – Loads all documents at once.
* `.lazy_load()` – Streams documents lazily, useful for large datasets.

```python  theme={null}
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # Integration-specific parameters here
)

---

## Next steps

**URL:** llms-txt#next-steps

Now that you've created a prompt, you can use it in your application code. See [how to pull a prompt programmatically](/langsmith/manage-prompts-programmatically#pull-a-prompt).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-a-prompt.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Try to access user 1's thread as user 2

**URL:** llms-txt#try-to-access-user-1's-thread-as-user-2

**Contents:**
- Next steps

user2_token = await login(email2, password)
user2_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user2_token}"}
)

try:
    await user2_client.threads.get(thread["thread_id"])
    print("❌ User 2 shouldn't see User 1's thread!")
except Exception as e:
    print("✅ User 2 blocked from User 1's thread:", e)
shell  theme={null}
✅ User 1 created thread: d6af3754-95df-4176-aa10-dbd8dca40f1a
✅ Unauthenticated access blocked: Client error '403 Forbidden' for url 'http://localhost:2024/threads'
✅ User 2 blocked from User 1's thread: Client error '404 Not Found' for url 'http://localhost:2024/threads/d6af3754-95df-4176-aa10-dbd8dca40f1a'
```

Your authentication and authorization are working together:

1. Users must log in to access the bot
2. Each user can only see their own threads

All users are managed by the Supabase auth provider, so you don't need to implement any additional user management logic.

You've successfully built a production-ready authentication system for your LangGraph application! Let's review what you've accomplished:

1. Set up an authentication provider (Supabase in this case)
2. Added real user accounts with email/password authentication
3. Integrated JWT token validation into your LangGraph server
4. Implemented proper authorization to ensure users can only access their own data
5. Created a foundation that's ready to handle your next authentication challenge 🚀

Now that you have production authentication, consider:

1. Building a web UI with your preferred framework (see the [Custom Auth](https://github.com/langchain-ai/custom-auth) template for an example)
2. Learn more about the other aspects of authentication and authorization in the [conceptual guide on authentication](/langsmith/auth).
3. Customize your handlers and setup further after reading the [reference docs](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-auth-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The output should look like this:
```

---

## This variable is just used for demonstration purposes to simulate a network failure.

**URL:** llms-txt#this-variable-is-just-used-for-demonstration-purposes-to-simulate-a-network-failure.

---

## thread_id is the persistent pointer (stores a stable ID in production)

**URL:** llms-txt#thread_id-is-the-persistent-pointer-(stores-a-stable-id-in-production)

config = {"configurable": {"thread_id": "thread-1"}}
result = graph.invoke({"input": "data"}, config=config)

---

## Configure your collector for LangSmith telemetry

**URL:** llms-txt#configure-your-collector-for-langsmith-telemetry

Source: https://docs.langchain.com/langsmith/langsmith-collector

The various services in a LangSmith deployment emit telemetry data in the form of logs, metrics, and traces. You may already have telemetry collectors set up in your Kubernetes cluster, or would like to deploy one to monitor your application.

This page describes how to configure an [OTel Collector](https://opentelemetry.io/docs/collector/configuration/) to gather telemetry data from LangSmith. Note that all of the concepts discussed below can be translated to other collectors such as [Fluentd](https://www.fluentd.org/) or [FluentBit](https://fluentbit.io/).

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

---

## 1. Create and/or select your dataset

**URL:** llms-txt#1.-create-and/or-select-your-dataset

ls_client = Client()
dataset = ls_client.clone_public_dataset(
    "https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d"
)

---

## Write your prompt with AI

**URL:** llms-txt#write-your-prompt-with-ai

**Contents:**
- Chat sidebar
- Quick actions
- Custom quick actions
- Diffing
- Saving and using prompts

Source: https://docs.langchain.com/langsmith/write-prompt-with-ai

The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-open.gif?s=480b38abe2797436e0b7969e2d961e23" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-open.gif" data-optimize="true" data-opv="3" />

You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-rewrite.gif?s=91ba55b2e7b1e3a18a799265250dedb0" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-rewrite.gif" data-optimize="true" data-opv="3" />

<Note>
  You can also edit the prompt directly - you don't **need** to use the LLM. This is useful if you know what edits you want to make and just want to make them directly
</Note>

There are quick actions to change the reading level or length of the prompt with a single mouse click:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-quick-actions.gif?s=6d2bb4ee78ec98fe551ce7f1a0e94ad9" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-quick-actions.gif" data-optimize="true" data-opv="3" />

## Custom quick actions

You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-custom-quick-action.gif?s=3fd706f2fdd87abe339f83a2639ce708" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-custom-quick-action.gif" data-optimize="true" data-opv="3" />

You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-diff.gif?s=be6259e6ac773c4a01c5b1ac4fe79e92" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-diff.gif" data-optimize="true" data-opv="3" />

## Saving and using prompts

Lastly, you can save the prompt you have created in the canvas by clicking the "Use this Version" button in the bottom right:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-save.gif?s=f54b0d1b6374e6b69750a6489c787172" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-save.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/write-prompt-with-ai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to use prebuilt evaluators

**URL:** llms-txt#how-to-use-prebuilt-evaluators

**Contents:**
- Setup
- Running an evaluator

Source: https://docs.langchain.com/langsmith/prebuilt-evaluators

LangSmith integrates with the open-source openevals package to provide a suite of prebuilt evaluators that you can use as starting points for evaluation.

<Note>
  This how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge). For a complete list of prebuilt evaluators with usage examples, refer to the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos.
</Note>

You'll need to install the `openevals` package to use the pre-built LLM-as-a-judge evaluator.

You'll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:

We'll also use LangSmith's [pytest](/langsmith/pytest) integration for Python and [Vitest/Jest](/langsmith/vitest-jest) for TypeScript to run our evals. `openevals` also integrates seamlessly with the [`evaluate`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method as well. See the [appropriate guides](/langsmith/pytest) for setup instructions.

## Running an evaluator

The general flow is simple: import the evaluator or factory function from `openevals`, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator's results as feedback.

Note that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.

Set up your test file like this:

The `feedback_key`/`feedbackKey` parameter will be used as the name of the feedback in your experiment.

Running the eval in your terminal will result in something like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c2351acb065520c3cef3c374bd762982" alt="Prebuilt evaluator terminal result" data-og-width="2114" width="2114" data-og-height="614" height="614" data-path="langsmith/images/prebuilt-eval-result.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5a091195ae1351d5b16b2ebe53632e1e 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=1e7488bb77662f71e60f01b9fa9609d6 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7e491cd83accabc3a56153a6c12d84fe 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2fbc03b560b082ae5f6de8d17d4ae626 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=20f6023215721383019659a0b99f3de5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=af97fb8ec7343f536704719294560dd0 2500w" />

You can also pass prebuilt evaluators directly into the `evaluate` method if you have already created a dataset in LangSmith. If using Python, this requires `langsmith>=0.3.11`:

For a complete list of available evaluators, see the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prebuilt-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

You'll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:
```

Example 3 (unknown):
```unknown
We'll also use LangSmith's [pytest](/langsmith/pytest) integration for Python and [Vitest/Jest](/langsmith/vitest-jest) for TypeScript to run our evals. `openevals` also integrates seamlessly with the [`evaluate`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method as well. See the [appropriate guides](/langsmith/pytest) for setup instructions.

## Running an evaluator

The general flow is simple: import the evaluator or factory function from `openevals`, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator's results as feedback.

Note that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.

Set up your test file like this:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Compile the workflow

**URL:** llms-txt#compile-the-workflow

orchestrator_worker = orchestrator_worker_builder.compile()

---

## Provider-native format (e.g., OpenAI)

**URL:** llms-txt#provider-native-format-(e.g.,-openai)

human_message = HumanMessage(content=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
])

---

## Multimodal invocation with gemini-pro-vision

**URL:** llms-txt#multimodal-invocation-with-gemini-pro-vision

**Contents:**
  - Embedding Models
  - LLMs
- Google Cloud
  - Chat models
  - LLMs
  - Embedding Models
  - Document loaders
  - Document Transformers
  - Vector Stores

message = HumanMessage(
    content=[
        {
            "type": "text",
            "text": "What's in this image?",
        },
        {"type": "image_url", "image_url": "https://picsum.photos/seed/picsum/200/300"},
    ]
)
result = llm.invoke([message])
print(result.content)
python  theme={null}
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
vector = embeddings.embed_query("What are embeddings?")
print(vector[:5])
python  theme={null}
from langchain_google_genai import GoogleGenerativeAI

llm = GoogleGenerativeAI(model="gemini-2.5-flash")
result = llm.invoke("Sing a ballad of LangChain.")
print(result)
bash pip theme={null}
  pip install langchain-google-vertexai
  # pip install langchain-google-community[...] # For other services
  bash uv theme={null}
  uv add langchain-google-vertexai
  # uv add langchain-google-community[...] # For other services
  python  theme={null}
from langchain_google_vertexai import ChatVertexAI
python  theme={null}
from langchain_google_vertexai.model_garden import ChatAnthropicVertex
python  theme={null}
from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama
python  theme={null}
from langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral
python  theme={null}
from langchain_google_vertexai.gemma import GemmaChatLocalHF
python  theme={null}
from langchain_google_vertexai.gemma import GemmaChatLocalKaggle
python  theme={null}
from langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden
python  theme={null}
from langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat
python  theme={null}
from langchain_google_vertexai.vision_models import VertexAIImageEditorChat
python  theme={null}
from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat
python  theme={null}
from langchain_google_vertexai.vision_models import VertexAIVisualQnAChat
python  theme={null}
from langchain_google_vertexai import VertexAIModelGarden
python  theme={null}
from langchain_google_vertexai.gemma import GemmaLocalHF
python  theme={null}
from langchain_google_vertexai.gemma import GemmaLocalKaggle
python  theme={null}
from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden
python  theme={null}
from langchain_google_vertexai.vision_models import VertexAIImageCaptioning
python  theme={null}
from langchain_google_vertexai import VertexAIEmbeddings
bash pip theme={null}
  pip install langchain-google-alloydb-pg
  bash uv theme={null}
  uv add langchain-google-alloydb-pg
  python  theme={null}
from langchain_google_alloydb_pg import AlloyDBLoader # AlloyDBEngine also available
bash pip theme={null}
  pip install langchain-google-community[bigquery]
  bash uv theme={null}
  uv add langchain-google-community[bigquery]
  python  theme={null}
from langchain_google_community import BigQueryLoader
bash pip theme={null}
  pip install langchain-google-bigtable
  bash uv theme={null}
  uv add langchain-google-bigtable
  python  theme={null}
from langchain_google_bigtable import BigtableLoader
bash pip theme={null}
  pip install langchain-google-cloud-sql-mysql
  bash uv theme={null}
  uv add langchain-google-cloud-sql-mysql
  python  theme={null}
from langchain_google_cloud_sql_mysql import MySQLLoader # MySQLEngine also available
bash pip theme={null}
  pip install langchain-google-cloud-sql-mssql
  bash uv theme={null}
  uv add langchain-google-cloud-sql-mssql
  python  theme={null}
from langchain_google_cloud_sql_mssql import MSSQLLoader # MSSQLEngine also available
bash pip theme={null}
  pip install langchain-google-cloud-sql-pg
  bash uv theme={null}
  uv add langchain-google-cloud-sql-pg
  python  theme={null}
from langchain_google_cloud_sql_pg import PostgresLoader # PostgresEngine also available
bash pip theme={null}
  pip install langchain-google-community[gcs]
  bash uv theme={null}
  uv add langchain-google-community[gcs]
  python  theme={null}
from langchain_google_community import GCSDirectoryLoader
python  theme={null}
from langchain_google_community import GCSFileLoader
bash pip theme={null}
  pip install langchain-google-community[vision]
  bash uv theme={null}
  uv add langchain-google-community[vision]
  python  theme={null}
from langchain_google_community.vision import CloudVisionLoader
bash pip theme={null}
  pip install langchain-google-el-carro
  bash uv theme={null}
  uv add langchain-google-el-carro
  python  theme={null}
from langchain_google_el_carro import ElCarroLoader
bash pip theme={null}
  pip install langchain-google-firestore
  bash uv theme={null}
  uv add langchain-google-firestore
  python  theme={null}
from langchain_google_firestore import FirestoreLoader
bash pip theme={null}
  pip install langchain-google-datastore
  bash uv theme={null}
  uv add langchain-google-datastore
  python  theme={null}
from langchain_google_datastore import DatastoreLoader
bash pip theme={null}
  pip install langchain-google-memorystore-redis
  bash uv theme={null}
  uv add langchain-google-memorystore-redis
  python  theme={null}
from langchain_google_memorystore_redis import MemorystoreDocumentLoader
bash pip theme={null}
  pip install langchain-google-spanner
  bash uv theme={null}
  uv add langchain-google-spanner
  python  theme={null}
from langchain_google_spanner import SpannerLoader
bash pip theme={null}
  pip install langchain-google-community[speech]
  bash uv theme={null}
  uv add langchain-google-community[speech]
  python  theme={null}
from langchain_google_community import SpeechToTextLoader
bash pip theme={null}
  pip install langchain-google-community[docai]
  bash uv theme={null}
  uv add langchain-google-community[docai]
  python  theme={null}
from langchain_core.document_loaders.blob_loaders import Blob
from langchain_google_community import DocAIParser
bash pip theme={null}
  pip install langchain-google-community[translate]
  bash uv theme={null}
  uv add langchain-google-community[translate]
  python  theme={null}
from langchain_google_community import GoogleTranslateTransformer
bash pip theme={null}
  pip install langchain-google-alloydb-pg
  bash uv theme={null}
  uv add langchain-google-alloydb-pg
  python  theme={null}
from langchain_google_alloydb_pg import AlloyDBVectorStore # AlloyDBEngine also available
bash pip theme={null}
  pip install google-cloud-bigquery
  bash uv theme={null}
  uv add google-cloud-bigquery
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The `image_url` can be a public URL, a GCS URI (`gs://...`), a local file path, a base64 encoded image string (`data:image/png;base64,...`), or a PIL Image object.

### Embedding Models

Generate text embeddings using models like `gemini-embedding-001` with the `GoogleGenerativeAIEmbeddings` class.

See a [usage example](/oss/python/integrations/text_embedding/google_generative_ai).
```

Example 2 (unknown):
```unknown
### LLMs

Access the same Gemini models using the (legacy) LLM
interface with the `GoogleGenerativeAI` class.

See a [usage example](/oss/python/integrations/llms/google_ai).
```

Example 3 (unknown):
```unknown
## Google Cloud

Access Gemini models, Vertex AI Model Garden and other Google Cloud services via Vertex AI and specific cloud integrations.

Vertex AI models require the `langchain-google-vertexai` package. Other services might require additional packages like `langchain-google-community`, `langchain-google-cloud-sql-pg`, etc.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initial run - hits the interrupt and pauses

**URL:** llms-txt#initial-run---hits-the-interrupt-and-pauses

---

## Define graph state

**URL:** llms-txt#define-graph-state

class State(TypedDict):
    foo: str

---

## Frequently Asked Questions

**URL:** llms-txt#frequently-asked-questions

**Contents:**
- Questions and Answers
  - I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?
  - Which plan is right for me?
  - What is a seat?
  - What is a trace?
  - What is an ingested event?
  - I've hit my rate or usage limits. What can I do?
  - I have a developer account, can I upgrade my account to the Plus or Enterprise plan?
  - How does billing work?
  - Can I limit how much I spend on tracing?

Source: https://docs.langchain.com/langsmith/pricing-faq

## Questions and Answers

### I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?

If you've been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by [contacting our sales team](https://www.langchain.com/contact-sales).

### Which plan is right for me?

If you're an individual developer, the Developer plan is a great choice for small projects.

For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application**, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.

If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.

A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.

A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.

### What is an ingested event?

An ingested event is any distinct, trace-related data sent to LangSmith. This includes:

* Inputs, outputs and metadata sent at the start of a run step within a trace
* Inputs, outputs and metadata sent at the end of a run step within a trace
* Feedback on run steps or traces

### I've hit my rate or usage limits. What can I do?

When you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to [Plans and Billing](https://smith.langchain.com/settings/payments) to upgrade.

Similarly, if you've hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or reach out to [support@langchain.dev](mailto:support@langchain.dev) with questions.

### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?

Yes, Developer plan users can easily upgrade to the Plus plan on the [Plans and Billing](https://smith.langchain.com/settings/payments) page. For the Enterprise plan, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss your needs.

### How does billing work?

Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.

As long as you have a card on file in your account, we'll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.

### Can I limit how much I spend on tracing?

You can set limits on the number of traces that can be sent to LangSmith per month on the [Usage configuration](https://smith.langchain.com/settings/payments) page.

<Note>
  While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.

You are not currently able to set a spend limit in the product.
</Note>

### How can I track my usage so far this month?

Under the Settings section for your Organization you will see subsection for **Usage**. There, you will be able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.

### I have a question about my bill...

Customers on the Developer and Plus plan tiers should email [support@langchain.dev](mailto:support@langchain.dev). Customers on the Enterprise plan should contact their sales representative directly.

Enterprise plan customers are billed annually by invoice.

### What can I expect from Support?

On the Developer plan, community-based support is available on [LangChain community Slack](https://www.langchain.com/join-community).

On the Plus plan, you will also receive preferential, email support at [support@langchain.dev](mailto:support@langchain.dev) for LangSmith-related questions only and we'll do our best to respond within the next business day.

On the Enterprise plan, you'll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we'll also support deployments and new releases with our infra engineering team on-call.

### Where is my data stored?

You may choose to sign up in either the US or EU region. See the [cloud architecture reference](/langsmith/cloud#cloud-architecture-and-scalability) for more details. If you're on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.

### Which security frameworks is LangSmith compliant with?

We are SOC 2 Type II, GDPR, and HIPAA compliant.

You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.

### Will you train on the data that I send LangSmith?

We will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-faq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Trace a RAG application tutorial

**URL:** llms-txt#trace-a-rag-application-tutorial

**Contents:**
- Prototyping
  - Set up your environment
  - Trace your LLM calls
  - Trace the whole chain
- Beta Testing
  - Collecting Feedback
  - Logging Metadata
- Production
  - Monitoring
  - A/B Testing

Source: https://docs.langchain.com/langsmith/observability-llm-tutorial

In this tutorial, we'll build a simple RAG application using the OpenAI SDK. We'll add observability to the application at each stage of development, from prototyping to production.

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).

<Note>
  You may see these variables referenced as `LANGCHAIN_*` in other places. These are all equivalent, however the best practice is to use `LANGSMITH_TRACING`, `LANGSMITH_API_KEY`, `LANGSMITH_PROJECT`.

The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Note>

### Trace your LLM calls

The first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:

Notice how we import `from langsmith.wrappers import wrap_openai` and use it to wrap the OpenAI client (`openai_client = wrap_openai(OpenAI())`).

What happens if you call it in the following way?

This will produce a trace of just the OpenAI call - it should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8b3ad3b0d00851bce313311efa4e8bbb" alt="" data-og-width="1027" width="1027" data-og-height="615" height="615" data-path="langsmith/images/tracing-tutorial-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c4ee9e306124a884702a7c0f5685e279 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=995ebe3b7342ea797887a052f962919d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24e6e8b84336197d23ed294d4d37c842 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=822b72e7b0ea10a96cc05eeeefad7b51 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=985061817d26e8f76b50d2638b34ecb7 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bf5bb01fa39af440f985666c39809cc5 2500w" />

### Trace the whole chain

Great - we've traced the LLM call. But it's often very informative to trace more than that. LangSmith is **built** for tracing the entire LLM pipeline - so let's do that! We can do this by modifying the code to now look something like this:

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable`).

What happens if you call it in the following way?

This will produce a trace of the entire RAG pipeline - it should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=00afea1ffa117b90159d30a53aac5a7f" alt="" data-og-width="1016" width="1016" data-og-height="635" height="635" data-path="langsmith/images/tracing-tutorial-chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01eb0588af8534c636796b1ffc673a14 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=38625d3b2d93cd41b344bc2610272ff4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f713b97cfd693fec7ea51d85d06c1358 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=039262907891da16b2a13d69ce3650ac 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=97e50a697c1ac249ca91a38c42beaaa9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=871e76f1794d7c3d0d7fc3bb875fd613 2500w" />

The next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous section

### Collecting Feedback

A huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that.

First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:

Associating feedback with that run would look something like:

Once the feedback is logged, you can then see it associated with each run by clicking into the `Metadata` tab when inspecting the run. It should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb81a556fe40895ebb29d4428d4c62d9" alt="" data-og-width="1025" width="1025" data-og-height="345" height="345" data-path="langsmith/images/tracing-tutorial-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=165c08ee4c4f96f9f3ebb6e8183dc539 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50e70bda816d314ac233430fe5703be 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=579ca04b25401fd98dbd1109e55f4a8c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9763f2b2f5cbb347f796e2c1951102cf 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8c84bd91d25dbb0514385aabf8b2dbc2 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b9f44b64e395eb392e9a6a0348189e84 2500w" />

You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=57ebc19f2e5443c21353064c082971bc" alt="" data-og-width="940" width="940" data-og-height="496" height="496" data-path="langsmith/images/tracing-tutorial-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=630d2ed8d85794026cbf07fcc186f791 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9bc6f8a5464f3c0bbdcf172ecb3e1f67 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9daf8b6d1ecbc23112dba100a77bc0a6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fa18baf671267ec4a1bfce3cfdc1c789 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b3184ab4bfe8361d1d52b4e63c054e58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2e1aeade5945fe20527a58c1966b5a10 2500w" />

It is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result.

For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:

Notice we added `@traceable(metadata={"llm": "gpt-4o-mini"})` to the `rag` function.

Keeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.

Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI [here](https://smith.langchain.com/public/37adf7e5-97aa-42d0-9850-99c0199bddf6/r).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=db49738eba3e0ce26514df3c9b72f87c" alt="" data-og-width="1016" width="1016" data-og-height="337" height="337" data-path="langsmith/images/tracing-tutorial-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9f46f9e36cc47a6b10cc37870e17ffc 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dba6e396adec67b74df789cfd9cd2491 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e31ff00027f6c5741935cec40d997697 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cf82f0eca41b0fb04ae181e38ae47ece 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fd57501d821496d68d5921e3cb8ce457 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=568f6fb60e84333f677775817dbc79e8 2500w" />

We can filter for these pieces of information by constructing a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3662dfc2d8fb2c274f622e3f67e14b34" alt="" data-og-width="932" width="932" data-og-height="436" height="436" data-path="langsmith/images/tracing-tutorial-metadata-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=561f6c48361533d9ddcfc005136ff6c3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ea92820bc977b16368e89d923a631f5 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137d1faa53ec75e8aaecf3aa2da32378 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9c7902430dbae7b10543bd9b747a5fc 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1494db66f66fceba483829a249a44a31 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06c2f159f5ce8bf601640a8484be187f 2500w" />

Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add?

First of all, let's note that the same observability you've already added will keep on providing value in production. You will continue to be able to drill down into particular runs.

In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.

If you click on the `Monitor` tab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=74f49882d9e6323e2ed467b525b81b9a" alt="" data-og-width="946" width="946" data-og-height="746" height="746" data-path="langsmith/images/tracing-tutorial-monitor.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d99a10225425733cc18b1da00c04ff27 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88a2737937bfa46e5d44f665a757fb68 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5825a3a92fc134fe8fb8f8f4a7b46e55 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=848482d02d40236a1911a9623c39a2f0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=278ba4c130e01f1042d49069093a7285 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dcef7cc8a6be73d02b2030f2bb9dc783 2500w" />

<Note>
  Group-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.
</Note>

You can also use this tab to perform a version of A/B Testing. In the previous tutorial we starting tracking a few different metadata attributes - one of which was `llm`. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.

In order to do this, we just need to click on the `Metadata` button at the top. This will give us a drop down of options to choose from to group by:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dc91e44dac01fcb3966c7b57b1f41d66" alt="" data-og-width="957" width="957" data-og-height="534" height="534" data-path="langsmith/images/tracing-tutorial-monitor-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8ddc7be16e3b0f6e09e83af55fbe917f 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=cbc8c4287707ff477695f12d9220c55f 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=70345f5ed014329e07046681734f412f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=73662cff14eebb9498e295834e6b8c86 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7a5e5124ffbc658a5a97d372f41b96c0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24dcb9a46ea8684c2fedd0b3f136d8a0 2500w" />

Once we select this, we will start to see charts grouped by this attribute:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a1bf8fb453d7721d85bca20fbd7cb431" alt="" data-og-width="973" width="973" data-og-height="621" height="621" data-path="langsmith/images/tracing-tutorial-monitor-grouped.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=896d99b5a7456e10d92aa58c8d3bb6d8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6cc318a5f30ed0e8e11accf1d6f7428d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=892af43a1ce178ec34ed4124a7a0f5d4 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7be53f1392b471c45b7d14febe0df6f9 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=61d68a5703c7e69fe4c96df6a861ab37 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e0a3af90e154e85c45f3de1cb0a908d5 2500w" />

One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify as problematic while looking at monitoring charts. In order to do this, you can simply hover over a datapoint in the monitoring chart. When you do this, you will be able to click the datapoint. This will lead you back to the runs table with a filtered view:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ca02e0473f1fdfff102f2ccba371828" alt="" data-og-width="952" width="952" data-og-height="708" height="708" data-path="langsmith/images/tracing-tutorial-monitor-drilldown.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ddf0256ff1e85656a8339e16a652480d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0fe248ccc799c0cef9e661c861e81605 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4c3f95dd9e31e1884f569bbf736b852c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8ce416eec3bdc7b4289ba9fbedf6959a 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2f93d8d48a5cf33f2f4b664c72644a7 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=506f485087e2e7ab91e7a53c44fd8205 2500w" />

In this tutorial you saw how to set up your LLM application with best-in-class observability. No matter what stage your application is in, you will still benefit from observability.

If you have more in-depth questions about observability, check out the [how-to section](/langsmith/observability-concepts) for guides on topics like testing, prompt management, and more.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-llm-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Prototyping

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).
```

---

## This is your PUBLIC anon key (which is safe to use client-side)

**URL:** llms-txt#this-is-your-public-anon-key-(which-is-safe-to-use-client-side)

---

## Text block

**URL:** llms-txt#text-block

text_block = {
    "type": "text",
    "text": "Hello world",
}

---

## Continue conversation

**URL:** llms-txt#continue-conversation

**Contents:**
- Message content

messages = [
    HumanMessage("What's the weather in San Francisco?"),
    ai_message,  # Model's tool call
    tool_message,  # Tool execution result
]
response = model.invoke(messages)  # Model processes the result
python  theme={null}
    from langchain.messages import ToolMessage

# Sent to model
    message_content = "It was the best of times, it was the worst of times."

# Artifact available downstream
    artifact = {"document_id": "doc_123", "page": 0}

tool_message = ToolMessage(
        content=message_content,
        tool_call_id="call_123",
        name="search_books",
        artifact=artifact,
    )
    python  theme={null}
from langchain.messages import HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="content" type="string" required>
    The stringified output of the tool call.
  </ParamField>

  <ParamField path="tool_call_id" type="string" required>
    The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage))
  </ParamField>

  <ParamField path="name" type="string" required>
    The name of the tool that was called.
  </ParamField>

  <ParamField path="artifact" type="dict">
    Additional data not sent to the model but can be accessed programmatically.
  </ParamField>
</Accordion>

<Note>
  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

  <Accordion title="Example: Using artifact for retrieval metadata">
    For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:
```

Example 2 (unknown):
```unknown
See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.
  </Accordion>
</Note>

***

## Message content

You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.

Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.

LangChain chat models accept message content in the `content` attribute, and can contain:

1. A string
2. A list of content blocks in a provider-native format
3. A list of [LangChain's standard content blocks](#standard-content-blocks)

See below for an example using [multimodal](#multimodal) inputs:
```

---

## Access the current conversation state

**URL:** llms-txt#access-the-current-conversation-state

@tool
def summarize_conversation(
    runtime: ToolRuntime
) -> str:
    """Summarize the conversation so far."""
    messages = runtime.state["messages"]

human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

---

## Retrievers

**URL:** llms-txt#retrievers

**Contents:**
- Bring-your-own documents
- External index
- All retrievers

Source: https://docs.langchain.com/oss/python/integrations/retrievers/index

A [retriever](/oss/python/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.
Retrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/python/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/python/integrations/retrievers/amazon_kendra_retriever/).

Retrievers accept a string query as input and return a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) as output.

Note that all [vector stores](/oss/python/integrations/vectorstores) can be cast to retrievers. Refer to the vector store [integration docs](/oss/python/integrations/vectorstores/) for available vector stores.
This page lists custom retrievers, implemented via subclassing BaseRetriever.

## Bring-your-own documents

The below retrievers allow you to index and search a custom corpus of documents.

| Retriever                                                                                | Self-host | Cloud offering | Package                                                                                                                                                                               |
| ---------------------------------------------------------------------------------------- | --------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`AmazonKnowledgeBasesRetriever`](/oss/python/integrations/retrievers/bedrock)           | ❌         | ✅              | [`langchain-aws`](https://python.langchain.com/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html)                                      |
| [`AzureAISearchRetriever`](/oss/python/integrations/retrievers/azure_ai_search)          | ❌         | ✅              | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html)                   |
| [`ElasticsearchRetriever`](/oss/python/integrations/retrievers/elasticsearch_retriever)  | ✅         | ✅              | [`langchain-elasticsearch`](https://python.langchain.com/api_reference/elasticsearch/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html)                       |
| [`VertexAISearchRetriever`](/oss/python/integrations/retrievers/google_vertex_ai_search) | ❌         | ✅              | [`langchain-google-community`](https://python.langchain.com/api_reference/google_community/vertex_ai_search/langchain_google_community.vertex_ai_search.VertexAISearchRetriever.html) |

The below retrievers will search over an external index (e.g., constructed from Internet data or similar).

| Retriever                                                                | Source                                                | Package                                                                                                                                                                 |
| ------------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`ArxivRetriever`](/oss/python/integrations/retrievers/arxiv)            | Scholarly articles on [arxiv.org](https://arxiv.org/) | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)                       |
| [`TavilySearchAPIRetriever`](/oss/python/integrations/retrievers/tavily) | Internet search                                       | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.tavily_search_api.TavilySearchAPIRetriever.html) |
| [`WikipediaRetriever`](/oss/python/integrations/retrievers/wikipedia)    | [Wikipedia](https://www.wikipedia.org/) articles      | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html)               |

> **Note:** The descriptions in the table below are truncated for readability.

<Columns cols={3}>
  <Card title="Activeloop Deep Memory" icon="link" href="/oss/python/integrations/retrievers/activeloop" arrow="true" cta="View guide" />

<Card title="Amazon Kendra" icon="link" href="/oss/python/integrations/retrievers/amazon_kendra_retriever" arrow="true" cta="View guide" />

<Card title="Arcee" icon="link" href="/oss/python/integrations/retrievers/arcee" arrow="true" cta="View guide" />

<Card title="Arxiv" icon="link" href="/oss/python/integrations/retrievers/arxiv" arrow="true" cta="View guide" />

<Card title="AskNews" icon="link" href="/oss/python/integrations/retrievers/asknews" arrow="true" cta="View guide" />

<Card title="Azure AI Search" icon="link" href="/oss/python/integrations/retrievers/azure_ai_search" arrow="true" cta="View guide" />

<Card title="Bedrock (Knowledge Bases)" icon="link" href="/oss/python/integrations/retrievers/bedrock" arrow="true" cta="View guide" />

<Card title="BM25" icon="link" href="/oss/python/integrations/retrievers/bm25" arrow="true" cta="View guide" />

<Card title="Box" icon="link" href="/oss/python/integrations/retrievers/box" arrow="true" cta="View guide" />

<Card title="BREEBS (Open Knowledge)" icon="link" href="/oss/python/integrations/retrievers/breebs" arrow="true" cta="View guide" />

<Card title="Chaindesk" icon="link" href="/oss/python/integrations/retrievers/chaindesk" arrow="true" cta="View guide" />

<Card title="ChatGPT plugin" icon="link" href="/oss/python/integrations/retrievers/chatgpt-plugin" arrow="true" cta="View guide" />

<Card title="Cognee" icon="link" href="/oss/python/integrations/retrievers/cognee" arrow="true" cta="View guide" />

<Card title="Cohere reranker" icon="link" href="/oss/python/integrations/retrievers/cohere-reranker" arrow="true" cta="View guide" />

<Card title="Cohere RAG" icon="link" href="/oss/python/integrations/retrievers/cohere" arrow="true" cta="View guide" />

<Card title="Contextual AI Reranker" icon="link" href="/oss/python/integrations/retrievers/contextual" arrow="true" cta="View guide" />

<Card title="Dappier" icon="link" href="/oss/python/integrations/retrievers/dappier" arrow="true" cta="View guide" />

<Card title="DocArray" icon="link" href="/oss/python/integrations/retrievers/docarray_retriever" arrow="true" cta="View guide" />

<Card title="Dria" icon="link" href="/oss/python/integrations/retrievers/dria_index" arrow="true" cta="View guide" />

<Card title="ElasticSearch BM25" icon="link" href="/oss/python/integrations/retrievers/elastic_search_bm25" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/retrievers/elasticsearch_retriever" arrow="true" cta="View guide" />

<Card title="Embedchain" icon="link" href="/oss/python/integrations/retrievers/embedchain" arrow="true" cta="View guide" />

<Card title="FlashRank reranker" icon="link" href="/oss/python/integrations/retrievers/flashrank-reranker" arrow="true" cta="View guide" />

<Card title="Fleet AI Context" icon="link" href="/oss/python/integrations/retrievers/fleet_context" arrow="true" cta="View guide" />

<Card title="Galaxia" icon="link" href="/oss/python/integrations/retrievers/galaxia-retriever" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/retrievers/google_drive" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Search" icon="link" href="/oss/python/integrations/retrievers/google_vertex_ai_search" arrow="true" cta="View guide" />

<Card title="Graph RAG" icon="link" href="/oss/python/integrations/retrievers/graph_rag" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/retrievers/greennode_reranker" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/retrievers/ibm_watsonx_ranker" arrow="true" cta="View guide" />

<Card title="JaguarDB Vector Database" icon="link" href="/oss/python/integrations/retrievers/jaguar" arrow="true" cta="View guide" />

<Card title="Kay.ai" icon="link" href="/oss/python/integrations/retrievers/kay" arrow="true" cta="View guide" />

<Card title="Kinetica Vectorstore" icon="link" href="/oss/python/integrations/retrievers/kinetica" arrow="true" cta="View guide" />

<Card title="kNN" icon="link" href="/oss/python/integrations/retrievers/knn" arrow="true" cta="View guide" />

<Card title="LinkupSearchRetriever" icon="link" href="/oss/python/integrations/retrievers/linkup_search" arrow="true" cta="View guide" />

<Card title="LLMLingua Document Compressor" icon="link" href="/oss/python/integrations/retrievers/llmlingua" arrow="true" cta="View guide" />

<Card title="LOTR (Merger Retriever)" icon="link" href="/oss/python/integrations/retrievers/merger_retriever" arrow="true" cta="View guide" />

<Card title="Metal" icon="link" href="/oss/python/integrations/retrievers/metal" arrow="true" cta="View guide" />

<Card title="NanoPQ (Product Quantization)" icon="link" href="/oss/python/integrations/retrievers/nanopq" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/retrievers/nebius" arrow="true" cta="View guide" />

<Card title="needle" icon="link" href="/oss/python/integrations/retrievers/needle" arrow="true" cta="View guide" />

<Card title="Nimble" icon="link" href="/oss/python/integrations/retrievers/nimble" arrow="true" cta="View guide" />

<Card title="Outline" icon="link" href="/oss/python/integrations/retrievers/outline" arrow="true" cta="View guide" />

<Card title="Permit" icon="link" href="/oss/python/integrations/retrievers/permit" arrow="true" cta="View guide" />

<Card title="Pinecone Hybrid Search" icon="link" href="/oss/python/integrations/retrievers/pinecone_hybrid_search" arrow="true" cta="View guide" />

<Card title="Pinecone Rerank" icon="link" href="/oss/python/integrations/retrievers/pinecone_rerank" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/retrievers/pubmed" arrow="true" cta="View guide" />

<Card title="Qdrant Sparse Vector" icon="link" href="/oss/python/integrations/retrievers/qdrant-sparse" arrow="true" cta="View guide" />

<Card title="RAGatouille" icon="link" href="/oss/python/integrations/retrievers/ragatouille" arrow="true" cta="View guide" />

<Card title="RePhraseQuery" icon="link" href="/oss/python/integrations/retrievers/re_phrase" arrow="true" cta="View guide" />

<Card title="Rememberizer" icon="link" href="/oss/python/integrations/retrievers/rememberizer" arrow="true" cta="View guide" />

<Card title="SEC filing" icon="link" href="/oss/python/integrations/retrievers/sec_filings" arrow="true" cta="View guide" />

<Card title="SVM" icon="link" href="/oss/python/integrations/retrievers/svm" arrow="true" cta="View guide" />

<Card title="TavilySearchAPI" icon="link" href="/oss/python/integrations/retrievers/tavily" arrow="true" cta="View guide" />

<Card title="TF-IDF" icon="link" href="/oss/python/integrations/retrievers/tf_idf" arrow="true" cta="View guide" />

<Card title="NeuralDB" icon="link" href="/oss/python/integrations/retrievers/thirdai_neuraldb" arrow="true" cta="View guide" />

<Card title="ValyuContext" icon="link" href="/oss/python/integrations/retrievers/valyu" arrow="true" cta="View guide" />

<Card title="Vectorize" icon="link" href="/oss/python/integrations/retrievers/vectorize" arrow="true" cta="View guide" />

<Card title="Vespa" icon="link" href="/oss/python/integrations/retrievers/vespa" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/retrievers/wikipedia" arrow="true" cta="View guide" />

<Card title="You.com" icon="link" href="/oss/python/integrations/retrievers/you-retriever" arrow="true" cta="View guide" />

<Card title="Zep Cloud" icon="link" href="/oss/python/integrations/retrievers/zep_cloud_memorystore" arrow="true" cta="View guide" />

<Card title="Zep Open Source" icon="link" href="/oss/python/integrations/retrievers/zep_memorystore" arrow="true" cta="View guide" />

<Card title="Zilliz Cloud Pipeline" icon="link" href="/oss/python/integrations/retrievers/zilliz_cloud_pipeline" arrow="true" cta="View guide" />

<Card title="Zotero" icon="link" href="/oss/python/integrations/retrievers/zotero" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/retrievers/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to define a target function to evaluate

**URL:** llms-txt#how-to-define-a-target-function-to-evaluate

**Contents:**
- Target function signature

Source: https://docs.langchain.com/langsmith/define-target-function

There are three main pieces need to run an evaluation:

1. A [dataset](/langsmith/evaluation-concepts#datasets) of test inputs and expected outputs.
2. A target function which is what you're evaluating.
3. [Evaluators](/langsmith/evaluation-concepts#evaluators) that score your target function's outputs.

This guide shows you how to define the target function depending on the part of your application you are evaluating. See here for [how to create a dataset](/langsmith/manage-datasets-programmatically) and [how to define evaluators](/langsmith/code-evaluator), and here for an [end-to-end example of running an evaluation](/langsmith/evaluate-llm-application).

## Target function signature

In order to evaluate an application in code, we need a way to run the application. When using `evaluate()` ([Python](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.evaluate)/[TypeScript](https://docs.smith.langchain.com/reference/js/functions/evaluation.evaluate))we'll do this by passing in a *target function* argument. This is a function that takes in a dataset [Example's](/langsmith/evaluation-concepts#examples) inputs and returns the application output as a dict. Within this function we can call our application however we'd like. We can also format the output however we'd like. The key is that any evaluator functions we define should work with the output format we return in our target function.

```python  theme={null}
from langsmith import Client

---

## Our SQL queries will only work if we filter on the exact string values that are in the DB.

**URL:** llms-txt#our-sql-queries-will-only-work-if-we-filter-on-the-exact-string-values-that-are-in-the-db.

---

## ... same as above

**URL:** llms-txt#...-same-as-above

**Contents:**
- Distributed tracing in TypeScript

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    my_application(langsmith_extra={"parent": request.headers})
typescript  theme={null}
// client.mts
import { getCurrentRunTree, traceable } from "langsmith/traceable";

const client = traceable(
    async () => {
        const runTree = getCurrentRunTree();
        return await fetch("...", {
            method: "POST",
            headers: runTree.toHeaders(),
        }).then((a) => a.text());
    },
    { name: "client" }
);

await client();
typescript Express.JS theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import express from "express";
  import bodyParser from "body-parser";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = express();
      app.use(bodyParser.text());

app.post("/", async (req, res) => {
      const runTree = RunTree.fromHeaders(req.headers);
      const result = await withRunTree(runTree, () => server(req.body));
      res.send(result);
  });
  typescript Hono theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import { Hono } from "hono";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = new Hono();

app.post("/", async (c) => {
      const body = await c.req.text();
      const runTree = RunTree.fromHeaders(c.req.raw.headers);
      const result = await withRunTree(runTree, () => server(body));
      return c.body(result);
  });
  ```
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/distributed-tracing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Distributed tracing in TypeScript

<Note>
  Distributed tracing in TypeScript requires `langsmith` version `>=0.1.31`
</Note>

First, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:
```

Example 2 (unknown):
```unknown
Then, the server converts the headers back to a run tree, which it uses to further continue the tracing.

To pass the newly created run tree to a traceable function, we can use the `withRunTree` helper, which will ensure the run tree is propagated within traceable invocations.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## test write permissions

**URL:** llms-txt#test-write-permissions

**Contents:**
  - Monitoring Runs
  - Common Errors

touch ./test.txt
aws s3 --endpoint-url=<endpoint_url> cp ./test.txt s3://<bucket-name>/tmp/test.txt
```

You can monitor your runs using the [List Runs API](#list-runs-for-an-export). If this is a known error, this will be added to the `errors` field of the run.

Here are some common errors:

| Error                              | Description                                                                                                                                                                                                                                                                                              |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Access denied                      | The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn't have the necessary permissions to access the specified bucket or perform the required operations.                                                                  |
| Bucket is not valid                | The specified blob store bucket is not valid. This error is thrown when the bucket doesn't exist or there is not enough access to perform writes on the bucket.                                                                                                                                          |
| Key ID you provided does not exist | The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key.                                                                                                                                                                  |
| Invalid endpoint                   | The endpoint\_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example `https://storage.googleapis.com` for GCS, `https://play.min.io` for minio, etc. If using AWS, you should omit the endpoint\_url. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-export.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Continue execution

**URL:** llms-txt#continue-execution

**Contents:**
  - Review tool calls
- Short-term memory
  - Manage checkpoints
  - Decouple return value from saved value
  - Chatbot example
- Long-term memory
- Workflows
- Integrate with other libraries

for event in graph.stream(Command(resume="baz"), config):
    print(event)
    print("\n")
python  theme={null}
from typing import Union

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call, returning a validated version."""
    human_review = interrupt(
        {
            "question": "Is this correct?",
            "tool_call": tool_call,
        }
    )
    review_action = human_review["action"]
    review_data = human_review.get("data")
    if review_action == "continue":
        return tool_call
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, name=tool_call["name"], tool_call_id=tool_call["id"]
        )
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

model_response = call_model(messages).result()
    while True:
        if not model_response.tool_calls:
            break

# Review tool calls
        tool_results = []
        tool_calls = []
        for i, tool_call in enumerate(model_response.tool_calls):
            review = review_tool_call(tool_call)
            if isinstance(review, ToolMessage):
                tool_results.append(review)
            else:  # is a validated tool call
                tool_calls.append(review)
                if review != tool_call:
                    model_response.tool_calls[i] = review  # update message

# Execute remaining tool calls
        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]
        remaining_tool_results = [fut.result() for fut in tool_result_futures]

# Append to message list
        messages = add_messages(
            messages,
            [model_response, *tool_results, *remaining_tool_results],
        )

# Call model again
        model_response = call_model(messages).result()

# Generate final response
    messages = add_messages(messages, model_response)
    return entrypoint.final(value=model_response, save=messages)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1",  # [!code highlight]
        # optionally provide an ID for a specific checkpoint,
        # otherwise the latest checkpoint is shown
        # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
}
graph.get_state(config)  # [!code highlight]

StateSnapshot(
    values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
    metadata={
        'source': 'loop',
        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
        'step': 4,
        'parents': {},
        'thread_id': '1'
    },
    created_at='2025-05-05T16:01:24.680462+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
    tasks=(),
    interrupts=()
)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1"  # [!code highlight]
    }
}
list(graph.get_state_history(config))  # [!code highlight]

[
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
        next=('call_model',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863421+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=('__start__',),
        config={...},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863173+00:00',
        parent_config={...}
        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=(),
        config={...},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.862295+00:00',
        parent_config={...}
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob")]},
        next=('call_model',),
        config={...},
        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.278960+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.277497+00:00',
        parent_config=None,
        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
        interrupts=()
    )
]
python  theme={null}
from langgraph.func import entrypoint
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def accumulate(n: int, *, previous: int | None) -> entrypoint.final[int, int]:
    previous = previous or 0
    total = previous + n
    # Return the *previous* value to the caller but save the *new* total to the checkpoint.
    return entrypoint.final(value=previous, save=total)

config = {"configurable": {"thread_id": "my-thread"}}

print(accumulate.invoke(1, config=config))  # 0
print(accumulate.invoke(2, config=config))  # 1
print(accumulate.invoke(3, config=config))  # 3
python  theme={null}
from langchain.messages import BaseMessage
from langgraph.graph import add_messages
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import InMemorySaver
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5")

@task
def call_model(messages: list[BaseMessage]):
    response = model.invoke(messages)
    return response

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):
    if previous:
        inputs = add_messages(previous, inputs)

response = call_model(inputs).result()
    return entrypoint.final(value=response, save=add_messages(inputs, response))

config = {"configurable": {"thread_id": "1"}}
input_message = {"role": "user", "content": "hi! I'm bob"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

[long-term memory](/oss/python/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.

* [Workflows and agent](/oss/python/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.

## Integrate with other libraries

* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
After resuming, the run proceeds through the remaining step and terminates as expected.

### Review tool calls

To review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/python/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.

Given a tool call, our function will [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) for human review. At that point we can either:

* Accept the tool call
* Revise the tool call and continue
* Generate a custom tool message (e.g., instructing the model to re-format its tool call)
```

Example 2 (unknown):
```unknown
We can now update our [entrypoint](/oss/python/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
```

Example 3 (unknown):
```unknown
## Short-term memory

Short-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/python/langgraph/functional-api#short-term-memory) for more details.

### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<a id="checkpoint" />

#### View thread state
```

Example 4 (unknown):
```unknown

```

---

## How we are sampling runs to include in our dataset

**URL:** llms-txt#how-we-are-sampling-runs-to-include-in-our-dataset

**Contents:**
  - Convert runs to experiment

end_time = datetime.now(tz=timezone.utc)
start_time = end_time - timedelta(days=1)
run_filter = f'and(gt(start_time, "{start_time.isoformat()}"), lt(end_time, "{end_time.isoformat()}"))'
prod_runs = list(
    client.list_runs(
        project_name=project_name,
        is_root=True,
        filter=run_filter,
    )
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Convert runs to experiment

`convert_runs_to_test` is a function which takes some runs and does the following:

1. The inputs, and optionally the outputs, are saved to a dataset as Examples.
2. The inputs and outputs are stored as an experiment, as if you had run the `evaluate` function and received those outputs.
```

---

## Unified access to content blocks

**URL:** llms-txt#unified-access-to-content-blocks

**Contents:**
  - Benefits
- Simplified package
  - Namespace

for block in response.content_blocks:
    if block["type"] == "reasoning":
        print(f"Model reasoning: {block['reasoning']}")
    elif block["type"] == "text":
        print(f"Response: {block['text']}")
    elif block["type"] == "tool_call":
        print(f"Tool call: {block['name']}({block['args']})")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Benefits

* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider
* **Type safe**: Full type hints for all content block types
* **Backward compatible**: Standard content can be [loaded lazily](/oss/python/langchain/messages#standard-content-blocks), so there are no associated breaking changes

For more information, see our guide on [content blocks](/oss/python/langchain/messages#standard-content-blocks).

***

## Simplified package

LangChain v1 streamlines the [`langchain`](https://pypi.org/project/langchain/) package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                                 |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality     |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from @\[`langchain-core`] |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from @\[`langchain-core`] |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization          |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                      |

Most of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents.
```

---

## Time travel using the server API

**URL:** llms-txt#time-travel-using-the-server-api

**Contents:**
- Use time travel in a workflow
  - 1. Run the graph
  - 2. Identify a checkpoint
  - 3. Update the state
  - 4. Resume execution from the checkpoint
- Learn more

Source: https://docs.langchain.com/langsmith/human-in-the-loop-time-travel

LangGraph provides the [**time travel**](/oss/python/langgraph/use-time-travel) functionality to resume execution from a prior checkpoint, either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.

To time travel using the LangGraph Server API (via the LangGraph SDK):

1. **Run the graph** with initial inputs using [LangGraph SDK](/langsmith/langgraph-python-sdk)'s [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs.
2. **Identify a checkpoint in an existing thread**: Use [client.threads.get\_history](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
   Alternatively, set a [breakpoint](/oss/python/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.
3. **(Optional) modify the graph state**: Use the [client.threads.update\_state](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.update_state) method to modify the graph’s state at the checkpoint and resume execution from alternative state.
4. **Resume execution from the checkpoint**: Use the [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs with an input of `None` and the appropriate `thread_id` and `checkpoint_id`.

## Use time travel in a workflow

<Accordion title="Example graph">
  
</Accordion>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

### 2. Identify a checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 4. Resume execution from the checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

* [**LangGraph time travel guide**](/oss/python/langgraph/use-time-travel): learn more about using time travel in LangGraph.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/human-in-the-loop-time-travel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 1. Run the graph

<Tabs>
  <Tab title="Python">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 4 (unknown):
```unknown
Run the graph:
```

---

## maxReplicas: 5

**URL:** llms-txt#maxreplicas:-5

---

## Run all tests

**URL:** llms-txt#run-all-tests

uv run --group test pytest tests/unit_tests/
uv run --group test --group test_integration pytest -n auto tests/integration_tests/

---

## LangGraph JS/TS SDK

**URL:** llms-txt#langgraph-js/ts-sdk

Source: https://docs.langchain.com/langsmith/langgraph-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-js-ts-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Interrupts

**URL:** llms-txt#interrupts

**Contents:**
- Pause using `interrupt`
- Resuming interrupts

Source: https://docs.langchain.com/oss/python/langgraph/interrupts

Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/python/langgraph/persistence) layer and waits indefinitely until you resume execution.

Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.

Unlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**—they can be placed anywhere in your code and can be conditional based on your application logic.

* **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.
* **`thread_id` is your pointer:** set `config={"configurable": {"thread_id": ...}}` to tell the checkpointer which state to load.
* **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` return to the caller in the `__interrupt__` field so you know what the graph is waiting on.

The `thread_id` you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.

## Pause using `interrupt`

The [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function pauses graph execution and returns a value to the caller. When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) within a node, LangGraph saves the current graph state and waits for you to resume execution with input.

To use [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), you need:

1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)
2. A **thread ID** in your config so the runtime knows which state to resume from
3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)

When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.

```python  theme={null}
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown
When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.
```

---

## from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists

**URL:** llms-txt#from-langchain_community.utilities.google_jobs-import-googlejobsapiwrapper-#-if-exists

bash  theme={null}
  pip install google-search-results langchain-community # Requires langchain-community
  bash uv theme={null}
  uv add google-search-results langchain-community # Requires langchain-community
  python  theme={null}
from langchain_community.tools.google_lens import GoogleLensQueryRun
from langchain_community.utilities.google_lens import GoogleLensAPIWrapper
bash pip theme={null}
  pip install googlemaps langchain # Requires base langchain
  bash uv theme={null}
  uv add googlemaps langchain # Requires base langchain
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
#### Google Lens

Perform visual searches. Requires `google-search-results` package and SerpApi key.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See [usage example and authorization instructions](/oss/python/integrations/tools/google_lens).
```

Example 4 (unknown):
```unknown
#### Google Places

Search for places information. Requires `googlemaps` package and a Google Maps API key.

<CodeGroup>
```

---

## How to add custom middleware

**URL:** llms-txt#how-to-add-custom-middleware

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-middleware

When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as [adding custom routes](/langsmith/custom-routes). You just need to provide your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps).

Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs.

Below is an example using FastAPI.

<Note>
  "Python only"
  We currently only support custom middleware in Python deployments with `langgraph-api>=0.0.26`.
</Note>

Starting from an **existing** LangSmith application, add the following middleware code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={5}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## Create the example

**URL:** llms-txt#create-the-example

**Contents:**
- 2. Run evaluations
  - Define a target function

ls_client.create_examples(
  dataset_id=dataset.id,
  examples=[example],
  # Uncomment this flag if you'd like to upload attachments from local files:
  # dangerously_allow_filesystem=True
)
typescript  theme={null}
import { Client } from "langsmith";
import { v4 as uuid4 } from "uuid";

// Publicly available test files
const pdfUrl = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf";
const wavUrl = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav";
const pngUrl = "https://www.w3.org/Graphics/PNG/nurbcup2si.png";

// Helper function to fetch file as ArrayBuffer
async function fetchArrayBuffer(url: string): Promise<ArrayBuffer> {
  const response = await fetch(url);
  if (!response.ok) {
    throw new Error(`Failed to fetch ${url}: ${response.statusText}`);
  }
  return response.arrayBuffer();
}

// Fetch files as ArrayBuffer
const pdfArrayBuffer = await fetchArrayBuffer(pdfUrl);
const wavArrayBuffer = await fetchArrayBuffer(wavUrl);
const pngArrayBuffer = await fetchArrayBuffer(pngUrl);

// Create the LangSmith client (Ensure LANGSMITH_API_KEY is set in env)
const langsmithClient = new Client();

// Create a unique dataset name
const datasetName = "attachment-test-dataset:" + uuid4().substring(0, 8);

// Create the dataset
const dataset = await langsmithClient.createDataset(datasetName, {
  description: "Test dataset for evals with publicly available attachments",
});

// Define the example with attachments
const exampleId = uuid4();
const example = {
  id: exampleId,
  inputs: {
      audio_question: "What is in this audio clip?",
      image_question: "What is in this image?",
  },
  outputs: {
      audio_answer: "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
      image_answer: "A mug with a blanket over it.",
  },
  attachments: {
    my_pdf: {
      mimeType: "application/pdf",
      data: pdfArrayBuffer
    },
    my_wav: {
      mimeType: "audio/wav",
      data: wavArrayBuffer
    },
    my_img: {
      mimeType: "image/png",
      data: pngArrayBuffer
    },
  },
};

// Upload the example with attachments to the dataset
await langsmithClient.uploadExamplesMultipart(dataset.id, [example]);
python  theme={null}
  client.create_examples(..., dangerously_allow_filesystem=True)
  python  theme={null}
{
    "presigned_url": str,
    "mime_type": str,
    "reader": BinaryIO
}
python  theme={null}
from langsmith.wrappers import wrap_openai
import base64
from openai import OpenAI

client = wrap_openai(OpenAI())

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

Requires version >= 0.2.13

You can use the `uploadExamplesMultipart` method to upload examples with attachments.

Note that this is a different method from the standard `createExamples` method, which currently does not support attachments. Each attachment requires either a `Uint8Array` or an `ArrayBuffer` as the data type.

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown
<Info>
  Along with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment `data` value and specify arg `dangerously_allow_filesystem=True`:
```

Example 3 (unknown):
```unknown
</Info>

## 2. Run evaluations

### Define a target function

Now that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAI's GPT-4o model to answer questions about an image and an audio clip.

#### Python

The target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be called `inputs` and the second must be called `attachments`.

* The `inputs` argument is a dictionary that contains the input data for the example, excluding the attachments.
* The `attachments` argument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime\_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:
```

Example 4 (unknown):
```unknown

```

---

## LangGraph Python SDK

**URL:** llms-txt#langgraph-python-sdk

Source: https://docs.langchain.com/langsmith/langgraph-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-python-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Transient files:

**URL:** llms-txt#transient-files:

---

## build based on the provided config

**URL:** llms-txt#build-based-on-the-provided-config

def make_graph(config: RunnableConfig):
    user_id = config.get("configurable", {}).get("user_id")
    # route to different graph state / structure based on the user ID
    if user_id == "1":
        return make_default_graph()
    else:
        return make_alternative_graph()

{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:make_graph",
    },
    "env": "./.env"
}
```

See more info on LangGraph API configuration file [here](/langsmith/cli#configuration-file)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/graph-rebuild.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Finally, you need to specify the path to your graph-making function (`make_graph`) in `langgraph.json`:
```

---

## and this is also supported

**URL:** llms-txt#and-this-is-also-supported

**Contents:**
- Nodes

{"messages": [{"type": "human", "content": "message"}]}
python  theme={null}
from langchain.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated
from typing_extensions import TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
python  theme={null}
from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
python  theme={null}
from dataclasses import dataclass
from typing_extensions import TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

class State(TypedDict):
    input: str
    results: str

@dataclass
class Context:
    user_id: str

builder = StateGraph(State)

def plain_node(state: State):
    return state

def node_with_runtime(state: State, runtime: Runtime[Context]):
    print("In node: ", runtime.context.user_id)
    return {"results": f"Hello, {state['input']}!"}

def node_with_config(state: State, config: RunnableConfig):
    print("In node with thread_id: ", config["configurable"]["thread_id"])
    return {"results": f"Hello, {state['input']}!"}

builder.add_node("plain_node", plain_node)
builder.add_node("node_with_runtime", node_with_runtime)
builder.add_node("node_with_config", node_with_config)
...
python  theme={null}
builder.add_node(my_node)

**Examples:**

Example 1 (unknown):
```unknown
Since the state updates are always deserialized into LangChain `Messages` when using [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages), you should use dot notation to access message attributes, like `state["messages"][-1].content`. Below is an example of a graph that uses [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) as its reducer function.
```

Example 2 (unknown):
```unknown
#### MessagesState

Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:
```

Example 3 (unknown):
```unknown
## Nodes

In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:

1. `state`: The [state](#state) of the graph
2. `config`: A [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) object that contains configuration information like `thread_id` and tracing information like `tags`
3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`

Similar to `NetworkX`, you add these nodes to a graph using the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) method:
```

Example 4 (unknown):
```unknown
Behind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.

If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.
```

---

## End runs

**URL:** llms-txt#end-runs

**Contents:**
- Batch Ingestion

patch_run(child_run_id, chat_completion.dict())
patch_run(parent_run_id, {"answer": chat_completion.choices[0].message.content})
python  theme={null}
import json
import os
import uuid
from datetime import datetime, timezone
from typing import Dict, List
import requests
from requests_toolbelt import MultipartEncoder

def create_dotted_order(
    start_time: datetime | None = None,
    run_id: uuid.UUID | None = None
) -> str:
    """Create a dotted order string for run ordering and hierarchy.

The dotted order is used to establish the sequence and relationships between runs.
    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.
    """
    st = start_time or datetime.now(timezone.utc)
    id_ = run_id or uuid.uuid4()
    return f"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}"

def create_run_base(
    name: str,
    run_type: str,
    inputs: dict,
    start_time: datetime
) -> dict:
    """Create the base structure for a run."""
    run_id = uuid.uuid4()
    return {
        "id": str(run_id),
        "trace_id": str(run_id),
        "name": name,
        "start_time": start_time.isoformat(),
        "inputs": inputs,
        "run_type": run_type,
    }

def construct_run(
    name: str,
    run_type: str,
    inputs: dict,
    parent_dotted_order: str | None = None,
) -> dict:
    """Construct a run dictionary with the given parameters.

This function creates a run with a unique ID and dotted order, establishing its place
    in the trace hierarchy if it's a child run.
    """
    start_time = datetime.now(timezone.utc)
    run = create_run_base(name, run_type, inputs, start_time)
    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run["id"]))

if parent_dotted_order:
        current_dotted_order = f"{parent_dotted_order}.{current_dotted_order}"
        run["trace_id"] = parent_dotted_order.split(".")[0].split("Z")[1]
        run["parent_run_id"] = parent_dotted_order.split(".")[-1].split("Z")[1]

run["dotted_order"] = current_dotted_order
    return run

def serialize_run(operation: str, run_data: dict) -> List[tuple]:
    """Serialize a run for the multipart request.

This function separates the run data into parts for efficient transmission and storage.
    The main run data and optional fields (inputs, outputs, events) are serialized separately.
    """
    run_id = run_data.get("id", str(uuid.uuid4()))

# Separate optional fields
    inputs = run_data.pop("inputs", None)
    outputs = run_data.pop("outputs", None)
    events = run_data.pop("events", None)

# Serialize main run data
    run_data_json = json.dumps(run_data).encode("utf-8")
    parts.append(
        (
            f"{operation}.{run_id}",
            (
                None,
                run_data_json,
                "application/json",
                {"Content-Length": str(len(run_data_json))},
            ),
        )
    )

# Serialize optional fields
    for key, value in [("inputs", inputs), ("outputs", outputs), ("events", events)]:
        if value:
            serialized_value = json.dumps(value).encode("utf-8")
            parts.append(
                (
                    f"{operation}.{run_id}.{key}",
                    (
                        None,
                        serialized_value,
                        "application/json",
                        {"Content-Length": str(len(serialized_value))},
                    ),
                )
            )

def batch_ingest_runs(
    api_url: str,
    api_key: str,
    posts: list[dict] | None = None,
    patches: list[dict] | None = None,
) -> None:
    """Ingest multiple runs in a single batch request.

This function handles both creating new runs (posts) and updating existing runs (patches).
    It's more efficient for ingesting multiple runs compared to individual API calls.
    """
    boundary = uuid.uuid4().hex
    all_parts = []

for operation, runs in zip(("post", "patch"), (posts, patches)):
        if runs:
            all_parts.extend(
                [part for run in runs for part in serialize_run(operation, run)]
            )

encoder = MultipartEncoder(fields=all_parts, boundary=boundary)
    headers = {"Content-Type": encoder.content_type, "x-api-key": api_key}

try:
        response = requests.post(
            f"{api_url}/runs/multipart",
            data=encoder,
            headers=headers
        )
        response.raise_for_status()
        print("Successfully ingested runs.")
    except requests.RequestException as e:
        print(f"Error ingesting runs: {e}")
        # In a production environment, you might want to log this error or handle it more robustly

**Examples:**

Example 1 (unknown):
```unknown
See the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.

## Batch Ingestion

For faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ) and `requests_toolbelt` to run
```

---

## LangChain Academy

**URL:** llms-txt#langchain-academy

Source: https://docs.langchain.com/oss/python/langchain/academy

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/academy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Example Collector Configuration: Logs Sidecar

**URL:** llms-txt#example-collector-configuration:-logs-sidecar

---

## Since this is **more specific** than the generic @auth.on handler, it will take precedence

**URL:** llms-txt#since-this-is-**more-specific**-than-the-generic-@auth.on-handler,-it-will-take-precedence

---

## You can customize the run name with the `name` keyword argument

**URL:** llms-txt#you-can-customize-the-run-name-with-the-`name`-keyword-argument

@traceable(name="Extract User Details")
def my_function(text: str) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": f"Extract {text}"},
        ]
    )

my_function("Jason is 25 years old")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-instructor.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Log LLM calls

**URL:** llms-txt#log-llm-calls

**Contents:**
- Messages Format
  - Examples
- Converting custom I/O formats into LangSmith compatible formats
- Identifying a custom model in traces
- Provide token and cost information
  - Setting run metadata
  - Setting run outputs
- Time-to-first-token

Source: https://docs.langchain.com/langsmith/log-llm-trace

This guide will cover how to log LLM calls to LangSmith when you are using a custom model or a custom input/output format. To make the most of LangSmith's LLM trace processing, you should log your LLM traces in one of the specified formats.

LangSmith offers the following benefits for LLM traces:

* Rich, structured rendering of message lists
* Token and cost tracking per LLM call, per trace and across traces over time

If you don't log your LLM traces in the suggested formats, you will still be able to log the data to LangSmith, but it may not be processed or rendered in expected ways.

If you are using [LangChain OSS](https://python.langchain.com/docs/tutorials/llm_chain/) to call language models or LangSmith wrappers ([OpenAI](/langsmith/trace-openai), [Anthropic](/langsmith/trace-anthropic)), these approaches will automatically log traces in the correct format.

<Note>
  The examples on this page use the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the [RunTree](/langsmith/annotate-code#use-the-runtree-api) or [API](https://api.smith.langchain.com/redoc) directly.
</Note>

When tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details,  refer to the [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create) or [Anthropic Messages](https://docs.claude.com/en/api/messages) documentation. The LangChain format is:

<Expandable title="LangChain format">
  <ParamField path="messages" type="array" required>
    A list of messages containing the content of the conversation.

<ParamField path="role" type="string" required>
      Identifies the message type. One of: <code>system</code> | <code>reasoning</code> | <code>user</code> | <code>assistant</code> | <code>tool</code>
    </ParamField>

<ParamField path="content" type="array" required>
      Content of the message. List of typed dictionaries.

<Expandable title="Content options">
        <ParamField path="type" type="string" required>
          One of: <code>text</code> | <code>image</code> | <code>file</code> | <code>audio</code> | <code>video</code> | <code>tool\_call</code> | <code>server\_tool\_call</code> | <code>server\_tool\_result</code>.
        </ParamField>

<Expandable title="text">
          <ParamField path="type" type="literal('text')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="annotations" type="object[]">
            List of annotations for the text
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="reasoning">
          <ParamField path="type" type="literal('reasoning')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="image">
          <ParamField path="type" type="literal('image')" required />

<ParamField path="url" type="string">
            URL pointing to the image location.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded image data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`).
          </ParamField>
        </Expandable>

<Expandable title="file (e.g., PDFs)">
          <ParamField path="type" type="literal('file')" required />

<ParamField path="url" type="string">
            URL pointing to the file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded file data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `application/pdf`).
          </ParamField>
        </Expandable>

<Expandable title="audio">
          <ParamField path="type" type="literal('audio')" required />

<ParamField path="url" type="string">
            URL pointing to the audio file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded audio data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `audio/mpeg`, `audio/wav`).
          </ParamField>
        </Expandable>

<Expandable title="video">
          <ParamField path="type" type="literal('video')" required />

<ParamField path="url" type="string">
            URL pointing to the video file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded video data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `video/mp4`, `video/webm`).
          </ParamField>
        </Expandable>

<Expandable title="tool_call">
          <ParamField path="type" type="literal('tool_call')" required />

<ParamField path="name" type="string" />

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_call">
          <ParamField path="type" type="literal('server_tool_call')" required />

<ParamField path="id" type="string" required>
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="name" type="string" required>
            The name of the tool to be called.
          </ParamField>

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_result">
          <ParamField path="type" type="literal('server_tool_result')" required />

<ParamField path="tool_call_id" type="string" required>
            Identifier of the corresponding server tool call.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="status" type="string" required>
            Execution status of the server-side tool. One of: <code>success</code> | <code>error</code>.
          </ParamField>

<ParamField path="output">
            Output of the executed tool.
          </ParamField>
        </Expandable>
      </Expandable>
    </ParamField>

<ParamField path="tool_call_id" type="string">
      Must match the <code>id</code> of a prior <code>assistant</code> message’s <code>tool\_calls\[i]</code> entry. Only valid when <code>role</code> is <code>tool</code>.
    </ParamField>

<ParamField path="usage_metadata" type="object">
      Use this field to send token counts and/or costs with your model's output. See [this guide](/langsmith/log-llm-trace#provide-token-and-cost-information) for more details.
    </ParamField>
  </ParamField>
</Expandable>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
    
  </CodeGroup>
</Expandable>

## Identifying a custom model in traces

When using a custom model, it is recommended to also provide the following `metadata` fields to identify the model when viewing traces and when filtering.

* `ls_provider`: The provider of the model, eg "openai", "anthropic", etc.
* `ls_model_name`: The name of the model, eg "gpt-4o-mini", "claude-3-opus-20240307", etc.

This code will log the following trace:

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f152f49a6313d98e29d3a7b42b76c11f" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1169" width="1169" data-og-height="548" height="548" data-path="langsmith/images/chat-model-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=083affd641c8eb41b0fcce26c8485076 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8ff2ced008bb1be8db587c40cc4a6cd8 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8891eae04f01247ec86c6e6b3de7a9cb 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=3287ed0315422c879ff151bf2561e199 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f9995381307324951553d7cfe8d00cdd 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=4fd10d74ce1253b1b3da84e49a439e33 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=1da2f0a1adc972aa6de6df94cbfc1407" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1168" width="1168" data-og-height="563" height="563" data-path="langsmith/images/chat-model-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=6662af6b4871f8250ab39659fd594df8 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=c3986c36300cef013831eb0ba951b0fc 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=05d5f21e2509e36e8176fb8ace2c1e79 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=464d2babbc58e4c42b3e720520af680c 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=db6c45b71fa4ff605edc8c7f88404ec2 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=cffed55abf3973cb5b908725433e001e 2500w" />
</div>

If you implement a custom streaming chat\_model, you can "reduce" the outputs into the same format as the non-streaming version. This is currently only supported in Python.

<Check>
  If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.metadata` for estimating token counts. The following fields are used in the order of precedence:

1. `metadata.ls_model_name`
  2. `inputs.model`
  3. `inputs.model_name`
</Check>

To learn more about how to use the `metadata` fields, refer to the [Add metadata and tags](/langsmith/add-metadata-tags) guide.

## Provide token and cost information

LangSmith calculates costs automatically by using the [model pricing table](https://smith.langchain.com/settings/workspaces/models) when token counts are provided. To learn how LangSmith calculates token-based costs, see [this guide](/langsmith/calculate-token-based-costs).

Many models include token counts as part of the response. You can provide token counts to LangSmith in one of two ways:

1. Extract usage within your traced function and set a `usage_metadata` field on the run's metadata.
2. Return a `usage_metadata` field in your traced function outputs.

In both cases, the usage metadata you send should contain a subset of the following LangSmith-recognized fields:

<Warning>
  You cannot set any fields other than the ones listed below. You do not need to include all fields.
</Warning>

Note that the usage data can also include cost information, in case you do not want to rely on LangSmith's token-based cost formula. This is useful for models with pricing that is not linear by token type.

### Setting run metadata

You can [modify the current run's metadata](/langsmith/add-metadata-tags) with usage information within your traced function. The advantage of this approach is that you do not need to change your traced function's runtime outputs. Here's an example:

<Note>
  Requires `langsmith>=0.3.43` (Python) and `langsmith>=0.3.30` (JS/TS).
</Note>

### Setting run outputs

You can add a `usage_metadata` key to the function's response to set manual token counts and costs.

## Time-to-first-token

If you are using `traceable` or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.
However, if you are using the `RunTree` API directly, you will need to add a `new_token` event to the run tree in order to properly populate time-to-first-token.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-llm-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
```

---

## We now add a conditional edge

**URL:** llms-txt#we-now-add-a-conditional-edge

workflow.add_conditional_edges(
    # First, we define the start node. We use 'agent'.
    # This means these are the edges taken after the 'agent' node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
)

---

## Evaluation job and results

**URL:** llms-txt#evaluation-job-and-results

**Contents:**
  - Trajectory evaluator
  - Single step evaluators

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[final_answer_correct],
    experiment_prefix="sql-agent-gpt4o-e2e",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}
def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:
    """Check how many of the desired steps the agent took."""
    if len(reference_outputs['trajectory']) > len(outputs['trajectory']):
        return False

i = j = 0
    while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):
        if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:
            i += 1
        j += 1

return i / len(reference_outputs['trajectory'])
python  theme={null}
async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    trajectory = []
    # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/
    # Set stream_mode="debug" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming
    async for namespace, chunk in graph.astream({"messages": [
            {
                "role": "user",
                "content": inputs['question'],
            }
        ]}, subgraphs=True, stream_mode="debug"):
        # Event type for entering a node
        if chunk['type'] == 'task':
            # Record the node name
            trajectory.append(chunk['payload']['name'])
            # Given how we defined our dataset, we also need to track when specific tools are
            # called by our question answering ReACT agent. These tool calls can be found
            # when the ToolsNode (named "tools") is invoked by looking at the AIMessage.tool_calls
            # of the latest input message.
            if chunk['payload']['name'] == 'tools' and chunk['type'] == 'task':
                for tc in chunk['payload']['input']['messages'][-1].tool_calls:
                    trajectory.append(tc['name'])
    return {"trajectory": trajectory}

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[trajectory_subsequence],
    experiment_prefix="sql-agent-gpt4o-trajectory",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Trajectory evaluator

As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it's often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn't reach the right final answer.

This is where trajectory evaluations come in. A trajectory evaluation:

1. Compares the actual sequence of steps the agent took against an expected sequence
2. Calculates a score based on how many of the expected steps were completed correctly

For this example, our end-to-end dataset contains an ordered list of steps that we expect the agent to take. Let's create an evaluator that checks the agent's actual trajectory against these expected steps and calculates what percentage were completed:
```

Example 2 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'trajectory' key, so lets define a target function that does so. We'll need to usage [LangGraph's streaming capabilities](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming/) to record the trajectory.

Note that we are reusing the same dataset as for our final response evaluation, so we could have run both evaluators together and defined a target function that returns both "response" and "trajectory". In practice it's often useful to have separate datasets for each type of evaluation, which is why we show them separately here:
```

Example 3 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Single step evaluators

While end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly.

In our case, a crucial part of our agent is that it routes the user's intention correctly into either the "refund" path or the "question answering" path. Let's create a dataset and run some evaluations to directly stress test this one component.
```

---

## (This can be done after putting memories into the store)

**URL:** llms-txt#(this-can-be-done-after-putting-memories-into-the-store)

memories = store.search(
    namespace_for_memory,
    query="What does the user like to eat?",
    limit=3  # Return top 3 matches
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:
```

---

## NOTE: there are no edges between nodes A, B and C!

**URL:** llms-txt#note:-there-are-no-edges-between-nodes-a,-b-and-c!

**Contents:**
  - Navigate to a node in a parent graph
  - Use inside tools
- Visualize your graph
  - Mermaid
  - PNG

graph = builder.compile()
python  theme={null}
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
import operator
from typing_extensions import Annotated

class State(TypedDict):
    # NOTE: we define a reducer here
    foo: Annotated[str, operator.add]  # [!code highlight]

def node_a(state: State):
    print("Called A")
    value = random.choice(["a", "b"])
    # this is a replacement for a conditional edge function
    if value == "a":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        update={"foo": value},
        goto=goto,
        # this tells LangGraph to navigate to node_b or node_c in the parent graph
        # NOTE: this will navigate to the closest parent graph relative to the subgraph
        graph=Command.PARENT,  # [!code highlight]
    )

subgraph = StateGraph(State).add_node(node_a).add_edge(START, "node_a").compile()

def node_b(state: State):
    print("Called B")
    # NOTE: since we've defined a reducer, we don't need to manually append
    # new characters to existing 'foo' value. instead, reducer will append these
    # automatically (via operator.add)
    return {"foo": "b"}  # [!code highlight]

def node_c(state: State):
    print("Called C")
    return {"foo": "c"}  # [!code highlight]

builder = StateGraph(State)
builder.add_edge(START, "subgraph")
builder.add_node("subgraph", subgraph)
builder.add_node(node_b)
builder.add_node(node_c)

graph = builder.compile()
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
@tool
def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):
    """Use this to look up user information to better assist them with their questions."""
    user_info = get_user_info(config.get("configurable", {}).get("user_id"))
    return Command(
        update={
            # update the state keys
            "user_info": user_info,
            # update the message history
            "messages": [ToolMessage("Successfully looked up user information", tool_call_id=tool_call_id)]
        }
    )
python  theme={null}
import random
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

class MyNode:
    def __init__(self, name: str):
        self.name = name
    def __call__(self, state: State):
        return {"messages": [("assistant", f"Called node {self.name}")]}

def route(state) -> Literal["entry_node", END]:
    if len(state["messages"]) > 10:
        return END
    return "entry_node"

def add_fractal_nodes(builder, current_node, level, max_level):
    if level > max_level:
        return
    # Number of nodes to create at this level
    num_nodes = random.randint(1, 3)  # Adjust randomness as needed
    for i in range(num_nodes):
        nm = ["A", "B", "C"][i]
        node_name = f"node_{current_node}_{nm}"
        builder.add_node(node_name, MyNode(node_name))
        builder.add_edge(current_node, node_name)
        # Recursively add more nodes
        r = random.random()
        if r > 0.2 and level + 1 < max_level:
            add_fractal_nodes(builder, node_name, level + 1, max_level)
        elif r > 0.05:
            builder.add_conditional_edges(node_name, route, node_name)
        else:
            # End
            builder.add_edge(node_name, END)

def build_fractal_graph(max_level: int):
    builder = StateGraph(State)
    entry_point = "entry_node"
    builder.add_node(entry_point, MyNode(entry_point))
    builder.add_edge(START, entry_point)
    add_fractal_nodes(builder, entry_point, 1, max_level)
    # Optional: set a finish point if required
    builder.add_edge(entry_point, END)  # or any specific node
    return builder.compile()

app = build_fractal_graph(3)
python  theme={null}
print(app.get_graph().draw_mermaid())

%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
    tart__([<p>__start__</p>]):::first
    ry_node(entry_node)
    e_entry_node_A(node_entry_node_A)
    e_entry_node_B(node_entry_node_B)
    e_node_entry_node_B_A(node_node_entry_node_B_A)
    e_node_entry_node_B_B(node_node_entry_node_B_B)
    e_node_entry_node_B_C(node_node_entry_node_B_C)
    nd__([<p>__end__</p>]):::last
    tart__ --> entry_node;
    ry_node --> __end__;
    ry_node --> node_entry_node_A;
    ry_node --> node_entry_node_B;
    e_entry_node_B --> node_node_entry_node_B_A;
    e_entry_node_B --> node_node_entry_node_B_B;
    e_entry_node_B --> node_node_entry_node_B_C;
    e_entry_node_A -.-> entry_node;
    e_entry_node_A -.-> __end__;
    e_node_entry_node_B_A -.-> entry_node;
    e_node_entry_node_B_A -.-> __end__;
    e_node_entry_node_B_B -.-> entry_node;
    e_node_entry_node_B_B -.-> __end__;
    e_node_entry_node_B_C -.-> entry_node;
    e_node_entry_node_B_C -.-> __end__;
    ssDef default fill:#f2f0ff,line-height:1.2
    ssDef first fill-opacity:0
    ssDef last fill:#bfb6fc
python  theme={null}
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(app.get_graph().draw_mermaid_png()))
python  theme={null}
import nest_asyncio

nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions

display(
    Image(
        app.get_graph().draw_mermaid_png(
            curve_style=CurveStyle.LINEAR,
            node_colors=NodeStyles(first="#ffdfba", last="#baffc9", default="#fad7de"),
            wrap_label_n_words=9,
            output_file_path=None,
            draw_method=MermaidDrawMethod.PYPPETEER,
            background_color="white",
            padding=10,
        )
    )
)
python  theme={null}
try:
    display(Image(app.get_graph().draw_png()))
except ImportError:
    print(
        "You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt"
    )
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  You might have noticed that we used [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) as a return type annotation, e.g. `Command[Literal["node_b", "node_c"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.
</Warning>
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f11e5cddedbf2760d40533f294c44aea" alt="Command-based graph navigation" data-og-width="232" width="232" data-og-height="333" height="333" data-path="oss/images/graph_api_image_11.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c1b27d92b257a6c4ac57f34f007d0ee1 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=695d0062e5fb8ebea5525379edbba476 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bd3f779df628beba60a397674f85b59 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=85a9194e8b4d9df2d01d10784dcf75d0 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=efd9118d4bcd6d1eb92760c573645fbd 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1eb2a132386a64d18582af6978e4ac24 2500w" />

If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Navigate to a node in a parent graph

If you are using [subgraphs](/oss/python/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:
```

---

## Check if 'is_concise' returned False.

**URL:** llms-txt#check-if-'is_concise'-returned-false.

failed = [r for r in results if not r["evaluation_results"]["results"][0].score]

---

## Build Docker image

**URL:** llms-txt#build-docker-image

langgraph build -t my-agent:latest

---

## LangChain v1 migration guide

**URL:** llms-txt#langchain-v1-migration-guide

**Contents:**
- Simplified package
  - Namespace
  - `langchain-classic`
- Migrate to `create_agent`
  - Import path
  - Prompts
  - Pre-model hook
  - Post-model hook
  - Custom state
  - Model

Source: https://docs.langchain.com/oss/python/migrate/langchain-v1

This guide outlines the major changes between [LangChain v1](/oss/python/releases/langchain-v1) and previous versions.

## Simplified package

The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |

### `langchain-classic`

If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:

* Legacy chains (`LLMChain`, `ConversationChain`, etc.)
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)
* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports
* Other deprecated functionality

## Migrate to `create_agent`

Prior to v1.0, we recommended using [`langgraph.prebuilt.create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to build agents. Now, we recommend you use [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build agents.

The table below outlines what functionality has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):

| Section                                            | TL;DR - What's changed                                                                                                                                                                     |
| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Import path](#import-path)                        | Package moved from `langgraph.prebuilt` to `langchain.agents`                                                                                                                              |
| [Prompts](#prompts)                                | Parameter renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(system_prompt\)), dynamic prompts use middleware            |
| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `before_model` method                                                                                                                                          |
| [Post-model hook](#post-model-hook)                | Replaced by middleware with `after_model` method                                                                                                                                           |
| [Custom state](#custom-state)                      | `TypedDict` only, can be defined via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) or middleware |
| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                                                                                                                           |
| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrap_tool_call`                                                                                                                              |
| [Structured output](#structured-output)            | prompted output removed, use `ToolStrategy`/`ProviderStrategy`                                                                                                                             |
| [Streaming node name](#streaming-node-name-rename) | Node name changed from `"agent"` to `"model"`                                                                                                                                              |
| [Runtime context](#runtime-context)                | Dependency injection via `context` argument instead of `config["configurable"]`                                                                                                            |
| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic`                                                                                                    |

The import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.
The name of the function has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):

For more information, see [Agents](/oss/python/langchain/agents).

#### Static prompt rename

The `prompt` parameter has been renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(system_prompt\)):

#### `SystemMessage` to string

If using [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects in the system prompt, extract the string content:

Dynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use the [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) decorator:

Pre-model hooks are now implemented as middleware with the `before_model` method.
This new pattern is more extensible--you can define multiple middlewares to run before the model is called,
reusing common patterns across different agents.

Common use cases include:

* Summarizing conversation history
* Trimming messages
* Input guardrails, like PII redaction

v1 now has summarization middleware as a built in option:

Post-model hooks are now implemented as middleware with the `after_model` method.
This new pattern is more extensible--you can define multiple middlewares to run after the model is called,
reusing common patterns across different agents.

Common use cases include:

* [Human in the loop](/oss/python/langchain/human-in-the-loop)
* Output guardrails

v1 has a built in middleware for human in the loop approval for tool calls:

Custom state extends the default agent state with additional fields. You can define custom state in two ways:

1. **Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)** - Best for state used in tools
2. **Via middleware** - Best for state managed by specific middleware hooks and tools attached to said middleware

<Note>
  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.

`state_schema` is still supported for backwards compatibility on `create_agent`.
</Note>

#### Defining state via `state_schema`

Use the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) parameter when your custom state needs to be accessed by tools:

#### Defining state via middleware

Middleware can also define custom state by setting the [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) attribute.
This helps to keep state extensions conceptually scoped to the relevant middleware and tools.

See the [middleware documentation](/oss/python/langchain/middleware#custom-state-schema) for more details on defining custom state via middleware.

#### State type restrictions

[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported.

Simply inherit from `langchain.agents.AgentState` instead of `BaseModel` or decorating with `dataclass`.
If you need to perform validation, handle it in middleware hooks instead.

Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) released in v0.6 of [`langgraph-prebuilt`](https://pypi.org/project/langgraph-prebuilt) supported dynamic model and tool selection via a callable passed to the `model` parameter.

This functionality has been ported to the middleware interface in v1.

#### Dynamic model selection

#### Pre-bound models

To better support structured output, [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) no longer accepts pre-bound models with tools or configuration:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Install with:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

***

## Migrate to `create_agent`

Prior to v1.0, we recommended using [`langgraph.prebuilt.create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to build agents. Now, we recommend you use [`langchain.agents.create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to build agents.

The table below outlines what functionality has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):

| Section                                            | TL;DR - What's changed                                                                                                                                                                     |
| -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Import path](#import-path)                        | Package moved from `langgraph.prebuilt` to `langchain.agents`                                                                                                                              |
| [Prompts](#prompts)                                | Parameter renamed to [`system_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(system_prompt\)), dynamic prompts use middleware            |
| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `before_model` method                                                                                                                                          |
| [Post-model hook](#post-model-hook)                | Replaced by middleware with `after_model` method                                                                                                                                           |
| [Custom state](#custom-state)                      | `TypedDict` only, can be defined via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) or middleware |
| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                                                                                                                           |
| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrap_tool_call`                                                                                                                              |
| [Structured output](#structured-output)            | prompted output removed, use `ToolStrategy`/`ProviderStrategy`                                                                                                                             |
| [Streaming node name](#streaming-node-name-rename) | Node name changed from `"agent"` to `"model"`                                                                                                                                              |
| [Runtime context](#runtime-context)                | Dependency injection via `context` argument instead of `config["configurable"]`                                                                                                            |
| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic`                                                                                                    |

### Import path

The import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.
The name of the function has changed from [`create_react_agent`](https://reference.langchain.com/python/langgraph/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent):
```

---

## 5. Analyze results locally

**URL:** llms-txt#5.-analyze-results-locally

results = list(experiment)

---

## Image block

**URL:** llms-txt#image-block

**Contents:**
  - Serialize standard content
- Simplified package
  - Namespace
  - `langchain-classic`
- Breaking changes
  - Dropped Python 3.9 support
  - Updated return type for chat models
  - Default message format for OpenAI Responses API

image_block = {
    "type": "image",
    "url": "https://example.com/image.png",
    "mime_type": "image/png",
}
bash Environment variable theme={null}
  export LC_OUTPUT_VERSION=v1
  python Initialization parameter theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model(
      "openai:gpt-5-nano",
      output_version="v1",
  )
  python v1 (new) theme={null}
  # Chains
  from langchain_classic.chains import LLMChain

# Retrievers
  from langchain_classic.retrievers import ...

# Indexing
  from langchain_classic.indexes import ...

# Hub
  from langchain_classic import hub
  python v0 (old) theme={null}
  # Chains
  from langchain.chains import LLMChain

# Retrievers
  from langchain.retrievers import ...

# Indexing
  from langchain.indexes import ...

# Hub
  from langchain import hub
  bash  theme={null}
uv pip install langchain-classic
python v1 (new) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, AIMessage]:
  python v0 (old) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, BaseMessage]:
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
See the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details.

### Serialize standard content

Standard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note>
  Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).
</Note>

***

## Simplified package

The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |

### `langchain-classic`

If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:

* Legacy chains (`LLMChain`, `ConversationChain`, etc.)
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)
* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports
* Other deprecated functionality

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Enable blob storage

**URL:** llms-txt#enable-blob-storage

**Contents:**
- Requirements
- Authentication
  - Amazon S3
  - Google Cloud Storage
  - Azure Blob Storage
- CH Search
- Configuration
- TTL Configuration
  - Amazon S3
  - Google Cloud Storage

Source: https://docs.langchain.com/langsmith/self-host-blob-storage

By default, LangSmith stores run inputs, outputs, errors, manifests, extras, and events in ClickHouse. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits. For the best results in production deployments, we **strongly** recommend using blob storage, which offers the following benefits:

1. In high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases.
2. If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system.

<Note>
  Azure blob storage is available in Helm chart versions 0.8.9 and greater. [Deleting trace projects](/langsmith/observability-concepts#deleting-traces-from-langsmith) is supported in Azure starting in Helm chart version 0.10.43.
</Note>

* Access to a valid blob storage service

* [Amazon S3](https://aws.amazon.com/s3/)
    * [Google Cloud Storage (GCS)](https://cloud.google.com/storage?hl=en)
  * [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs)

* A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.
  * **If you are using TTLs**, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs [here](/langsmith/self-host-ttl). These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See [here](#ttl-configuration) on how to setup the lifecycle rules for TTLs for blob storage.

* Credentials to permit LangSmith Services to access the bucket/directory
  * You will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication [section](#authentication) below for more information.

* If using S3 or GCS, an API url for your blob storage service

* This will be the URL that LangSmith uses to access your blob storage system
  * For Amazon S3, this will be the URL of the S3 endpoint. Something like: `https://s3.amazonaws.com` or `https://s3.us-west-1.amazonaws.com` if using a regional endpoint.
  * For Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: `https://storage.googleapis.com`

To authenticate to [Amazon S3](https://aws.amazon.com/s3/), you will need to create an IAM policy granting the following permissions on your bucket.

Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:

### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:

### Google Cloud Storage

You will need to setup lifecycle conditions for your GCS buckets that you are using. You can find information for this [in the Google Documentation](https://cloud.google.com/storage/docs/lifecycle#conditions), specifically using matchesPrefix.

As an example, if you are using Terraform to manage your GCS bucket, you would setup something like this:

### Azure blob storage

You will need to configure a [lifecycle management policy](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure) on the container in order to expire objects matching the prefixes above.

As an example, if you are [using Terraform to manage your blob storage container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_management_policy), you would setup something like this:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-blob-storage.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

   1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

   1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:
```

Example 2 (unknown):
```unknown
### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

## CH Search

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

## Configuration

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

## TTL Configuration

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

### Amazon S3

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:
```

---

## Set up automation rules

**URL:** llms-txt#set-up-automation-rules

**Contents:**
- View automation rules
- Create a rule
- View logs for your automations
- Video guide

Source: https://docs.langchain.com/langsmith/rules

While you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called **Automations** that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a **filter**, **sampling rate**, and **action**.

Automation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:

* Send all traces with negative feedback to an annotation queue for human review
* Send 10% of all traces to an annotation queue for human review to spot check for issues
* Upgrade all traces with errors for extended data retention

<Info>
  To configure online evaluations, visit the [online evaluations](/langsmith/online-evaluations) page.
</Info>

<Note>If an automation rule matches any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your automation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View automation rules

Head to the **Tracing Projects** tab and select a tracing project. To view existing automation rules for that tracing project, click on the **Automations** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9206c95784e0d572adddf7ad60e58717" alt="View automation rules" data-og-width="1349" width="1349" data-og-height="521" height="521" data-path="langsmith/images/view-automation-rules.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88988b3e687419b0494b36e20d424965 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8091094d8e938a31367fd456cdb16778 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=08bce1184a23c4345c41abc2ede29826 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=faf56babdc5b9396a38e780600a31b4a 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8996953f130ed137172b26af817a5c22 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fea915cf43defcf09e6b7161a2c2af8a 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aq-spot-check-rule.gif?s=0ea58303ab04ffa8ed16c49ab39701ef" alt="" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/aq-spot-check-rule.gif" data-optimize="true" data-opv="3" />

#### 1. Navigate to rule creation

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Automation**.

#### 2. Name your rule

#### 3. Create a filter

Automation rule filters work the same way as filters applied to traces in the project. For more information on filters, you can refer to [this guide](./filter-traces-in-application)

#### 4. Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action.

You can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can [view logs for your automations](./rules#view-logs-for-your-automations)

#### 6. Select an action to trigger when the rule is applied.

There are four actions you can take with an automation rule:

* **Add to dataset**: Add the inputs and outputs of the trace to a [dataset](/langsmith/evaluation-concepts#datasets).
* **Add to annotation queue**: Add the trace to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).
* **Trigger webhook**: Trigger a webhook with the trace data. For more information on webhooks, you can refer to [this guide](./webhooks).
* **Extend data retention**: Extends the data retention period on matching traces that use base retention [(see data retention docs for more details)](/langsmith/administration-overview#data-retention).
  Note that all other rules will also extend data retention on matching traces through the
  auto-upgrade mechanism described in the aforementioned data retention docs,
  but this rule takes no additional action.

## View logs for your automations

Logs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the **Automations** tab within a tracing project and clicking the **Logs** button for the rule you created.

The logs tab allows you to:

* View all runs processed by a given rule for the time period selected
* If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon
* You can monitor the progress of a backfill job by filtering to the rule's creation timestamp. This is because the backfill starts from when the rule was created.
* Inspect the run that the automation rule applied to using the **View run** button. For rules that add runs as examples to datasets, you can view the example produced.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rule-logs.gif?s=2ec5562fea9e2079b687da149d1609b3" alt="Logs_Gif" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/rule-logs.gif" data-optimize="true" data-opv="3" />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rules.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Graph API

**URL:** llms-txt#graph-api

graph = StateGraph(...).compile(checkpointer=checkpointer)

---

## Define the structure for email classification

**URL:** llms-txt#define-the-structure-for-email-classification

**Contents:**
- Step 4: Build your nodes
  - Handle errors appropriately
  - Implementing our email agent nodes
- Step 5: Wire it together
  - Try out your agent
- Summary and next steps
  - Key Insights
  - Advanced considerations
  - Where to go from here

class EmailClassification(TypedDict):
    intent: Literal["question", "bug", "billing", "feature", "complex"]
    urgency: Literal["low", "medium", "high", "critical"]
    topic: str
    summary: str

class EmailAgentState(TypedDict):
    # Raw email data
    email_content: str
    sender_email: str
    email_id: str

# Classification result
    classification: EmailClassification | None

# Raw search/API results
    search_results: list[str] | None  # List of raw document chunks
    customer_history: dict | None  # Raw customer data from CRM

# Generated content
    draft_response: str | None
    messages: list[str] | None
python  theme={null}
    from langgraph.types import RetryPolicy

workflow.add_node(
        "search_documentation",
        search_documentation,
        retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)
    )
    python  theme={null}
    from langgraph.types import Command

def execute_tool(state: State) -> Command[Literal["agent", "execute_tool"]]:
        try:
            result = run_tool(state['tool_call'])
            return Command(update={"tool_result": result}, goto="agent")
        except ToolError as e:
            # Let the LLM see what went wrong and try again
            return Command(
                update={"tool_result": f"Tool error: {str(e)}"},
                goto="agent"
            )
    python  theme={null}
    from langgraph.types import Command

def lookup_customer_history(state: State) -> Command[Literal["draft_response"]]:
        if not state.get('customer_id'):
            user_input = interrupt({
                "message": "Customer ID needed",
                "request": "Please provide the customer's account ID to look up their subscription history"
            })
            return Command(
                update={"customer_id": user_input['customer_id']},
                goto="lookup_customer_history"
            )
        # Now proceed with the lookup
        customer_data = fetch_customer_history(state['customer_id'])
        return Command(update={"customer_history": customer_data}, goto="draft_response")
    python  theme={null}
    def send_reply(state: EmailAgentState):
        try:
            email_service.send(state["draft_response"])
        except Exception:
            raise  # Surface unexpected errors
    python  theme={null}
    from typing import Literal
    from langgraph.graph import StateGraph, START, END
    from langgraph.types import interrupt, Command, RetryPolicy
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4")

def read_email(state: EmailAgentState) -> dict:
        """Extract and parse email content"""
        # In production, this would connect to your email service
        return {
            "messages": [HumanMessage(content=f"Processing email: {state['email_content']}")]
        }

def classify_intent(state: EmailAgentState) -> Command[Literal["search_documentation", "human_review", "draft_response", "bug_tracking"]]:
        """Use LLM to classify email intent and urgency, then route accordingly"""

# Create structured LLM that returns EmailClassification dict
        structured_llm = llm.with_structured_output(EmailClassification)

# Format the prompt on-demand, not stored in state
        classification_prompt = f"""
        Analyze this customer email and classify it:

Email: {state['email_content']}
        From: {state['sender_email']}

Provide classification including intent, urgency, topic, and summary.
        """

# Get structured response directly as dict
        classification = structured_llm.invoke(classification_prompt)

# Determine next node based on classification
        if classification['intent'] == 'billing' or classification['urgency'] == 'critical':
            goto = "human_review"
        elif classification['intent'] in ['question', 'feature']:
            goto = "search_documentation"
        elif classification['intent'] == 'bug':
            goto = "bug_tracking"
        else:
            goto = "draft_response"

# Store classification as a single dict in state
        return Command(
            update={"classification": classification},
            goto=goto
        )
    python  theme={null}
    def search_documentation(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Search knowledge base for relevant information"""

# Build search query from classification
        classification = state.get('classification', {})
        query = f"{classification.get('intent', '')} {classification.get('topic', '')}"

try:
            # Implement your search logic here
            # Store raw search results, not formatted text
            search_results = [
                "Reset password via Settings > Security > Change Password",
                "Password must be at least 12 characters",
                "Include uppercase, lowercase, numbers, and symbols"
            ]
        except SearchAPIError as e:
            # For recoverable search errors, store error and continue
            search_results = [f"Search temporarily unavailable: {str(e)}"]

return Command(
            update={"search_results": search_results},  # Store raw results or error
            goto="draft_response"
        )

def bug_tracking(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Create or update bug tracking ticket"""

# Create ticket in your bug tracking system
        ticket_id = "BUG-12345"  # Would be created via API

return Command(
            update={
                "search_results": [f"Bug ticket {ticket_id} created"],
                "current_step": "bug_tracked"
            },
            goto="draft_response"
        )
    python  theme={null}
    def draft_response(state: EmailAgentState) -> Command[Literal["human_review", "send_reply"]]:
        """Generate response using context and route based on quality"""

classification = state.get('classification', {})

# Format context from raw state data on-demand
        context_sections = []

if state.get('search_results'):
            # Format search results for the prompt
            formatted_docs = "\n".join([f"- {doc}" for doc in state['search_results']])
            context_sections.append(f"Relevant documentation:\n{formatted_docs}")

if state.get('customer_history'):
            # Format customer data for the prompt
            context_sections.append(f"Customer tier: {state['customer_history'].get('tier', 'standard')}")

# Build the prompt with formatted context
        draft_prompt = f"""
        Draft a response to this customer email:
        {state['email_content']}

Email intent: {classification.get('intent', 'unknown')}
        Urgency level: {classification.get('urgency', 'medium')}

{chr(10).join(context_sections)}

Guidelines:
        - Be professional and helpful
        - Address their specific concern
        - Use the provided documentation when relevant
        """

response = llm.invoke(draft_prompt)

# Determine if human review needed based on urgency and intent
        needs_review = (
            classification.get('urgency') in ['high', 'critical'] or
            classification.get('intent') == 'complex'
        )

# Route to appropriate next node
        goto = "human_review" if needs_review else "send_reply"

return Command(
            update={"draft_response": response.content},  # Store only the raw response
            goto=goto
        )

def human_review(state: EmailAgentState) -> Command[Literal["send_reply", END]]:
        """Pause for human review using interrupt and route based on decision"""

classification = state.get('classification', {})
        
        # interrupt() must come first - any code before it will re-run on resume
        human_decision = interrupt({
            "email_id": state.get('email_id',''),
            "original_email": state.get('email_content',''),
            "draft_response": state.get('draft_response',''),
            "urgency": classification.get('urgency'),
            "intent": classification.get('intent'),
            "action": "Please review and approve/edit this response"
        })

# Now process the human's decision
        if human_decision.get("approved"):
            return Command(
                update={"draft_response": human_decision.get("edited_response", state.get('draft_response',''))},
                goto="send_reply"
            )
        else:
            # Rejection means human will handle directly
            return Command(update={}, goto=END)

def send_reply(state: EmailAgentState) -> dict:
        """Send the email response"""
        # Integrate with email service
        print(f"Sending reply: {state['draft_response'][:100]}...")
        return {}
    python  theme={null}
  from langgraph.checkpoint.memory import MemorySaver
  from langgraph.types import RetryPolicy

# Create the graph
  workflow = StateGraph(EmailAgentState)

# Add nodes with appropriate error handling
  workflow.add_node("read_email", read_email)
  workflow.add_node("classify_intent", classify_intent)

# Add retry policy for nodes that might have transient failures
  workflow.add_node(
      "search_documentation",
      search_documentation,
      retry_policy=RetryPolicy(max_attempts=3)
  )
  workflow.add_node("bug_tracking", bug_tracking)
  workflow.add_node("draft_response", draft_response)
  workflow.add_node("human_review", human_review)
  workflow.add_node("send_reply", send_reply)

# Add only the essential edges
  workflow.add_edge(START, "read_email")
  workflow.add_edge("read_email", "classify_intent")
  workflow.add_edge("send_reply", END)

# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer
  memory = MemorySaver()
  app = workflow.compile(checkpointer=memory)
  python  theme={null}
  # Test with an urgent billing issue
  initial_state = {
      "email_content": "I was charged twice for my subscription! This is urgent!",
      "sender_email": "customer@example.com",
      "email_id": "email_123",
      "messages": []
  }

# Run with a thread_id for persistence
  config = {"configurable": {"thread_id": "customer_123"}}
  result = app.invoke(initial_state, config)
  # The graph will pause at human_review
  print(f"Draft ready for review: {result['draft_response'][:100]}...")

# When ready, provide human input to resume
  from langgraph.types import Command

human_response = Command(
      resume={
          "approved": True,
          "edited_response": "We sincerely apologize for the double charge. I've initiated an immediate refund..."
      }
  )

# Resume execution
  final_result = app.invoke(human_response, config)
  print(f"Email sent successfully!")
  ```
</Accordion>

The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.

## Summary and next steps

Building this email agent has shown us the LangGraph way of thinking:

<CardGroup cols={2}>
  <Card title="Break into discrete steps" icon="sitemap" href="#step-1-map-out-your-workflow-as-discrete-steps">
    Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.
  </Card>

<Card title="State is shared memory" icon="database" href="#step-3-design-your-state">
    Store raw data, not formatted text. This lets different nodes use the same information in different ways.
  </Card>

<Card title="Nodes are functions" icon="code" href="#step-4-build-your-nodes">
    They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.
  </Card>

<Card title="Errors are part of the flow" icon="triangle-exclamation" href="#handle-errors-appropriately">
    Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.
  </Card>

<Card title="Human input is first-class" icon="user" href="/oss/python/langgraph/interrupts">
    The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.
  </Card>

<Card title="Graph structure emerges naturally" icon="diagram-project" href="#step-5-wire-it-together">
    You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.
  </Card>
</CardGroup>

### Advanced considerations

<Accordion title="Node granularity trade-offs" icon="sliders">
  <Info>
    This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.
  </Info>

You might wonder: why not combine `Read Email` and `Classify Intent` into one node?

Or why separate Doc Search from Draft Reply?

The answer involves trade-offs between resilience and observability.

**The resilience consideration:** LangGraph's [durable execution](/oss/python/langgraph/durable-execution) creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.

Why we chose this breakdown for the email agent:

* **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.

* **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.

* **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.

* **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.

A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You'd lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.

Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn't prescribe this.

Performance considerations: More nodes doesn't mean slower execution. LangGraph writes checkpoints in the background by default ([async durability mode](/oss/python/langgraph/durable-execution#durability-modes)), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `"exit"` mode to checkpoint only at completion, or `"sync"` mode to block execution until each checkpoint is written.
</Accordion>

### Where to go from here

This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:

<CardGroup cols={2}>
  <Card title="Human-in-the-loop patterns" icon="user-check" href="/oss/python/langgraph/interrupts">
    Learn how to add tool approval before execution, batch approval, and other patterns
  </Card>

<Card title="Subgraphs" icon="diagram-nested" href="/oss/python/langgraph/use-subgraphs">
    Create subgraphs for complex multi-step operations
  </Card>

<Card title="Streaming" icon="tower-broadcast" href="/oss/python/langgraph/streaming">
    Add streaming to show real-time progress to users
  </Card>

<Card title="Observability" icon="chart-line" href="/oss/python/langgraph/observability">
    Add observability with LangSmith for debugging and monitoring
  </Card>

<Card title="Tool Integration" icon="wrench" href="/oss/python/langchain/tools">
    Integrate more tools for web search, database queries, and API calls
  </Card>

<Card title="Retry Logic" icon="rotate" href="/oss/python/langgraph/use-graph-api#add-retry-policies">
    Implement retry logic with exponential backoff for failed operations
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.

### Handle errors appropriately

Different errors need different handling strategies:

| Error Type                                                      | Who Fixes It       | Strategy                           | When to Use                                      |
| --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ |
| Transient errors (network issues, rate limits)                  | System (automatic) | Retry policy                       | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues)          | LLM                | Store error in state and loop back | LLM can see the error and adjust its approach    |
| User-fixable errors (missing information, unclear instructions) | Human              | Pause with `interrupt()`           | Need user input to proceed                       |
| Unexpected errors                                               | Developer          | Let them bubble up                 | Unknown issues that need debugging               |

<Tabs>
  <Tab title="Transient errors" icon="rotate">
    Add a retry policy to automatically retry network issues and rate limits:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="LLM-recoverable" icon="brain">
    Store the error in state and loop back so the LLM can see what went wrong and try again:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="User-fixable" icon="user">
    Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Unexpected" icon="triangle-exclamation">
    Let them bubble up for debugging. Don't catch what you can't handle:
```

---

## Dynamic prompts

**URL:** llms-txt#dynamic-prompts

@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context.user_name  # [!code highlight]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt

---

## We strongly recommend setting up a replicated clickhouse cluster for this load.

**URL:** llms-txt#we-strongly-recommend-setting-up-a-replicated-clickhouse-cluster-for-this-load.

---

## Persistent file (survives across threads)

**URL:** llms-txt#persistent-file-(survives-across-threads)

**Contents:**
- Cross-thread persistence

agent.invoke({
    "messages": [{"role": "user", "content": "Save final report to /memories/report.txt"}]
})
python  theme={null}
import uuid

**Examples:**

Example 1 (unknown):
```unknown
## Cross-thread persistence

Files in `/memories/` can be accessed from any thread:
```

---

## If prod tag points to commit a1b2c3d4, this is equivalent to:

**URL:** llms-txt#if-prod-tag-points-to-commit-a1b2c3d4,-this-is-equivalent-to:

**Contents:**
- Trigger a webhook on prompt commit
  - Configure a webhook
  - Trigger the webhook
- Public prompt hub

prompt = client.pull_prompt("joke-generator:a1b2c3d4")
```

For more information on how to use prompts in code, refer to [Managing prompts programmatically](/langsmith/manage-prompts-programmatically).

## Trigger a webhook on prompt commit

You can configure a webhook to be triggered whenever a commit is made to a prompt.

Some common use cases of this include:

* Triggering a CI/CD pipeline when prompts are updated.
* Synchronizing prompts with a GitHub repository.
* Notifying team members about prompt modifications.

### Configure a webhook

Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage. In the top right corner, click on the `+ Webhook` button.

Add a webhook URL and any required headers.

<Note>
  You can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the [LangChain Forum](https://forum.langchain.com/).
</Note>

To test out your webhook, click the **Send test notification** button. This will send a test notification to the webhook URL you provided with a sample payload.

The sample payload is a JSON object with the following fields:

* `prompt_id`: The ID of the prompt that was committed.
* `prompt_name`: The name of the prompt that was committed.
* `commit_hash`: The commit hash of the prompt.
* `created_at`: The date of the commit.
* `created_by`: The author of the commit.
* `manifest`: The manifest of the prompt.

### Trigger the webhook

Commit to a prompt to trigger the webhook you've configured.

#### Use the Playground

If you do this in the Playground, you'll be prompted to deselect the webhooks you'd like to avoid triggering.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84f487c929ab7894bbd1e2c8922b6a9e" alt="" data-og-width="736" width="736" data-og-height="540" height="540" data-path="langsmith/images/commit-prompt-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=68e38eb869ab7f97b5e53bb793a0ba24 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84bbdbb22d53ca87b9e49b164d4dd190 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f679941d9c42a52f30c858a00174749d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c8ba79844ab070cd1a7b443677bbef4a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d011a2ebe87104d006e71470204d158d 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cdfcfddfdad692fd623b502efdd6589d 2500w" />

If you commit via the API, you can specify to skip triggering the webhook by setting the `skip_webhooks` parameter to `true` or to an array of webhook ids to ignore. Refer to the [API docs](https://api.smith.langchain.com/redoc#tag/commits/operation/create_commit_api_v1_commits__owner___repo__post) for more information.

LangSmith's public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.

<Note>
  Note that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our [Terms of Service](https://www.langchain.com/terms-of-service).
</Note>

Navigate to the **Prompts** section of the left-hand sidebar and click on **Browse all Public Prompts in the LangChain Hub**.

Here you'll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the Playground. You can [pull any public prompt into your code](/langsmith/manage-prompts-programmatically) using the SDK.

To view prompts tied to your workspace, visit the **Prompts** tab in the sidebar.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d689013c2158309249c547086e145783" alt="" data-og-width="3012" width="3012" data-og-height="1704" height="1704" data-path="langsmith/images/prompts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ad811ae37a565d22851d6162d2f45e07 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d48f24e8fcc7d5813e7093f117c21cac 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b93e12cdf73897ab5373eb5019ac58cf 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cb361802a4bb5d823dda53605b647567 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8450e66dcc99a90ea84b1fc66bdfc6ad 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5de8982f6602902e9fa239ba3f4157d4 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Contributing to code

**URL:** llms-txt#contributing-to-code

**Contents:**
- Philosophy
- Getting started
  - Quick fix: submit a bugfix
  - Full development setup
  - Development environment
- Repository structure
- Development workflow
  - Testing requirements
  - Code quality standards
  - Manual formatting and linting

Source: https://docs.langchain.com/oss/python/contributing/code

Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.

<Note>
  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.

This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. Be sure to link any relevant issues in your PR description. Use <Tooltip tip="(e.g. `Fixes #123`)">closing keywords</Tooltip> to automatically close issues when the PR is merged.

New integrations should follow the [integration guidelines](/oss/python/contributing#add-a-new-integration).
</Note>

Aim to follow these core principles for all code contributions:

<CardGroup cols={2}>
  <Card title="Backwards compatibility" icon="shield" href="#backwards-compatibility" arrow>
    Maintain stable public interfaces and avoid breaking changes
  </Card>

<Card title="Testing first" icon="flask" href="#testing-requirements" arrow>
    Every change must include comprehensive tests to verify correctness and prevent regressions
  </Card>

<Card title="Code quality" icon="star" href="#code-quality-standards" arrow>
    Follow consistent style, documentation, and architecture patterns
  </Card>

<Card title="Security focused" icon="lock" href="#security-guidelines" arrow>
    Prioritize secure coding practices and vulnerability prevention
  </Card>
</CardGroup>

### Quick fix: submit a bugfix

For simple bugfixes, you can get started immediately:

<Steps>
  <Step title="Fork the repository">
    Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your <Tooltip tip="If you fork to an organization account, maintainers will be unable to make edits.">personal GitHub account</Tooltip>
  </Step>

<Step title="Clone and setup">

You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously.
  </Step>

<Step title="Create a branch">
    
  </Step>

<Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards)
  </Step>

<Step title="Add tests">
    Include [unit tests](#test-writing-guidelines) that fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

<Step title="Run tests">
    Ensure all tests pass locally before submitting your PR

<Step title="Submit a pull request">
    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #123`).
  </Step>
</Steps>

### Full development setup

For ongoing development or larger contributions:

<Steps>
  <Step title="Development environment">
    Set up your environment following our [setup guide](#development-environment) below
  </Step>

<Step title="Repository structure">
    Understand the [repository structure](#repository-structure) and package organization
  </Step>

<Step title="Development workflow">
    Learn our [development workflow](#development-workflow) including testing and linting
  </Step>

<Step title="Contribution guidelines">
    Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations
  </Step>
</Steps>

### Development environment

<Warning>
  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.
</Warning>

Set up a development environment for the package(s) you're working on.

<Tabs>
  <Tab title="LangChain" icon="link">
    <AccordionGroup>
      <Accordion title="Core abstractions">
        For changes to `langchain-core`:

<Accordion title="Main package">
        For changes to `langchain`:

<Accordion title="Partner packages">
        For changes to [partner integrations](/oss/python/integrations/providers/overview):

<Accordion title="Community packages">
        For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)):

</Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Repository structure

<Tabs>
  <Tab title="LangChain" icon="link">
    LangChain is organized as a monorepo with multiple packages:

<AccordionGroup>
      <Accordion title="Core packages" defaultOpen>
        * **[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic
        * **[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)** (located in `libs/core/`): Base interfaces and core abstractions
      </Accordion>

<Accordion title="Partner packages">
        Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:

* **[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)**: [OpenAI](/oss/python/integrations/providers/openai) integrations
        * **[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)**: [Anthropic](/oss/python/integrations/providers/anthropic) integrations
        * **[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)**: [Google Generative AI](/oss/python/integrations/chat/google_generative_ai) integrations

Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.
      </Accordion>

<Accordion title="Supporting packages">
        * **[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)**: Text splitting utilities
        * **[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)**: Standard test suites for integrations
        * **[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)**: Command line interface
        * **[`langchain-community`](https://github.com/langchain-ai/langchain-community)**: Community maintained integrations (located in a separate repo)
      </Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Development workflow

### Testing requirements

<Info>
  Directories are relative to the package you're working in.
</Info>

Every code change must include comprehensive tests.

<Steps>
  <Step title="Unit tests">
    **Location**: `tests/unit_tests/`

* No network calls allowed
    * Test all code paths including edge cases
    * Use mocks for external dependencies

<Step title="Integration tests">
    Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.

Not every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.

**Location**: `tests/integration_tests/`

* Test real integrations with external services
    * Use environment variables for API keys
    * Skip gracefully if credentials unavailable

<Step title="Test quality checklist">
    * Tests fail when your code is broken
    * Edge cases and error conditions are tested
    * Proper use of fixtures and mocks
  </Step>
</Steps>

### Code quality standards

Quality requirements:

<Tabs>
  <Tab title="Type hints">
    **Required**: Complete type annotations for all functions

<Tab title="Documentation">
    **Required**: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions

* Document all parameters and return values
    * Include usage examples for complex functions
    * Document raised exceptions
    * Focus on "why" rather than "what"
  </Tab>

<Tab title="Code style">
    **Automated**: Formatting and linting via [`ruff`](https://docs.astral.sh/ruff/)

* Descriptive variable names
    * Break up complex functions (aim for fewer than 20 lines)
    * Follow existing patterns in the codebase
  </Tab>
</Tabs>

### Manual formatting and linting

<Note>
  Code formatting and linting are enforced via CI/CD. Run these commands before committing to ensure your code passes checks.
</Note>

Run formatting and linting:

<Steps>
  <Step title="Format code">
    
  </Step>

<Step title="Run linting checks">
    
  </Step>

<Step title="Verify changes">
    Both commands will show you any formatting or linting issues that need to be addressed before committing.
  </Step>
</Steps>

## Contribution guidelines

### Backwards compatibility

<Warning>
  Breaking changes to public APIs are not allowed except for critical security fixes.

See our [versioning policy](/oss/python/versioning) for details on major version releases.
</Warning>

Maintain compatibility:

<AccordionGroup>
  <Accordion title="Stable interfaces">
    **Always preserve**:

* Function signatures and parameter names
    * Class interfaces and method names
    * Return value structure and types
    * Import paths for public APIs
  </Accordion>

<Accordion title="Safe changes">
    **Acceptable modifications**:

* Adding new optional parameters

* Adding new methods to classes

* Improving performance without changing behavior

* Adding new modules or functions
  </Accordion>

<Accordion title="Before making changes">
    * **Would this break existing user code?**

* Check if your target is public

* If needed, is it exported in `__init__.py`?

* Are there existing usage patterns in tests?
  </Accordion>
</AccordionGroup>

For bugfix contributions:

<Steps>
  <Step title="Reproduce the issue">
    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification
  </Step>

<Step title="Write failing tests">
    Add unit tests that would fail without your fix
  </Step>

<Step title="Implement the fix">
    Make the **minimal change necessary** to resolve the issue
  </Step>

<Step title="Verify the fix">
    Ensure that tests pass and no regressions are introduced
  </Step>

<Step title="Document the change">
    Update docstrings if behavior changes, add comments for complex logic
  </Step>
</Steps>

We aim to keep the bar high for new features. We generally don't accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them.

In general, feature contribution requirements include:

<Steps>
  <Step title="Design discussion">
    Open an issue describing:

* The problem you're solving
    * Proposed API design
    * Expected usage patterns
  </Step>

<Step title="Implementation">
    * Follow existing code patterns
    * Include comprehensive tests and documentation
    * Consider security implications
  </Step>

<Step title="Integration considerations">
    * How does this interact with existing features?
    * Are there performance implications?
    * Does this introduce new dependencies?

We will reject features that are likely to lead to security vulnerabilities or reports.
  </Step>
</Steps>

### Security guidelines

<Warning>
  Security is paramount. Never introduce vulnerabilities or unsafe patterns.
</Warning>

<AccordionGroup>
  <Accordion title="Input validation">
    * Validate and sanitize all user inputs
    * Properly escape data in templates and queries
    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities
  </Accordion>

<Accordion title="Error handling">
    * Use specific exception types
    * Don't expose sensitive information in error messages
    * Implement proper resource cleanup
  </Accordion>

<Accordion title="Dependencies">
    * Avoid adding hard dependencies
    * Keep optional dependencies minimal
    * Review third-party packages for security issues
  </Accordion>
</AccordionGroup>

## Testing and validation

### Running tests locally

Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.

<Tabs>
  <Tab title="LangChain" icon="link">
    <Steps>
      <Step title="Unit tests">

All unit tests must pass
      </Step>

<Step title="Integration tests">

(Run if your changes affect integrations)
      </Step>

<Step title="Formatting">

Code must pass all style checks
      </Step>

<Step title="Type checking">

All type hints must be valid
      </Step>

<Step title="PR submission">
        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.
      </Step>
    </Steps>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

### Test writing guidelines

In order to write effective tests, there's a few good practices to follow:

* Use natural language to describe the test in docstrings
* Use descriptive variable names
* Be exhaustive with assertions

<Tabs>
  <Tab title="Unit tests">
    
  </Tab>

<Tab title="Integration tests">
    
  </Tab>

<Tab title="Mock usage">
    
  </Tab>
</Tabs>

Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You're now ready to contribute high-quality code to LangChain!
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously.
  </Step>

  <Step title="Create a branch">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards)
  </Step>

  <Step title="Add tests">
    Include [unit tests](#test-writing-guidelines) that fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

  <Step title="Run tests">
    Ensure all tests pass locally before submitting your PR
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Submit a pull request">
    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #123`).
  </Step>
</Steps>

### Full development setup

For ongoing development or larger contributions:

<Steps>
  <Step title="Development environment">
    Set up your environment following our [setup guide](#development-environment) below
  </Step>

  <Step title="Repository structure">
    Understand the [repository structure](#repository-structure) and package organization
  </Step>

  <Step title="Development workflow">
    Learn our [development workflow](#development-workflow) including testing and linting
  </Step>

  <Step title="Contribution guidelines">
    Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations
  </Step>
</Steps>

### Development environment

<Warning>
  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.
</Warning>

Set up a development environment for the package(s) you're working on.

<Tabs>
  <Tab title="LangChain" icon="link">
    <AccordionGroup>
      <Accordion title="Core abstractions">
        For changes to `langchain-core`:
```

---

## minReplicas: 2

**URL:** llms-txt#minreplicas:-2

platformBackend:
  deployment:
    replicas: 20 # OR enable autoscaling to this level (example below)

---

## Create a child run, linked to the parent

**URL:** llms-txt#create-a-child-run,-linked-to-the-parent

child_run = construct_run(
    name="Child Run",
    run_type="llm",
    inputs={"question": "What is the capital of France?"},
    parent_dotted_order=parent_run["dotted_order"],
)

---

## Generic / global handler catches calls that aren't handled by more specific handlers

**URL:** llms-txt#generic-/-global-handler-catches-calls-that-aren't-handled-by-more-specific-handlers

@auth.on
async def reject_unhandled_requests(ctx: Auth.types.AuthContext, value: Any) -> False:
    print(f"Request to {ctx.path} by {ctx.user.identity}")
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="Forbidden"
    )

---

## You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs

**URL:** llms-txt#you-can-iterate-over-the-runs-in-the-experiments-belonging-to-the-comparative-experiment-and-preferentially-rank-the-outputs

---

## Use threads

**URL:** llms-txt#use-threads

**Contents:**
- Create a thread
  - Empty thread
  - Copy thread
  - Prepopulated State
- List threads
  - LangGraph SDK
  - LangSmith UI
- Inspect threads
  - LangGraph SDK
  - LangSmith UI

Source: https://docs.langchain.com/langsmith/use-threads

In this guide, we will show how to create, view, and inspect [threads](/oss/python/langgraph/persistence#threads).

To run your graph and the state persisted, you must first create a thread.

To create a new thread, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create_3) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Alternatively, if you already have a thread in your application whose state you wish to copy, you can use the `copy` method. This will create an independent thread whose history is identical to the original thread at the time of the operation. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.copy) and [JS](/langsmith/langgraph-js-ts-sdk#copy) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

### Prepopulated State

Finally, you can create a thread with an arbitrary pre-defined state by providing a list of `supersteps` into the `create` method. The `supersteps` describe a list of a sequence of state updates. For example:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

To list threads, use the [LangGraph SDK](/langsmith/sdk) `search` method. This will list the threads in the application that match the provided filters. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.search) and [JS](/langsmith/langgraph-js-ts-sdk#search_2) SDK reference docs for more information.

#### Filter by thread status

Use the `status` field to filter threads based on their status. Supported values are `idle`, `busy`, `interrupted`, and `error`. See [here](/langsmith/langgraph-python-sdk?h=thread+status#langgraph_sdk.auth.types.ThreadStatus) for information on each status. For example, to view `idle` threads:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Filter by metadata

The `search` method allows you to filter on metadata:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

The SDK also supports sorting threads by `thread_id`, `status`, `created_at`, and `updated_at` using the `sort_by` and `sort_order` params.

You can also view threads in a deployment via the LangSmith UI.

Inside your deployment, select the "Threads" tab. This will load a table of all of the threads in your deployment.

To filter by thread status, select a status in the top bar. To sort by a supported property, click on the arrow icon for the desired column.

To view a specific thread given its `thread_id`, use the `get` method:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Inspect Thread State

To view the current state of a given thread, use the `get_state` method:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Optionally, to view the state of a thread at a given checkpoint, simply pass in the checkpoint id (or the entire checkpoint object):

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Inspect Full Thread History

To view a thread's history, use the `get_history` method. This returns a list of every state the thread experienced. For more information see the [Python](/langsmith/langgraph-python-sdk?h=thread+status#langgraph_sdk.client.ThreadsClient.get_history) and [JS](/langsmith/langgraph-js-ts-sdk#gethistory) reference docs.

You can also view threads in a deployment via the LangSmith UI.

Inside your deployment, select the "Threads" tab. This will load a table of all of the threads in your deployment.

Select a thread to inspect its current state. To view its full history and for further debugging, open the thread in [Studio](/langsmith/studio).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-threads.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
### Copy thread

Alternatively, if you already have a thread in your application whose state you wish to copy, you can use the `copy` method. This will create an independent thread whose history is identical to the original thread at the time of the operation. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.copy) and [JS](/langsmith/langgraph-js-ts-sdk#copy) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
```

---

## Rollback Concurrent

**URL:** llms-txt#rollback-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/rollback-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `rollback` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option is very similar to the `interrupt` option, but in this case the first run is completely deleted from the database and cannot be restarted. Below is a quick example of using the `rollback` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's run a thread with the multitask parameter set to "rollback":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has data only from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, rolled back run was deleted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rollback-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Trace without setting environment variables

**URL:** llms-txt#trace-without-setting-environment-variables

Source: https://docs.langchain.com/langsmith/trace-without-env-vars

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

* `LANGSMITH_TRACING`
* `LANGSMITH_API_KEY`
* `LANGSMITH_ENDPOINT`
* `LANGSMITH_PROJECT`

If you need to trace runs with a custom configuration, are working in an environment that doesn’t support typical environment variables (such as Cloudflare Workers), or would simply prefer not to rely on environment variables, LangSmith allows you to configure tracing programmatically.

<Warning>
  Due to a number of asks for finer-grained control of tracing using the `trace` context manager, **we changed the behavior** of `with trace` to honor the `LANGSMITH_TRACING` environment variable in version **0.1.95** of the Python SDK. You can find more details in the [release notes](https://github.com/langchain-ai/langsmith-sdk/releases/tag/v0.1.95). The recommended way to disable/enable tracing without setting environment variables is to use the `with tracing_context` context manager, as shown in the example below.
</Warning>

* Python: The recommended way to do this in Python is to use the `tracing_context` context manager. This works for both code annotated with `traceable` and code within the `trace` context manager.
* TypeScript: You can pass in both the client and the `tracingEnabled` flag to the `traceable` decorator.

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-without-env-vars.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## This WILL be traced

**URL:** llms-txt#this-will-be-traced

with ls.tracing_context(enabled=True):
    agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production.

store = InMemoryStore() # [!code highlight]

@dataclass
class Context:
    user_id: str

---

## Let's say hi again

**URL:** llms-txt#let's-say-hi-again

**Contents:**
- Checkpointer libraries
  - Checkpointer interface
  - Serializer

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates"
):
    print(update)
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "openai:text-embeddings-3-small",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

**Examples:**

Example 1 (unknown):
```unknown
When we use the LangSmith, either locally (e.g., in [Studio](/langsmith/studio)) or [hosted with LangSmith](/langsmith/hosting), the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:
```

Example 2 (unknown):
```unknown
See the [deployment guide](/langsmith/semantic-search) for more details and configuration options.

## Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

* `langgraph-checkpoint`: The base interface for checkpointer savers ([`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)) and serialization/deserialization interface ([`SerializerProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included.
* `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([`SqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately.
* `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([`PostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### Checkpointer interface

Each checkpointer conforms to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface and implements the following methods:

* `.put` - Store a checkpoint with its configuration and metadata.
* `.put_writes` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)).
* `.get_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.
* `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`

If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`).

<Note>
  For running your graph asynchronously, you can use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver), or async versions of Sqlite/Postgres checkpointers -- [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver) checkpointers.
</Note>

### Serializer

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.

`langgraph_checkpoint` defines [protocol](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol) for implementing serializers provides a default implementation ([`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

#### Serialization with `pickle`

The default serializer, [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer), uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.

If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),
you can use the `pickle_fallback` argument of the [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer):
```

---

## How to run an evaluation asynchronously

**URL:** llms-txt#how-to-run-an-evaluation-asynchronously

**Contents:**
- Use `aevaluate()`

Source: https://docs.langchain.com/langsmith/evaluation-async

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)
</Info>

We can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](/langsmith/evaluate-llm-application).

<Info>
  This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](/langsmith/evaluate-llm-application).
</Info>

Requires `langsmith>=0.3.13`

```python  theme={null}
from langsmith import wrappers, Client
from openai import AsyncOpenAI

---

## Bob creates his own thread

**URL:** llms-txt#bob-creates-his-own-thread

bob_thread = await bob.threads.create()
await bob.runs.create(
    thread_id=bob_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Bob's private chat"}]}
)
print(f"✅ Bob created his own thread: {bob_thread['thread_id']}")

---

## MISSING_CHECKPOINTER

**URL:** llms-txt#missing_checkpointer

**Contents:**
- Troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/errors/MISSING_CHECKPOINTER

You are attempting to use built-in LangGraph persistence without providing a checkpointer.

This happens when a `checkpointer` is missing in the `compile()` method of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint).

The following may help resolve this error:

* Initialize and pass a checkpointer to the `compile()` method of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint).

```python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
checkpointer = InMemorySaver()

---

## Configure LangSmith tracing

**URL:** llms-txt#configure-langsmith-tracing

configure(project_name="multi-framework-app")

---

## An experiment is a collection of runs with a reference to the dataset used

**URL:** llms-txt#an-experiment-is-a-collection-of-runs-with-a-reference-to-the-dataset-used

---

## path/to/embedding_function.py

**URL:** llms-txt#path/to/embedding_function.py

**Contents:**
- Querying via the API

from openai import AsyncOpenAI

client = AsyncOpenAI()

async def aembed_texts(texts: list[str]) -> list[list[float]]:
    """Custom embedding function that must:
    1. Be async
    2. Accept a list of strings
    3. Return a list of float arrays (embeddings)
    """
    response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [e.embedding for e in response.data]
python  theme={null}
from langgraph_sdk import get_client

async def search_store():
    client = get_client()
    results = await client.store.search_items(
        ("memory", "facts"),
        query="your search query",
        limit=3  # number of results to return
    )
    return results

**Examples:**

Example 1 (unknown):
```unknown
## Querying via the API

You can also query the store using the LangGraph SDK. Since the SDK uses async operations:
```

---

## Define regex patterns for various PII

**URL:** llms-txt#define-regex-patterns-for-various-pii

SSN_PATTERN = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
CREDIT_CARD_PATTERN = re.compile(r'\b(?:\d[ -]*?){13,16}\b')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b')
PHONE_PATTERN = re.compile(r'\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b')
FULL_NAME_PATTERN = re.compile(r'\b([A-Z][a-z]*\s[A-Z][a-z]*)\b')

def regex_anonymize(text):
    """
    Anonymize sensitive information in the text using regex patterns.
    Args:
        text (str): The input text to be anonymized.
    Returns:
        str: The anonymized text.
    """
    # Replace sensitive information with placeholders
    text = SSN_PATTERN.sub('[REDACTED SSN]', text)
    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)
    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)
    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)
    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)
    return text

def recursive_anonymize(data, depth=10):
    """
    Recursively traverse the data structure and anonymize sensitive information.
    Args:
        data (any): The input data to be anonymized.
        depth (int): The current recursion depth to prevent excessive recursion.
    Returns:
        any: The anonymized data.
    """
    if depth == 0:
        return data
    if isinstance(data, dict):
        anonymized_dict = {}
        for k, v in data.items():
            anonymized_value = recursive_anonymize(v, depth - 1)
            anonymized_dict[k] = anonymized_value
        return anonymized_dict
    elif isinstance(data, list):
        anonymized_list = []
        for item in data:
            anonymized_item = recursive_anonymize(item, depth - 1)
            anonymized_list.append(anonymized_item)
        return anonymized_list
    elif isinstance(data, str):
        anonymized_data = regex_anonymize(data)
        return anonymized_data
    else:
        return data

openai_client = wrap_openai(openai.Client())

---

## MULTIPLE_SUBGRAPHS

**URL:** llms-txt#multiple_subgraphs

**Contents:**
- Troubleshooting

Source: https://docs.langchain.com/oss/python/langgraph/errors/MULTIPLE_SUBGRAPHS

You are calling subgraphs multiple times within a single LangGraph node with checkpointing enabled for each subgraph.

This is currently not allowed due to internal restrictions on how checkpoint namespacing for subgraphs works.

The following may help resolve this error:

* If you don't need to interrupt/resume from a subgraph, pass `checkpointer=False` when compiling it like this: `.compile(checkpointer=False)`

* Don't imperatively call graphs multiple times in the same node, and instead use the [`Send`](/oss/python/langgraph/graph-api#send) API.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/errors/MULTIPLE_SUBGRAPHS.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Graph state.

**URL:** llms-txt#graph-state.

class State(TypedDict):
    """Agent state."""
    messages: Annotated[list[AnyMessage], add_messages]
    followup: str | None

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None

---

## Output from node_1 contains private data that is not part of the overall state

**URL:** llms-txt#output-from-node_1-contains-private-data-that-is-not-part-of-the-overall-state

class Node1Output(TypedDict):
    private_data: str

---

## Trace with LangGraph

**URL:** llms-txt#trace-with-langgraph

**Contents:**
- With LangChain
  - 1. Installation
  - 2. Configure your environment
  - 3. Log a trace
- Without LangChain
  - 1. Installation
  - 2. Configure your environment
  - 3. Log a trace

Source: https://docs.langchain.com/langsmith/trace-with-langgraph

LangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agents, whether you're using LangChain modules or other SDKs.

If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.

This guide will walk through a basic example. For more detailed information on configuration, see the [Trace With LangChain](/langsmith/trace-with-langchain) guide.

Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).

For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).

### 2. Configure your environment

<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

`export LANGCHAIN_CALLBACKS_BACKGROUND=true`

If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

`export LANGCHAIN_CALLBACKS_BACKGROUND=false`

See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

Once you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:

An example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea" alt="Trace tree for a LangGraph run with LangChain" data-og-width="3314" width="3314" data-og-height="1766" height="1766" data-path="langsmith/images/langgraph-with-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w" />

If you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.

Here's an example. You can also see this page for more information.

Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).

### 2. Configure your environment

<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

`export LANGCHAIN_CALLBACKS_BACKGROUND=true`

If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

`export LANGCHAIN_CALLBACKS_BACKGROUND=false`

See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

Once you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:

An example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2" alt="Trace tree for a LangGraph run without LangChain" data-og-width="3296" width="3296" data-og-height="1774" height="1774" data-path="langsmith/images/langgraph-without-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### 2. Configure your environment

<CodeGroup>
```

---

## You must provide a thread ID to associate the execution with a conversation thread,

**URL:** llms-txt#you-must-provide-a-thread-id-to-associate-the-execution-with-a-conversation-thread,

---

## Callbacks

**URL:** llms-txt#callbacks

Source: https://docs.langchain.com/oss/javascript/integrations/callbacks/index

<Columns cols={3}>
  <Card title="Datadog Tracer" icon="link" href="/oss/javascript/integrations/callbacks/datadog_tracer" arrow="true" cta="View guide" />

<Card title="Upstash Rate Limit" icon="link" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/callbacks/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to add custom routes

**URL:** llms-txt#how-to-add-custom-routes

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-routes

When deploying agents to LangSmith Deployment, your server automatically exposes routes for creating runs and threads, interacting with the long-term memory store, managing configurable assistants, and other core functionality ([see all default API endpoints](/langsmith/server-api-ref)).

You can add custom routes by providing your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps). You make LangSmith aware of this by providing a path to the app in your `langgraph.json` configuration file.

Defining a custom app object lets you add any routes you'd like, so you can do anything from adding a `/login` endpoint to writing an entire full-stack web-app, all deployed in a single LangGraph Server.

Below is an example using FastAPI.

Starting from an **existing** LangSmith application, add the following custom route code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={4}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## Compare traces

**URL:** llms-txt#compare-traces

Source: https://docs.langchain.com/langsmith/compare-traces

To compare traces, click on the **Compare** button in the upper right hand side of any trace view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=794720bc9ef293984be25f19bffc89e7" alt="" data-og-width="2936" width="2936" data-og-height="1860" height="1860" data-path="langsmith/images/compare-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f64c79f89cf94802ed0676cb4d175be7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=111db8eecaadd0458001072616ef95fa 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1df1b17af22d13e24a613bea71cb2e98 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=476560e917219525466629c49284de13 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=12cf8bf414cd5abd48355693f93d3bcf 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7242812503868254fde6688b27e3586f 2500w" />

This will show the trace run table. Select the trace you want to compare against the original trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89c9c16780ac8d129736ee800124625a" alt="" data-og-width="2388" width="2388" data-og-height="1856" height="1856" data-path="langsmith/images/select-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1a7d4eb56fb4c2f8814b0584bce967b1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=72c89c532151d35facb58cb62b825213 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=acd522e4aa3f54caaa3ef4036eb654b2 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aac705cc1574309b670d54e96bff666f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=db82f5ea3c9ea13b3919f892db65bd9c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b926b0a266e8945063a71a7496034e97 2500w" />

The pane will open with both traces selected in a side by side comparison view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0f3b4479d399d18c3d6d48fb91681f73" alt="" data-og-width="2930" width="2930" data-og-height="1868" height="1868" data-path="langsmith/images/compare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=05c151e7c9d0733ca6bfee6b6ce5ccc6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=579b34d6aa74caea5e055cba22f6fea8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9a06b98a43f3a195eed05292cacae2d2 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=968d1316e524123352412791515af047 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=29065b3b65b98841a47ebe6487a26974 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=dfe7e02a6db6693d4ff8490e170014bf 2500w" />

To stop comparing, close the pane or click on **Stop comparing** in the upper right hand side of the pane.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---
